{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks - A Practical Introduction\n",
    "by _Minho Menezes_  \n",
    "\n",
    "---\n",
    "\n",
    "## Neural Networks - Representation\n",
    "\n",
    "### Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## LIBRARIES ##\n",
    "import numpy as np                         # Library for Numerical and Matricial Operations\n",
    "import matplotlib.pyplot as plt            # Library for Generating Visualizations\n",
    "from mpl_toolkits.mplot3d import Axes3D    # Library for Generating 3D Visualizations\n",
    "import pandas as pd                        # Library for Handling Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## CLASS: Multilayer Perceptron ##\n",
    "class MultilayerPerceptron:\n",
    "    \n",
    "    # CLASS CONSTRUCTOR\n",
    "    def __init__(self, n_neurons=[2, 5, 1]):\n",
    "        if(len(n_neurons) < 2):\n",
    "            raise ValueError(\"The network must have at least two layers! (The input and the output layers)\")\n",
    "        \n",
    "        # Network Architecture\n",
    "        self.hidden_layers = len(n_neurons)-2\n",
    "        self.n_neurons = n_neurons\n",
    "        self.W = []\n",
    "        \n",
    "        # Adjusting the Network architecture\n",
    "        for i in range(1, len(n_neurons)):\n",
    "            self.W.append( np.random.randn(self.n_neurons[i-1]+1 , self.n_neurons[i]) )\n",
    "        \n",
    "    # ACTIVATION FUNCTION\n",
    "    def activate(self,Z):\n",
    "        return 1 / (1 + np.exp(-Z))\n",
    "    \n",
    "    # FORWARD PROPAGATION\n",
    "    def forward(self, X):\n",
    "        # Activation List\n",
    "        A = []\n",
    "        \n",
    "        # Input Layer Activation\n",
    "        A.append( np.vstack([np.ones([1, X.shape[1]]), X]) )\n",
    "        \n",
    "        # Hidden Layer Activation\n",
    "        for i in range(0, self.hidden_layers):\n",
    "            Z = np.matmul(self.W[i].T, A[-1])\n",
    "            Z = self.activate(Z)\n",
    "            \n",
    "            A.append( np.vstack([np.ones([1, Z.shape[1]]), Z]) )\n",
    "        \n",
    "        # Output Layer Activation\n",
    "        Z = np.matmul(self.W[-1].T, A[-1])\n",
    "        Z = self.activate(Z)\n",
    "\n",
    "        A.append(Z)\n",
    "        \n",
    "        return A\n",
    "        \n",
    "## ---------------------------- ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single-Layer Perceptron\n",
    "\n",
    "The **Single-Layer Perceptron**, also known by the alias of Logistic Units, are the fundamental component of most of the connected Neural Networks that we study, so they are refered as to the _neurons_ of such Networks. This is a graphic representation of a Single-Layer Perceptron:\n",
    "\n",
    "[img]\n",
    "\n",
    "Write the code for the activation of such neuron in below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def activate(Z):\n",
    "    return 1 / (1 + np.exp(-Z))\n",
    "\n",
    "def forward(X, W):\n",
    "    Z = np.matmul(W.T, X)\n",
    "    A = activate(Z)\n",
    "    \n",
    "    return A "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.80773421, 1.        ]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.array([[ 1, 1],\n",
    "              [ 2, 8],\n",
    "              [-2, 6]])\n",
    "\n",
    "Y = np.array([[1, 0]])\n",
    "\n",
    "W = np.random.randn(3,1)\n",
    "\n",
    "forward(X, W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib qt5\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "x = np.linspace(-15, 20, 30)\n",
    "y = np.linspace(-15, 20, 30)\n",
    "\n",
    "xx, yy = np.meshgrid(x, y)\n",
    "data = np.vstack([np.ones([1, xx.ravel().shape[0]]), xx.ravel(), yy.ravel()])\n",
    "z = (forward(data, W) > 0.5).astype(int)\n",
    "zz = z.reshape(xx.shape)\n",
    "\n",
    "ax.plot_wireframe(xx,yy,zz)\n",
    "ax.scatter3D(X[1,:],X[2,:], Y, c=\"Red\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multilayer Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(X, W):\n",
    "        # Activation List\n",
    "        A = []\n",
    "        \n",
    "        # Input Layer Activation\n",
    "        A.append( np.vstack([np.ones([1, X.shape[1]]), X]) )\n",
    "        \n",
    "        # Hidden Layer Activation\n",
    "        for i in range(0, len(W)-1):\n",
    "            Z = np.matmul(W[i].T, A[-1])\n",
    "            Z = activate(Z)\n",
    "            \n",
    "            A.append( np.vstack([np.ones([1, Z.shape[1]]), Z]) )\n",
    "        \n",
    "        # Output Layer Activation\n",
    "        Z = np.matmul(W[-1].T, A[-1])\n",
    "        Z = activate(Z)\n",
    "\n",
    "        A.append(Z)\n",
    "        \n",
    "        return A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.69236995, 0.7535169 ]])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.array([[ 2, 8],\n",
    "              [-2, 6]])\n",
    "\n",
    "Y = np.array([[1, 0]])\n",
    "\n",
    "W = [np.random.randn(3,125), np.random.randn(126,1)]\n",
    "\n",
    "forward(X, W)[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib qt5\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "x = np.linspace(-15, 20, 50)\n",
    "y = np.linspace(-15, 20, 50)\n",
    "\n",
    "xx, yy = np.meshgrid(x, y)\n",
    "data = np.vstack([xx.ravel(), yy.ravel()])\n",
    "z = (forward(data, W)[-1] > 0.5).astype(int)\n",
    "zz = z.reshape(xx.shape)\n",
    "\n",
    "ax.plot_wireframe(xx,yy,zz)\n",
    "ax.scatter3D(X[0,:],X[1,:], Y, c=\"Red\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
