{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks - A Practical Introduction\n",
    "by _Minho Menezes_  \n",
    "\n",
    "---\n",
    "\n",
    "## Neural Networks - Representation\n",
    "\n",
    "In this first notebook, we build the structure necessary for a Neural Network to be represented, as well as the methods that it can perform with data.\n",
    "\n",
    "* [1. The Single Layer Perceptron](#1.-The-Single-Layer-Perceptron)  \n",
    "* [2. Classification Using a SLP](#2.-Classification-Using-a-SLP)  \n",
    "* [3. The Multilayer Perceptron](#3.-The-Multilayer-Perceptron)  \n",
    "* [4. Classification Using a MLP](#4.-Classification-Using-a-MLP)  \n",
    "* [5. Multiclass Classification Using a MLP](#5.-Multiclass-Classification-Using-a-MLP)  \n",
    "\n",
    "---\n",
    "### Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## LIBRARIES ##\n",
    "import numpy as np                         # Library for Numerical and Matricial Operations\n",
    "import matplotlib.pyplot as plt            # Library for Generating Visualizations\n",
    "from mpl_toolkits.mplot3d import Axes3D    # Library for Generating 3D Visualizations\n",
    "import pandas as pd                        # Library for Handling Datasets\n",
    "from tools.tools import Tools as tl        # Library for some Utilitary Tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## CLASS: Multilayer Perceptron ##\n",
    "class MultilayerPerceptron(object):\n",
    "    \n",
    "    # CLASS CONSTRUCTOR\n",
    "    def __init__(self, n_neurons=[2, 5, 1]):\n",
    "        if(len(n_neurons) < 2):\n",
    "            raise ValueError(\"The network must have at least two layers! (The input and the output layers)\")\n",
    "        \n",
    "        # Network Architecture\n",
    "        self.hidden_layers = len(n_neurons)-2\n",
    "        self.n_neurons = n_neurons\n",
    "        self.W = []\n",
    "        \n",
    "        # Adjusting the Network architecture\n",
    "        for i in range(1, len(n_neurons)):\n",
    "            self.W.append( np.random.randn(self.n_neurons[i-1]+1 , self.n_neurons[i]) )\n",
    "        \n",
    "    # ACTIVATION FUNCTION\n",
    "    def activate(self,Z):\n",
    "        pass\n",
    "    \n",
    "    # FORWARD PROPAGATION\n",
    "    def forward(self, X):\n",
    "        pass\n",
    "    \n",
    "    # CLASSIFICATION PREDICTION\n",
    "    def predict(self, X):\n",
    "        pass\n",
    "        \n",
    "## ---------------------------- ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "## 1. The Single-Layer Perceptron\n",
    "\n",
    "The **Single-Layer Perceptron**, also known by the alias of Logistic Units, are the fundamental component of most of the connected Neural Networks that we study, so they are refered as to the _neurons_ of such Networks. This is a graphic representation of a Single-Layer Perceptron neuron:\n",
    "\n",
    "<img src=\"imgs/slp_01.png\" alt=\"singlelayer perceptron neuron\" width=\"350px\"/>\n",
    "\n",
    "We have seen that the output of such neuron is calculated as:\n",
    "\n",
    "$$\n",
    "    \\hat{y} = \\varphi \\left(S_{i_\\text{net}} \\right) = \\varphi \\left( W_0 + \\sum_i W_i X_i \\right)\n",
    "$$\n",
    "\n",
    "Where $S_{i_\\text{net}} = W_0 + \\sum_i W_i X_i$ is the **net potential** of a sample $j$.  \n",
    "If we include, for each sample, a feature $X_0 = 1$, then this operation can be summarized as:\n",
    "\n",
    "$$\n",
    "    \\hat{\\mathbf{y}} = \\varphi \\left(\\mathbf{W}^T \\mathbf{X} \\right)\n",
    "$$\n",
    "\n",
    "\n",
    "Write codes for the _activate()_ and _forward()_ methods of such neuron in below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def activate(self, Z):\n",
    "    # YOUR CODE HERE\n",
    "\n",
    "def forward(self, X):\n",
    "    # YOUR CODE HERE\n",
    "\n",
    "MultilayerPerceptron.activate = activate\n",
    "MultilayerPerceptron.forward = forward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a simple Neural Network to try to classify the examples from the matrix $X$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([[  1, 0, -2,  5, -7],\n",
    "              [ -2, 9,  1, -1, -4]])\n",
    "\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Classification Using a SLP\n",
    "\n",
    "Once that a Neural Network architecture is built, and the method of forwarding is well-implemented, we can use this network to try to classify samples from real datasets.\n",
    "\n",
    "**Consider the Artificial Dataset below, where the color indicates the correct class of each sample**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = tl.loadData(\"data/toy_data_01.csv\")\n",
    "tl.plotData(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can implement a method of inference to use the information of the output signal (from the forward stage) to classify the samples. For this, we just need to \"clip\" the output in the following way:\n",
    "\n",
    "$$\n",
    "    \\hat{\\mathbf{y}} = \n",
    "    \\begin{cases}\n",
    "     1 & \\text{if }\\  \\hat{\\mathbf{y}}_\\text{forward} \\ge 0.5 \\\\\n",
    "     0 & \\text{otherwise}\n",
    "    \\end{cases}\n",
    "$$\n",
    "\n",
    "Implement below the method that clips all the output from the forward stage and return the prediction of the class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(self, X):\n",
    "    # YOUR CODE HERE\n",
    "\n",
    "MultilayerPerceptron.predict = predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now predict all the samples from the artificial dataset using a Neural Network and visualize the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also classify the entire dataset space in order to visualize the behaviour of the network, and the **decision boundary**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. The Multilayer Perceptron\n",
    "\n",
    "The Singlelayer Perceptron can be useful in some situations, but it is restricted to linearly separable problems.  \n",
    "The **Multilayer Perceptron** is an evolution of such architecture that solves this problem by adding intermediary layers (known as _hidden layers_), that adds non-linearity to the network inference process. This is a graphic representation of a Multilayer Perceptron with only one hidden layer:\n",
    "\n",
    "<img src=\"imgs/mlp_01.png\" alt=\"binary multilayer perceptron\" width=\"350px\"/>\n",
    "\n",
    "The forward process of the MLP, using the matrix operations, is very simple. All we need to do is to have a list of weights matrices $\\mathbf{W}^{(l)}$ for transition to layer $l-1$ to $l$, and perform matrix multiplications in the order of the layers:\n",
    "\n",
    "$$\n",
    "    \\mathbf{A}^{(i)} = \\varphi \\left(\\mathbf{W}^{(i)T} \\mathbf{A}^{(i-1)} \\right)\n",
    "$$\n",
    "\n",
    "Update the _forward()_ method to account for the multilayer architecture:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(self, X):\n",
    "        # YOUR CODE HERE\n",
    "    \n",
    "MultilayerPerceptron.forward = forward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a Neural Network with multiple layers to try to classify the examples from the matrix $X$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([[  1, 0, -2,  5, -7],\n",
    "              [ -2, 9,  1, -1, -4]])\n",
    "\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Classification Using a MLP\n",
    "Once that a Neural Network architecture is built, and the method of forwarding is well-implemented, we can use this network to try to classify samples from real datasets. **We will use the same dataset as before**.\n",
    "\n",
    "Start by updating the _predict()_ method to only return the \"clipped\" results from the activations in the Output Layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(self, X):\n",
    "    # YOUR CODE HERE\n",
    "\n",
    "MultilayerPerceptron.predict = predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now predict all the samples from the artificial dataset using a Neural Network and visualize the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "We can also classify the entire dataset space in order to visualize the behaviour of the network, and the **decision boundary**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Multiclass Classification Using a MLP\n",
    "\n",
    "In a lot of applications, the Neural Network has to identify the sample within a set of possible classes (not only the binary case). Fortunately, the Multilayer Perceptron already covers this functionality. This is a representation of a MLP for a multiclass classification:\n",
    "\n",
    "<img src=\"imgs/mlp_02.png\" alt=\"multiclass multilayer perceptron\" width=\"350px\"/>\n",
    "\n",
    "Now the case is that the output layer will have a set of neurons, each representative of a class. The probability accumulated in such neuron is the probability of a certain sample to be part of that class. \n",
    "\n",
    "**Consider the Artificial Dataset below, where the color indicates the correct class of each sample**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = tl.loadData(\"data/toy_data_02.csv\")\n",
    "tl.plotData(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We know update (again) the _predict()_ method to also account for the multiclass case. \n",
    "\n",
    "In this case, the method return directly the number of the class that the sample belongs, given that the probability of it belonging to such class is the higher among all the neurons in the output layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(self, X):\n",
    "    # YOUR CODE HERE\n",
    "\n",
    "MultilayerPerceptron.predict = predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now predict all the samples from the artificial dataset using a Neural Network and visualize the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also classify the entire dataset space in order to visualize the behaviour of the network, and the **decision boundary**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
