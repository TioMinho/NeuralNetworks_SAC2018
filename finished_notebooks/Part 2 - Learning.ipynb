{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks - A Practical Introduction\n",
    "by _Minho Menezes_  \n",
    "\n",
    "---\n",
    "\n",
    "## Neural Networks - Learning\n",
    "\n",
    "In this second notebook, we build the intelligent algorithms that will learn the optimal set of weights for the Neural Network task. This is the Supervisioned Learning approach, and it is fundamental in Machine Learning.\n",
    "\n",
    "* [1. Evaluating Performance](#1.-Evaluating-Performance)  \n",
    "* [2. Backpropagation](#2.-Backpropagation)  \n",
    "* [3. Gradient Descent Training](#3.-Gradient-Descent-Training)  \n",
    "* [4. Training a MLP for Binary Classification](#4.-Training-a-MLP-for-Binary-Classification)  \n",
    "* [5. Training a MLP for Multiclass Classification](#5.-Training-a-MLP-for-Multiclass-Classification)  \n",
    "\n",
    "---\n",
    "\n",
    "### Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## LIBRARIES ##\n",
    "import numpy as np                         # Library for Numerical and Matricial Operations\n",
    "import matplotlib.pyplot as plt            # Library for Generating Visualizations\n",
    "import pandas as pd                        # Library for Handling Datasets\n",
    "from tools import Tools as tl              # Library for some Utilitary Tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## CLASS: Multilayer Perceptron ##\n",
    "class MultilayerPerceptron:\n",
    "    \n",
    "    # CLASS CONSTRUCTOR\n",
    "    def __init__(self, n_neurons=[2, 5, 1]):\n",
    "        if(len(n_neurons) < 2):\n",
    "            raise ValueError(\"The network must have at least two layers! (The input and the output layers)\")\n",
    "        \n",
    "        # Network Architecture\n",
    "        self.hidden_layers = len(n_neurons)-2\n",
    "        self.n_neurons = n_neurons\n",
    "        self.W = []\n",
    "        \n",
    "        # Adjusting the Network architecture\n",
    "        for i in range(1, len(n_neurons)):\n",
    "            self.W.append( np.random.randn(self.n_neurons[i-1]+1 , self.n_neurons[i]) )\n",
    "        \n",
    "    # ACTIVATION FUNCTION\n",
    "    def activate(self,Z):\n",
    "        return 1 / (1 + np.exp(-Z))\n",
    "    \n",
    "    # FORWARD PROPAGATION\n",
    "    def forward(self, X):\n",
    "        # Activation List\n",
    "        A = []\n",
    "        \n",
    "        # Input Layer Activation\n",
    "        A.append( np.vstack([np.ones([1, X.shape[1]]), X]) )\n",
    "        \n",
    "        # Hidden Layer Activation\n",
    "        for i in range(0, self.hidden_layers):\n",
    "            Z = np.matmul(self.W[i].T, A[-1])\n",
    "            Z = self.activate(Z)\n",
    "            \n",
    "            A.append( np.vstack([np.ones([1, Z.shape[1]]), Z]) )\n",
    "        \n",
    "        # Output Layer Activation\n",
    "        Z = np.matmul(self.W[-1].T, A[-1])\n",
    "        Z = self.activate(Z)\n",
    "\n",
    "        A.append(Z)\n",
    "        \n",
    "        return A\n",
    "    \n",
    "    # CLASSIFICATION PREDICTION\n",
    "    def predict(self, X):\n",
    "        A = self.forward(X)\n",
    "\n",
    "        # Case: Multiclass\n",
    "        if(self.n_neurons[-1] > 1):\n",
    "            return A[-1].argmax(axis=0)\n",
    "\n",
    "        # Case: Singleclass\n",
    "        else:\n",
    "            return (A[-1] > 0.5).astype(int)\n",
    "    \n",
    "    # LOSS FUNCTION\n",
    "    def loss(self, y, y_hat):\n",
    "        pass\n",
    "    \n",
    "    # ACCURACY FUNCTION\n",
    "    def accuracy(self, y, y_hat):\n",
    "        pass\n",
    "    \n",
    "    # BACKPROPAGATION\n",
    "    def backpropagate(self, A, y):\n",
    "        pass\n",
    "    \n",
    "    # GRADIENT DESCENT TRAINING\n",
    "    def train(self, X_train, y_train, alpha=1e-3, maxIt=50000, tol=1e-5, verbose=False):\n",
    "        pass\n",
    "        \n",
    "## ---------------------------- ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 1. Evaluating Performance\n",
    "\n",
    "One of the first things to be defined in any Supervisioned Learning approach is the evaluation and error metrics used to tell the model how it performed, and allow it to correct his parameters.\n",
    "\n",
    "An important metric is the **Loss Function**, that direcly be used in the training. In our case, we will use the function known as _Cross-Entropy Loss_:\n",
    "\n",
    "$$\n",
    "    \\mathcal{L}(W) = -\\frac{1}{m} \\sum y\\ log( \\hat{y} ) + (1-y)\\ log(1 - \\hat{y} )\n",
    "$$\n",
    "\n",
    "While it is mathematically useful for the training, its values are hard to interpret. So, a more human-like performance metric consists in the **Accuracy Function**, that can be calculated as:\n",
    "\n",
    "$$\n",
    "    \\text{Acc}(W) = -\\frac{100}{m} \\sum ( y = \\hat{y} )\n",
    "$$\n",
    "\n",
    "Implement the two evaluation metrics below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOSS FUNCTION\n",
    "def loss(self, y, y_hat):\n",
    "    m = y.shape[1]\n",
    "    return -(1/m) * np.sum(y * np.log(y_hat) + (1-y) * np.log(1 - y_hat))\n",
    "\n",
    "# ACCURACY FUNCTION\n",
    "def accuracy(self, y, y_hat):\n",
    "    m = y.shape[1]\n",
    "    return (1/m) * np.sum(y == y_hat) * 100\n",
    "\n",
    "# Updates the methods directly in the MLP class\n",
    "MultilayerPerceptron.loss = loss\n",
    "MultilayerPerceptron.accuracy = accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, use these metrics to evaluate the performance of classifiction for the examples in the matrix $X$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## RESULTS ##\n",
      "Real:       [[1 1 0 0]]\n",
      "Predicted:  [[0 0 0 0]]\n",
      "Activation: [[0.10608102 0.27377442 0.31464858 0.17368901]]\n",
      "\n",
      "Cross-Entropy Loss:  1.026902634736987\n",
      "Prediction Accuracy: 50.0%\n"
     ]
    }
   ],
   "source": [
    "# Creates a artificial toy dataset (Rows -> Attributes; Columns --> Samples)\n",
    "X = np.array([[ 5,  1, -2, -1],\n",
    "              [ 4,  2,  0,  4],\n",
    "              [ 3,  3,  1,  4],\n",
    "              [ 2,  4, -1, -3]])    \n",
    "\n",
    "y = np.array([[ 1,  1,  0,  0]])\n",
    "\n",
    "# Creates the Neural Network with 4 Input Neurons, 5 Hidden Neurons and 1 Output Neuron\n",
    "# (The weights are randomnly initiated)\n",
    "brain = MultilayerPerceptron(n_neurons=[4, 5, 1])\n",
    "\n",
    "A = brain.forward(X) # Activation (Probability)\n",
    "P = brain.predict(X) # Prediction (Class Index)\n",
    "\n",
    "print(\"## RESULTS ##\")\n",
    "print(\"Real:       {}\".format(y))\n",
    "print(\"Predicted:  {}\".format(P))\n",
    "print(\"Activation: {}\".format(A[-1]))\n",
    "\n",
    "print()\n",
    "print(\"Cross-Entropy Loss:  {}\".format(brain.loss(y, A[-1])))\n",
    "print(\"Prediction Accuracy: {}%\".format(brain.accuracy(y, P)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Backpropagation\n",
    "\n",
    "The **Backpropagation** algorithm is one of the most popular and powerful techniques in traning multilayer models.\n",
    "\n",
    "This algorithm allows the misclassification error to be distributed to the entire network, responsabilizing each neuron individually for its contribution to the faults. The algorithm work as follows.\n",
    "\n",
    "The error in the output layer is directly the difference between the real value of the class for each sample subtracted by the probabilities calculated by the network. For all the subsequent layers, the error in each neuron is equal to:\n",
    "\n",
    "$$\n",
    "    e_i^{(l)} = \\left( e_1^{(l+1)} W_1^{(l)} + e_2^{(l+1)} W_2^{(l)} + \\cdots + e_n^{(l+1)} W_n^{(l)} \\right) \\cfrac{d \\varphi(S_{i_\\text{net}}^{(l)})}{dW}\n",
    "$$\n",
    "\n",
    "Where the derivative is equal to:\n",
    "\n",
    "$$\n",
    "    \\cfrac{d \\varphi(S_{i_\\text{net}}^{(l)})}{dW} = A_i^{(l)}(1-A_i^{(l)})\n",
    "$$\n",
    "\n",
    "Using matrix multiplication, the backpropagation of the error between two layers is:\n",
    "\n",
    "$$\n",
    "    \\mathbf{E}^{(l)} = \\mathbf{W}^{(l)} \\mathbf{E}^{(l+1)} \\mathbf{A}^{(l)} (1 - \\mathbf{A}^{(l)})\n",
    "$$\n",
    "\n",
    "Implement the backpropagation method in the cell below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BACKPROPAGATION\n",
    "def backpropagate(self, A, y):\n",
    "    # Calculates the error in the Output Layer (difference between the real and the predicted)\n",
    "    E = []\n",
    "    E.append( A[-1] - y )\n",
    "\n",
    "    # Backpropagates the error to all the Hidden Layers\n",
    "    for i in range(self.hidden_layers, 0, -1):\n",
    "        E.append( np.matmul(self.W[i], E[-1]) * A[i] * (1-A[i]) )\n",
    "        E[-1] = E[-1][1:,:]\n",
    "        \n",
    "    # Returns the list of Error Matrices\n",
    "    return E[::-1]\n",
    "\n",
    "# Updates the methods directly in the MLP class\n",
    "MultilayerPerceptron.backpropagate = backpropagate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, experiment the _backpropagation()_ method in the example below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## RESULTS ##\n",
      "Real:       [[1 1 0 0]]\n",
      "Activation: [[0.93567557 0.94520244 0.8693045  0.79767477]]\n",
      "\n",
      "Error in Output Layer:\n",
      " [[-0.06432443 -0.05479756  0.8693045   0.79767477]]\n",
      "\n",
      "Error in Hidden Layer (Backpropagated):\n",
      " [[ 1.23606604e-04  5.41458912e-05 -3.72958947e-02 -2.50401231e-03]\n",
      " [-1.83559903e-02 -2.63747920e-04  2.49730218e-01  9.80805541e-02]\n",
      " [-6.43349561e-04 -1.29062779e-03  3.15731616e-02  6.81315213e-06]\n",
      " [-6.91364117e-04 -1.29145022e-03  5.89025750e-02  1.04673683e-02]\n",
      " [-1.67313455e-02 -1.95198332e-03  4.20083209e-02  2.00812901e-04]]\n"
     ]
    }
   ],
   "source": [
    "# Creates a artificial toy dataset (Rows -> Attributes; Columns --> Samples)\n",
    "X = np.array([[ 5,  1, -2, -1],\n",
    "              [ 4,  2,  0,  4],\n",
    "              [ 3,  3,  1,  4],\n",
    "              [ 2,  4, -1, -3]])    \n",
    "\n",
    "y = np.array([[ 1,  1,  0,  0]])\n",
    "\n",
    "# Creates the Neural Network with 4 Input Neurons, 5 Hidden Neurons and 1 Output Neuron\n",
    "# (The weights are randomnly initiated)\n",
    "brain = MultilayerPerceptron(n_neurons=[4, 5, 1])\n",
    "\n",
    "A = brain.forward(X)           # Activation\n",
    "E = brain.backpropagate(A, y)  # Errors\n",
    "\n",
    "# Show the results\n",
    "print(\"## RESULTS ##\")\n",
    "print(\"Real:       {}\".format(y))\n",
    "print(\"Activation: {}\".format(A[-1]))\n",
    "\n",
    "print()\n",
    "print(\"Error in Output Layer:\\n\", E[1])\n",
    "print(\"\\nError in Hidden Layer (Backpropagated):\\n\", E[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Gradient Descent Training\n",
    "\n",
    "The **Gradient Descent** is the most popular iterative algorithm to optimize loss functions.\n",
    "\n",
    "The algorithm works as follows:\n",
    "\n",
    "Given a Multilayer Neural Network with $L$ layers, where $A^{(l)}$ is the activation matrix in each layer $l$, and $E^{(l)}$ is the error matrix for each layer $l$:\n",
    "\n",
    "1. Calculate the activation $\\mathbf{A}^{(l)}$ and the errors $\\mathbf{E}^{(l)}$ using $\\mathbf{W}^{(i)}$;  \n",
    "\n",
    "2. Evaluate the current performance using the _Loss Function_ and the _Accuracy Metric_;\n",
    "   \n",
    "3. Update the network weights:\n",
    "\n",
    "    $$ \n",
    "        \\mathbf{W}^{(i+1)} = \\mathbf{W}^{(i+1)} - \\alpha \\nabla \\mathcal{L}(\\mathbf{W}^{(i)})\n",
    "    $$\n",
    "    \n",
    "    where $\\alpha$ is a scaling factor (mostly between 0 and 1) called _Learning Rate_, and $\\nabla \\mathcal{L}(\\mathbf{W}^{(i)})$ represents the gradient of the _cost function_, that can be calculated as:\n",
    "    \n",
    "    $$ \n",
    "        \\nabla \\mathcal{L}(\\mathbf{W}^{(i)}) = (A^{(l)}) E^{(l)^T}\n",
    "    $$  <br>\n",
    "    \n",
    "4. Print the training results at each 50 epochs  <br>\n",
    "\n",
    "5. Check for convergence by comparing the decrease in the _Loss Function_; <br>\n",
    "\n",
    "6. If the training did not converged, go back to Step 1. <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADIENT DESCENT TRAINING\n",
    "def train(self, X_train, y_train, alpha=1e-3, maxIt=50000, tol=1e-5, verbose=True):\n",
    "    # Returns the total number of samples in the training\n",
    "    m = X_train.shape[1]\n",
    "    \n",
    "    # Defines the Error History and other auxiliary variables\n",
    "    errorHist = []\n",
    "    \n",
    "    # Gradient Descent Loop\n",
    "    for it in range(0, maxIt):\n",
    "        # 1. Calculates all the activations and prediction (Forward Propagation) and \n",
    "        #    the errors (Backpropagation), using the current weights self.Wˆ(i)\n",
    "        A = self.forward(X_train)\n",
    "        E = self.backpropagate(A, y_train)\n",
    "        P = self.predict(X_train)\n",
    "        \n",
    "        # 2. Calculates the Evaluation Metrics\n",
    "        actualLoss = self.loss(y_train, A[-1])\n",
    "        actualAcc = self.accuracy(y_train, P)\n",
    "        errorHist.append(actualLoss)\n",
    "        \n",
    "        # 3. Updates the Neural Networks weights\n",
    "        for i in range(0, self.hidden_layers+1):\n",
    "            self.W[i] = self.W[i] - (alpha/m) * np.matmul(A[i], E[i].T)\n",
    "        \n",
    "        # 4. Prints the training results\n",
    "        if(verbose): \n",
    "            print(\"# Iteration {0:5} -> Loss: {1:} \\t| Accuracy: {2:.3f}\".format(it+1, actualLoss, actualAcc))\n",
    "        \n",
    "        # 5. Check for convergence and prints the final result for the training.\n",
    "        if(it > 1 and abs(errorHist[-1] - errorHist[-2]) <= tol):\n",
    "            print(\"\\n!!! Convergence reached !!!\")\n",
    "            print(\"# Iteration\", it, \"#\")\n",
    "            print(\"Cross-Entropy Loss:      {}\".format(actualLoss))\n",
    "            print(\"Accuracy (Training Set): {0:.3f}%\".format(actualAcc))\n",
    "            print(\"Weights\\nS -> H:\\n\", self.W[0], \"\\nH -> O:\\n\", self.W[1])\n",
    "            print(\"\\n\")\n",
    "            break;\n",
    "        \n",
    "    # End of the Training\n",
    "    return errorHist\n",
    "\n",
    "# Updates the methods directly in the MLP class\n",
    "MultilayerPerceptron.train = train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Training a MLP for Binary Classification\n",
    "\n",
    "The training process updates the weights to a optimum set of values, that minimizes the classification error, i.e., the Loss Function. Once that our algorithm for training is successfully implemented, we can use our Neural Network to train over a specific training set of data in order to learn the pattern of classification in that data.\n",
    "\n",
    "Consider our already known Multilayer Perceptron for binary classification:\n",
    "\n",
    "<img src=\"../imgs/mlp_01.png\" alt=\"binary multilayer perceptron\" width=\"350px\"/>\n",
    "\n",
    "**We will train this network using the dataset from the previous notebook:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Loads and plot the data from the artificial dataset\n",
    "X_train, X_test, y_train, y_test = tl.loadData(\"../data/toy_data_01.csv\")\n",
    "tl.plotData(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we create the Neural Network with the wanted architecture, all we need to do is run the _train()_ method passing the desired training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Iteration     1 -> Loss: 1.1069656913486885 \t| Accuracy: 51.875\n",
      "# Iteration     2 -> Loss: 1.0602507470415503 \t| Accuracy: 51.875\n",
      "# Iteration     3 -> Loss: 1.0171770814496108 \t| Accuracy: 51.875\n",
      "# Iteration     4 -> Loss: 0.9777485546637303 \t| Accuracy: 51.875\n",
      "# Iteration     5 -> Loss: 0.9419212225881606 \t| Accuracy: 51.875\n",
      "# Iteration     6 -> Loss: 0.9096035816464069 \t| Accuracy: 51.875\n",
      "# Iteration     7 -> Loss: 0.8806596524394888 \t| Accuracy: 51.875\n",
      "# Iteration     8 -> Loss: 0.8549146318848074 \t| Accuracy: 51.875\n",
      "# Iteration     9 -> Loss: 0.8321625225149082 \t| Accuracy: 51.875\n",
      "# Iteration    10 -> Loss: 0.8121749392285347 \t| Accuracy: 51.875\n",
      "# Iteration    11 -> Loss: 0.7947102326670278 \t| Accuracy: 51.875\n",
      "# Iteration    12 -> Loss: 0.7795221486345958 \t| Accuracy: 51.875\n",
      "# Iteration    13 -> Loss: 0.7663674255583541 \t| Accuracy: 51.875\n",
      "# Iteration    14 -> Loss: 0.7550119621973284 \t| Accuracy: 51.875\n",
      "# Iteration    15 -> Loss: 0.745235413801606 \t| Accuracy: 51.875\n",
      "# Iteration    16 -> Loss: 0.7368342601780324 \t| Accuracy: 51.458\n",
      "# Iteration    17 -> Loss: 0.7296235165131801 \t| Accuracy: 50.625\n",
      "# Iteration    18 -> Loss: 0.7234373268501177 \t| Accuracy: 50.208\n",
      "# Iteration    19 -> Loss: 0.7181287008859397 \t| Accuracy: 49.167\n",
      "# Iteration    20 -> Loss: 0.7135686416540574 \t| Accuracy: 51.042\n",
      "# Iteration    21 -> Loss: 0.7096448788220149 \t| Accuracy: 51.875\n",
      "# Iteration    22 -> Loss: 0.7062603811627977 \t| Accuracy: 52.292\n",
      "# Iteration    23 -> Loss: 0.7033317801077899 \t| Accuracy: 53.333\n",
      "# Iteration    24 -> Loss: 0.7007877987816238 \t| Accuracy: 56.042\n",
      "# Iteration    25 -> Loss: 0.6985677496825184 \t| Accuracy: 57.708\n",
      "# Iteration    26 -> Loss: 0.696620139674021 \t| Accuracy: 58.542\n",
      "# Iteration    27 -> Loss: 0.6949014027024958 \t| Accuracy: 59.375\n",
      "# Iteration    28 -> Loss: 0.693374767704239 \t| Accuracy: 61.458\n",
      "# Iteration    29 -> Loss: 0.6920092604456242 \t| Accuracy: 62.917\n",
      "# Iteration    30 -> Loss: 0.6907788325355579 \t| Accuracy: 64.167\n",
      "# Iteration    31 -> Loss: 0.6896616076914651 \t| Accuracy: 63.542\n",
      "# Iteration    32 -> Loss: 0.6886392338269622 \t| Accuracy: 62.500\n",
      "# Iteration    33 -> Loss: 0.6876963291223804 \t| Accuracy: 61.875\n",
      "# Iteration    34 -> Loss: 0.6868200105394519 \t| Accuracy: 61.458\n",
      "# Iteration    35 -> Loss: 0.6859994939631395 \t| Accuracy: 60.833\n",
      "# Iteration    36 -> Loss: 0.6852257560987222 \t| Accuracy: 61.042\n",
      "# Iteration    37 -> Loss: 0.684491249288101 \t| Accuracy: 61.458\n",
      "# Iteration    38 -> Loss: 0.6837896614504922 \t| Accuracy: 61.458\n",
      "# Iteration    39 -> Loss: 0.6831157143471899 \t| Accuracy: 61.458\n",
      "# Iteration    40 -> Loss: 0.6824649942888484 \t| Accuracy: 61.458\n",
      "# Iteration    41 -> Loss: 0.6818338102330245 \t| Accuracy: 61.250\n",
      "# Iteration    42 -> Loss: 0.6812190749556479 \t| Accuracy: 61.250\n",
      "# Iteration    43 -> Loss: 0.6806182056249613 \t| Accuracy: 61.250\n",
      "# Iteration    44 -> Loss: 0.6800290406660499 \t| Accuracy: 61.042\n",
      "# Iteration    45 -> Loss: 0.6794497702859972 \t| Accuracy: 61.042\n",
      "# Iteration    46 -> Loss: 0.6788788784422344 \t| Accuracy: 61.042\n",
      "# Iteration    47 -> Loss: 0.6783150943881333 \t| Accuracy: 61.042\n",
      "# Iteration    48 -> Loss: 0.6777573522281715 \t| Accuracy: 61.042\n",
      "# Iteration    49 -> Loss: 0.6772047571673793 \t| Accuracy: 60.833\n",
      "# Iteration    50 -> Loss: 0.676656557352757 \t| Accuracy: 60.833\n",
      "# Iteration    51 -> Loss: 0.6761121203837099 \t| Accuracy: 60.833\n",
      "# Iteration    52 -> Loss: 0.6755709137193454 \t| Accuracy: 60.833\n",
      "# Iteration    53 -> Loss: 0.6750324883370706 \t| Accuracy: 60.833\n",
      "# Iteration    54 -> Loss: 0.6744964651030783 \t| Accuracy: 60.833\n",
      "# Iteration    55 -> Loss: 0.6739625234042305 \t| Accuracy: 60.833\n",
      "# Iteration    56 -> Loss: 0.673430391665283 \t| Accuracy: 60.833\n",
      "# Iteration    57 -> Loss: 0.6728998394376328 \t| Accuracy: 60.833\n",
      "# Iteration    58 -> Loss: 0.6723706707978144 \t| Accuracy: 60.833\n",
      "# Iteration    59 -> Loss: 0.671842718837436 \t| Accuracy: 61.042\n",
      "# Iteration    60 -> Loss: 0.6713158410625452 \t| Accuracy: 61.042\n",
      "# Iteration    61 -> Loss: 0.6707899155507219 \t| Accuracy: 61.042\n",
      "# Iteration    62 -> Loss: 0.6702648377394702 \t| Accuracy: 60.833\n",
      "# Iteration    63 -> Loss: 0.6697405177405796 \t| Accuracy: 60.833\n",
      "# Iteration    64 -> Loss: 0.6692168780927052 \t| Accuracy: 60.833\n",
      "# Iteration    65 -> Loss: 0.6686938518790873 \t| Accuracy: 60.833\n",
      "# Iteration    66 -> Loss: 0.6681713811495446 \t| Accuracy: 60.833\n",
      "# Iteration    67 -> Loss: 0.6676494155960654 \t| Accuracy: 60.833\n",
      "# Iteration    68 -> Loss: 0.6671279114398004 \t| Accuracy: 60.833\n",
      "# Iteration    69 -> Loss: 0.6666068304943323 \t| Accuracy: 60.833\n",
      "# Iteration    70 -> Loss: 0.6660861393759826 \t| Accuracy: 61.042\n",
      "# Iteration    71 -> Loss: 0.6655658088368154 \t| Accuracy: 61.042\n",
      "# Iteration    72 -> Loss: 0.6650458132000828 \t| Accuracy: 61.042\n",
      "# Iteration    73 -> Loss: 0.6645261298812476 \t| Accuracy: 61.042\n",
      "# Iteration    74 -> Loss: 0.664006738980552 \t| Accuracy: 61.250\n",
      "# Iteration    75 -> Loss: 0.6634876229354513 \t| Accuracy: 61.250\n",
      "# Iteration    76 -> Loss: 0.6629687662231882 \t| Accuracy: 61.250\n",
      "# Iteration    77 -> Loss: 0.6624501551054147 \t| Accuracy: 61.250\n",
      "# Iteration    78 -> Loss: 0.6619317774081225 \t| Accuracy: 61.250\n",
      "# Iteration    79 -> Loss: 0.6614136223312695 \t| Accuracy: 61.250\n",
      "# Iteration    80 -> Loss: 0.6608956802834312 \t| Accuracy: 61.250\n",
      "# Iteration    81 -> Loss: 0.6603779427375767 \t| Accuracy: 61.458\n",
      "# Iteration    82 -> Loss: 0.6598604021047304 \t| Accuracy: 61.458\n",
      "# Iteration    83 -> Loss: 0.6593430516228085 \t| Accuracy: 61.458\n",
      "# Iteration    84 -> Loss: 0.6588258852583713 \t| Accuracy: 61.458\n",
      "# Iteration    85 -> Loss: 0.6583088976194091 \t| Accuracy: 62.292\n",
      "# Iteration    86 -> Loss: 0.6577920838775859 \t| Accuracy: 62.292\n",
      "# Iteration    87 -> Loss: 0.6572754396986175 \t| Accuracy: 62.292\n",
      "# Iteration    88 -> Loss: 0.6567589611796895 \t| Accuracy: 62.292\n",
      "# Iteration    89 -> Loss: 0.6562426447929827 \t| Accuracy: 62.292\n",
      "# Iteration    90 -> Loss: 0.6557264873345326 \t| Accuracy: 62.292\n",
      "# Iteration    91 -> Loss: 0.655210485877767 \t| Accuracy: 62.292\n",
      "# Iteration    92 -> Loss: 0.6546946377311728 \t| Accuracy: 62.292\n",
      "# Iteration    93 -> Loss: 0.654178940399623 \t| Accuracy: 62.292\n",
      "# Iteration    94 -> Loss: 0.653663391548972 \t| Accuracy: 62.708\n",
      "# Iteration    95 -> Loss: 0.6531479889735805 \t| Accuracy: 62.708\n",
      "# Iteration    96 -> Loss: 0.652632730566484 \t| Accuracy: 62.708\n",
      "# Iteration    97 -> Loss: 0.6521176142919605 \t| Accuracy: 62.708\n",
      "# Iteration    98 -> Loss: 0.651602638160284 \t| Accuracy: 62.708\n",
      "# Iteration    99 -> Loss: 0.6510878002044795 \t| Accuracy: 62.708\n",
      "# Iteration   100 -> Loss: 0.6505730984589256 \t| Accuracy: 62.708\n",
      "# Iteration   101 -> Loss: 0.6500585309396567 \t| Accuracy: 62.708\n",
      "# Iteration   102 -> Loss: 0.6495440956262523 \t| Accuracy: 62.708\n",
      "# Iteration   103 -> Loss: 0.6490297904451933 \t| Accuracy: 62.708\n",
      "# Iteration   104 -> Loss: 0.6485156132546 \t| Accuracy: 62.917\n",
      "# Iteration   105 -> Loss: 0.6480015618302547 \t| Accuracy: 63.125\n",
      "# Iteration   106 -> Loss: 0.6474876338528358 \t| Accuracy: 63.125\n",
      "# Iteration   107 -> Loss: 0.6469738268962912 \t| Accuracy: 63.125\n",
      "# Iteration   108 -> Loss: 0.646460138417283 \t| Accuracy: 63.125\n",
      "# Iteration   109 -> Loss: 0.6459465657456452 \t| Accuracy: 63.125\n",
      "# Iteration   110 -> Loss: 0.645433106075799 \t| Accuracy: 63.125\n",
      "# Iteration   111 -> Loss: 0.6449197564590688 \t| Accuracy: 63.125\n",
      "# Iteration   112 -> Loss: 0.6444065137968588 \t| Accuracy: 63.125\n",
      "# Iteration   113 -> Loss: 0.6438933748346329 \t| Accuracy: 63.333\n",
      "# Iteration   114 -> Loss: 0.6433803361566662 \t| Accuracy: 63.333\n",
      "# Iteration   115 -> Loss: 0.642867394181517 \t| Accuracy: 63.333\n",
      "# Iteration   116 -> Loss: 0.6423545451581868 \t| Accuracy: 63.333\n",
      "# Iteration   117 -> Loss: 0.6418417851629294 \t| Accuracy: 63.542\n",
      "# Iteration   118 -> Loss: 0.6413291100966712 \t| Accuracy: 63.750\n",
      "# Iteration   119 -> Loss: 0.6408165156830116 \t| Accuracy: 63.958\n",
      "# Iteration   120 -> Loss: 0.6403039974667704 \t| Accuracy: 63.958\n",
      "# Iteration   121 -> Loss: 0.6397915508130504 \t| Accuracy: 63.958\n",
      "# Iteration   122 -> Loss: 0.6392791709067838 \t| Accuracy: 63.958\n",
      "# Iteration   123 -> Loss: 0.6387668527527375 \t| Accuracy: 63.958\n",
      "# Iteration   124 -> Loss: 0.6382545911759462 \t| Accuracy: 64.375\n",
      "# Iteration   125 -> Loss: 0.6377423808225473 \t| Accuracy: 64.375\n",
      "# Iteration   126 -> Loss: 0.6372302161609944 \t| Accuracy: 64.375\n",
      "# Iteration   127 -> Loss: 0.6367180914836211 \t| Accuracy: 64.375\n",
      "# Iteration   128 -> Loss: 0.6362060009085353 \t| Accuracy: 64.375\n",
      "# Iteration   129 -> Loss: 0.6356939383818181 \t| Accuracy: 64.792\n",
      "# Iteration   130 -> Loss: 0.6351818976800082 \t| Accuracy: 65.208\n",
      "# Iteration   131 -> Loss: 0.6346698724128497 \t| Accuracy: 65.208\n",
      "# Iteration   132 -> Loss: 0.6341578560262817 \t| Accuracy: 65.208\n",
      "# Iteration   133 -> Loss: 0.6336458418056562 \t| Accuracy: 65.417\n",
      "# Iteration   134 -> Loss: 0.6331338228791562 \t| Accuracy: 65.833\n",
      "# Iteration   135 -> Loss: 0.6326217922214088 \t| Accuracy: 66.250\n",
      "# Iteration   136 -> Loss: 0.6321097426572639 \t| Accuracy: 66.250\n",
      "# Iteration   137 -> Loss: 0.6315976668657342 \t| Accuracy: 66.250\n",
      "# Iteration   138 -> Loss: 0.6310855573840702 \t| Accuracy: 66.458\n",
      "# Iteration   139 -> Loss: 0.6305734066119657 \t| Accuracy: 66.458\n",
      "# Iteration   140 -> Loss: 0.6300612068158725 \t| Accuracy: 66.667\n",
      "# Iteration   141 -> Loss: 0.6295489501334148 \t| Accuracy: 66.875\n",
      "# Iteration   142 -> Loss: 0.6290366285778902 \t| Accuracy: 66.875\n",
      "# Iteration   143 -> Loss: 0.6285242340428456 \t| Accuracy: 67.083\n",
      "# Iteration   144 -> Loss: 0.6280117583067143 \t| Accuracy: 67.083\n",
      "# Iteration   145 -> Loss: 0.6274991930375087 \t| Accuracy: 67.083\n",
      "# Iteration   146 -> Loss: 0.6269865297975538 \t| Accuracy: 67.083\n",
      "# Iteration   147 -> Loss: 0.6264737600482541 \t| Accuracy: 67.083\n",
      "# Iteration   148 -> Loss: 0.6259608751548854 \t| Accuracy: 67.083\n",
      "# Iteration   149 -> Loss: 0.6254478663914029 \t| Accuracy: 67.500\n",
      "# Iteration   150 -> Loss: 0.6249347249452559 \t| Accuracy: 67.917\n",
      "# Iteration   151 -> Loss: 0.6244214419222051 \t| Accuracy: 67.917\n",
      "# Iteration   152 -> Loss: 0.6239080083511328 \t| Accuracy: 68.125\n",
      "# Iteration   153 -> Loss: 0.6233944151888421 \t| Accuracy: 68.333\n",
      "# Iteration   154 -> Loss: 0.6228806533248358 \t| Accuracy: 68.333\n",
      "# Iteration   155 -> Loss: 0.6223667135860735 \t| Accuracy: 68.333\n",
      "# Iteration   156 -> Loss: 0.6218525867416991 \t| Accuracy: 68.333\n",
      "# Iteration   157 -> Loss: 0.6213382635077338 \t| Accuracy: 68.750\n",
      "# Iteration   158 -> Loss: 0.6208237345517341 \t| Accuracy: 68.750\n",
      "# Iteration   159 -> Loss: 0.6203089904974051 \t| Accuracy: 68.750\n",
      "# Iteration   160 -> Loss: 0.61979402192917 \t| Accuracy: 68.750\n",
      "# Iteration   161 -> Loss: 0.6192788193966917 \t| Accuracy: 68.750\n",
      "# Iteration   162 -> Loss: 0.6187633734193412 \t| Accuracy: 68.750\n",
      "# Iteration   163 -> Loss: 0.6182476744906137 \t| Accuracy: 68.750\n",
      "# Iteration   164 -> Loss: 0.6177317130824878 \t| Accuracy: 68.958\n",
      "# Iteration   165 -> Loss: 0.617215479649726 \t| Accuracy: 68.958\n",
      "# Iteration   166 -> Loss: 0.616698964634115 \t| Accuracy: 70.208\n",
      "# Iteration   167 -> Loss: 0.616182158468645 \t| Accuracy: 70.417\n",
      "# Iteration   168 -> Loss: 0.6156650515816248 \t| Accuracy: 71.250\n",
      "# Iteration   169 -> Loss: 0.6151476344007349 \t| Accuracy: 71.250\n",
      "# Iteration   170 -> Loss: 0.6146298973570126 \t| Accuracy: 71.250\n",
      "# Iteration   171 -> Loss: 0.6141118308887739 \t| Accuracy: 71.250\n",
      "# Iteration   172 -> Loss: 0.6135934254454688 \t| Accuracy: 71.250\n",
      "# Iteration   173 -> Loss: 0.6130746714914695 \t| Accuracy: 72.708\n",
      "# Iteration   174 -> Loss: 0.6125555595097919 \t| Accuracy: 72.708\n",
      "# Iteration   175 -> Loss: 0.6120360800057516 \t| Accuracy: 72.917\n",
      "# Iteration   176 -> Loss: 0.6115162235105522 \t| Accuracy: 73.125\n",
      "# Iteration   177 -> Loss: 0.6109959805848085 \t| Accuracy: 73.333\n",
      "# Iteration   178 -> Loss: 0.6104753418220028 \t| Accuracy: 73.333\n",
      "# Iteration   179 -> Loss: 0.6099542978518769 \t| Accuracy: 73.542\n",
      "# Iteration   180 -> Loss: 0.6094328393437588 \t| Accuracy: 73.750\n",
      "# Iteration   181 -> Loss: 0.6089109570098259 \t| Accuracy: 73.750\n",
      "# Iteration   182 -> Loss: 0.6083886416083047 \t| Accuracy: 73.750\n",
      "# Iteration   183 -> Loss: 0.607865883946609 \t| Accuracy: 73.750\n",
      "# Iteration   184 -> Loss: 0.6073426748844151 \t| Accuracy: 74.375\n",
      "# Iteration   185 -> Loss: 0.606819005336679 \t| Accuracy: 74.375\n",
      "# Iteration   186 -> Loss: 0.6062948662765931 \t| Accuracy: 74.375\n",
      "# Iteration   187 -> Loss: 0.6057702487384846 \t| Accuracy: 74.375\n",
      "# Iteration   188 -> Loss: 0.605245143820659 \t| Accuracy: 74.583\n",
      "# Iteration   189 -> Loss: 0.604719542688185 \t| Accuracy: 75.000\n",
      "# Iteration   190 -> Loss: 0.6041934365756279 \t| Accuracy: 75.208\n",
      "# Iteration   191 -> Loss: 0.6036668167897287 \t| Accuracy: 75.625\n",
      "# Iteration   192 -> Loss: 0.6031396747120321 \t| Accuracy: 75.833\n",
      "# Iteration   193 -> Loss: 0.602612001801463 \t| Accuracy: 76.458\n",
      "# Iteration   194 -> Loss: 0.6020837895968559 \t| Accuracy: 76.458\n",
      "# Iteration   195 -> Loss: 0.6015550297194356 \t| Accuracy: 76.458\n",
      "# Iteration   196 -> Loss: 0.6010257138752524 \t| Accuracy: 76.458\n",
      "# Iteration   197 -> Loss: 0.6004958338575715 \t| Accuracy: 76.458\n",
      "# Iteration   198 -> Loss: 0.5999653815492212 \t| Accuracy: 76.458\n",
      "# Iteration   199 -> Loss: 0.5994343489248966 \t| Accuracy: 76.458\n",
      "# Iteration   200 -> Loss: 0.5989027280534257 \t| Accuracy: 76.458\n",
      "# Iteration   201 -> Loss: 0.5983705110999942 \t| Accuracy: 76.667\n",
      "# Iteration   202 -> Loss: 0.5978376903283336 \t| Accuracy: 76.875\n",
      "# Iteration   203 -> Loss: 0.5973042581028734 \t| Accuracy: 76.875\n",
      "# Iteration   204 -> Loss: 0.5967702068908571 \t| Accuracy: 76.875\n",
      "# Iteration   205 -> Loss: 0.596235529264427 \t| Accuracy: 76.875\n",
      "# Iteration   206 -> Loss: 0.5957002179026747 \t| Accuracy: 76.875\n",
      "# Iteration   207 -> Loss: 0.5951642655936614 \t| Accuracy: 76.875\n",
      "# Iteration   208 -> Loss: 0.5946276652364095 \t| Accuracy: 77.292\n",
      "# Iteration   209 -> Loss: 0.5940904098428653 \t| Accuracy: 77.292\n",
      "# Iteration   210 -> Loss: 0.5935524925398348 \t| Accuracy: 77.500\n",
      "# Iteration   211 -> Loss: 0.5930139065708936 \t| Accuracy: 77.708\n",
      "# Iteration   212 -> Loss: 0.592474645298273 \t| Accuracy: 77.917\n",
      "# Iteration   213 -> Loss: 0.5919347022047233 \t| Accuracy: 77.917\n",
      "# Iteration   214 -> Loss: 0.591394070895353 \t| Accuracy: 78.333\n",
      "# Iteration   215 -> Loss: 0.5908527450994487 \t| Accuracy: 78.333\n",
      "# Iteration   216 -> Loss: 0.5903107186722756 \t| Accuracy: 78.958\n",
      "# Iteration   217 -> Loss: 0.5897679855968576 \t| Accuracy: 78.958\n",
      "# Iteration   218 -> Loss: 0.5892245399857418 \t| Accuracy: 79.167\n",
      "# Iteration   219 -> Loss: 0.5886803760827444 \t| Accuracy: 79.375\n",
      "# Iteration   220 -> Loss: 0.5881354882646832 \t| Accuracy: 79.375\n",
      "# Iteration   221 -> Loss: 0.5875898710430933 \t| Accuracy: 79.375\n",
      "# Iteration   222 -> Loss: 0.5870435190659319 \t| Accuracy: 79.375\n",
      "# Iteration   223 -> Loss: 0.586496427119268 \t| Accuracy: 79.792\n",
      "# Iteration   224 -> Loss: 0.5859485901289612 \t| Accuracy: 79.792\n",
      "# Iteration   225 -> Loss: 0.5854000031623313 \t| Accuracy: 79.792\n",
      "# Iteration   226 -> Loss: 0.5848506614298159 \t| Accuracy: 79.792\n",
      "# Iteration   227 -> Loss: 0.5843005602866194 \t| Accuracy: 79.792\n",
      "# Iteration   228 -> Loss: 0.5837496952343552 \t| Accuracy: 79.792\n",
      "# Iteration   229 -> Loss: 0.5831980619226784 \t| Accuracy: 79.792\n",
      "# Iteration   230 -> Loss: 0.5826456561509138 \t| Accuracy: 79.792\n",
      "# Iteration   231 -> Loss: 0.5820924738696767 \t| Accuracy: 80.000\n",
      "# Iteration   232 -> Loss: 0.5815385111824887 \t| Accuracy: 80.000\n",
      "# Iteration   233 -> Loss: 0.5809837643473904 \t| Accuracy: 80.208\n",
      "# Iteration   234 -> Loss: 0.5804282297785486 \t| Accuracy: 80.625\n",
      "# Iteration   235 -> Loss: 0.5798719040478612 \t| Accuracy: 80.625\n",
      "# Iteration   236 -> Loss: 0.5793147838865602 \t| Accuracy: 80.625\n",
      "# Iteration   237 -> Loss: 0.5787568661868127 \t| Accuracy: 81.250\n",
      "# Iteration   238 -> Loss: 0.5781981480033188 \t| Accuracy: 81.250\n",
      "# Iteration   239 -> Loss: 0.5776386265549128 \t| Accuracy: 81.250\n",
      "# Iteration   240 -> Loss: 0.5770782992261606 \t| Accuracy: 81.250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Iteration   241 -> Loss: 0.5765171635689592 \t| Accuracy: 81.250\n",
      "# Iteration   242 -> Loss: 0.5759552173041376 \t| Accuracy: 81.250\n",
      "# Iteration   243 -> Loss: 0.5753924583230574 \t| Accuracy: 81.250\n",
      "# Iteration   244 -> Loss: 0.5748288846892178 \t| Accuracy: 81.250\n",
      "# Iteration   245 -> Loss: 0.5742644946398597 \t| Accuracy: 81.458\n",
      "# Iteration   246 -> Loss: 0.5736992865875754 \t| Accuracy: 81.458\n",
      "# Iteration   247 -> Loss: 0.573133259121919 \t| Accuracy: 82.083\n",
      "# Iteration   248 -> Loss: 0.5725664110110216 \t| Accuracy: 82.083\n",
      "# Iteration   249 -> Loss: 0.5719987412032084 \t| Accuracy: 82.292\n",
      "# Iteration   250 -> Loss: 0.571430248828621 \t| Accuracy: 82.292\n",
      "# Iteration   251 -> Loss: 0.5708609332008424 \t| Accuracy: 82.292\n",
      "# Iteration   252 -> Loss: 0.5702907938185261 \t| Accuracy: 82.292\n",
      "# Iteration   253 -> Loss: 0.5697198303670292 \t| Accuracy: 82.292\n",
      "# Iteration   254 -> Loss: 0.5691480427200493 \t| Accuracy: 82.292\n",
      "# Iteration   255 -> Loss: 0.5685754309412645 \t| Accuracy: 82.292\n",
      "# Iteration   256 -> Loss: 0.5680019952859787 \t| Accuracy: 82.292\n",
      "# Iteration   257 -> Loss: 0.5674277362027671 \t| Accuracy: 82.083\n",
      "# Iteration   258 -> Loss: 0.5668526543351274 \t| Accuracy: 82.083\n",
      "# Iteration   259 -> Loss: 0.5662767505231315 \t| Accuracy: 82.500\n",
      "# Iteration   260 -> Loss: 0.5657000258050798 \t| Accuracy: 82.500\n",
      "# Iteration   261 -> Loss: 0.5651224814191567 \t| Accuracy: 82.500\n",
      "# Iteration   262 -> Loss: 0.5645441188050858 \t| Accuracy: 82.500\n",
      "# Iteration   263 -> Loss: 0.5639649396057846 \t| Accuracy: 82.500\n",
      "# Iteration   264 -> Loss: 0.5633849456690176 \t| Accuracy: 82.500\n",
      "# Iteration   265 -> Loss: 0.5628041390490467 \t| Accuracy: 82.500\n",
      "# Iteration   266 -> Loss: 0.5622225220082762 \t| Accuracy: 82.500\n",
      "# Iteration   267 -> Loss: 0.5616400970188934 \t| Accuracy: 82.500\n",
      "# Iteration   268 -> Loss: 0.5610568667645012 \t| Accuracy: 83.125\n",
      "# Iteration   269 -> Loss: 0.5604728341417401 \t| Accuracy: 83.125\n",
      "# Iteration   270 -> Loss: 0.5598880022619004 \t| Accuracy: 83.125\n",
      "# Iteration   271 -> Loss: 0.5593023744525195 \t| Accuracy: 83.125\n",
      "# Iteration   272 -> Loss: 0.5587159542589637 \t| Accuracy: 83.125\n",
      "# Iteration   273 -> Loss: 0.5581287454459909 \t| Accuracy: 83.125\n",
      "# Iteration   274 -> Loss: 0.557540751999291 \t| Accuracy: 83.125\n",
      "# Iteration   275 -> Loss: 0.5569519781270023 \t| Accuracy: 83.125\n",
      "# Iteration   276 -> Loss: 0.5563624282611999 \t| Accuracy: 83.125\n",
      "# Iteration   277 -> Loss: 0.5557721070593507 \t| Accuracy: 83.333\n",
      "# Iteration   278 -> Loss: 0.5551810194057345 \t| Accuracy: 83.333\n",
      "# Iteration   279 -> Loss: 0.5545891704128254 \t| Accuracy: 83.333\n",
      "# Iteration   280 -> Loss: 0.5539965654226279 \t| Accuracy: 83.333\n",
      "# Iteration   281 -> Loss: 0.5534032100079658 \t| Accuracy: 83.333\n",
      "# Iteration   282 -> Loss: 0.552809109973717 \t| Accuracy: 83.333\n",
      "# Iteration   283 -> Loss: 0.5522142713579911 \t| Accuracy: 83.542\n",
      "# Iteration   284 -> Loss: 0.5516187004332413 \t| Accuracy: 84.167\n",
      "# Iteration   285 -> Loss: 0.5510224037073094 \t| Accuracy: 84.167\n",
      "# Iteration   286 -> Loss: 0.5504253879243933 \t| Accuracy: 84.167\n",
      "# Iteration   287 -> Loss: 0.5498276600659358 \t| Accuracy: 84.375\n",
      "# Iteration   288 -> Loss: 0.5492292273514248 \t| Accuracy: 84.583\n",
      "# Iteration   289 -> Loss: 0.5486300972390984 \t| Accuracy: 84.583\n",
      "# Iteration   290 -> Loss: 0.5480302774265529 \t| Accuracy: 84.583\n",
      "# Iteration   291 -> Loss: 0.5474297758512391 \t| Accuracy: 84.583\n",
      "# Iteration   292 -> Loss: 0.5468286006908476 \t| Accuracy: 84.583\n",
      "# Iteration   293 -> Loss: 0.5462267603635702 \t| Accuracy: 84.583\n",
      "# Iteration   294 -> Loss: 0.5456242635282323 \t| Accuracy: 84.583\n",
      "# Iteration   295 -> Loss: 0.5450211190842902 \t| Accuracy: 84.583\n",
      "# Iteration   296 -> Loss: 0.5444173361716816 \t| Accuracy: 84.583\n",
      "# Iteration   297 -> Loss: 0.5438129241705261 \t| Accuracy: 84.583\n",
      "# Iteration   298 -> Loss: 0.5432078927006647 \t| Accuracy: 84.583\n",
      "# Iteration   299 -> Loss: 0.5426022516210323 \t| Accuracy: 84.583\n",
      "# Iteration   300 -> Loss: 0.5419960110288554 \t| Accuracy: 84.583\n",
      "# Iteration   301 -> Loss: 0.5413891812586655 \t| Accuracy: 84.583\n",
      "# Iteration   302 -> Loss: 0.540781772881123 \t| Accuracy: 85.000\n",
      "# Iteration   303 -> Loss: 0.5401737967016425 \t| Accuracy: 85.625\n",
      "# Iteration   304 -> Loss: 0.5395652637588134 \t| Accuracy: 85.625\n",
      "# Iteration   305 -> Loss: 0.538956185322607 \t| Accuracy: 85.625\n",
      "# Iteration   306 -> Loss: 0.5383465728923641 \t| Accuracy: 85.625\n",
      "# Iteration   307 -> Loss: 0.5377364381945585 \t| Accuracy: 85.625\n",
      "# Iteration   308 -> Loss: 0.5371257931803247 \t| Accuracy: 85.625\n",
      "# Iteration   309 -> Loss: 0.5365146500227513 \t| Accuracy: 85.625\n",
      "# Iteration   310 -> Loss: 0.5359030211139273 \t| Accuracy: 85.625\n",
      "# Iteration   311 -> Loss: 0.5352909190617418 \t| Accuracy: 85.417\n",
      "# Iteration   312 -> Loss: 0.5346783566864294 \t| Accuracy: 85.417\n",
      "# Iteration   313 -> Loss: 0.5340653470168595 \t| Accuracy: 85.417\n",
      "# Iteration   314 -> Loss: 0.5334519032865629 \t| Accuracy: 85.417\n",
      "# Iteration   315 -> Loss: 0.5328380389294973 \t| Accuracy: 85.417\n",
      "# Iteration   316 -> Loss: 0.5322237675755467 \t| Accuracy: 85.417\n",
      "# Iteration   317 -> Loss: 0.5316091030457528 \t| Accuracy: 85.417\n",
      "# Iteration   318 -> Loss: 0.5309940593472814 \t| Accuracy: 85.417\n",
      "# Iteration   319 -> Loss: 0.5303786506681197 \t| Accuracy: 85.417\n",
      "# Iteration   320 -> Loss: 0.5297628913715091 \t| Accuracy: 85.417\n",
      "# Iteration   321 -> Loss: 0.529146795990112 \t| Accuracy: 86.042\n",
      "# Iteration   322 -> Loss: 0.5285303792199197 \t| Accuracy: 86.042\n",
      "# Iteration   323 -> Loss: 0.5279136559139003 \t| Accuracy: 86.042\n",
      "# Iteration   324 -> Loss: 0.5272966410753941 \t| Accuracy: 86.042\n",
      "# Iteration   325 -> Loss: 0.5266793498512619 \t| Accuracy: 86.042\n",
      "# Iteration   326 -> Loss: 0.5260617975247914 \t| Accuracy: 86.042\n",
      "# Iteration   327 -> Loss: 0.5254439995083701 \t| Accuracy: 86.042\n",
      "# Iteration   328 -> Loss: 0.5248259713359316 \t| Accuracy: 86.042\n",
      "# Iteration   329 -> Loss: 0.5242077286551868 \t| Accuracy: 86.042\n",
      "# Iteration   330 -> Loss: 0.5235892872196483 \t| Accuracy: 86.042\n",
      "# Iteration   331 -> Loss: 0.5229706628804585 \t| Accuracy: 86.042\n",
      "# Iteration   332 -> Loss: 0.5223518715780362 \t| Accuracy: 86.042\n",
      "# Iteration   333 -> Loss: 0.5217329293335509 \t| Accuracy: 86.458\n",
      "# Iteration   334 -> Loss: 0.5211138522402406 \t| Accuracy: 86.667\n",
      "# Iteration   335 -> Loss: 0.5204946564545877 \t| Accuracy: 86.667\n",
      "# Iteration   336 -> Loss: 0.5198753581873647 \t| Accuracy: 86.667\n",
      "# Iteration   337 -> Loss: 0.5192559736945704 \t| Accuracy: 86.667\n",
      "# Iteration   338 -> Loss: 0.5186365192682688 \t| Accuracy: 86.667\n",
      "# Iteration   339 -> Loss: 0.5180170112273492 \t| Accuracy: 86.667\n",
      "# Iteration   340 -> Loss: 0.5173974659082252 \t| Accuracy: 86.667\n",
      "# Iteration   341 -> Loss: 0.5167778996554898 \t| Accuracy: 86.667\n",
      "# Iteration   342 -> Loss: 0.5161583288125438 \t| Accuracy: 87.083\n",
      "# Iteration   343 -> Loss: 0.5155387697122181 \t| Accuracy: 87.083\n",
      "# Iteration   344 -> Loss: 0.5149192386674075 \t| Accuracy: 87.083\n",
      "# Iteration   345 -> Loss: 0.5142997519617335 \t| Accuracy: 87.083\n",
      "# Iteration   346 -> Loss: 0.5136803258402559 \t| Accuracy: 87.083\n",
      "# Iteration   347 -> Loss: 0.5130609765002538 \t| Accuracy: 87.083\n",
      "# Iteration   348 -> Loss: 0.5124417200820905 \t| Accuracy: 87.083\n",
      "# Iteration   349 -> Loss: 0.5118225726601852 \t| Accuracy: 87.083\n",
      "# Iteration   350 -> Loss: 0.5112035502341057 \t| Accuracy: 87.083\n",
      "# Iteration   351 -> Loss: 0.5105846687198032 \t| Accuracy: 87.083\n",
      "# Iteration   352 -> Loss: 0.5099659439410051 \t| Accuracy: 87.500\n",
      "# Iteration   353 -> Loss: 0.5093473916207811 \t| Accuracy: 87.500\n",
      "# Iteration   354 -> Loss: 0.5087290273733037 \t| Accuracy: 87.500\n",
      "# Iteration   355 -> Loss: 0.5081108666958122 \t| Accuracy: 88.125\n",
      "# Iteration   356 -> Loss: 0.5074929249608018 \t| Accuracy: 88.125\n",
      "# Iteration   357 -> Loss: 0.5068752174084467 \t| Accuracy: 88.125\n",
      "# Iteration   358 -> Loss: 0.5062577591392742 \t| Accuracy: 88.125\n",
      "# Iteration   359 -> Loss: 0.5056405651071006 \t| Accuracy: 88.125\n",
      "# Iteration   360 -> Loss: 0.5050236501122405 \t| Accuracy: 88.125\n",
      "# Iteration   361 -> Loss: 0.5044070287950012 \t| Accuracy: 88.125\n",
      "# Iteration   362 -> Loss: 0.5037907156294714 \t| Accuracy: 88.125\n",
      "# Iteration   363 -> Loss: 0.5031747249176123 \t| Accuracy: 88.125\n",
      "# Iteration   364 -> Loss: 0.5025590707836598 \t| Accuracy: 88.125\n",
      "# Iteration   365 -> Loss: 0.5019437671688437 \t| Accuracy: 88.125\n",
      "# Iteration   366 -> Loss: 0.5013288278264305 \t| Accuracy: 88.125\n",
      "# Iteration   367 -> Loss: 0.5007142663170918 \t| Accuracy: 88.125\n",
      "# Iteration   368 -> Loss: 0.5001000960046056 \t| Accuracy: 88.125\n",
      "# Iteration   369 -> Loss: 0.49948633005188936 \t| Accuracy: 88.125\n",
      "# Iteration   370 -> Loss: 0.4988729814173682 \t| Accuracy: 88.125\n",
      "# Iteration   371 -> Loss: 0.49826006285167823 \t| Accuracy: 88.333\n",
      "# Iteration   372 -> Loss: 0.4976475868947035 \t| Accuracy: 88.333\n",
      "# Iteration   373 -> Loss: 0.4970355658729463 \t| Accuracy: 88.333\n",
      "# Iteration   374 -> Loss: 0.496424011897227 \t| Accuracy: 88.333\n",
      "# Iteration   375 -> Loss: 0.4958129368607113 \t| Accuracy: 88.333\n",
      "# Iteration   376 -> Loss: 0.49520235243725846 \t| Accuracy: 88.542\n",
      "# Iteration   377 -> Loss: 0.49459227008008866 \t| Accuracy: 88.542\n",
      "# Iteration   378 -> Loss: 0.49398270102075914 \t| Accuracy: 88.333\n",
      "# Iteration   379 -> Loss: 0.4933736562684474 \t| Accuracy: 88.333\n",
      "# Iteration   380 -> Loss: 0.49276514660952975 \t| Accuracy: 88.333\n",
      "# Iteration   381 -> Loss: 0.49215718260745017 \t| Accuracy: 88.333\n",
      "# Iteration   382 -> Loss: 0.49154977460286936 \t| Accuracy: 88.542\n",
      "# Iteration   383 -> Loss: 0.49094293271408584 \t| Accuracy: 88.542\n",
      "# Iteration   384 -> Loss: 0.4903366668377189 \t| Accuracy: 88.542\n",
      "# Iteration   385 -> Loss: 0.4897309866496437 \t| Accuracy: 88.542\n",
      "# Iteration   386 -> Loss: 0.4891259016061686 \t| Accuracy: 88.542\n",
      "# Iteration   387 -> Loss: 0.4885214209454428 \t| Accuracy: 88.542\n",
      "# Iteration   388 -> Loss: 0.4879175536890857 \t| Accuracy: 88.750\n",
      "# Iteration   389 -> Loss: 0.4873143086440239 \t| Accuracy: 88.750\n",
      "# Iteration   390 -> Loss: 0.4867116944045275 \t| Accuracy: 88.750\n",
      "# Iteration   391 -> Loss: 0.4861097193544324 \t| Accuracy: 88.750\n",
      "# Iteration   392 -> Loss: 0.4855083916695378 \t| Accuracy: 88.750\n",
      "# Iteration   393 -> Loss: 0.4849077193201675 \t| Accuracy: 88.750\n",
      "# Iteration   394 -> Loss: 0.4843077100738841 \t| Accuracy: 88.750\n",
      "# Iteration   395 -> Loss: 0.48370837149834334 \t| Accuracy: 88.750\n",
      "# Iteration   396 -> Loss: 0.483109710964279 \t| Accuracy: 88.750\n",
      "# Iteration   397 -> Loss: 0.48251173564860594 \t| Accuracy: 88.750\n",
      "# Iteration   398 -> Loss: 0.4819144525376314 \t| Accuracy: 88.750\n",
      "# Iteration   399 -> Loss: 0.481317868430363 \t| Accuracy: 88.958\n",
      "# Iteration   400 -> Loss: 0.48072198994190346 \t| Accuracy: 88.958\n",
      "# Iteration   401 -> Loss: 0.4801268235069214 \t| Accuracy: 88.958\n",
      "# Iteration   402 -> Loss: 0.47953237538318905 \t| Accuracy: 89.167\n",
      "# Iteration   403 -> Loss: 0.47893865165517613 \t| Accuracy: 89.167\n",
      "# Iteration   404 -> Loss: 0.47834565823769115 \t| Accuracy: 89.167\n",
      "# Iteration   405 -> Loss: 0.4777534008795616 \t| Accuracy: 89.167\n",
      "# Iteration   406 -> Loss: 0.47716188516734315 \t| Accuracy: 89.583\n",
      "# Iteration   407 -> Loss: 0.47657111652905054 \t| Accuracy: 89.583\n",
      "# Iteration   408 -> Loss: 0.47598110023790224 \t| Accuracy: 89.583\n",
      "# Iteration   409 -> Loss: 0.47539184141607016 \t| Accuracy: 89.792\n",
      "# Iteration   410 -> Loss: 0.4748033450384289 \t| Accuracy: 89.792\n",
      "# Iteration   411 -> Loss: 0.4742156159362966 \t| Accuracy: 89.792\n",
      "# Iteration   412 -> Loss: 0.4736286588011604 \t| Accuracy: 89.792\n",
      "# Iteration   413 -> Loss: 0.473042478188383 \t| Accuracy: 89.792\n",
      "# Iteration   414 -> Loss: 0.4724570785208808 \t| Accuracy: 89.792\n",
      "# Iteration   415 -> Loss: 0.47187246409277167 \t| Accuracy: 89.792\n",
      "# Iteration   416 -> Loss: 0.4712886390729861 \t| Accuracy: 89.792\n",
      "# Iteration   417 -> Loss: 0.4707056075088367 \t| Accuracy: 89.792\n",
      "# Iteration   418 -> Loss: 0.47012337332954207 \t| Accuracy: 89.792\n",
      "# Iteration   419 -> Loss: 0.46954194034970215 \t| Accuracy: 89.792\n",
      "# Iteration   420 -> Loss: 0.46896131227272 \t| Accuracy: 89.792\n",
      "# Iteration   421 -> Loss: 0.4683814926941672 \t| Accuracy: 89.792\n",
      "# Iteration   422 -> Loss: 0.4678024851050917 \t| Accuracy: 89.792\n",
      "# Iteration   423 -> Loss: 0.4672242928952619 \t| Accuracy: 89.792\n",
      "# Iteration   424 -> Loss: 0.46664691935634856 \t| Accuracy: 89.792\n",
      "# Iteration   425 -> Loss: 0.46607036768503995 \t| Accuracy: 89.792\n",
      "# Iteration   426 -> Loss: 0.46549464098608895 \t| Accuracy: 89.792\n",
      "# Iteration   427 -> Loss: 0.46491974227529187 \t| Accuracy: 89.792\n",
      "# Iteration   428 -> Loss: 0.46434567448239594 \t| Accuracy: 89.792\n",
      "# Iteration   429 -> Loss: 0.4637724404539355 \t| Accuracy: 89.792\n",
      "# Iteration   430 -> Loss: 0.46320004295599637 \t| Accuracy: 89.792\n",
      "# Iteration   431 -> Loss: 0.4626284846769065 \t| Accuracy: 89.792\n",
      "# Iteration   432 -> Loss: 0.46205776822985395 \t| Accuracy: 89.792\n",
      "# Iteration   433 -> Loss: 0.46148789615543145 \t| Accuracy: 89.792\n",
      "# Iteration   434 -> Loss: 0.46091887092410677 \t| Accuracy: 89.792\n",
      "# Iteration   435 -> Loss: 0.4603506949386209 \t| Accuracy: 89.792\n",
      "# Iteration   436 -> Loss: 0.45978337053631196 \t| Accuracy: 89.792\n",
      "# Iteration   437 -> Loss: 0.45921689999136683 \t| Accuracy: 89.792\n",
      "# Iteration   438 -> Loss: 0.45865128551700185 \t| Accuracy: 89.792\n",
      "# Iteration   439 -> Loss: 0.4580865292675696 \t| Accuracy: 89.792\n",
      "# Iteration   440 -> Loss: 0.45752263334059806 \t| Accuracy: 89.792\n",
      "# Iteration   441 -> Loss: 0.4569595997787578 \t| Accuracy: 89.792\n",
      "# Iteration   442 -> Loss: 0.4563974305717612 \t| Accuracy: 89.792\n",
      "# Iteration   443 -> Loss: 0.45583612765819453 \t| Accuracy: 89.792\n",
      "# Iteration   444 -> Loss: 0.45527569292728254 \t| Accuracy: 89.792\n",
      "# Iteration   445 -> Loss: 0.45471612822058866 \t| Accuracy: 89.792\n",
      "# Iteration   446 -> Loss: 0.45415743533364983 \t| Accuracy: 89.792\n",
      "# Iteration   447 -> Loss: 0.4535996160175504 \t| Accuracy: 90.000\n",
      "# Iteration   448 -> Loss: 0.4530426719804328 \t| Accuracy: 90.000\n",
      "# Iteration   449 -> Loss: 0.4524866048889498 \t| Accuracy: 90.000\n",
      "# Iteration   450 -> Loss: 0.45193141636965667 \t| Accuracy: 90.000\n",
      "# Iteration   451 -> Loss: 0.4513771080103475 \t| Accuracy: 90.000\n",
      "# Iteration   452 -> Loss: 0.4508236813613354 \t| Accuracy: 90.000\n",
      "# Iteration   453 -> Loss: 0.450271137936678 \t| Accuracy: 90.000\n",
      "# Iteration   454 -> Loss: 0.4497194792153512 \t| Accuracy: 90.000\n",
      "# Iteration   455 -> Loss: 0.4491687066423713 \t| Accuracy: 90.000\n",
      "# Iteration   456 -> Loss: 0.44861882162986705 \t| Accuracy: 90.000\n",
      "# Iteration   457 -> Loss: 0.4480698255581044 \t| Accuracy: 90.000\n",
      "# Iteration   458 -> Loss: 0.4475217197764641 \t| Accuracy: 90.000\n",
      "# Iteration   459 -> Loss: 0.4469745056043737 \t| Accuracy: 90.000\n",
      "# Iteration   460 -> Loss: 0.44642818433219694 \t| Accuracy: 90.000\n",
      "# Iteration   461 -> Loss: 0.4458827572220798 \t| Accuracy: 90.000\n",
      "# Iteration   462 -> Loss: 0.44533822550875624 \t| Accuracy: 90.000\n",
      "# Iteration   463 -> Loss: 0.4447945904003147 \t| Accuracy: 90.000\n",
      "# Iteration   464 -> Loss: 0.4442518530789263 \t| Accuracy: 90.000\n",
      "# Iteration   465 -> Loss: 0.44371001470153654 \t| Accuracy: 90.000\n",
      "# Iteration   466 -> Loss: 0.4431690764005219 \t| Accuracy: 90.000\n",
      "# Iteration   467 -> Loss: 0.4426290392843122 \t| Accuracy: 90.000\n",
      "# Iteration   468 -> Loss: 0.4420899044379811 \t| Accuracy: 90.000\n",
      "# Iteration   469 -> Loss: 0.44155167292380415 \t| Accuracy: 90.000\n",
      "# Iteration   470 -> Loss: 0.4410143457817877 \t| Accuracy: 90.000\n",
      "# Iteration   471 -> Loss: 0.44047792403016867 \t| Accuracy: 90.000\n",
      "# Iteration   472 -> Loss: 0.4399424086658862 \t| Accuracy: 90.208\n",
      "# Iteration   473 -> Loss: 0.43940780066502777 \t| Accuracy: 90.208\n",
      "# Iteration   474 -> Loss: 0.4388741009832487 \t| Accuracy: 90.208\n",
      "# Iteration   475 -> Loss: 0.43834131055616826 \t| Accuracy: 90.208\n",
      "# Iteration   476 -> Loss: 0.4378094302997428 \t| Accuracy: 90.417\n",
      "# Iteration   477 -> Loss: 0.4372784611106148 \t| Accuracy: 90.417\n",
      "# Iteration   478 -> Loss: 0.4367484038664434 \t| Accuracy: 90.208\n",
      "# Iteration   479 -> Loss: 0.43621925942621287 \t| Accuracy: 90.208\n",
      "# Iteration   480 -> Loss: 0.4356910286305222 \t| Accuracy: 90.208\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Iteration   481 -> Loss: 0.43516371230185635 \t| Accuracy: 90.208\n",
      "# Iteration   482 -> Loss: 0.4346373112448407 \t| Accuracy: 90.208\n",
      "# Iteration   483 -> Loss: 0.43411182624647693 \t| Accuracy: 90.208\n",
      "# Iteration   484 -> Loss: 0.43358725807636594 \t| Accuracy: 90.208\n",
      "# Iteration   485 -> Loss: 0.4330636074869132 \t| Accuracy: 90.208\n",
      "# Iteration   486 -> Loss: 0.4325408752135208 \t| Accuracy: 90.208\n",
      "# Iteration   487 -> Loss: 0.4320190619747663 \t| Accuracy: 90.208\n",
      "# Iteration   488 -> Loss: 0.431498168472568 \t| Accuracy: 90.208\n",
      "# Iteration   489 -> Loss: 0.43097819539233917 \t| Accuracy: 90.208\n",
      "# Iteration   490 -> Loss: 0.43045914340312913 \t| Accuracy: 90.208\n",
      "# Iteration   491 -> Loss: 0.42994101315775524 \t| Accuracy: 89.583\n",
      "# Iteration   492 -> Loss: 0.42942380529292357 \t| Accuracy: 89.583\n",
      "# Iteration   493 -> Loss: 0.42890752042933994 \t| Accuracy: 89.583\n",
      "# Iteration   494 -> Loss: 0.42839215917181217 \t| Accuracy: 89.583\n",
      "# Iteration   495 -> Loss: 0.4278777221093441 \t| Accuracy: 89.583\n",
      "# Iteration   496 -> Loss: 0.4273642098152203 \t| Accuracy: 89.583\n",
      "# Iteration   497 -> Loss: 0.4268516228470846 \t| Accuracy: 89.583\n",
      "# Iteration   498 -> Loss: 0.4263399617470109 \t| Accuracy: 89.583\n",
      "# Iteration   499 -> Loss: 0.42582922704156684 \t| Accuracy: 89.583\n",
      "# Iteration   500 -> Loss: 0.4253194192418724 \t| Accuracy: 89.583\n",
      "# Iteration   501 -> Loss: 0.42481053884365155 \t| Accuracy: 89.583\n",
      "# Iteration   502 -> Loss: 0.4243025863272794 \t| Accuracy: 89.583\n",
      "# Iteration   503 -> Loss: 0.4237955621578238 \t| Accuracy: 89.583\n",
      "# Iteration   504 -> Loss: 0.42328946678508217 \t| Accuracy: 89.583\n",
      "# Iteration   505 -> Loss: 0.42278430064361433 \t| Accuracy: 90.000\n",
      "# Iteration   506 -> Loss: 0.42228006415277164 \t| Accuracy: 90.000\n",
      "# Iteration   507 -> Loss: 0.4217767577167213 \t| Accuracy: 90.000\n",
      "# Iteration   508 -> Loss: 0.4212743817244688 \t| Accuracy: 90.000\n",
      "# Iteration   509 -> Loss: 0.42077293654987596 \t| Accuracy: 90.000\n",
      "# Iteration   510 -> Loss: 0.4202724225516764 \t| Accuracy: 90.000\n",
      "# Iteration   511 -> Loss: 0.41977284007349 \t| Accuracy: 90.000\n",
      "# Iteration   512 -> Loss: 0.41927418944383205 \t| Accuracy: 90.000\n",
      "# Iteration   513 -> Loss: 0.4187764709761234 \t| Accuracy: 90.000\n",
      "# Iteration   514 -> Loss: 0.41827968496869666 \t| Accuracy: 90.000\n",
      "# Iteration   515 -> Loss: 0.4177838317048017 \t| Accuracy: 90.000\n",
      "# Iteration   516 -> Loss: 0.41728891145260943 \t| Accuracy: 90.000\n",
      "# Iteration   517 -> Loss: 0.41679492446521443 \t| Accuracy: 90.000\n",
      "# Iteration   518 -> Loss: 0.41630187098063665 \t| Accuracy: 90.000\n",
      "# Iteration   519 -> Loss: 0.4158097512218213 \t| Accuracy: 90.000\n",
      "# Iteration   520 -> Loss: 0.41531856539663975 \t| Accuracy: 90.000\n",
      "# Iteration   521 -> Loss: 0.41482831369788825 \t| Accuracy: 90.208\n",
      "# Iteration   522 -> Loss: 0.4143389963032869 \t| Accuracy: 90.208\n",
      "# Iteration   523 -> Loss: 0.4138506133754786 \t| Accuracy: 90.208\n",
      "# Iteration   524 -> Loss: 0.4133631650620271 \t| Accuracy: 90.208\n",
      "# Iteration   525 -> Loss: 0.4128766514954157 \t| Accuracy: 90.208\n",
      "# Iteration   526 -> Loss: 0.4123910727930459 \t| Accuracy: 90.208\n",
      "# Iteration   527 -> Loss: 0.4119064290572361 \t| Accuracy: 90.208\n",
      "# Iteration   528 -> Loss: 0.41142272037522065 \t| Accuracy: 90.208\n",
      "# Iteration   529 -> Loss: 0.4109399468191496 \t| Accuracy: 90.208\n",
      "# Iteration   530 -> Loss: 0.4104581084460887 \t| Accuracy: 90.208\n",
      "# Iteration   531 -> Loss: 0.4099772052980199 \t| Accuracy: 90.208\n",
      "# Iteration   532 -> Loss: 0.409497237401843 \t| Accuracy: 90.208\n",
      "# Iteration   533 -> Loss: 0.4090182047693779 \t| Accuracy: 90.208\n",
      "# Iteration   534 -> Loss: 0.4085401073973671 \t| Accuracy: 90.208\n",
      "# Iteration   535 -> Loss: 0.4080629452674806 \t| Accuracy: 90.208\n",
      "# Iteration   536 -> Loss: 0.4075867183463198 \t| Accuracy: 90.208\n",
      "# Iteration   537 -> Loss: 0.4071114265854246 \t| Accuracy: 90.208\n",
      "# Iteration   538 -> Loss: 0.4066370699212796 \t| Accuracy: 90.208\n",
      "# Iteration   539 -> Loss: 0.40616364827532336 \t| Accuracy: 90.208\n",
      "# Iteration   540 -> Loss: 0.4056911615539571 \t| Accuracy: 90.208\n",
      "# Iteration   541 -> Loss: 0.40521960964855624 \t| Accuracy: 90.208\n",
      "# Iteration   542 -> Loss: 0.40474899243548224 \t| Accuracy: 90.208\n",
      "# Iteration   543 -> Loss: 0.40427930977609633 \t| Accuracy: 90.208\n",
      "# Iteration   544 -> Loss: 0.40381056151677436 \t| Accuracy: 90.208\n",
      "# Iteration   545 -> Loss: 0.40334274748892335 \t| Accuracy: 90.208\n",
      "# Iteration   546 -> Loss: 0.40287586750899923 \t| Accuracy: 90.208\n",
      "# Iteration   547 -> Loss: 0.40240992137852655 \t| Accuracy: 90.208\n",
      "# Iteration   548 -> Loss: 0.40194490888411866 \t| Accuracy: 90.208\n",
      "# Iteration   549 -> Loss: 0.4014808297975015 \t| Accuracy: 90.208\n",
      "# Iteration   550 -> Loss: 0.40101768387553655 \t| Accuracy: 90.208\n",
      "# Iteration   551 -> Loss: 0.40055547086024695 \t| Accuracy: 90.208\n",
      "# Iteration   552 -> Loss: 0.40009419047884515 \t| Accuracy: 90.208\n",
      "# Iteration   553 -> Loss: 0.39963384244376127 \t| Accuracy: 90.208\n",
      "# Iteration   554 -> Loss: 0.3991744264526744 \t| Accuracy: 90.208\n",
      "# Iteration   555 -> Loss: 0.3987159421885445 \t| Accuracy: 90.208\n",
      "# Iteration   556 -> Loss: 0.3982583893196464 \t| Accuracy: 90.208\n",
      "# Iteration   557 -> Loss: 0.39780176749960583 \t| Accuracy: 90.208\n",
      "# Iteration   558 -> Loss: 0.3973460763674365 \t| Accuracy: 90.208\n",
      "# Iteration   559 -> Loss: 0.3968913155475792 \t| Accuracy: 90.208\n",
      "# Iteration   560 -> Loss: 0.3964374846499429 \t| Accuracy: 90.208\n",
      "# Iteration   561 -> Loss: 0.3959845832699469 \t| Accuracy: 90.208\n",
      "# Iteration   562 -> Loss: 0.39553261098856546 \t| Accuracy: 90.208\n",
      "# Iteration   563 -> Loss: 0.39508156737237327 \t| Accuracy: 90.208\n",
      "# Iteration   564 -> Loss: 0.3946314519735934 \t| Accuracy: 90.208\n",
      "# Iteration   565 -> Loss: 0.39418226433014664 \t| Accuracy: 90.208\n",
      "# Iteration   566 -> Loss: 0.39373400396570235 \t| Accuracy: 90.208\n",
      "# Iteration   567 -> Loss: 0.3932866703897315 \t| Accuracy: 90.208\n",
      "# Iteration   568 -> Loss: 0.3928402630975607 \t| Accuracy: 90.208\n",
      "# Iteration   569 -> Loss: 0.39239478157042856 \t| Accuracy: 90.208\n",
      "# Iteration   570 -> Loss: 0.39195022527554324 \t| Accuracy: 90.208\n",
      "# Iteration   571 -> Loss: 0.39150659366614193 \t| Accuracy: 90.208\n",
      "# Iteration   572 -> Loss: 0.391063886181552 \t| Accuracy: 90.208\n",
      "# Iteration   573 -> Loss: 0.3906221022472533 \t| Accuracy: 90.208\n",
      "# Iteration   574 -> Loss: 0.3901812412749428 \t| Accuracy: 90.208\n",
      "# Iteration   575 -> Loss: 0.3897413026626001 \t| Accuracy: 90.208\n",
      "# Iteration   576 -> Loss: 0.38930228579455506 \t| Accuracy: 90.208\n",
      "# Iteration   577 -> Loss: 0.38886419004155653 \t| Accuracy: 90.208\n",
      "# Iteration   578 -> Loss: 0.3884270147608431 \t| Accuracy: 90.208\n",
      "# Iteration   579 -> Loss: 0.38799075929621496 \t| Accuracy: 90.208\n",
      "# Iteration   580 -> Loss: 0.38755542297810747 \t| Accuracy: 90.208\n",
      "# Iteration   581 -> Loss: 0.387121005123666 \t| Accuracy: 90.208\n",
      "# Iteration   582 -> Loss: 0.38668750503682237 \t| Accuracy: 90.625\n",
      "# Iteration   583 -> Loss: 0.38625492200837286 \t| Accuracy: 90.625\n",
      "# Iteration   584 -> Loss: 0.3858232553160575 \t| Accuracy: 90.625\n",
      "# Iteration   585 -> Loss: 0.38539250422464016 \t| Accuracy: 90.625\n",
      "# Iteration   586 -> Loss: 0.38496266798599105 \t| Accuracy: 90.625\n",
      "# Iteration   587 -> Loss: 0.38453374583917005 \t| Accuracy: 90.625\n",
      "# Iteration   588 -> Loss: 0.3841057370105106 \t| Accuracy: 90.625\n",
      "# Iteration   589 -> Loss: 0.38367864071370655 \t| Accuracy: 90.625\n",
      "# Iteration   590 -> Loss: 0.38325245614989867 \t| Accuracy: 90.625\n",
      "# Iteration   591 -> Loss: 0.3828271825077626 \t| Accuracy: 90.625\n",
      "# Iteration   592 -> Loss: 0.38240281896359923 \t| Accuracy: 90.625\n",
      "# Iteration   593 -> Loss: 0.38197936468142496 \t| Accuracy: 90.625\n",
      "# Iteration   594 -> Loss: 0.3815568188130629 \t| Accuracy: 90.625\n",
      "# Iteration   595 -> Loss: 0.381135180498237 \t| Accuracy: 90.625\n",
      "# Iteration   596 -> Loss: 0.38071444886466455 \t| Accuracy: 90.625\n",
      "# Iteration   597 -> Loss: 0.38029462302815226 \t| Accuracy: 90.625\n",
      "# Iteration   598 -> Loss: 0.3798757020926917 \t| Accuracy: 90.625\n",
      "# Iteration   599 -> Loss: 0.3794576851505562 \t| Accuracy: 90.625\n",
      "# Iteration   600 -> Loss: 0.3790405712823993 \t| Accuracy: 90.625\n",
      "# Iteration   601 -> Loss: 0.3786243595573526 \t| Accuracy: 90.625\n",
      "# Iteration   602 -> Loss: 0.3782090490331261 \t| Accuracy: 90.625\n",
      "# Iteration   603 -> Loss: 0.37779463875610886 \t| Accuracy: 90.625\n",
      "# Iteration   604 -> Loss: 0.37738112776146954 \t| Accuracy: 90.625\n",
      "# Iteration   605 -> Loss: 0.37696851507325924 \t| Accuracy: 90.625\n",
      "# Iteration   606 -> Loss: 0.37655679970451383 \t| Accuracy: 90.625\n",
      "# Iteration   607 -> Loss: 0.376145980657358 \t| Accuracy: 90.625\n",
      "# Iteration   608 -> Loss: 0.3757360569231089 \t| Accuracy: 90.625\n",
      "# Iteration   609 -> Loss: 0.3753270274823814 \t| Accuracy: 90.625\n",
      "# Iteration   610 -> Loss: 0.3749188913051937 \t| Accuracy: 90.625\n",
      "# Iteration   611 -> Loss: 0.3745116473510728 \t| Accuracy: 90.625\n",
      "# Iteration   612 -> Loss: 0.374105294569162 \t| Accuracy: 90.625\n",
      "# Iteration   613 -> Loss: 0.37369983189832706 \t| Accuracy: 90.625\n",
      "# Iteration   614 -> Loss: 0.3732952582672648 \t| Accuracy: 90.625\n",
      "# Iteration   615 -> Loss: 0.3728915725946102 \t| Accuracy: 90.625\n",
      "# Iteration   616 -> Loss: 0.37248877378904577 \t| Accuracy: 90.625\n",
      "# Iteration   617 -> Loss: 0.37208686074940983 \t| Accuracy: 91.250\n",
      "# Iteration   618 -> Loss: 0.371685832364806 \t| Accuracy: 91.250\n",
      "# Iteration   619 -> Loss: 0.3712856875147128 \t| Accuracy: 91.250\n",
      "# Iteration   620 -> Loss: 0.37088642506909325 \t| Accuracy: 91.250\n",
      "# Iteration   621 -> Loss: 0.3704880438885052 \t| Accuracy: 91.250\n",
      "# Iteration   622 -> Loss: 0.3700905428242117 \t| Accuracy: 91.250\n",
      "# Iteration   623 -> Loss: 0.3696939207182915 \t| Accuracy: 91.250\n",
      "# Iteration   624 -> Loss: 0.3692981764037494 \t| Accuracy: 91.250\n",
      "# Iteration   625 -> Loss: 0.3689033087046279 \t| Accuracy: 91.250\n",
      "# Iteration   626 -> Loss: 0.3685093164361174 \t| Accuracy: 91.250\n",
      "# Iteration   627 -> Loss: 0.3681161984046675 \t| Accuracy: 91.250\n",
      "# Iteration   628 -> Loss: 0.3677239534080983 \t| Accuracy: 91.250\n",
      "# Iteration   629 -> Loss: 0.36733258023571147 \t| Accuracy: 91.250\n",
      "# Iteration   630 -> Loss: 0.36694207766840087 \t| Accuracy: 91.250\n",
      "# Iteration   631 -> Loss: 0.36655244447876445 \t| Accuracy: 91.250\n",
      "# Iteration   632 -> Loss: 0.3661636794312146 \t| Accuracy: 91.250\n",
      "# Iteration   633 -> Loss: 0.3657757812820893 \t| Accuracy: 91.250\n",
      "# Iteration   634 -> Loss: 0.36538874877976324 \t| Accuracy: 91.250\n",
      "# Iteration   635 -> Loss: 0.3650025806647581 \t| Accuracy: 91.250\n",
      "# Iteration   636 -> Loss: 0.3646172756698533 \t| Accuracy: 91.250\n",
      "# Iteration   637 -> Loss: 0.3642328325201962 \t| Accuracy: 91.250\n",
      "# Iteration   638 -> Loss: 0.3638492499334123 \t| Accuracy: 91.250\n",
      "# Iteration   639 -> Loss: 0.3634665266197152 \t| Accuracy: 91.250\n",
      "# Iteration   640 -> Loss: 0.3630846612820162 \t| Accuracy: 91.250\n",
      "# Iteration   641 -> Loss: 0.3627036526160333 \t| Accuracy: 91.250\n",
      "# Iteration   642 -> Loss: 0.362323499310401 \t| Accuracy: 91.250\n",
      "# Iteration   643 -> Loss: 0.361944200046778 \t| Accuracy: 91.250\n",
      "# Iteration   644 -> Loss: 0.36156575349995645 \t| Accuracy: 91.250\n",
      "# Iteration   645 -> Loss: 0.36118815833796947 \t| Accuracy: 91.250\n",
      "# Iteration   646 -> Loss: 0.3608114132221987 \t| Accuracy: 91.250\n",
      "# Iteration   647 -> Loss: 0.3604355168074818 \t| Accuracy: 91.250\n",
      "# Iteration   648 -> Loss: 0.3600604677422189 \t| Accuracy: 91.250\n",
      "# Iteration   649 -> Loss: 0.35968626466847897 \t| Accuracy: 91.250\n",
      "# Iteration   650 -> Loss: 0.35931290622210577 \t| Accuracy: 91.250\n",
      "# Iteration   651 -> Loss: 0.3589403910328232 \t| Accuracy: 91.250\n",
      "# Iteration   652 -> Loss: 0.35856871772433974 \t| Accuracy: 91.250\n",
      "# Iteration   653 -> Loss: 0.35819788491445326 \t| Accuracy: 91.250\n",
      "# Iteration   654 -> Loss: 0.35782789121515474 \t| Accuracy: 91.250\n",
      "# Iteration   655 -> Loss: 0.35745873523273103 \t| Accuracy: 91.250\n",
      "# Iteration   656 -> Loss: 0.35709041556786797 \t| Accuracy: 91.250\n",
      "# Iteration   657 -> Loss: 0.3567229308157526 \t| Accuracy: 91.250\n",
      "# Iteration   658 -> Loss: 0.356356279566174 \t| Accuracy: 91.250\n",
      "# Iteration   659 -> Loss: 0.3559904604036247 \t| Accuracy: 91.250\n",
      "# Iteration   660 -> Loss: 0.3556254719074008 \t| Accuracy: 91.250\n",
      "# Iteration   661 -> Loss: 0.3552613126517014 \t| Accuracy: 91.250\n",
      "# Iteration   662 -> Loss: 0.3548979812057279 \t| Accuracy: 91.250\n",
      "# Iteration   663 -> Loss: 0.354535476133782 \t| Accuracy: 91.250\n",
      "# Iteration   664 -> Loss: 0.35417379599536336 \t| Accuracy: 91.250\n",
      "# Iteration   665 -> Loss: 0.35381293934526675 \t| Accuracy: 91.250\n",
      "# Iteration   666 -> Loss: 0.3534529047336782 \t| Accuracy: 91.250\n",
      "# Iteration   667 -> Loss: 0.35309369070627045 \t| Accuracy: 91.250\n",
      "# Iteration   668 -> Loss: 0.35273529580429797 \t| Accuracy: 91.250\n",
      "# Iteration   669 -> Loss: 0.352377718564691 \t| Accuracy: 91.250\n",
      "# Iteration   670 -> Loss: 0.352020957520149 \t| Accuracy: 91.250\n",
      "# Iteration   671 -> Loss: 0.35166501119923327 \t| Accuracy: 91.250\n",
      "# Iteration   672 -> Loss: 0.35130987812645903 \t| Accuracy: 91.250\n",
      "# Iteration   673 -> Loss: 0.3509555568223862 \t| Accuracy: 91.250\n",
      "# Iteration   674 -> Loss: 0.3506020458037105 \t| Accuracy: 91.250\n",
      "# Iteration   675 -> Loss: 0.3502493435833525 \t| Accuracy: 91.250\n",
      "# Iteration   676 -> Loss: 0.3498974486705468 \t| Accuracy: 91.250\n",
      "# Iteration   677 -> Loss: 0.34954635957093017 \t| Accuracy: 91.250\n",
      "# Iteration   678 -> Loss: 0.3491960747866288 \t| Accuracy: 91.250\n",
      "# Iteration   679 -> Loss: 0.34884659281634517 \t| Accuracy: 91.250\n",
      "# Iteration   680 -> Loss: 0.34849791215544323 \t| Accuracy: 91.250\n",
      "# Iteration   681 -> Loss: 0.3481500312960344 \t| Accuracy: 91.250\n",
      "# Iteration   682 -> Loss: 0.3478029487270605 \t| Accuracy: 91.250\n",
      "# Iteration   683 -> Loss: 0.34745666293437877 \t| Accuracy: 91.250\n",
      "# Iteration   684 -> Loss: 0.3471111724008429 \t| Accuracy: 91.250\n",
      "# Iteration   685 -> Loss: 0.3467664756063859 \t| Accuracy: 91.250\n",
      "# Iteration   686 -> Loss: 0.34642257102810103 \t| Accuracy: 91.250\n",
      "# Iteration   687 -> Loss: 0.34607945714032173 \t| Accuracy: 91.458\n",
      "# Iteration   688 -> Loss: 0.3457371324147012 \t| Accuracy: 91.458\n",
      "# Iteration   689 -> Loss: 0.34539559532029146 \t| Accuracy: 91.458\n",
      "# Iteration   690 -> Loss: 0.34505484432362027 \t| Accuracy: 91.458\n",
      "# Iteration   691 -> Loss: 0.3447148778887694 \t| Accuracy: 91.458\n",
      "# Iteration   692 -> Loss: 0.3443756944774499 \t| Accuracy: 91.458\n",
      "# Iteration   693 -> Loss: 0.3440372925490781 \t| Accuracy: 91.458\n",
      "# Iteration   694 -> Loss: 0.3436996705608498 \t| Accuracy: 91.458\n",
      "# Iteration   695 -> Loss: 0.3433628269678149 \t| Accuracy: 91.458\n",
      "# Iteration   696 -> Loss: 0.34302676022294976 \t| Accuracy: 91.458\n",
      "# Iteration   697 -> Loss: 0.34269146877722984 \t| Accuracy: 91.458\n",
      "# Iteration   698 -> Loss: 0.3423569510797012 \t| Accuracy: 91.458\n",
      "# Iteration   699 -> Loss: 0.34202320557755095 \t| Accuracy: 91.458\n",
      "# Iteration   700 -> Loss: 0.34169023071617755 \t| Accuracy: 91.458\n",
      "# Iteration   701 -> Loss: 0.34135802493925954 \t| Accuracy: 91.458\n",
      "# Iteration   702 -> Loss: 0.34102658668882446 \t| Accuracy: 91.458\n",
      "# Iteration   703 -> Loss: 0.3406959144053159 \t| Accuracy: 91.458\n",
      "# Iteration   704 -> Loss: 0.3403660065276604 \t| Accuracy: 91.458\n",
      "# Iteration   705 -> Loss: 0.340036861493334 \t| Accuracy: 91.458\n",
      "# Iteration   706 -> Loss: 0.33970847773842683 \t| Accuracy: 91.458\n",
      "# Iteration   707 -> Loss: 0.3393808536977082 \t| Accuracy: 91.458\n",
      "# Iteration   708 -> Loss: 0.33905398780468987 \t| Accuracy: 91.458\n",
      "# Iteration   709 -> Loss: 0.3387278784916896 \t| Accuracy: 91.458\n",
      "# Iteration   710 -> Loss: 0.3384025241898931 \t| Accuracy: 91.458\n",
      "# Iteration   711 -> Loss: 0.33807792332941533 \t| Accuracy: 91.458\n",
      "# Iteration   712 -> Loss: 0.3377540743393619 \t| Accuracy: 91.458\n",
      "# Iteration   713 -> Loss: 0.33743097564788854 \t| Accuracy: 91.458\n",
      "# Iteration   714 -> Loss: 0.33710862568226085 \t| Accuracy: 91.458\n",
      "# Iteration   715 -> Loss: 0.3367870228689126 \t| Accuracy: 91.458\n",
      "# Iteration   716 -> Loss: 0.33646616563350373 \t| Accuracy: 91.458\n",
      "# Iteration   717 -> Loss: 0.33614605240097734 \t| Accuracy: 91.458\n",
      "# Iteration   718 -> Loss: 0.3358266815956167 \t| Accuracy: 91.458\n",
      "# Iteration   719 -> Loss: 0.3355080516411003 \t| Accuracy: 91.458\n",
      "# Iteration   720 -> Loss: 0.3351901609605574 \t| Accuracy: 91.458\n",
      "# Iteration   721 -> Loss: 0.33487300797662223 \t| Accuracy: 91.458\n",
      "# Iteration   722 -> Loss: 0.3345565911114875 \t| Accuracy: 91.458\n",
      "# Iteration   723 -> Loss: 0.33424090878695784 \t| Accuracy: 91.458\n",
      "# Iteration   724 -> Loss: 0.33392595942450165 \t| Accuracy: 91.458\n",
      "# Iteration   725 -> Loss: 0.33361174144530326 \t| Accuracy: 91.458\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Iteration   726 -> Loss: 0.3332982532703134 \t| Accuracy: 91.458\n",
      "# Iteration   727 -> Loss: 0.33298549332029986 \t| Accuracy: 91.458\n",
      "# Iteration   728 -> Loss: 0.33267346001589715 \t| Accuracy: 91.458\n",
      "# Iteration   729 -> Loss: 0.33236215177765555 \t| Accuracy: 91.458\n",
      "# Iteration   730 -> Loss: 0.3320515670260896 \t| Accuracy: 91.458\n",
      "# Iteration   731 -> Loss: 0.3317417041817255 \t| Accuracy: 91.458\n",
      "# Iteration   732 -> Loss: 0.3314325616651488 \t| Accuracy: 91.458\n",
      "# Iteration   733 -> Loss: 0.3311241378970506 \t| Accuracy: 91.458\n",
      "# Iteration   734 -> Loss: 0.3308164312982735 \t| Accuracy: 91.458\n",
      "# Iteration   735 -> Loss: 0.3305094402898571 \t| Accuracy: 91.458\n",
      "# Iteration   736 -> Loss: 0.33020316329308247 \t| Accuracy: 91.458\n",
      "# Iteration   737 -> Loss: 0.32989759872951646 \t| Accuracy: 91.458\n",
      "# Iteration   738 -> Loss: 0.32959274502105496 \t| Accuracy: 91.458\n",
      "# Iteration   739 -> Loss: 0.3292886005899665 \t| Accuracy: 91.458\n",
      "# Iteration   740 -> Loss: 0.32898516385893356 \t| Accuracy: 91.458\n",
      "# Iteration   741 -> Loss: 0.3286824332510955 \t| Accuracy: 91.458\n",
      "# Iteration   742 -> Loss: 0.32838040719008893 \t| Accuracy: 91.458\n",
      "# Iteration   743 -> Loss: 0.3280790841000891 \t| Accuracy: 91.458\n",
      "# Iteration   744 -> Loss: 0.3277784624058491 \t| Accuracy: 91.458\n",
      "# Iteration   745 -> Loss: 0.32747854053274056 \t| Accuracy: 91.458\n",
      "# Iteration   746 -> Loss: 0.3271793169067918 \t| Accuracy: 91.458\n",
      "# Iteration   747 -> Loss: 0.3268807899547269 \t| Accuracy: 91.458\n",
      "# Iteration   748 -> Loss: 0.3265829581040033 \t| Accuracy: 91.667\n",
      "# Iteration   749 -> Loss: 0.3262858197828494 \t| Accuracy: 91.667\n",
      "# Iteration   750 -> Loss: 0.3259893734203016 \t| Accuracy: 91.667\n",
      "# Iteration   751 -> Loss: 0.3256936174462407 \t| Accuracy: 91.667\n",
      "# Iteration   752 -> Loss: 0.32539855029142745 \t| Accuracy: 91.667\n",
      "# Iteration   753 -> Loss: 0.32510417038753875 \t| Accuracy: 91.667\n",
      "# Iteration   754 -> Loss: 0.3248104761672019 \t| Accuracy: 91.667\n",
      "# Iteration   755 -> Loss: 0.32451746606402915 \t| Accuracy: 91.667\n",
      "# Iteration   756 -> Loss: 0.324225138512652 \t| Accuracy: 91.667\n",
      "# Iteration   757 -> Loss: 0.3239334919487543 \t| Accuracy: 91.667\n",
      "# Iteration   758 -> Loss: 0.32364252480910577 \t| Accuracy: 91.667\n",
      "# Iteration   759 -> Loss: 0.323352235531594 \t| Accuracy: 91.667\n",
      "# Iteration   760 -> Loss: 0.32306262255525664 \t| Accuracy: 91.667\n",
      "# Iteration   761 -> Loss: 0.3227736843203134 \t| Accuracy: 91.667\n",
      "# Iteration   762 -> Loss: 0.32248541926819707 \t| Accuracy: 91.667\n",
      "# Iteration   763 -> Loss: 0.3221978258415838 \t| Accuracy: 91.667\n",
      "# Iteration   764 -> Loss: 0.32191090248442444 \t| Accuracy: 91.667\n",
      "# Iteration   765 -> Loss: 0.3216246476419731 \t| Accuracy: 91.667\n",
      "# Iteration   766 -> Loss: 0.32133905976081784 \t| Accuracy: 91.667\n",
      "# Iteration   767 -> Loss: 0.3210541372889087 \t| Accuracy: 91.667\n",
      "# Iteration   768 -> Loss: 0.320769878675587 \t| Accuracy: 91.667\n",
      "# Iteration   769 -> Loss: 0.3204862823716132 \t| Accuracy: 91.667\n",
      "# Iteration   770 -> Loss: 0.32020334682919466 \t| Accuracy: 91.667\n",
      "# Iteration   771 -> Loss: 0.31992107050201296 \t| Accuracy: 91.667\n",
      "# Iteration   772 -> Loss: 0.31963945184525133 \t| Accuracy: 91.667\n",
      "# Iteration   773 -> Loss: 0.31935848931562066 \t| Accuracy: 91.667\n",
      "# Iteration   774 -> Loss: 0.31907818137138594 \t| Accuracy: 91.667\n",
      "# Iteration   775 -> Loss: 0.31879852647239215 \t| Accuracy: 91.667\n",
      "# Iteration   776 -> Loss: 0.31851952308008935 \t| Accuracy: 91.667\n",
      "# Iteration   777 -> Loss: 0.3182411696575582 \t| Accuracy: 91.667\n",
      "# Iteration   778 -> Loss: 0.31796346466953446 \t| Accuracy: 91.667\n",
      "# Iteration   779 -> Loss: 0.3176864065824327 \t| Accuracy: 91.667\n",
      "# Iteration   780 -> Loss: 0.3174099938643714 \t| Accuracy: 91.667\n",
      "# Iteration   781 -> Loss: 0.31713422498519533 \t| Accuracy: 91.667\n",
      "# Iteration   782 -> Loss: 0.3168590984164999 \t| Accuracy: 91.667\n",
      "# Iteration   783 -> Loss: 0.31658461263165294 \t| Accuracy: 91.667\n",
      "# Iteration   784 -> Loss: 0.3163107661058181 \t| Accuracy: 91.667\n",
      "# Iteration   785 -> Loss: 0.31603755731597655 \t| Accuracy: 91.667\n",
      "# Iteration   786 -> Loss: 0.315764984740949 \t| Accuracy: 91.667\n",
      "# Iteration   787 -> Loss: 0.31549304686141716 \t| Accuracy: 91.667\n",
      "# Iteration   788 -> Loss: 0.3152217421599449 \t| Accuracy: 91.667\n",
      "# Iteration   789 -> Loss: 0.3149510691209991 \t| Accuracy: 91.667\n",
      "# Iteration   790 -> Loss: 0.3146810262309703 \t| Accuracy: 91.667\n",
      "# Iteration   791 -> Loss: 0.3144116119781925 \t| Accuracy: 91.667\n",
      "# Iteration   792 -> Loss: 0.3141428248529639 \t| Accuracy: 91.667\n",
      "# Iteration   793 -> Loss: 0.31387466334756536 \t| Accuracy: 91.667\n",
      "# Iteration   794 -> Loss: 0.31360712595628043 \t| Accuracy: 91.667\n",
      "# Iteration   795 -> Loss: 0.31334021117541416 \t| Accuracy: 91.667\n",
      "# Iteration   796 -> Loss: 0.3130739175033115 \t| Accuracy: 91.667\n",
      "# Iteration   797 -> Loss: 0.31280824344037583 \t| Accuracy: 91.667\n",
      "# Iteration   798 -> Loss: 0.31254318748908716 \t| Accuracy: 91.667\n",
      "# Iteration   799 -> Loss: 0.31227874815401974 \t| Accuracy: 91.667\n",
      "# Iteration   800 -> Loss: 0.31201492394185926 \t| Accuracy: 91.667\n",
      "# Iteration   801 -> Loss: 0.3117517133614205 \t| Accuracy: 91.667\n",
      "# Iteration   802 -> Loss: 0.311489114923664 \t| Accuracy: 91.667\n",
      "# Iteration   803 -> Loss: 0.31122712714171263 \t| Accuracy: 91.667\n",
      "# Iteration   804 -> Loss: 0.31096574853086806 \t| Accuracy: 91.667\n",
      "# Iteration   805 -> Loss: 0.31070497760862664 \t| Accuracy: 91.667\n",
      "# Iteration   806 -> Loss: 0.3104448128946956 \t| Accuracy: 91.667\n",
      "# Iteration   807 -> Loss: 0.31018525291100785 \t| Accuracy: 91.667\n",
      "# Iteration   808 -> Loss: 0.3099262961817379 \t| Accuracy: 91.667\n",
      "# Iteration   809 -> Loss: 0.30966794123331665 \t| Accuracy: 91.667\n",
      "# Iteration   810 -> Loss: 0.30941018659444597 \t| Accuracy: 91.667\n",
      "# Iteration   811 -> Loss: 0.3091530307961133 \t| Accuracy: 91.667\n",
      "# Iteration   812 -> Loss: 0.30889647237160567 \t| Accuracy: 91.667\n",
      "# Iteration   813 -> Loss: 0.3086405098565243 \t| Accuracy: 91.667\n",
      "# Iteration   814 -> Loss: 0.3083851417887974 \t| Accuracy: 91.667\n",
      "# Iteration   815 -> Loss: 0.3081303667086943 \t| Accuracy: 91.667\n",
      "# Iteration   816 -> Loss: 0.3078761831588384 \t| Accuracy: 91.667\n",
      "# Iteration   817 -> Loss: 0.3076225896842202 \t| Accuracy: 91.667\n",
      "# Iteration   818 -> Loss: 0.30736958483221005 \t| Accuracy: 91.667\n",
      "# Iteration   819 -> Loss: 0.30711716715257054 \t| Accuracy: 91.667\n",
      "# Iteration   820 -> Loss: 0.306865335197469 \t| Accuracy: 91.667\n",
      "# Iteration   821 -> Loss: 0.3066140875214893 \t| Accuracy: 91.667\n",
      "# Iteration   822 -> Loss: 0.3063634226816437 \t| Accuracy: 91.667\n",
      "# Iteration   823 -> Loss: 0.30611333923738465 \t| Accuracy: 91.667\n",
      "# Iteration   824 -> Loss: 0.3058638357506156 \t| Accuracy: 91.667\n",
      "# Iteration   825 -> Loss: 0.305614910785703 \t| Accuracy: 91.667\n",
      "# Iteration   826 -> Loss: 0.30536656290948616 \t| Accuracy: 91.667\n",
      "# Iteration   827 -> Loss: 0.3051187906912886 \t| Accuracy: 91.667\n",
      "# Iteration   828 -> Loss: 0.3048715927029286 \t| Accuracy: 91.667\n",
      "# Iteration   829 -> Loss: 0.3046249675187288 \t| Accuracy: 91.667\n",
      "# Iteration   830 -> Loss: 0.30437891371552694 \t| Accuracy: 91.667\n",
      "# Iteration   831 -> Loss: 0.3041334298726853 \t| Accuracy: 91.667\n",
      "# Iteration   832 -> Loss: 0.3038885145721005 \t| Accuracy: 91.667\n",
      "# Iteration   833 -> Loss: 0.3036441663982127 \t| Accuracy: 91.667\n",
      "# Iteration   834 -> Loss: 0.30340038393801494 \t| Accuracy: 91.667\n",
      "# Iteration   835 -> Loss: 0.30315716578106233 \t| Accuracy: 91.667\n",
      "# Iteration   836 -> Loss: 0.3029145105194804 \t| Accuracy: 91.667\n",
      "# Iteration   837 -> Loss: 0.30267241674797424 \t| Accuracy: 91.875\n",
      "# Iteration   838 -> Loss: 0.3024308830638365 \t| Accuracy: 91.875\n",
      "# Iteration   839 -> Loss: 0.30218990806695556 \t| Accuracy: 91.875\n",
      "# Iteration   840 -> Loss: 0.3019494903598243 \t| Accuracy: 91.875\n",
      "# Iteration   841 -> Loss: 0.301709628547547 \t| Accuracy: 91.875\n",
      "# Iteration   842 -> Loss: 0.30147032123784745 \t| Accuracy: 91.875\n",
      "# Iteration   843 -> Loss: 0.30123156704107645 \t| Accuracy: 91.875\n",
      "# Iteration   844 -> Loss: 0.3009933645702186 \t| Accuracy: 91.875\n",
      "# Iteration   845 -> Loss: 0.3007557124409004 \t| Accuracy: 91.875\n",
      "# Iteration   846 -> Loss: 0.30051860927139595 \t| Accuracy: 91.875\n",
      "# Iteration   847 -> Loss: 0.3002820536826347 \t| Accuracy: 91.875\n",
      "# Iteration   848 -> Loss: 0.3000460442982073 \t| Accuracy: 91.875\n",
      "# Iteration   849 -> Loss: 0.29981057974437253 \t| Accuracy: 91.875\n",
      "# Iteration   850 -> Loss: 0.29957565865006297 \t| Accuracy: 91.875\n",
      "# Iteration   851 -> Loss: 0.29934127964689133 \t| Accuracy: 91.875\n",
      "# Iteration   852 -> Loss: 0.2991074413691565 \t| Accuracy: 91.875\n",
      "# Iteration   853 -> Loss: 0.298874142453849 \t| Accuracy: 91.875\n",
      "# Iteration   854 -> Loss: 0.2986413815406563 \t| Accuracy: 91.875\n",
      "# Iteration   855 -> Loss: 0.2984091572719688 \t| Accuracy: 91.875\n",
      "# Iteration   856 -> Loss: 0.29817746829288444 \t| Accuracy: 91.875\n",
      "# Iteration   857 -> Loss: 0.29794631325121407 \t| Accuracy: 91.875\n",
      "# Iteration   858 -> Loss: 0.2977156907974862 \t| Accuracy: 91.875\n",
      "# Iteration   859 -> Loss: 0.29748559958495174 \t| Accuracy: 91.875\n",
      "# Iteration   860 -> Loss: 0.2972560382695888 \t| Accuracy: 91.875\n",
      "# Iteration   861 -> Loss: 0.2970270055101067 \t| Accuracy: 91.875\n",
      "# Iteration   862 -> Loss: 0.2967984999679504 \t| Accuracy: 91.875\n",
      "# Iteration   863 -> Loss: 0.2965705203073048 \t| Accuracy: 91.875\n",
      "# Iteration   864 -> Loss: 0.2963430651950985 \t| Accuracy: 91.875\n",
      "# Iteration   865 -> Loss: 0.2961161333010076 \t| Accuracy: 91.875\n",
      "# Iteration   866 -> Loss: 0.2958897232974595 \t| Accuracy: 91.875\n",
      "# Iteration   867 -> Loss: 0.29566383385963635 \t| Accuracy: 91.875\n",
      "# Iteration   868 -> Loss: 0.29543846366547827 \t| Accuracy: 91.875\n",
      "# Iteration   869 -> Loss: 0.2952136113956869 \t| Accuracy: 91.875\n",
      "# Iteration   870 -> Loss: 0.29498927573372846 \t| Accuracy: 91.875\n",
      "# Iteration   871 -> Loss: 0.29476545536583626 \t| Accuracy: 91.875\n",
      "# Iteration   872 -> Loss: 0.29454214898101394 \t| Accuracy: 91.875\n",
      "# Iteration   873 -> Loss: 0.29431935527103825 \t| Accuracy: 91.875\n",
      "# Iteration   874 -> Loss: 0.2940970729304613 \t| Accuracy: 91.875\n",
      "# Iteration   875 -> Loss: 0.293875300656613 \t| Accuracy: 91.875\n",
      "# Iteration   876 -> Loss: 0.29365403714960364 \t| Accuracy: 91.875\n",
      "# Iteration   877 -> Loss: 0.29343328111232564 \t| Accuracy: 91.875\n",
      "# Iteration   878 -> Loss: 0.29321303125045595 \t| Accuracy: 91.875\n",
      "# Iteration   879 -> Loss: 0.2929932862724577 \t| Accuracy: 91.875\n",
      "# Iteration   880 -> Loss: 0.2927740448895822 \t| Accuracy: 91.875\n",
      "# Iteration   881 -> Loss: 0.2925553058158703 \t| Accuracy: 91.875\n",
      "# Iteration   882 -> Loss: 0.2923370677681543 \t| Accuracy: 91.875\n",
      "# Iteration   883 -> Loss: 0.2921193294660591 \t| Accuracy: 91.875\n",
      "# Iteration   884 -> Loss: 0.2919020896320036 \t| Accuracy: 91.875\n",
      "# Iteration   885 -> Loss: 0.29168534699120163 \t| Accuracy: 91.875\n",
      "# Iteration   886 -> Loss: 0.29146910027166356 \t| Accuracy: 91.875\n",
      "# Iteration   887 -> Loss: 0.2912533482041966 \t| Accuracy: 91.875\n",
      "# Iteration   888 -> Loss: 0.2910380895224064 \t| Accuracy: 91.875\n",
      "# Iteration   889 -> Loss: 0.29082332296269664 \t| Accuracy: 91.875\n",
      "# Iteration   890 -> Loss: 0.2906090472642712 \t| Accuracy: 91.875\n",
      "# Iteration   891 -> Loss: 0.29039526116913306 \t| Accuracy: 92.083\n",
      "# Iteration   892 -> Loss: 0.29018196342208585 \t| Accuracy: 92.083\n",
      "# Iteration   893 -> Loss: 0.28996915277073376 \t| Accuracy: 92.083\n",
      "# Iteration   894 -> Loss: 0.2897568279654813 \t| Accuracy: 92.083\n",
      "# Iteration   895 -> Loss: 0.2895449877595343 \t| Accuracy: 92.083\n",
      "# Iteration   896 -> Loss: 0.28933363090889913 \t| Accuracy: 92.083\n",
      "# Iteration   897 -> Loss: 0.28912275617238276 \t| Accuracy: 92.083\n",
      "# Iteration   898 -> Loss: 0.2889123623115929 \t| Accuracy: 92.083\n",
      "# Iteration   899 -> Loss: 0.28870244809093704 \t| Accuracy: 92.083\n",
      "# Iteration   900 -> Loss: 0.28849301227762275 \t| Accuracy: 92.083\n",
      "# Iteration   901 -> Loss: 0.2882840536416567 \t| Accuracy: 92.083\n",
      "# Iteration   902 -> Loss: 0.2880755709558443 \t| Accuracy: 92.083\n",
      "# Iteration   903 -> Loss: 0.2878675629957885 \t| Accuracy: 92.083\n",
      "# Iteration   904 -> Loss: 0.2876600285398901 \t| Accuracy: 92.083\n",
      "# Iteration   905 -> Loss: 0.2874529663693456 \t| Accuracy: 92.083\n",
      "# Iteration   906 -> Loss: 0.287246375268147 \t| Accuracy: 92.083\n",
      "# Iteration   907 -> Loss: 0.287040254023081 \t| Accuracy: 92.292\n",
      "# Iteration   908 -> Loss: 0.28683460142372674 \t| Accuracy: 92.292\n",
      "# Iteration   909 -> Loss: 0.28662941626245597 \t| Accuracy: 92.292\n",
      "# Iteration   910 -> Loss: 0.2864246973344307 \t| Accuracy: 92.292\n",
      "# Iteration   911 -> Loss: 0.2862204434376023 \t| Accuracy: 92.292\n",
      "# Iteration   912 -> Loss: 0.28601665337271 \t| Accuracy: 92.292\n",
      "# Iteration   913 -> Loss: 0.28581332594327913 \t| Accuracy: 92.292\n",
      "# Iteration   914 -> Loss: 0.28561045995561996 \t| Accuracy: 92.292\n",
      "# Iteration   915 -> Loss: 0.2854080542188254 \t| Accuracy: 92.292\n",
      "# Iteration   916 -> Loss: 0.2852061075447697 \t| Accuracy: 92.292\n",
      "# Iteration   917 -> Loss: 0.28500461874810645 \t| Accuracy: 92.292\n",
      "# Iteration   918 -> Loss: 0.28480358664626637 \t| Accuracy: 92.292\n",
      "# Iteration   919 -> Loss: 0.2846030100594561 \t| Accuracy: 92.292\n",
      "# Iteration   920 -> Loss: 0.28440288781065526 \t| Accuracy: 92.292\n",
      "# Iteration   921 -> Loss: 0.28420321872561466 \t| Accuracy: 92.292\n",
      "# Iteration   922 -> Loss: 0.28400400163285433 \t| Accuracy: 92.292\n",
      "# Iteration   923 -> Loss: 0.28380523536366103 \t| Accuracy: 92.292\n",
      "# Iteration   924 -> Loss: 0.2836069187520857 \t| Accuracy: 92.292\n",
      "# Iteration   925 -> Loss: 0.28340905063494165 \t| Accuracy: 92.292\n",
      "# Iteration   926 -> Loss: 0.28321162985180154 \t| Accuracy: 92.292\n",
      "# Iteration   927 -> Loss: 0.28301465524499486 \t| Accuracy: 92.292\n",
      "# Iteration   928 -> Loss: 0.282818125659606 \t| Accuracy: 92.292\n",
      "# Iteration   929 -> Loss: 0.2826220399434708 \t| Accuracy: 92.292\n",
      "# Iteration   930 -> Loss: 0.2824263969471741 \t| Accuracy: 92.292\n",
      "# Iteration   931 -> Loss: 0.2822311955240474 \t| Accuracy: 92.292\n",
      "# Iteration   932 -> Loss: 0.2820364345301654 \t| Accuracy: 92.292\n",
      "# Iteration   933 -> Loss: 0.2818421128243436 \t| Accuracy: 92.292\n",
      "# Iteration   934 -> Loss: 0.281648229268135 \t| Accuracy: 92.292\n",
      "# Iteration   935 -> Loss: 0.28145478272582736 \t| Accuracy: 92.292\n",
      "# Iteration   936 -> Loss: 0.2812617720644399 \t| Accuracy: 92.292\n",
      "# Iteration   937 -> Loss: 0.28106919615372095 \t| Accuracy: 92.292\n",
      "# Iteration   938 -> Loss: 0.2808770538661435 \t| Accuracy: 92.292\n",
      "# Iteration   939 -> Loss: 0.28068534407690304 \t| Accuracy: 92.292\n",
      "# Iteration   940 -> Loss: 0.28049406566391427 \t| Accuracy: 92.292\n",
      "# Iteration   941 -> Loss: 0.2803032175078072 \t| Accuracy: 92.292\n",
      "# Iteration   942 -> Loss: 0.2801127984919243 \t| Accuracy: 92.292\n",
      "# Iteration   943 -> Loss: 0.27992280750231696 \t| Accuracy: 92.292\n",
      "# Iteration   944 -> Loss: 0.2797332434277422 \t| Accuracy: 92.292\n",
      "# Iteration   945 -> Loss: 0.27954410515965866 \t| Accuracy: 92.292\n",
      "# Iteration   946 -> Loss: 0.2793553915922241 \t| Accuracy: 92.292\n",
      "# Iteration   947 -> Loss: 0.27916710162229075 \t| Accuracy: 92.292\n",
      "# Iteration   948 -> Loss: 0.27897923414940234 \t| Accuracy: 92.292\n",
      "# Iteration   949 -> Loss: 0.27879178807579025 \t| Accuracy: 92.708\n",
      "# Iteration   950 -> Loss: 0.27860476230637005 \t| Accuracy: 92.708\n",
      "# Iteration   951 -> Loss: 0.27841815574873746 \t| Accuracy: 92.708\n",
      "# Iteration   952 -> Loss: 0.2782319673131647 \t| Accuracy: 92.708\n",
      "# Iteration   953 -> Loss: 0.2780461959125969 \t| Accuracy: 92.708\n",
      "# Iteration   954 -> Loss: 0.2778608404626479 \t| Accuracy: 92.708\n",
      "# Iteration   955 -> Loss: 0.277675899881597 \t| Accuracy: 92.708\n",
      "# Iteration   956 -> Loss: 0.2774913730903841 \t| Accuracy: 92.708\n",
      "# Iteration   957 -> Loss: 0.2773072590126066 \t| Accuracy: 92.708\n",
      "# Iteration   958 -> Loss: 0.2771235565745152 \t| Accuracy: 92.708\n",
      "# Iteration   959 -> Loss: 0.2769402647050099 \t| Accuracy: 92.708\n",
      "# Iteration   960 -> Loss: 0.2767573823356356 \t| Accuracy: 92.708\n",
      "# Iteration   961 -> Loss: 0.2765749084005787 \t| Accuracy: 92.708\n",
      "# Iteration   962 -> Loss: 0.27639284183666246 \t| Accuracy: 92.708\n",
      "# Iteration   963 -> Loss: 0.2762111815833433 \t| Accuracy: 92.708\n",
      "# Iteration   964 -> Loss: 0.27602992658270625 \t| Accuracy: 92.708\n",
      "# Iteration   965 -> Loss: 0.2758490757794611 \t| Accuracy: 92.708\n",
      "# Iteration   966 -> Loss: 0.27566862812093806 \t| Accuracy: 92.708\n",
      "# Iteration   967 -> Loss: 0.27548858255708364 \t| Accuracy: 92.708\n",
      "# Iteration   968 -> Loss: 0.2753089380404561 \t| Accuracy: 92.708\n",
      "# Iteration   969 -> Loss: 0.2751296935262217 \t| Accuracy: 92.708\n",
      "# Iteration   970 -> Loss: 0.27495084797214975 \t| Accuracy: 92.708\n",
      "# Iteration   971 -> Loss: 0.27477240033860895 \t| Accuracy: 92.708\n",
      "# Iteration   972 -> Loss: 0.2745943495885627 \t| Accuracy: 92.708\n",
      "# Iteration   973 -> Loss: 0.27441669468756463 \t| Accuracy: 92.708\n",
      "# Iteration   974 -> Loss: 0.2742394346037546 \t| Accuracy: 92.708\n",
      "# Iteration   975 -> Loss: 0.2740625683078537 \t| Accuracy: 92.708\n",
      "# Iteration   976 -> Loss: 0.2738860947731606 \t| Accuracy: 92.708\n",
      "# Iteration   977 -> Loss: 0.2737100129755465 \t| Accuracy: 92.708\n",
      "# Iteration   978 -> Loss: 0.2735343218934507 \t| Accuracy: 92.708\n",
      "# Iteration   979 -> Loss: 0.27335902050787697 \t| Accuracy: 92.708\n",
      "# Iteration   980 -> Loss: 0.27318410780238744 \t| Accuracy: 92.708\n",
      "# Iteration   981 -> Loss: 0.2730095827630999 \t| Accuracy: 92.708\n",
      "# Iteration   982 -> Loss: 0.2728354443786818 \t| Accuracy: 92.708\n",
      "# Iteration   983 -> Loss: 0.2726616916403466 \t| Accuracy: 92.708\n",
      "# Iteration   984 -> Loss: 0.2724883235418491 \t| Accuracy: 92.708\n",
      "# Iteration   985 -> Loss: 0.27231533907948025 \t| Accuracy: 92.708\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Iteration   986 -> Loss: 0.27214273725206356 \t| Accuracy: 92.708\n",
      "# Iteration   987 -> Loss: 0.2719705170609495 \t| Accuracy: 92.708\n",
      "# Iteration   988 -> Loss: 0.27179867751001163 \t| Accuracy: 92.708\n",
      "# Iteration   989 -> Loss: 0.27162721760564185 \t| Accuracy: 92.708\n",
      "# Iteration   990 -> Loss: 0.27145613635674537 \t| Accuracy: 92.708\n",
      "# Iteration   991 -> Loss: 0.2712854327747365 \t| Accuracy: 92.708\n",
      "# Iteration   992 -> Loss: 0.2711151058735337 \t| Accuracy: 92.708\n",
      "# Iteration   993 -> Loss: 0.27094515466955527 \t| Accuracy: 92.708\n",
      "# Iteration   994 -> Loss: 0.27077557818171427 \t| Accuracy: 92.708\n",
      "# Iteration   995 -> Loss: 0.27060637543141425 \t| Accuracy: 92.708\n",
      "# Iteration   996 -> Loss: 0.2704375454425443 \t| Accuracy: 92.708\n",
      "# Iteration   997 -> Loss: 0.2702690872414742 \t| Accuracy: 92.708\n",
      "# Iteration   998 -> Loss: 0.2701009998570502 \t| Accuracy: 92.708\n",
      "# Iteration   999 -> Loss: 0.26993328232059005 \t| Accuracy: 92.708\n",
      "# Iteration  1000 -> Loss: 0.26976593366587814 \t| Accuracy: 92.708\n",
      "# Iteration  1001 -> Loss: 0.26959895292916086 \t| Accuracy: 92.708\n",
      "# Iteration  1002 -> Loss: 0.2694323391491422 \t| Accuracy: 92.708\n",
      "# Iteration  1003 -> Loss: 0.2692660913669786 \t| Accuracy: 92.708\n",
      "# Iteration  1004 -> Loss: 0.2691002086262743 \t| Accuracy: 92.708\n",
      "# Iteration  1005 -> Loss: 0.26893468997307685 \t| Accuracy: 92.708\n",
      "# Iteration  1006 -> Loss: 0.26876953445587193 \t| Accuracy: 92.708\n",
      "# Iteration  1007 -> Loss: 0.2686047411255792 \t| Accuracy: 92.708\n",
      "# Iteration  1008 -> Loss: 0.2684403090355468 \t| Accuracy: 92.708\n",
      "# Iteration  1009 -> Loss: 0.2682762372415474 \t| Accuracy: 92.708\n",
      "# Iteration  1010 -> Loss: 0.2681125248017727 \t| Accuracy: 92.708\n",
      "# Iteration  1011 -> Loss: 0.26794917077682917 \t| Accuracy: 92.708\n",
      "# Iteration  1012 -> Loss: 0.26778617422973316 \t| Accuracy: 92.708\n",
      "# Iteration  1013 -> Loss: 0.26762353422590607 \t| Accuracy: 92.708\n",
      "# Iteration  1014 -> Loss: 0.2674612498331695 \t| Accuracy: 92.708\n",
      "# Iteration  1015 -> Loss: 0.2672993201217408 \t| Accuracy: 92.708\n",
      "# Iteration  1016 -> Loss: 0.2671377441642279 \t| Accuracy: 92.708\n",
      "# Iteration  1017 -> Loss: 0.26697652103562497 \t| Accuracy: 92.708\n",
      "# Iteration  1018 -> Loss: 0.26681564981330724 \t| Accuracy: 92.708\n",
      "# Iteration  1019 -> Loss: 0.26665512957702653 \t| Accuracy: 92.708\n",
      "# Iteration  1020 -> Loss: 0.26649495940890633 \t| Accuracy: 92.708\n",
      "# Iteration  1021 -> Loss: 0.26633513839343714 \t| Accuracy: 92.708\n",
      "# Iteration  1022 -> Loss: 0.26617566561747164 \t| Accuracy: 92.708\n",
      "# Iteration  1023 -> Loss: 0.26601654017021986 \t| Accuracy: 92.708\n",
      "# Iteration  1024 -> Loss: 0.26585776114324444 \t| Accuracy: 92.708\n",
      "# Iteration  1025 -> Loss: 0.26569932763045623 \t| Accuracy: 92.708\n",
      "# Iteration  1026 -> Loss: 0.2655412387281089 \t| Accuracy: 92.708\n",
      "# Iteration  1027 -> Loss: 0.2653834935347946 \t| Accuracy: 92.708\n",
      "# Iteration  1028 -> Loss: 0.26522609115143914 \t| Accuracy: 92.708\n",
      "# Iteration  1029 -> Loss: 0.2650690306812972 \t| Accuracy: 92.708\n",
      "# Iteration  1030 -> Loss: 0.26491231122994763 \t| Accuracy: 92.708\n",
      "# Iteration  1031 -> Loss: 0.2647559319052886 \t| Accuracy: 92.708\n",
      "# Iteration  1032 -> Loss: 0.26459989181753296 \t| Accuracy: 92.708\n",
      "# Iteration  1033 -> Loss: 0.26444419007920344 \t| Accuracy: 92.708\n",
      "# Iteration  1034 -> Loss: 0.26428882580512797 \t| Accuracy: 92.708\n",
      "# Iteration  1035 -> Loss: 0.264133798112435 \t| Accuracy: 92.708\n",
      "# Iteration  1036 -> Loss: 0.2639791061205484 \t| Accuracy: 92.708\n",
      "# Iteration  1037 -> Loss: 0.2638247489511834 \t| Accuracy: 92.708\n",
      "# Iteration  1038 -> Loss: 0.26367072572834127 \t| Accuracy: 92.708\n",
      "# Iteration  1039 -> Loss: 0.2635170355783049 \t| Accuracy: 92.708\n",
      "# Iteration  1040 -> Loss: 0.26336367762963403 \t| Accuracy: 92.708\n",
      "# Iteration  1041 -> Loss: 0.2632106510131605 \t| Accuracy: 92.708\n",
      "# Iteration  1042 -> Loss: 0.2630579548619836 \t| Accuracy: 92.708\n",
      "# Iteration  1043 -> Loss: 0.2629055883114654 \t| Accuracy: 92.708\n",
      "# Iteration  1044 -> Loss: 0.26275355049922594 \t| Accuracy: 92.708\n",
      "# Iteration  1045 -> Loss: 0.2626018405651388 \t| Accuracy: 92.708\n",
      "# Iteration  1046 -> Loss: 0.262450457651326 \t| Accuracy: 92.708\n",
      "# Iteration  1047 -> Loss: 0.26229940090215376 \t| Accuracy: 92.708\n",
      "# Iteration  1048 -> Loss: 0.26214866946422755 \t| Accuracy: 92.708\n",
      "# Iteration  1049 -> Loss: 0.2619982624863876 \t| Accuracy: 92.708\n",
      "# Iteration  1050 -> Loss: 0.26184817911970404 \t| Accuracy: 92.708\n",
      "# Iteration  1051 -> Loss: 0.26169841851747255 \t| Accuracy: 92.708\n",
      "# Iteration  1052 -> Loss: 0.26154897983520936 \t| Accuracy: 92.708\n",
      "# Iteration  1053 -> Loss: 0.2613998622306468 \t| Accuracy: 92.708\n",
      "# Iteration  1054 -> Loss: 0.2612510648637289 \t| Accuracy: 92.708\n",
      "# Iteration  1055 -> Loss: 0.26110258689660615 \t| Accuracy: 92.708\n",
      "# Iteration  1056 -> Loss: 0.26095442749363157 \t| Accuracy: 92.708\n",
      "# Iteration  1057 -> Loss: 0.2608065858213554 \t| Accuracy: 93.125\n",
      "# Iteration  1058 -> Loss: 0.2606590610485213 \t| Accuracy: 93.125\n",
      "# Iteration  1059 -> Loss: 0.2605118523460611 \t| Accuracy: 93.125\n",
      "# Iteration  1060 -> Loss: 0.2603649588870904 \t| Accuracy: 93.125\n",
      "# Iteration  1061 -> Loss: 0.2602183798469039 \t| Accuracy: 93.125\n",
      "# Iteration  1062 -> Loss: 0.2600721144029711 \t| Accuracy: 93.125\n",
      "# Iteration  1063 -> Loss: 0.2599261617349316 \t| Accuracy: 93.125\n",
      "# Iteration  1064 -> Loss: 0.25978052102459037 \t| Accuracy: 93.125\n",
      "# Iteration  1065 -> Loss: 0.2596351914559131 \t| Accuracy: 93.125\n",
      "# Iteration  1066 -> Loss: 0.25949017221502213 \t| Accuracy: 93.125\n",
      "# Iteration  1067 -> Loss: 0.2593454624901916 \t| Accuracy: 93.125\n",
      "# Iteration  1068 -> Loss: 0.25920106147184274 \t| Accuracy: 93.125\n",
      "# Iteration  1069 -> Loss: 0.25905696835253966 \t| Accuracy: 93.125\n",
      "# Iteration  1070 -> Loss: 0.2589131823269847 \t| Accuracy: 93.125\n",
      "# Iteration  1071 -> Loss: 0.25876970259201376 \t| Accuracy: 93.125\n",
      "# Iteration  1072 -> Loss: 0.258626528346592 \t| Accuracy: 93.125\n",
      "# Iteration  1073 -> Loss: 0.25848365879180935 \t| Accuracy: 93.125\n",
      "# Iteration  1074 -> Loss: 0.2583410931308757 \t| Accuracy: 93.125\n",
      "# Iteration  1075 -> Loss: 0.25819883056911697 \t| Accuracy: 93.125\n",
      "# Iteration  1076 -> Loss: 0.25805687031396995 \t| Accuracy: 93.125\n",
      "# Iteration  1077 -> Loss: 0.25791521157497826 \t| Accuracy: 93.125\n",
      "# Iteration  1078 -> Loss: 0.25777385356378785 \t| Accuracy: 93.125\n",
      "# Iteration  1079 -> Loss: 0.2576327954941425 \t| Accuracy: 93.125\n",
      "# Iteration  1080 -> Loss: 0.2574920365818791 \t| Accuracy: 93.125\n",
      "# Iteration  1081 -> Loss: 0.2573515760449236 \t| Accuracy: 93.125\n",
      "# Iteration  1082 -> Loss: 0.2572114131032865 \t| Accuracy: 93.125\n",
      "# Iteration  1083 -> Loss: 0.25707154697905804 \t| Accuracy: 93.125\n",
      "# Iteration  1084 -> Loss: 0.25693197689640435 \t| Accuracy: 93.125\n",
      "# Iteration  1085 -> Loss: 0.25679270208156246 \t| Accuracy: 93.125\n",
      "# Iteration  1086 -> Loss: 0.25665372176283635 \t| Accuracy: 93.125\n",
      "# Iteration  1087 -> Loss: 0.25651503517059226 \t| Accuracy: 93.125\n",
      "# Iteration  1088 -> Loss: 0.25637664153725437 \t| Accuracy: 93.125\n",
      "# Iteration  1089 -> Loss: 0.2562385400973005 \t| Accuracy: 93.125\n",
      "# Iteration  1090 -> Loss: 0.2561007300872575 \t| Accuracy: 93.125\n",
      "# Iteration  1091 -> Loss: 0.2559632107456971 \t| Accuracy: 93.125\n",
      "# Iteration  1092 -> Loss: 0.2558259813132315 \t| Accuracy: 93.125\n",
      "# Iteration  1093 -> Loss: 0.2556890410325089 \t| Accuracy: 93.125\n",
      "# Iteration  1094 -> Loss: 0.25555238914820916 \t| Accuracy: 93.125\n",
      "# Iteration  1095 -> Loss: 0.2554160249070395 \t| Accuracy: 93.125\n",
      "# Iteration  1096 -> Loss: 0.2552799475577303 \t| Accuracy: 93.125\n",
      "# Iteration  1097 -> Loss: 0.2551441563510304 \t| Accuracy: 93.125\n",
      "# Iteration  1098 -> Loss: 0.255008650539703 \t| Accuracy: 93.125\n",
      "# Iteration  1099 -> Loss: 0.2548734293785215 \t| Accuracy: 93.125\n",
      "# Iteration  1100 -> Loss: 0.25473849212426486 \t| Accuracy: 93.125\n",
      "# Iteration  1101 -> Loss: 0.2546038380357134 \t| Accuracy: 93.125\n",
      "# Iteration  1102 -> Loss: 0.25446946637364454 \t| Accuracy: 93.125\n",
      "# Iteration  1103 -> Loss: 0.2543353764008285 \t| Accuracy: 93.125\n",
      "# Iteration  1104 -> Loss: 0.2542015673820241 \t| Accuracy: 93.125\n",
      "# Iteration  1105 -> Loss: 0.2540680385839742 \t| Accuracy: 93.125\n",
      "# Iteration  1106 -> Loss: 0.25393478927540175 \t| Accuracy: 93.125\n",
      "# Iteration  1107 -> Loss: 0.25380181872700525 \t| Accuracy: 93.125\n",
      "# Iteration  1108 -> Loss: 0.2536691262114545 \t| Accuracy: 93.125\n",
      "# Iteration  1109 -> Loss: 0.2535367110033868 \t| Accuracy: 93.125\n",
      "# Iteration  1110 -> Loss: 0.25340457237940195 \t| Accuracy: 93.125\n",
      "# Iteration  1111 -> Loss: 0.2532727096180586 \t| Accuracy: 93.125\n",
      "# Iteration  1112 -> Loss: 0.2531411219998698 \t| Accuracy: 93.125\n",
      "# Iteration  1113 -> Loss: 0.2530098088072986 \t| Accuracy: 93.125\n",
      "# Iteration  1114 -> Loss: 0.2528787693247542 \t| Accuracy: 93.125\n",
      "# Iteration  1115 -> Loss: 0.25274800283858745 \t| Accuracy: 93.125\n",
      "# Iteration  1116 -> Loss: 0.2526175086370866 \t| Accuracy: 92.917\n",
      "# Iteration  1117 -> Loss: 0.2524872860104734 \t| Accuracy: 92.917\n",
      "# Iteration  1118 -> Loss: 0.2523573342508984 \t| Accuracy: 92.917\n",
      "# Iteration  1119 -> Loss: 0.25222765265243735 \t| Accuracy: 92.917\n",
      "# Iteration  1120 -> Loss: 0.25209824051108654 \t| Accuracy: 92.917\n",
      "# Iteration  1121 -> Loss: 0.25196909712475873 \t| Accuracy: 92.917\n",
      "# Iteration  1122 -> Loss: 0.2518402217932792 \t| Accuracy: 92.917\n",
      "# Iteration  1123 -> Loss: 0.25171161381838125 \t| Accuracy: 92.917\n",
      "# Iteration  1124 -> Loss: 0.2515832725037021 \t| Accuracy: 92.917\n",
      "# Iteration  1125 -> Loss: 0.251455197154779 \t| Accuracy: 92.917\n",
      "# Iteration  1126 -> Loss: 0.2513273870790448 \t| Accuracy: 92.917\n",
      "# Iteration  1127 -> Loss: 0.2511998415858237 \t| Accuracy: 92.917\n",
      "# Iteration  1128 -> Loss: 0.2510725599863275 \t| Accuracy: 92.917\n",
      "# Iteration  1129 -> Loss: 0.2509455415936511 \t| Accuracy: 92.917\n",
      "# Iteration  1130 -> Loss: 0.25081878572276867 \t| Accuracy: 92.917\n",
      "# Iteration  1131 -> Loss: 0.2506922916905291 \t| Accuracy: 92.917\n",
      "# Iteration  1132 -> Loss: 0.2505660588156522 \t| Accuracy: 92.917\n",
      "# Iteration  1133 -> Loss: 0.2504400864187246 \t| Accuracy: 92.917\n",
      "# Iteration  1134 -> Loss: 0.2503143738221954 \t| Accuracy: 92.917\n",
      "# Iteration  1135 -> Loss: 0.2501889203503723 \t| Accuracy: 92.917\n",
      "# Iteration  1136 -> Loss: 0.25006372532941734 \t| Accuracy: 92.917\n",
      "# Iteration  1137 -> Loss: 0.2499387880873428 \t| Accuracy: 92.917\n",
      "# Iteration  1138 -> Loss: 0.24981410795400727 \t| Accuracy: 92.917\n",
      "# Iteration  1139 -> Loss: 0.24968968426111127 \t| Accuracy: 92.917\n",
      "# Iteration  1140 -> Loss: 0.24956551634219354 \t| Accuracy: 92.917\n",
      "# Iteration  1141 -> Loss: 0.24944160353262662 \t| Accuracy: 92.917\n",
      "# Iteration  1142 -> Loss: 0.24931794516961292 \t| Accuracy: 92.917\n",
      "# Iteration  1143 -> Loss: 0.24919454059218074 \t| Accuracy: 92.917\n",
      "# Iteration  1144 -> Loss: 0.24907138914117996 \t| Accuracy: 92.917\n",
      "# Iteration  1145 -> Loss: 0.2489484901592783 \t| Accuracy: 92.917\n",
      "# Iteration  1146 -> Loss: 0.2488258429909568 \t| Accuracy: 92.917\n",
      "# Iteration  1147 -> Loss: 0.24870344698250643 \t| Accuracy: 92.917\n",
      "# Iteration  1148 -> Loss: 0.24858130148202337 \t| Accuracy: 92.917\n",
      "# Iteration  1149 -> Loss: 0.24845940583940535 \t| Accuracy: 92.917\n",
      "# Iteration  1150 -> Loss: 0.24833775940634753 \t| Accuracy: 92.917\n",
      "# Iteration  1151 -> Loss: 0.24821636153633855 \t| Accuracy: 92.917\n",
      "# Iteration  1152 -> Loss: 0.24809521158465628 \t| Accuracy: 92.917\n",
      "# Iteration  1153 -> Loss: 0.24797430890836394 \t| Accuracy: 92.917\n",
      "# Iteration  1154 -> Loss: 0.24785365286630606 \t| Accuracy: 92.917\n",
      "# Iteration  1155 -> Loss: 0.24773324281910453 \t| Accuracy: 92.917\n",
      "# Iteration  1156 -> Loss: 0.24761307812915442 \t| Accuracy: 92.917\n",
      "# Iteration  1157 -> Loss: 0.2474931581606202 \t| Accuracy: 92.917\n",
      "# Iteration  1158 -> Loss: 0.24737348227943154 \t| Accuracy: 92.917\n",
      "# Iteration  1159 -> Loss: 0.24725404985327928 \t| Accuracy: 92.917\n",
      "# Iteration  1160 -> Loss: 0.24713486025161166 \t| Accuracy: 92.917\n",
      "# Iteration  1161 -> Loss: 0.24701591284563013 \t| Accuracy: 93.125\n",
      "# Iteration  1162 -> Loss: 0.2468972070082854 \t| Accuracy: 93.125\n",
      "# Iteration  1163 -> Loss: 0.24677874211427364 \t| Accuracy: 93.125\n",
      "# Iteration  1164 -> Loss: 0.24666051754003213 \t| Accuracy: 93.125\n",
      "# Iteration  1165 -> Loss: 0.24654253266373558 \t| Accuracy: 93.125\n",
      "# Iteration  1166 -> Loss: 0.2464247868652921 \t| Accuracy: 93.125\n",
      "# Iteration  1167 -> Loss: 0.2463072795263391 \t| Accuracy: 93.125\n",
      "# Iteration  1168 -> Loss: 0.2461900100302396 \t| Accuracy: 93.125\n",
      "# Iteration  1169 -> Loss: 0.24607297776207787 \t| Accuracy: 93.125\n",
      "# Iteration  1170 -> Loss: 0.24595618210865589 \t| Accuracy: 93.125\n",
      "# Iteration  1171 -> Loss: 0.245839622458489 \t| Accuracy: 93.125\n",
      "# Iteration  1172 -> Loss: 0.24572329820180236 \t| Accuracy: 93.125\n",
      "# Iteration  1173 -> Loss: 0.24560720873052658 \t| Accuracy: 93.125\n",
      "# Iteration  1174 -> Loss: 0.24549135343829415 \t| Accuracy: 93.125\n",
      "# Iteration  1175 -> Loss: 0.24537573172043523 \t| Accuracy: 93.125\n",
      "# Iteration  1176 -> Loss: 0.24526034297397387 \t| Accuracy: 93.125\n",
      "# Iteration  1177 -> Loss: 0.24514518659762402 \t| Accuracy: 93.125\n",
      "# Iteration  1178 -> Loss: 0.24503026199178554 \t| Accuracy: 93.125\n",
      "# Iteration  1179 -> Loss: 0.24491556855854044 \t| Accuracy: 93.125\n",
      "# Iteration  1180 -> Loss: 0.24480110570164865 \t| Accuracy: 93.125\n",
      "# Iteration  1181 -> Loss: 0.24468687282654464 \t| Accuracy: 93.125\n",
      "# Iteration  1182 -> Loss: 0.24457286934033287 \t| Accuracy: 93.125\n",
      "# Iteration  1183 -> Loss: 0.24445909465178428 \t| Accuracy: 93.125\n",
      "# Iteration  1184 -> Loss: 0.24434554817133233 \t| Accuracy: 93.125\n",
      "# Iteration  1185 -> Loss: 0.24423222931106892 \t| Accuracy: 93.125\n",
      "# Iteration  1186 -> Loss: 0.24411913748474068 \t| Accuracy: 93.125\n",
      "# Iteration  1187 -> Loss: 0.24400627210774506 \t| Accuracy: 93.125\n",
      "# Iteration  1188 -> Loss: 0.24389363259712618 \t| Accuracy: 93.125\n",
      "# Iteration  1189 -> Loss: 0.24378121837157132 \t| Accuracy: 93.125\n",
      "# Iteration  1190 -> Loss: 0.24366902885140673 \t| Accuracy: 93.125\n",
      "# Iteration  1191 -> Loss: 0.2435570634585939 \t| Accuracy: 93.125\n",
      "# Iteration  1192 -> Loss: 0.2434453216167256 \t| Accuracy: 93.125\n",
      "# Iteration  1193 -> Loss: 0.243333802751022 \t| Accuracy: 93.125\n",
      "# Iteration  1194 -> Loss: 0.24322250628832698 \t| Accuracy: 93.125\n",
      "# Iteration  1195 -> Loss: 0.24311143165710378 \t| Accuracy: 93.125\n",
      "# Iteration  1196 -> Loss: 0.2430005782874318 \t| Accuracy: 93.125\n",
      "# Iteration  1197 -> Loss: 0.2428899456110022 \t| Accuracy: 93.125\n",
      "# Iteration  1198 -> Loss: 0.2427795330611142 \t| Accuracy: 93.125\n",
      "# Iteration  1199 -> Loss: 0.24266934007267127 \t| Accuracy: 93.125\n",
      "# Iteration  1200 -> Loss: 0.2425593660821773 \t| Accuracy: 93.125\n",
      "# Iteration  1201 -> Loss: 0.24244961052773253 \t| Accuracy: 93.125\n",
      "# Iteration  1202 -> Loss: 0.24234007284902997 \t| Accuracy: 93.125\n",
      "# Iteration  1203 -> Loss: 0.2422307524873515 \t| Accuracy: 93.125\n",
      "# Iteration  1204 -> Loss: 0.24212164888556373 \t| Accuracy: 93.125\n",
      "# Iteration  1205 -> Loss: 0.24201276148811454 \t| Accuracy: 93.125\n",
      "# Iteration  1206 -> Loss: 0.24190408974102914 \t| Accuracy: 93.125\n",
      "# Iteration  1207 -> Loss: 0.24179563309190608 \t| Accuracy: 93.125\n",
      "# Iteration  1208 -> Loss: 0.24168739098991346 \t| Accuracy: 93.125\n",
      "# Iteration  1209 -> Loss: 0.24157936288578533 \t| Accuracy: 93.125\n",
      "# Iteration  1210 -> Loss: 0.24147154823181757 \t| Accuracy: 93.125\n",
      "# Iteration  1211 -> Loss: 0.2413639464818642 \t| Accuracy: 93.125\n",
      "# Iteration  1212 -> Loss: 0.24125655709133356 \t| Accuracy: 93.125\n",
      "# Iteration  1213 -> Loss: 0.24114937951718454 \t| Accuracy: 93.125\n",
      "# Iteration  1214 -> Loss: 0.24104241321792264 \t| Accuracy: 93.125\n",
      "# Iteration  1215 -> Loss: 0.24093565765359615 \t| Accuracy: 93.125\n",
      "# Iteration  1216 -> Loss: 0.2408291122857926 \t| Accuracy: 93.125\n",
      "# Iteration  1217 -> Loss: 0.2407227765776346 \t| Accuracy: 93.125\n",
      "# Iteration  1218 -> Loss: 0.24061664999377635 \t| Accuracy: 93.125\n",
      "# Iteration  1219 -> Loss: 0.2405107320003997 \t| Accuracy: 93.125\n",
      "# Iteration  1220 -> Loss: 0.24040502206521014 \t| Accuracy: 93.125\n",
      "# Iteration  1221 -> Loss: 0.24029951965743349 \t| Accuracy: 93.125\n",
      "# Iteration  1222 -> Loss: 0.24019422424781176 \t| Accuracy: 93.125\n",
      "# Iteration  1223 -> Loss: 0.24008913530859932 \t| Accuracy: 93.125\n",
      "# Iteration  1224 -> Loss: 0.2399842523135595 \t| Accuracy: 93.125\n",
      "# Iteration  1225 -> Loss: 0.23987957473796037 \t| Accuracy: 93.125\n",
      "# Iteration  1226 -> Loss: 0.2397751020585711 \t| Accuracy: 93.125\n",
      "# Iteration  1227 -> Loss: 0.23967083375365844 \t| Accuracy: 93.125\n",
      "# Iteration  1228 -> Loss: 0.2395667693029827 \t| Accuracy: 93.125\n",
      "# Iteration  1229 -> Loss: 0.23946290818779392 \t| Accuracy: 93.125\n",
      "# Iteration  1230 -> Loss: 0.23935924989082827 \t| Accuracy: 93.125\n",
      "# Iteration  1231 -> Loss: 0.2392557938963043 \t| Accuracy: 93.125\n",
      "# Iteration  1232 -> Loss: 0.239152539689919 \t| Accuracy: 93.125\n",
      "# Iteration  1233 -> Loss: 0.2390494867588444 \t| Accuracy: 93.125\n",
      "# Iteration  1234 -> Loss: 0.2389466345917233 \t| Accuracy: 93.125\n",
      "# Iteration  1235 -> Loss: 0.23884398267866605 \t| Accuracy: 93.125\n",
      "# Iteration  1236 -> Loss: 0.23874153051124633 \t| Accuracy: 93.125\n",
      "# Iteration  1237 -> Loss: 0.23863927758249795 \t| Accuracy: 93.125\n",
      "# Iteration  1238 -> Loss: 0.23853722338691055 \t| Accuracy: 93.125\n",
      "# Iteration  1239 -> Loss: 0.2384353674204263 \t| Accuracy: 93.125\n",
      "# Iteration  1240 -> Loss: 0.23833370918043595 \t| Accuracy: 93.125\n",
      "# Iteration  1241 -> Loss: 0.23823224816577498 \t| Accuracy: 93.125\n",
      "# Iteration  1242 -> Loss: 0.23813098387672038 \t| Accuracy: 93.125\n",
      "# Iteration  1243 -> Loss: 0.23802991581498625 \t| Accuracy: 93.125\n",
      "# Iteration  1244 -> Loss: 0.23792904348372082 \t| Accuracy: 93.125\n",
      "# Iteration  1245 -> Loss: 0.2378283663875019 \t| Accuracy: 93.125\n",
      "# Iteration  1246 -> Loss: 0.237727884032334 \t| Accuracy: 93.125\n",
      "# Iteration  1247 -> Loss: 0.23762759592564406 \t| Accuracy: 93.125\n",
      "# Iteration  1248 -> Loss: 0.23752750157627797 \t| Accuracy: 93.125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Iteration  1249 -> Loss: 0.23742760049449682 \t| Accuracy: 93.125\n",
      "# Iteration  1250 -> Loss: 0.2373278921919733 \t| Accuracy: 93.125\n",
      "# Iteration  1251 -> Loss: 0.23722837618178771 \t| Accuracy: 93.125\n",
      "# Iteration  1252 -> Loss: 0.23712905197842477 \t| Accuracy: 93.125\n",
      "# Iteration  1253 -> Loss: 0.23702991909776952 \t| Accuracy: 93.125\n",
      "# Iteration  1254 -> Loss: 0.23693097705710375 \t| Accuracy: 93.125\n",
      "# Iteration  1255 -> Loss: 0.23683222537510235 \t| Accuracy: 93.125\n",
      "# Iteration  1256 -> Loss: 0.23673366357182965 \t| Accuracy: 93.125\n",
      "# Iteration  1257 -> Loss: 0.23663529116873583 \t| Accuracy: 93.125\n",
      "# Iteration  1258 -> Loss: 0.23653710768865294 \t| Accuracy: 93.125\n",
      "# Iteration  1259 -> Loss: 0.23643911265579168 \t| Accuracy: 93.125\n",
      "# Iteration  1260 -> Loss: 0.2363413055957374 \t| Accuracy: 93.125\n",
      "# Iteration  1261 -> Loss: 0.23624368603544663 \t| Accuracy: 93.125\n",
      "# Iteration  1262 -> Loss: 0.23614625350324325 \t| Accuracy: 93.125\n",
      "# Iteration  1263 -> Loss: 0.23604900752881516 \t| Accuracy: 93.125\n",
      "# Iteration  1264 -> Loss: 0.2359519476432102 \t| Accuracy: 93.125\n",
      "# Iteration  1265 -> Loss: 0.23585507337883288 \t| Accuracy: 93.125\n",
      "# Iteration  1266 -> Loss: 0.23575838426944065 \t| Accuracy: 93.125\n",
      "# Iteration  1267 -> Loss: 0.23566187985014017 \t| Accuracy: 93.125\n",
      "# Iteration  1268 -> Loss: 0.2355655596573836 \t| Accuracy: 93.125\n",
      "# Iteration  1269 -> Loss: 0.2354694232289655 \t| Accuracy: 93.125\n",
      "# Iteration  1270 -> Loss: 0.23537347010401843 \t| Accuracy: 93.125\n",
      "# Iteration  1271 -> Loss: 0.23527769982301003 \t| Accuracy: 93.125\n",
      "# Iteration  1272 -> Loss: 0.23518211192773886 \t| Accuracy: 93.125\n",
      "# Iteration  1273 -> Loss: 0.23508670596133133 \t| Accuracy: 93.125\n",
      "# Iteration  1274 -> Loss: 0.23499148146823767 \t| Accuracy: 93.125\n",
      "# Iteration  1275 -> Loss: 0.23489643799422846 \t| Accuracy: 93.125\n",
      "# Iteration  1276 -> Loss: 0.23480157508639113 \t| Accuracy: 93.125\n",
      "# Iteration  1277 -> Loss: 0.23470689229312627 \t| Accuracy: 93.125\n",
      "# Iteration  1278 -> Loss: 0.23461238916414406 \t| Accuracy: 93.125\n",
      "# Iteration  1279 -> Loss: 0.23451806525046076 \t| Accuracy: 93.125\n",
      "# Iteration  1280 -> Loss: 0.234423920104395 \t| Accuracy: 93.125\n",
      "# Iteration  1281 -> Loss: 0.2343299532795644 \t| Accuracy: 93.125\n",
      "# Iteration  1282 -> Loss: 0.23423616433088176 \t| Accuracy: 93.125\n",
      "# Iteration  1283 -> Loss: 0.23414255281455174 \t| Accuracy: 93.125\n",
      "# Iteration  1284 -> Loss: 0.23404911828806704 \t| Accuracy: 93.125\n",
      "# Iteration  1285 -> Loss: 0.2339558603102052 \t| Accuracy: 93.125\n",
      "# Iteration  1286 -> Loss: 0.2338627784410247 \t| Accuracy: 93.125\n",
      "# Iteration  1287 -> Loss: 0.23376987224186158 \t| Accuracy: 93.125\n",
      "# Iteration  1288 -> Loss: 0.23367714127532588 \t| Accuracy: 93.125\n",
      "# Iteration  1289 -> Loss: 0.2335845851052982 \t| Accuracy: 93.125\n",
      "# Iteration  1290 -> Loss: 0.23349220329692588 \t| Accuracy: 93.125\n",
      "# Iteration  1291 -> Loss: 0.23339999541661982 \t| Accuracy: 93.125\n",
      "# Iteration  1292 -> Loss: 0.23330796103205084 \t| Accuracy: 93.125\n",
      "# Iteration  1293 -> Loss: 0.2332160997121459 \t| Accuracy: 93.125\n",
      "# Iteration  1294 -> Loss: 0.2331244110270851 \t| Accuracy: 93.125\n",
      "# Iteration  1295 -> Loss: 0.23303289454829765 \t| Accuracy: 93.125\n",
      "# Iteration  1296 -> Loss: 0.23294154984845875 \t| Accuracy: 93.125\n",
      "# Iteration  1297 -> Loss: 0.2328503765014859 \t| Accuracy: 93.542\n",
      "# Iteration  1298 -> Loss: 0.23275937408253541 \t| Accuracy: 93.542\n",
      "# Iteration  1299 -> Loss: 0.23266854216799904 \t| Accuracy: 93.542\n",
      "# Iteration  1300 -> Loss: 0.23257788033550042 \t| Accuracy: 93.542\n",
      "# Iteration  1301 -> Loss: 0.23248738816389156 \t| Accuracy: 93.542\n",
      "# Iteration  1302 -> Loss: 0.23239706523324927 \t| Accuracy: 93.542\n",
      "# Iteration  1303 -> Loss: 0.23230691112487206 \t| Accuracy: 93.542\n",
      "# Iteration  1304 -> Loss: 0.23221692542127634 \t| Accuracy: 93.542\n",
      "# Iteration  1305 -> Loss: 0.23212710770619302 \t| Accuracy: 93.542\n",
      "# Iteration  1306 -> Loss: 0.2320374575645641 \t| Accuracy: 93.542\n",
      "# Iteration  1307 -> Loss: 0.23194797458253927 \t| Accuracy: 93.542\n",
      "# Iteration  1308 -> Loss: 0.23185865834747243 \t| Accuracy: 93.542\n",
      "# Iteration  1309 -> Loss: 0.23176950844791822 \t| Accuracy: 93.542\n",
      "# Iteration  1310 -> Loss: 0.2316805244736285 \t| Accuracy: 93.542\n",
      "# Iteration  1311 -> Loss: 0.23159170601554932 \t| Accuracy: 93.542\n",
      "# Iteration  1312 -> Loss: 0.2315030526658168 \t| Accuracy: 93.542\n",
      "# Iteration  1313 -> Loss: 0.23141456401775445 \t| Accuracy: 93.542\n",
      "# Iteration  1314 -> Loss: 0.23132623966586932 \t| Accuracy: 93.542\n",
      "# Iteration  1315 -> Loss: 0.2312380792058488 \t| Accuracy: 93.542\n",
      "# Iteration  1316 -> Loss: 0.23115008223455688 \t| Accuracy: 93.542\n",
      "# Iteration  1317 -> Loss: 0.23106224835003128 \t| Accuracy: 93.542\n",
      "# Iteration  1318 -> Loss: 0.2309745771514796 \t| Accuracy: 93.542\n",
      "# Iteration  1319 -> Loss: 0.2308870682392762 \t| Accuracy: 93.542\n",
      "# Iteration  1320 -> Loss: 0.2307997212149588 \t| Accuracy: 93.542\n",
      "# Iteration  1321 -> Loss: 0.23071253568122485 \t| Accuracy: 93.542\n",
      "# Iteration  1322 -> Loss: 0.23062551124192862 \t| Accuracy: 93.542\n",
      "# Iteration  1323 -> Loss: 0.23053864750207734 \t| Accuracy: 93.542\n",
      "# Iteration  1324 -> Loss: 0.2304519440678282 \t| Accuracy: 93.542\n",
      "# Iteration  1325 -> Loss: 0.23036540054648488 \t| Accuracy: 93.542\n",
      "# Iteration  1326 -> Loss: 0.23027901654649424 \t| Accuracy: 93.542\n",
      "# Iteration  1327 -> Loss: 0.2301927916774428 \t| Accuracy: 93.542\n",
      "# Iteration  1328 -> Loss: 0.23010672555005374 \t| Accuracy: 93.542\n",
      "# Iteration  1329 -> Loss: 0.2300208177761833 \t| Accuracy: 93.542\n",
      "# Iteration  1330 -> Loss: 0.22993506796881752 \t| Accuracy: 93.542\n",
      "# Iteration  1331 -> Loss: 0.22984947574206901 \t| Accuracy: 93.542\n",
      "# Iteration  1332 -> Loss: 0.22976404071117354 \t| Accuracy: 93.542\n",
      "# Iteration  1333 -> Loss: 0.22967876249248675 \t| Accuracy: 93.542\n",
      "# Iteration  1334 -> Loss: 0.22959364070348096 \t| Accuracy: 93.542\n",
      "# Iteration  1335 -> Loss: 0.22950867496274174 \t| Accuracy: 93.542\n",
      "# Iteration  1336 -> Loss: 0.2294238648899647 \t| Accuracy: 93.542\n",
      "# Iteration  1337 -> Loss: 0.22933921010595212 \t| Accuracy: 93.542\n",
      "# Iteration  1338 -> Loss: 0.22925471023260985 \t| Accuracy: 93.542\n",
      "# Iteration  1339 -> Loss: 0.22917036489294382 \t| Accuracy: 93.542\n",
      "# Iteration  1340 -> Loss: 0.229086173711057 \t| Accuracy: 93.542\n",
      "# Iteration  1341 -> Loss: 0.229002136312146 \t| Accuracy: 93.542\n",
      "# Iteration  1342 -> Loss: 0.22891825232249782 \t| Accuracy: 93.542\n",
      "# Iteration  1343 -> Loss: 0.22883452136948665 \t| Accuracy: 93.542\n",
      "# Iteration  1344 -> Loss: 0.2287509430815706 \t| Accuracy: 93.542\n",
      "# Iteration  1345 -> Loss: 0.2286675170882886 \t| Accuracy: 93.542\n",
      "# Iteration  1346 -> Loss: 0.22858424302025698 \t| Accuracy: 93.542\n",
      "# Iteration  1347 -> Loss: 0.2285011205091663 \t| Accuracy: 93.542\n",
      "# Iteration  1348 -> Loss: 0.2284181491877782 \t| Accuracy: 93.542\n",
      "# Iteration  1349 -> Loss: 0.2283353286899222 \t| Accuracy: 93.542\n",
      "# Iteration  1350 -> Loss: 0.22825265865049243 \t| Accuracy: 93.750\n",
      "# Iteration  1351 -> Loss: 0.22817013870544453 \t| Accuracy: 93.750\n",
      "# Iteration  1352 -> Loss: 0.22808776849179233 \t| Accuracy: 93.750\n",
      "# Iteration  1353 -> Loss: 0.22800554764760472 \t| Accuracy: 93.750\n",
      "# Iteration  1354 -> Loss: 0.22792347581200254 \t| Accuracy: 93.750\n",
      "# Iteration  1355 -> Loss: 0.22784155262515532 \t| Accuracy: 93.750\n",
      "# Iteration  1356 -> Loss: 0.22775977772827827 \t| Accuracy: 93.750\n",
      "# Iteration  1357 -> Loss: 0.22767815076362893 \t| Accuracy: 93.750\n",
      "# Iteration  1358 -> Loss: 0.2275966713745041 \t| Accuracy: 93.750\n",
      "# Iteration  1359 -> Loss: 0.22751533920523664 \t| Accuracy: 93.750\n",
      "# Iteration  1360 -> Loss: 0.2274341539011925 \t| Accuracy: 93.750\n",
      "# Iteration  1361 -> Loss: 0.22735311510876752 \t| Accuracy: 93.750\n",
      "# Iteration  1362 -> Loss: 0.22727222247538414 \t| Accuracy: 93.750\n",
      "# Iteration  1363 -> Loss: 0.22719147564948852 \t| Accuracy: 93.750\n",
      "# Iteration  1364 -> Loss: 0.22711087428054716 \t| Accuracy: 93.542\n",
      "# Iteration  1365 -> Loss: 0.2270304180190441 \t| Accuracy: 93.542\n",
      "# Iteration  1366 -> Loss: 0.22695010651647773 \t| Accuracy: 93.542\n",
      "# Iteration  1367 -> Loss: 0.2268699394253575 \t| Accuracy: 93.542\n",
      "# Iteration  1368 -> Loss: 0.22678991639920104 \t| Accuracy: 93.542\n",
      "# Iteration  1369 -> Loss: 0.226710037092531 \t| Accuracy: 93.542\n",
      "# Iteration  1370 -> Loss: 0.22663030116087207 \t| Accuracy: 93.542\n",
      "# Iteration  1371 -> Loss: 0.22655070826074783 \t| Accuracy: 93.542\n",
      "# Iteration  1372 -> Loss: 0.22647125804967766 \t| Accuracy: 93.542\n",
      "# Iteration  1373 -> Loss: 0.2263919501861738 \t| Accuracy: 93.542\n",
      "# Iteration  1374 -> Loss: 0.22631278432973817 \t| Accuracy: 93.542\n",
      "# Iteration  1375 -> Loss: 0.2262337601408595 \t| Accuracy: 93.542\n",
      "# Iteration  1376 -> Loss: 0.22615487728101014 \t| Accuracy: 93.542\n",
      "# Iteration  1377 -> Loss: 0.22607613541264313 \t| Accuracy: 93.542\n",
      "# Iteration  1378 -> Loss: 0.22599753419918914 \t| Accuracy: 93.542\n",
      "# Iteration  1379 -> Loss: 0.22591907330505354 \t| Accuracy: 93.542\n",
      "# Iteration  1380 -> Loss: 0.22584075239561324 \t| Accuracy: 93.542\n",
      "# Iteration  1381 -> Loss: 0.22576257113721385 \t| Accuracy: 93.542\n",
      "# Iteration  1382 -> Loss: 0.22568452919716653 \t| Accuracy: 93.542\n",
      "# Iteration  1383 -> Loss: 0.22560662624374528 \t| Accuracy: 93.542\n",
      "# Iteration  1384 -> Loss: 0.2255288619461836 \t| Accuracy: 93.542\n",
      "# Iteration  1385 -> Loss: 0.22545123597467184 \t| Accuracy: 93.542\n",
      "# Iteration  1386 -> Loss: 0.225373748000354 \t| Accuracy: 93.958\n",
      "# Iteration  1387 -> Loss: 0.22529639769532495 \t| Accuracy: 93.958\n",
      "# Iteration  1388 -> Loss: 0.22521918473262742 \t| Accuracy: 93.958\n",
      "# Iteration  1389 -> Loss: 0.225142108786249 \t| Accuracy: 93.958\n",
      "# Iteration  1390 -> Loss: 0.2250651695311192 \t| Accuracy: 93.958\n",
      "# Iteration  1391 -> Loss: 0.22498836664310679 \t| Accuracy: 93.958\n",
      "# Iteration  1392 -> Loss: 0.2249116997990164 \t| Accuracy: 93.958\n",
      "# Iteration  1393 -> Loss: 0.22483516867658612 \t| Accuracy: 93.958\n",
      "# Iteration  1394 -> Loss: 0.22475877295448413 \t| Accuracy: 93.958\n",
      "# Iteration  1395 -> Loss: 0.22468251231230613 \t| Accuracy: 93.958\n",
      "# Iteration  1396 -> Loss: 0.22460638643057237 \t| Accuracy: 93.958\n",
      "# Iteration  1397 -> Loss: 0.2245303949907246 \t| Accuracy: 93.958\n",
      "# Iteration  1398 -> Loss: 0.22445453767512344 \t| Accuracy: 93.958\n",
      "# Iteration  1399 -> Loss: 0.2243788141670453 \t| Accuracy: 93.958\n",
      "# Iteration  1400 -> Loss: 0.22430322415067971 \t| Accuracy: 93.958\n",
      "# Iteration  1401 -> Loss: 0.22422776731112626 \t| Accuracy: 93.958\n",
      "# Iteration  1402 -> Loss: 0.22415244333439188 \t| Accuracy: 93.958\n",
      "# Iteration  1403 -> Loss: 0.22407725190738792 \t| Accuracy: 93.958\n",
      "# Iteration  1404 -> Loss: 0.2240021927179275 \t| Accuracy: 93.958\n",
      "# Iteration  1405 -> Loss: 0.22392726545472239 \t| Accuracy: 93.958\n",
      "# Iteration  1406 -> Loss: 0.22385246980738036 \t| Accuracy: 93.958\n",
      "# Iteration  1407 -> Loss: 0.22377780546640239 \t| Accuracy: 93.958\n",
      "# Iteration  1408 -> Loss: 0.22370327212317984 \t| Accuracy: 93.750\n",
      "# Iteration  1409 -> Loss: 0.22362886946999147 \t| Accuracy: 93.750\n",
      "# Iteration  1410 -> Loss: 0.22355459720000107 \t| Accuracy: 93.750\n",
      "# Iteration  1411 -> Loss: 0.2234804550072541 \t| Accuracy: 93.750\n",
      "# Iteration  1412 -> Loss: 0.2234064425866755 \t| Accuracy: 93.750\n",
      "# Iteration  1413 -> Loss: 0.22333255963406634 \t| Accuracy: 93.750\n",
      "# Iteration  1414 -> Loss: 0.2232588058461016 \t| Accuracy: 93.750\n",
      "# Iteration  1415 -> Loss: 0.22318518092032716 \t| Accuracy: 93.750\n",
      "# Iteration  1416 -> Loss: 0.22311168455515681 \t| Accuracy: 93.750\n",
      "# Iteration  1417 -> Loss: 0.22303831644986996 \t| Accuracy: 93.750\n",
      "# Iteration  1418 -> Loss: 0.2229650763046087 \t| Accuracy: 93.750\n",
      "# Iteration  1419 -> Loss: 0.22289196382037493 \t| Accuracy: 93.750\n",
      "# Iteration  1420 -> Loss: 0.22281897869902798 \t| Accuracy: 93.750\n",
      "# Iteration  1421 -> Loss: 0.22274612064328142 \t| Accuracy: 93.750\n",
      "# Iteration  1422 -> Loss: 0.2226733893567008 \t| Accuracy: 93.750\n",
      "# Iteration  1423 -> Loss: 0.22260078454370083 \t| Accuracy: 93.750\n",
      "# Iteration  1424 -> Loss: 0.22252830590954242 \t| Accuracy: 93.750\n",
      "# Iteration  1425 -> Loss: 0.22245595316033046 \t| Accuracy: 93.750\n",
      "# Iteration  1426 -> Loss: 0.22238372600301068 \t| Accuracy: 93.750\n",
      "# Iteration  1427 -> Loss: 0.2223116241453673 \t| Accuracy: 93.750\n",
      "# Iteration  1428 -> Loss: 0.22223964729602017 \t| Accuracy: 93.750\n",
      "# Iteration  1429 -> Loss: 0.2221677951644223 \t| Accuracy: 93.750\n",
      "# Iteration  1430 -> Loss: 0.22209606746085708 \t| Accuracy: 93.750\n",
      "# Iteration  1431 -> Loss: 0.22202446389643563 \t| Accuracy: 93.750\n",
      "# Iteration  1432 -> Loss: 0.22195298418309417 \t| Accuracy: 93.750\n",
      "# Iteration  1433 -> Loss: 0.2218816280335916 \t| Accuracy: 93.750\n",
      "# Iteration  1434 -> Loss: 0.22181039516150644 \t| Accuracy: 93.750\n",
      "# Iteration  1435 -> Loss: 0.22173928528123474 \t| Accuracy: 93.750\n",
      "# Iteration  1436 -> Loss: 0.22166829810798705 \t| Accuracy: 93.750\n",
      "# Iteration  1437 -> Loss: 0.2215974333577861 \t| Accuracy: 93.750\n",
      "# Iteration  1438 -> Loss: 0.22152669074746392 \t| Accuracy: 93.750\n",
      "# Iteration  1439 -> Loss: 0.22145606999465967 \t| Accuracy: 93.750\n",
      "# Iteration  1440 -> Loss: 0.22138557081781668 \t| Accuracy: 93.750\n",
      "# Iteration  1441 -> Loss: 0.22131519293618004 \t| Accuracy: 93.750\n",
      "# Iteration  1442 -> Loss: 0.2212449360697941 \t| Accuracy: 93.750\n",
      "# Iteration  1443 -> Loss: 0.2211747999394997 \t| Accuracy: 93.750\n",
      "# Iteration  1444 -> Loss: 0.22110478426693198 \t| Accuracy: 93.750\n",
      "# Iteration  1445 -> Loss: 0.22103488877451744 \t| Accuracy: 93.750\n",
      "# Iteration  1446 -> Loss: 0.22096511318547163 \t| Accuracy: 93.750\n",
      "# Iteration  1447 -> Loss: 0.22089545722379658 \t| Accuracy: 93.750\n",
      "# Iteration  1448 -> Loss: 0.22082592061427836 \t| Accuracy: 93.750\n",
      "# Iteration  1449 -> Loss: 0.22075650308248446 \t| Accuracy: 93.750\n",
      "# Iteration  1450 -> Loss: 0.22068720435476127 \t| Accuracy: 93.750\n",
      "# Iteration  1451 -> Loss: 0.22061802415823173 \t| Accuracy: 93.750\n",
      "# Iteration  1452 -> Loss: 0.2205489622207926 \t| Accuracy: 93.750\n",
      "# Iteration  1453 -> Loss: 0.22048001827111227 \t| Accuracy: 93.750\n",
      "# Iteration  1454 -> Loss: 0.22041119203862797 \t| Accuracy: 93.750\n",
      "# Iteration  1455 -> Loss: 0.22034248325354375 \t| Accuracy: 93.750\n",
      "# Iteration  1456 -> Loss: 0.22027389164682742 \t| Accuracy: 93.750\n",
      "# Iteration  1457 -> Loss: 0.22020541695020862 \t| Accuracy: 93.750\n",
      "# Iteration  1458 -> Loss: 0.22013705889617613 \t| Accuracy: 93.750\n",
      "# Iteration  1459 -> Loss: 0.22006881721797533 \t| Accuracy: 93.750\n",
      "# Iteration  1460 -> Loss: 0.22000069164960612 \t| Accuracy: 93.750\n",
      "# Iteration  1461 -> Loss: 0.21993268192582013 \t| Accuracy: 93.750\n",
      "# Iteration  1462 -> Loss: 0.21986478778211851 \t| Accuracy: 93.750\n",
      "# Iteration  1463 -> Loss: 0.21979700895474952 \t| Accuracy: 93.750\n",
      "# Iteration  1464 -> Loss: 0.21972934518070594 \t| Accuracy: 93.750\n",
      "# Iteration  1465 -> Loss: 0.21966179619772289 \t| Accuracy: 93.750\n",
      "# Iteration  1466 -> Loss: 0.21959436174427538 \t| Accuracy: 93.750\n",
      "# Iteration  1467 -> Loss: 0.2195270415595758 \t| Accuracy: 93.750\n",
      "# Iteration  1468 -> Loss: 0.21945983538357178 \t| Accuracy: 93.750\n",
      "# Iteration  1469 -> Loss: 0.2193927429569436 \t| Accuracy: 93.750\n",
      "# Iteration  1470 -> Loss: 0.219325764021102 \t| Accuracy: 93.750\n",
      "# Iteration  1471 -> Loss: 0.2192588983181856 \t| Accuracy: 93.750\n",
      "# Iteration  1472 -> Loss: 0.21919214559105887 \t| Accuracy: 93.750\n",
      "# Iteration  1473 -> Loss: 0.21912550558330957 \t| Accuracy: 93.750\n",
      "# Iteration  1474 -> Loss: 0.21905897803924645 \t| Accuracy: 93.750\n",
      "# Iteration  1475 -> Loss: 0.21899256270389694 \t| Accuracy: 93.750\n",
      "# Iteration  1476 -> Loss: 0.21892625932300488 \t| Accuracy: 93.750\n",
      "# Iteration  1477 -> Loss: 0.2188600676430281 \t| Accuracy: 93.750\n",
      "# Iteration  1478 -> Loss: 0.21879398741113634 \t| Accuracy: 93.750\n",
      "# Iteration  1479 -> Loss: 0.21872801837520855 \t| Accuracy: 93.750\n",
      "# Iteration  1480 -> Loss: 0.21866216028383112 \t| Accuracy: 93.750\n",
      "# Iteration  1481 -> Loss: 0.21859641288629514 \t| Accuracy: 93.750\n",
      "# Iteration  1482 -> Loss: 0.21853077593259437 \t| Accuracy: 93.750\n",
      "# Iteration  1483 -> Loss: 0.218465249173423 \t| Accuracy: 93.750\n",
      "# Iteration  1484 -> Loss: 0.21839983236017319 \t| Accuracy: 93.750\n",
      "# Iteration  1485 -> Loss: 0.21833452524493296 \t| Accuracy: 93.750\n",
      "# Iteration  1486 -> Loss: 0.21826932758048395 \t| Accuracy: 93.750\n",
      "# Iteration  1487 -> Loss: 0.2182042391202991 \t| Accuracy: 93.750\n",
      "# Iteration  1488 -> Loss: 0.21813925961854044 \t| Accuracy: 93.750\n",
      "# Iteration  1489 -> Loss: 0.21807438883005703 \t| Accuracy: 93.750\n",
      "# Iteration  1490 -> Loss: 0.21800962651038244 \t| Accuracy: 93.750\n",
      "# Iteration  1491 -> Loss: 0.2179449724157327 \t| Accuracy: 93.750\n",
      "# Iteration  1492 -> Loss: 0.21788042630300422 \t| Accuracy: 93.750\n",
      "# Iteration  1493 -> Loss: 0.21781598792977133 \t| Accuracy: 93.750\n",
      "# Iteration  1494 -> Loss: 0.21775165705428431 \t| Accuracy: 93.750\n",
      "# Iteration  1495 -> Loss: 0.21768743343546704 \t| Accuracy: 93.750\n",
      "# Iteration  1496 -> Loss: 0.21762331683291494 \t| Accuracy: 93.750\n",
      "# Iteration  1497 -> Loss: 0.21755930700689277 \t| Accuracy: 93.750\n",
      "# Iteration  1498 -> Loss: 0.21749540371833243 \t| Accuracy: 93.750\n",
      "# Iteration  1499 -> Loss: 0.2174316067288308 \t| Accuracy: 93.750\n",
      "# Iteration  1500 -> Loss: 0.2173679158006477 \t| Accuracy: 93.750\n",
      "# Iteration  1501 -> Loss: 0.21730433069670366 \t| Accuracy: 93.750\n",
      "# Iteration  1502 -> Loss: 0.21724085118057765 \t| Accuracy: 93.750\n",
      "# Iteration  1503 -> Loss: 0.21717747701650528 \t| Accuracy: 93.750\n",
      "# Iteration  1504 -> Loss: 0.21711420796937636 \t| Accuracy: 93.750\n",
      "# Iteration  1505 -> Loss: 0.21705104380473297 \t| Accuracy: 93.750\n",
      "# Iteration  1506 -> Loss: 0.21698798428876737 \t| Accuracy: 93.750\n",
      "# Iteration  1507 -> Loss: 0.2169250291883196 \t| Accuracy: 93.750\n",
      "# Iteration  1508 -> Loss: 0.2168621782708758 \t| Accuracy: 93.750\n",
      "# Iteration  1509 -> Loss: 0.2167994313045659 \t| Accuracy: 93.750\n",
      "# Iteration  1510 -> Loss: 0.2167367880581615 \t| Accuracy: 93.750\n",
      "# Iteration  1511 -> Loss: 0.21667424830107393 \t| Accuracy: 93.750\n",
      "# Iteration  1512 -> Loss: 0.21661181180335215 \t| Accuracy: 93.750\n",
      "# Iteration  1513 -> Loss: 0.21654947833568056 \t| Accuracy: 93.750\n",
      "# Iteration  1514 -> Loss: 0.21648724766937705 \t| Accuracy: 93.750\n",
      "# Iteration  1515 -> Loss: 0.21642511957639102 \t| Accuracy: 93.750\n",
      "# Iteration  1516 -> Loss: 0.21636309382930116 \t| Accuracy: 93.750\n",
      "# Iteration  1517 -> Loss: 0.2163011702013137 \t| Accuracy: 93.750\n",
      "# Iteration  1518 -> Loss: 0.21623934846625986 \t| Accuracy: 93.750\n",
      "# Iteration  1519 -> Loss: 0.2161776283985946 \t| Accuracy: 93.750\n",
      "# Iteration  1520 -> Loss: 0.21611600977339387 \t| Accuracy: 93.750\n",
      "# Iteration  1521 -> Loss: 0.216054492366353 \t| Accuracy: 93.750\n",
      "# Iteration  1522 -> Loss: 0.21599307595378459 \t| Accuracy: 93.750\n",
      "# Iteration  1523 -> Loss: 0.21593176031261652 \t| Accuracy: 93.750\n",
      "# Iteration  1524 -> Loss: 0.21587054522039006 \t| Accuracy: 93.750\n",
      "# Iteration  1525 -> Loss: 0.2158094304552577 \t| Accuracy: 93.750\n",
      "# Iteration  1526 -> Loss: 0.2157484157959813 \t| Accuracy: 93.750\n",
      "# Iteration  1527 -> Loss: 0.2156875010219301 \t| Accuracy: 93.750\n",
      "# Iteration  1528 -> Loss: 0.2156266859130788 \t| Accuracy: 93.750\n",
      "# Iteration  1529 -> Loss: 0.2155659702500055 \t| Accuracy: 93.750\n",
      "# Iteration  1530 -> Loss: 0.21550535381388994 \t| Accuracy: 93.750\n",
      "# Iteration  1531 -> Loss: 0.21544483638651116 \t| Accuracy: 93.750\n",
      "# Iteration  1532 -> Loss: 0.21538441775024617 \t| Accuracy: 93.750\n",
      "# Iteration  1533 -> Loss: 0.21532409768806737 \t| Accuracy: 93.750\n",
      "# Iteration  1534 -> Loss: 0.21526387598354124 \t| Accuracy: 93.750\n",
      "# Iteration  1535 -> Loss: 0.21520375242082587 \t| Accuracy: 93.750\n",
      "# Iteration  1536 -> Loss: 0.21514372678466945 \t| Accuracy: 93.750\n",
      "# Iteration  1537 -> Loss: 0.21508379886040827 \t| Accuracy: 93.750\n",
      "# Iteration  1538 -> Loss: 0.2150239684339646 \t| Accuracy: 93.750\n",
      "# Iteration  1539 -> Loss: 0.21496423529184508 \t| Accuracy: 93.750\n",
      "# Iteration  1540 -> Loss: 0.21490459922113878 \t| Accuracy: 93.750\n",
      "# Iteration  1541 -> Loss: 0.21484506000951514 \t| Accuracy: 93.750\n",
      "# Iteration  1542 -> Loss: 0.21478561744522237 \t| Accuracy: 93.750\n",
      "# Iteration  1543 -> Loss: 0.21472627131708533 \t| Accuracy: 93.750\n",
      "# Iteration  1544 -> Loss: 0.21466702141450386 \t| Accuracy: 93.750\n",
      "# Iteration  1545 -> Loss: 0.2146078675274508 \t| Accuracy: 93.750\n",
      "# Iteration  1546 -> Loss: 0.21454880944647028 \t| Accuracy: 93.750\n",
      "# Iteration  1547 -> Loss: 0.21448984696267567 \t| Accuracy: 93.750\n",
      "# Iteration  1548 -> Loss: 0.21443097986774803 \t| Accuracy: 93.750\n",
      "# Iteration  1549 -> Loss: 0.214372207953934 \t| Accuracy: 93.750\n",
      "# Iteration  1550 -> Loss: 0.2143135310140441 \t| Accuracy: 93.750\n",
      "# Iteration  1551 -> Loss: 0.21425494884145105 \t| Accuracy: 93.750\n",
      "# Iteration  1552 -> Loss: 0.21419646123008762 \t| Accuracy: 93.750\n",
      "# Iteration  1553 -> Loss: 0.21413806797444526 \t| Accuracy: 93.750\n",
      "# Iteration  1554 -> Loss: 0.21407976886957195 \t| Accuracy: 93.750\n",
      "# Iteration  1555 -> Loss: 0.2140215637110705 \t| Accuracy: 93.750\n",
      "# Iteration  1556 -> Loss: 0.21396345229509695 \t| Accuracy: 93.750\n",
      "# Iteration  1557 -> Loss: 0.2139054344183585 \t| Accuracy: 93.750\n",
      "# Iteration  1558 -> Loss: 0.21384750987811194 \t| Accuracy: 93.750\n",
      "# Iteration  1559 -> Loss: 0.21378967847216188 \t| Accuracy: 93.750\n",
      "# Iteration  1560 -> Loss: 0.2137319399988588 \t| Accuracy: 93.750\n",
      "# Iteration  1561 -> Loss: 0.2136742942570976 \t| Accuracy: 93.750\n",
      "# Iteration  1562 -> Loss: 0.21361674104631553 \t| Accuracy: 93.750\n",
      "# Iteration  1563 -> Loss: 0.2135592801664907 \t| Accuracy: 93.750\n",
      "# Iteration  1564 -> Loss: 0.21350191141814026 \t| Accuracy: 93.750\n",
      "# Iteration  1565 -> Loss: 0.21344463460231858 \t| Accuracy: 93.750\n",
      "# Iteration  1566 -> Loss: 0.21338744952061556 \t| Accuracy: 93.750\n",
      "# Iteration  1567 -> Loss: 0.2133303559751551 \t| Accuracy: 93.750\n",
      "# Iteration  1568 -> Loss: 0.21327335376859313 \t| Accuracy: 93.750\n",
      "# Iteration  1569 -> Loss: 0.21321644270411605 \t| Accuracy: 93.750\n",
      "# Iteration  1570 -> Loss: 0.21315962258543897 \t| Accuracy: 93.750\n",
      "# Iteration  1571 -> Loss: 0.21310289321680406 \t| Accuracy: 93.750\n",
      "# Iteration  1572 -> Loss: 0.21304625440297892 \t| Accuracy: 93.750\n",
      "# Iteration  1573 -> Loss: 0.21298970594925462 \t| Accuracy: 93.750\n",
      "# Iteration  1574 -> Loss: 0.21293324766144436 \t| Accuracy: 93.750\n",
      "# Iteration  1575 -> Loss: 0.21287687934588165 \t| Accuracy: 93.750\n",
      "# Iteration  1576 -> Loss: 0.21282060080941861 \t| Accuracy: 93.750\n",
      "# Iteration  1577 -> Loss: 0.21276441185942443 \t| Accuracy: 93.750\n",
      "# Iteration  1578 -> Loss: 0.21270831230378356 \t| Accuracy: 93.750\n",
      "# Iteration  1579 -> Loss: 0.2126523019508941 \t| Accuracy: 93.750\n",
      "# Iteration  1580 -> Loss: 0.2125963806096664 \t| Accuracy: 93.750\n",
      "# Iteration  1581 -> Loss: 0.21254054808952094 \t| Accuracy: 93.750\n",
      "# Iteration  1582 -> Loss: 0.21248480420038718 \t| Accuracy: 93.750\n",
      "# Iteration  1583 -> Loss: 0.2124291487527017 \t| Accuracy: 93.750\n",
      "# Iteration  1584 -> Loss: 0.21237358155740663 \t| Accuracy: 93.750\n",
      "# Iteration  1585 -> Loss: 0.21231810242594792 \t| Accuracy: 93.750\n",
      "# Iteration  1586 -> Loss: 0.2122627111702739 \t| Accuracy: 93.750\n",
      "# Iteration  1587 -> Loss: 0.21220740760283374 \t| Accuracy: 93.750\n",
      "# Iteration  1588 -> Loss: 0.2121521915365754 \t| Accuracy: 93.750\n",
      "# Iteration  1589 -> Loss: 0.21209706278494478 \t| Accuracy: 93.750\n",
      "# Iteration  1590 -> Loss: 0.2120420211618833 \t| Accuracy: 93.750\n",
      "# Iteration  1591 -> Loss: 0.21198706648182708 \t| Accuracy: 93.750\n",
      "# Iteration  1592 -> Loss: 0.2119321985597048 \t| Accuracy: 93.750\n",
      "# Iteration  1593 -> Loss: 0.2118774172109364 \t| Accuracy: 93.750\n",
      "# Iteration  1594 -> Loss: 0.21182272225143153 \t| Accuracy: 93.750\n",
      "# Iteration  1595 -> Loss: 0.21176811349758798 \t| Accuracy: 93.750\n",
      "# Iteration  1596 -> Loss: 0.21171359076628982 \t| Accuracy: 93.750\n",
      "# Iteration  1597 -> Loss: 0.21165915387490639 \t| Accuracy: 93.750\n",
      "# Iteration  1598 -> Loss: 0.21160480264129036 \t| Accuracy: 93.750\n",
      "# Iteration  1599 -> Loss: 0.21155053688377634 \t| Accuracy: 93.750\n",
      "# Iteration  1600 -> Loss: 0.21149635642117925 \t| Accuracy: 93.750\n",
      "# Iteration  1601 -> Loss: 0.21144226107279293 \t| Accuracy: 93.750\n",
      "# Iteration  1602 -> Loss: 0.21138825065838857 \t| Accuracy: 93.750\n",
      "# Iteration  1603 -> Loss: 0.21133432499821306 \t| Accuracy: 93.750\n",
      "# Iteration  1604 -> Loss: 0.2112804839129878 \t| Accuracy: 93.750\n",
      "# Iteration  1605 -> Loss: 0.21122672722390679 \t| Accuracy: 93.750\n",
      "# Iteration  1606 -> Loss: 0.21117305475263545 \t| Accuracy: 93.750\n",
      "# Iteration  1607 -> Loss: 0.21111946632130904 \t| Accuracy: 93.750\n",
      "# Iteration  1608 -> Loss: 0.21106596175253103 \t| Accuracy: 93.750\n",
      "# Iteration  1609 -> Loss: 0.21101254086937182 \t| Accuracy: 93.750\n",
      "# Iteration  1610 -> Loss: 0.21095920349536718 \t| Accuracy: 93.750\n",
      "# Iteration  1611 -> Loss: 0.2109059494545167 \t| Accuracy: 93.750\n",
      "# Iteration  1612 -> Loss: 0.2108527785712824 \t| Accuracy: 93.750\n",
      "# Iteration  1613 -> Loss: 0.21079969067058743 \t| Accuracy: 93.750\n",
      "# Iteration  1614 -> Loss: 0.21074668557781412 \t| Accuracy: 93.750\n",
      "# Iteration  1615 -> Loss: 0.21069376311880314 \t| Accuracy: 93.750\n",
      "# Iteration  1616 -> Loss: 0.21064092311985164 \t| Accuracy: 93.750\n",
      "# Iteration  1617 -> Loss: 0.21058816540771197 \t| Accuracy: 93.750\n",
      "# Iteration  1618 -> Loss: 0.21053548980959017 \t| Accuracy: 93.750\n",
      "# Iteration  1619 -> Loss: 0.2104828961531446 \t| Accuracy: 93.750\n",
      "# Iteration  1620 -> Loss: 0.21043038426648447 \t| Accuracy: 93.750\n",
      "# Iteration  1621 -> Loss: 0.2103779539781685 \t| Accuracy: 93.750\n",
      "# Iteration  1622 -> Loss: 0.21032560511720333 \t| Accuracy: 93.750\n",
      "# Iteration  1623 -> Loss: 0.21027333751304234 \t| Accuracy: 93.750\n",
      "# Iteration  1624 -> Loss: 0.210221150995584 \t| Accuracy: 93.750\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Iteration  1625 -> Loss: 0.21016904539517067 \t| Accuracy: 93.750\n",
      "# Iteration  1626 -> Loss: 0.21011702054258719 \t| Accuracy: 93.750\n",
      "# Iteration  1627 -> Loss: 0.21006507626905924 \t| Accuracy: 93.750\n",
      "# Iteration  1628 -> Loss: 0.21001321240625226 \t| Accuracy: 93.750\n",
      "# Iteration  1629 -> Loss: 0.20996142878626997 \t| Accuracy: 93.750\n",
      "# Iteration  1630 -> Loss: 0.20990972524165297 \t| Accuracy: 93.750\n",
      "# Iteration  1631 -> Loss: 0.2098581016053772 \t| Accuracy: 93.750\n",
      "# Iteration  1632 -> Loss: 0.20980655771085305 \t| Accuracy: 93.750\n",
      "# Iteration  1633 -> Loss: 0.20975509339192336 \t| Accuracy: 93.750\n",
      "# Iteration  1634 -> Loss: 0.20970370848286254 \t| Accuracy: 93.750\n",
      "# Iteration  1635 -> Loss: 0.20965240281837522 \t| Accuracy: 93.750\n",
      "# Iteration  1636 -> Loss: 0.20960117623359442 \t| Accuracy: 93.750\n",
      "# Iteration  1637 -> Loss: 0.20955002856408086 \t| Accuracy: 93.750\n",
      "# Iteration  1638 -> Loss: 0.20949895964582108 \t| Accuracy: 93.750\n",
      "# Iteration  1639 -> Loss: 0.2094479693152265 \t| Accuracy: 93.750\n",
      "# Iteration  1640 -> Loss: 0.20939705740913178 \t| Accuracy: 93.750\n",
      "# Iteration  1641 -> Loss: 0.20934622376479375 \t| Accuracy: 93.750\n",
      "# Iteration  1642 -> Loss: 0.20929546821988995 \t| Accuracy: 93.750\n",
      "# Iteration  1643 -> Loss: 0.20924479061251716 \t| Accuracy: 93.750\n",
      "# Iteration  1644 -> Loss: 0.2091941907811906 \t| Accuracy: 93.750\n",
      "# Iteration  1645 -> Loss: 0.2091436685648421 \t| Accuracy: 93.750\n",
      "# Iteration  1646 -> Loss: 0.20909322380281894 \t| Accuracy: 93.750\n",
      "# Iteration  1647 -> Loss: 0.20904285633488282 \t| Accuracy: 93.750\n",
      "# Iteration  1648 -> Loss: 0.20899256600120827 \t| Accuracy: 93.750\n",
      "# Iteration  1649 -> Loss: 0.20894235264238134 \t| Accuracy: 93.750\n",
      "# Iteration  1650 -> Loss: 0.20889221609939856 \t| Accuracy: 93.750\n",
      "# Iteration  1651 -> Loss: 0.20884215621366564 \t| Accuracy: 93.750\n",
      "# Iteration  1652 -> Loss: 0.20879217282699583 \t| Accuracy: 93.750\n",
      "# Iteration  1653 -> Loss: 0.2087422657816091 \t| Accuracy: 93.750\n",
      "# Iteration  1654 -> Loss: 0.20869243492013065 \t| Accuracy: 93.750\n",
      "# Iteration  1655 -> Loss: 0.20864268008558975 \t| Accuracy: 93.750\n",
      "# Iteration  1656 -> Loss: 0.20859300112141832 \t| Accuracy: 93.750\n",
      "# Iteration  1657 -> Loss: 0.20854339787144993 \t| Accuracy: 93.750\n",
      "# Iteration  1658 -> Loss: 0.20849387017991824 \t| Accuracy: 93.958\n",
      "# Iteration  1659 -> Loss: 0.2084444178914561 \t| Accuracy: 93.958\n",
      "# Iteration  1660 -> Loss: 0.20839504085109403 \t| Accuracy: 93.958\n",
      "# Iteration  1661 -> Loss: 0.20834573890425917 \t| Accuracy: 93.958\n",
      "# Iteration  1662 -> Loss: 0.20829651189677403 \t| Accuracy: 93.958\n",
      "# Iteration  1663 -> Loss: 0.20824735967485508 \t| Accuracy: 93.958\n",
      "# Iteration  1664 -> Loss: 0.2081982820851118 \t| Accuracy: 93.958\n",
      "# Iteration  1665 -> Loss: 0.20814927897454533 \t| Accuracy: 93.958\n",
      "# Iteration  1666 -> Loss: 0.20810035019054715 \t| Accuracy: 93.958\n",
      "# Iteration  1667 -> Loss: 0.2080514955808982 \t| Accuracy: 93.958\n",
      "# Iteration  1668 -> Loss: 0.20800271499376724 \t| Accuracy: 93.958\n",
      "# Iteration  1669 -> Loss: 0.20795400827771007 \t| Accuracy: 93.958\n",
      "# Iteration  1670 -> Loss: 0.2079053752816681 \t| Accuracy: 93.958\n",
      "# Iteration  1671 -> Loss: 0.2078568158549671 \t| Accuracy: 93.958\n",
      "# Iteration  1672 -> Loss: 0.20780832984731631 \t| Accuracy: 93.958\n",
      "# Iteration  1673 -> Loss: 0.20775991710880692 \t| Accuracy: 93.958\n",
      "# Iteration  1674 -> Loss: 0.20771157748991112 \t| Accuracy: 93.958\n",
      "# Iteration  1675 -> Loss: 0.20766331084148087 \t| Accuracy: 93.958\n",
      "# Iteration  1676 -> Loss: 0.20761511701474678 \t| Accuracy: 93.958\n",
      "# Iteration  1677 -> Loss: 0.20756699586131663 \t| Accuracy: 93.958\n",
      "# Iteration  1678 -> Loss: 0.20751894723317485 \t| Accuracy: 93.958\n",
      "# Iteration  1679 -> Loss: 0.20747097098268064 \t| Accuracy: 93.958\n",
      "# Iteration  1680 -> Loss: 0.2074230669625673 \t| Accuracy: 93.958\n",
      "# Iteration  1681 -> Loss: 0.20737523502594096 \t| Accuracy: 93.958\n",
      "# Iteration  1682 -> Loss: 0.20732747502627943 \t| Accuracy: 93.958\n",
      "# Iteration  1683 -> Loss: 0.20727978681743103 \t| Accuracy: 93.958\n",
      "# Iteration  1684 -> Loss: 0.2072321702536134 \t| Accuracy: 93.958\n",
      "# Iteration  1685 -> Loss: 0.20718462518941258 \t| Accuracy: 93.958\n",
      "# Iteration  1686 -> Loss: 0.2071371514797816 \t| Accuracy: 93.958\n",
      "# Iteration  1687 -> Loss: 0.20708974898003962 \t| Accuracy: 93.958\n",
      "# Iteration  1688 -> Loss: 0.20704241754587072 \t| Accuracy: 93.958\n",
      "# Iteration  1689 -> Loss: 0.20699515703332255 \t| Accuracy: 93.958\n",
      "# Iteration  1690 -> Loss: 0.20694796729880569 \t| Accuracy: 93.958\n",
      "# Iteration  1691 -> Loss: 0.20690084819909219 \t| Accuracy: 93.958\n",
      "# Iteration  1692 -> Loss: 0.20685379959131447 \t| Accuracy: 93.958\n",
      "# Iteration  1693 -> Loss: 0.20680682133296438 \t| Accuracy: 93.958\n",
      "# Iteration  1694 -> Loss: 0.20675991328189205 \t| Accuracy: 93.958\n",
      "# Iteration  1695 -> Loss: 0.20671307529630484 \t| Accuracy: 93.958\n",
      "# Iteration  1696 -> Loss: 0.206666307234766 \t| Accuracy: 93.958\n",
      "# Iteration  1697 -> Loss: 0.20661960895619397 \t| Accuracy: 93.958\n",
      "# Iteration  1698 -> Loss: 0.20657298031986115 \t| Accuracy: 93.958\n",
      "# Iteration  1699 -> Loss: 0.20652642118539258 \t| Accuracy: 93.958\n",
      "# Iteration  1700 -> Loss: 0.20647993141276513 \t| Accuracy: 93.958\n",
      "# Iteration  1701 -> Loss: 0.20643351086230652 \t| Accuracy: 93.958\n",
      "# Iteration  1702 -> Loss: 0.20638715939469407 \t| Accuracy: 93.958\n",
      "# Iteration  1703 -> Loss: 0.20634087687095354 \t| Accuracy: 93.958\n",
      "# Iteration  1704 -> Loss: 0.20629466315245837 \t| Accuracy: 93.958\n",
      "# Iteration  1705 -> Loss: 0.20624851810092845 \t| Accuracy: 93.958\n",
      "# Iteration  1706 -> Loss: 0.2062024415784289 \t| Accuracy: 93.958\n",
      "# Iteration  1707 -> Loss: 0.20615643344736959 \t| Accuracy: 93.958\n",
      "# Iteration  1708 -> Loss: 0.20611049357050343 \t| Accuracy: 93.958\n",
      "# Iteration  1709 -> Loss: 0.20606462181092564 \t| Accuracy: 93.958\n",
      "# Iteration  1710 -> Loss: 0.20601881803207286 \t| Accuracy: 93.958\n",
      "# Iteration  1711 -> Loss: 0.2059730820977218 \t| Accuracy: 93.958\n",
      "# Iteration  1712 -> Loss: 0.2059274138719885 \t| Accuracy: 93.958\n",
      "# Iteration  1713 -> Loss: 0.2058818132193271 \t| Accuracy: 93.958\n",
      "# Iteration  1714 -> Loss: 0.205836280004529 \t| Accuracy: 93.958\n",
      "# Iteration  1715 -> Loss: 0.20579081409272157 \t| Accuracy: 93.958\n",
      "# Iteration  1716 -> Loss: 0.2057454153493675 \t| Accuracy: 93.958\n",
      "# Iteration  1717 -> Loss: 0.20570008364026346 \t| Accuracy: 93.958\n",
      "# Iteration  1718 -> Loss: 0.20565481883153938 \t| Accuracy: 93.958\n",
      "# Iteration  1719 -> Loss: 0.20560962078965714 \t| Accuracy: 93.958\n",
      "# Iteration  1720 -> Loss: 0.20556448938140992 \t| Accuracy: 93.958\n",
      "# Iteration  1721 -> Loss: 0.20551942447392077 \t| Accuracy: 93.958\n",
      "# Iteration  1722 -> Loss: 0.20547442593464216 \t| Accuracy: 93.958\n",
      "# Iteration  1723 -> Loss: 0.20542949363135452 \t| Accuracy: 93.958\n",
      "# Iteration  1724 -> Loss: 0.2053846274321655 \t| Accuracy: 93.958\n",
      "# Iteration  1725 -> Loss: 0.20533982720550892 \t| Accuracy: 93.958\n",
      "# Iteration  1726 -> Loss: 0.20529509282014372 \t| Accuracy: 93.958\n",
      "# Iteration  1727 -> Loss: 0.20525042414515327 \t| Accuracy: 93.958\n",
      "# Iteration  1728 -> Loss: 0.20520582104994392 \t| Accuracy: 93.958\n",
      "# Iteration  1729 -> Loss: 0.20516128340424455 \t| Accuracy: 93.958\n",
      "# Iteration  1730 -> Loss: 0.20511681107810528 \t| Accuracy: 93.958\n",
      "# Iteration  1731 -> Loss: 0.20507240394189652 \t| Accuracy: 93.958\n",
      "# Iteration  1732 -> Loss: 0.20502806186630823 \t| Accuracy: 93.958\n",
      "# Iteration  1733 -> Loss: 0.2049837847223487 \t| Accuracy: 93.958\n",
      "# Iteration  1734 -> Loss: 0.20493957238134378 \t| Accuracy: 93.958\n",
      "# Iteration  1735 -> Loss: 0.20489542471493588 \t| Accuracy: 93.958\n",
      "# Iteration  1736 -> Loss: 0.20485134159508303 \t| Accuracy: 93.958\n",
      "# Iteration  1737 -> Loss: 0.2048073228940579 \t| Accuracy: 93.958\n",
      "# Iteration  1738 -> Loss: 0.20476336848444684 \t| Accuracy: 93.958\n",
      "# Iteration  1739 -> Loss: 0.20471947823914924 \t| Accuracy: 93.958\n",
      "# Iteration  1740 -> Loss: 0.20467565203137608 \t| Accuracy: 93.958\n",
      "# Iteration  1741 -> Loss: 0.2046318897346495 \t| Accuracy: 93.958\n",
      "# Iteration  1742 -> Loss: 0.20458819122280147 \t| Accuracy: 93.958\n",
      "# Iteration  1743 -> Loss: 0.2045445563699733 \t| Accuracy: 93.958\n",
      "# Iteration  1744 -> Loss: 0.20450098505061423 \t| Accuracy: 93.958\n",
      "# Iteration  1745 -> Loss: 0.20445747713948095 \t| Accuracy: 93.958\n",
      "# Iteration  1746 -> Loss: 0.20441403251163645 \t| Accuracy: 93.958\n",
      "# Iteration  1747 -> Loss: 0.20437065104244917 \t| Accuracy: 93.958\n",
      "# Iteration  1748 -> Loss: 0.2043273326075921 \t| Accuracy: 93.958\n",
      "# Iteration  1749 -> Loss: 0.20428407708304186 \t| Accuracy: 93.958\n",
      "# Iteration  1750 -> Loss: 0.20424088434507798 \t| Accuracy: 93.958\n",
      "# Iteration  1751 -> Loss: 0.20419775427028153 \t| Accuracy: 93.958\n",
      "# Iteration  1752 -> Loss: 0.2041546867355348 \t| Accuracy: 93.958\n",
      "# Iteration  1753 -> Loss: 0.2041116816180201 \t| Accuracy: 93.958\n",
      "# Iteration  1754 -> Loss: 0.20406873879521883 \t| Accuracy: 93.958\n",
      "# Iteration  1755 -> Loss: 0.2040258581449109 \t| Accuracy: 93.958\n",
      "# Iteration  1756 -> Loss: 0.20398303954517338 \t| Accuracy: 93.958\n",
      "# Iteration  1757 -> Loss: 0.2039402828743801 \t| Accuracy: 93.958\n",
      "# Iteration  1758 -> Loss: 0.20389758801120048 \t| Accuracy: 93.958\n",
      "# Iteration  1759 -> Loss: 0.20385495483459876 \t| Accuracy: 93.958\n",
      "# Iteration  1760 -> Loss: 0.20381238322383313 \t| Accuracy: 93.958\n",
      "# Iteration  1761 -> Loss: 0.20376987305845481 \t| Accuracy: 93.958\n",
      "# Iteration  1762 -> Loss: 0.2037274242183074 \t| Accuracy: 93.958\n",
      "# Iteration  1763 -> Loss: 0.2036850365835256 \t| Accuracy: 93.958\n",
      "# Iteration  1764 -> Loss: 0.2036427100345348 \t| Accuracy: 93.958\n",
      "# Iteration  1765 -> Loss: 0.20360044445205 \t| Accuracy: 93.958\n",
      "# Iteration  1766 -> Loss: 0.20355823971707493 \t| Accuracy: 93.958\n",
      "# Iteration  1767 -> Loss: 0.20351609571090143 \t| Accuracy: 93.958\n",
      "# Iteration  1768 -> Loss: 0.20347401231510823 \t| Accuracy: 93.958\n",
      "# Iteration  1769 -> Loss: 0.20343198941156057 \t| Accuracy: 93.958\n",
      "# Iteration  1770 -> Loss: 0.20339002688240898 \t| Accuracy: 93.958\n",
      "# Iteration  1771 -> Loss: 0.20334812461008858 \t| Accuracy: 93.958\n",
      "# Iteration  1772 -> Loss: 0.2033062824773183 \t| Accuracy: 93.958\n",
      "# Iteration  1773 -> Loss: 0.20326450036710006 \t| Accuracy: 93.958\n",
      "# Iteration  1774 -> Loss: 0.2032227781627178 \t| Accuracy: 93.958\n",
      "# Iteration  1775 -> Loss: 0.20318111574773687 \t| Accuracy: 93.958\n",
      "# Iteration  1776 -> Loss: 0.2031395130060029 \t| Accuracy: 93.958\n",
      "# Iteration  1777 -> Loss: 0.20309796982164136 \t| Accuracy: 93.958\n",
      "# Iteration  1778 -> Loss: 0.20305648607905646 \t| Accuracy: 93.958\n",
      "# Iteration  1779 -> Loss: 0.20301506166293054 \t| Accuracy: 93.958\n",
      "# Iteration  1780 -> Loss: 0.20297369645822302 \t| Accuracy: 93.958\n",
      "# Iteration  1781 -> Loss: 0.20293239035016974 \t| Accuracy: 93.958\n",
      "# Iteration  1782 -> Loss: 0.2028911432242824 \t| Accuracy: 93.958\n",
      "# Iteration  1783 -> Loss: 0.20284995496634714 \t| Accuracy: 93.958\n",
      "# Iteration  1784 -> Loss: 0.20280882546242437 \t| Accuracy: 93.958\n",
      "# Iteration  1785 -> Loss: 0.20276775459884763 \t| Accuracy: 93.958\n",
      "# Iteration  1786 -> Loss: 0.20272674226222295 \t| Accuracy: 93.958\n",
      "# Iteration  1787 -> Loss: 0.2026857883394279 \t| Accuracy: 93.958\n",
      "# Iteration  1788 -> Loss: 0.20264489271761096 \t| Accuracy: 93.958\n",
      "# Iteration  1789 -> Loss: 0.20260405528419068 \t| Accuracy: 93.958\n",
      "# Iteration  1790 -> Loss: 0.20256327592685477 \t| Accuracy: 93.958\n",
      "# Iteration  1791 -> Loss: 0.20252255453355972 \t| Accuracy: 93.958\n",
      "# Iteration  1792 -> Loss: 0.20248189099252953 \t| Accuracy: 93.958\n",
      "# Iteration  1793 -> Loss: 0.20244128519225513 \t| Accuracy: 93.958\n",
      "# Iteration  1794 -> Loss: 0.20240073702149383 \t| Accuracy: 93.958\n",
      "# Iteration  1795 -> Loss: 0.2023602463692682 \t| Accuracy: 93.958\n",
      "# Iteration  1796 -> Loss: 0.20231981312486547 \t| Accuracy: 93.958\n",
      "# Iteration  1797 -> Loss: 0.2022794371778369 \t| Accuracy: 93.958\n",
      "# Iteration  1798 -> Loss: 0.20223911841799677 \t| Accuracy: 93.958\n",
      "# Iteration  1799 -> Loss: 0.2021988567354217 \t| Accuracy: 93.958\n",
      "# Iteration  1800 -> Loss: 0.20215865202045002 \t| Accuracy: 93.958\n",
      "# Iteration  1801 -> Loss: 0.20211850416368085 \t| Accuracy: 93.958\n",
      "# Iteration  1802 -> Loss: 0.20207841305597357 \t| Accuracy: 93.958\n",
      "# Iteration  1803 -> Loss: 0.20203837858844675 \t| Accuracy: 93.958\n",
      "# Iteration  1804 -> Loss: 0.20199840065247768 \t| Accuracy: 93.958\n",
      "# Iteration  1805 -> Loss: 0.20195847913970155 \t| Accuracy: 93.958\n",
      "# Iteration  1806 -> Loss: 0.20191861394201066 \t| Accuracy: 93.958\n",
      "# Iteration  1807 -> Loss: 0.20187880495155372 \t| Accuracy: 93.958\n",
      "# Iteration  1808 -> Loss: 0.2018390520607351 \t| Accuracy: 93.958\n",
      "# Iteration  1809 -> Loss: 0.20179935516221423 \t| Accuracy: 93.958\n",
      "# Iteration  1810 -> Loss: 0.20175971414890456 \t| Accuracy: 93.958\n",
      "# Iteration  1811 -> Loss: 0.20172012891397315 \t| Accuracy: 93.958\n",
      "# Iteration  1812 -> Loss: 0.20168059935083982 \t| Accuracy: 93.958\n",
      "# Iteration  1813 -> Loss: 0.20164112535317646 \t| Accuracy: 93.958\n",
      "# Iteration  1814 -> Loss: 0.2016017068149062 \t| Accuracy: 93.958\n",
      "# Iteration  1815 -> Loss: 0.2015623436302027 \t| Accuracy: 93.958\n",
      "# Iteration  1816 -> Loss: 0.20152303569348978 \t| Accuracy: 93.958\n",
      "# Iteration  1817 -> Loss: 0.2014837828994402 \t| Accuracy: 93.958\n",
      "# Iteration  1818 -> Loss: 0.2014445851429753 \t| Accuracy: 93.958\n",
      "# Iteration  1819 -> Loss: 0.2014054423192641 \t| Accuracy: 93.958\n",
      "# Iteration  1820 -> Loss: 0.20136635432372274 \t| Accuracy: 93.958\n",
      "# Iteration  1821 -> Loss: 0.20132732105201376 \t| Accuracy: 93.958\n",
      "# Iteration  1822 -> Loss: 0.20128834240004523 \t| Accuracy: 93.958\n",
      "# Iteration  1823 -> Loss: 0.20124941826397028 \t| Accuracy: 93.958\n",
      "# Iteration  1824 -> Loss: 0.20121054854018622 \t| Accuracy: 93.958\n",
      "# Iteration  1825 -> Loss: 0.20117173312533393 \t| Accuracy: 93.958\n",
      "# Iteration  1826 -> Loss: 0.2011329719162974 \t| Accuracy: 93.958\n",
      "# Iteration  1827 -> Loss: 0.20109426481020243 \t| Accuracy: 93.958\n",
      "# Iteration  1828 -> Loss: 0.20105561170441652 \t| Accuracy: 93.958\n",
      "# Iteration  1829 -> Loss: 0.20101701249654808 \t| Accuracy: 93.958\n",
      "# Iteration  1830 -> Loss: 0.20097846708444553 \t| Accuracy: 93.958\n",
      "# Iteration  1831 -> Loss: 0.2009399753661968 \t| Accuracy: 93.958\n",
      "# Iteration  1832 -> Loss: 0.20090153724012852 \t| Accuracy: 93.958\n",
      "# Iteration  1833 -> Loss: 0.20086315260480556 \t| Accuracy: 93.958\n",
      "# Iteration  1834 -> Loss: 0.20082482135903013 \t| Accuracy: 93.958\n",
      "# Iteration  1835 -> Loss: 0.20078654340184124 \t| Accuracy: 93.958\n",
      "# Iteration  1836 -> Loss: 0.200748318632514 \t| Accuracy: 93.958\n",
      "# Iteration  1837 -> Loss: 0.20071014695055897 \t| Accuracy: 93.958\n",
      "# Iteration  1838 -> Loss: 0.20067202825572145 \t| Accuracy: 93.958\n",
      "# Iteration  1839 -> Loss: 0.20063396244798085 \t| Accuracy: 93.958\n",
      "# Iteration  1840 -> Loss: 0.20059594942755013 \t| Accuracy: 93.958\n",
      "# Iteration  1841 -> Loss: 0.20055798909487485 \t| Accuracy: 93.958\n",
      "# Iteration  1842 -> Loss: 0.20052008135063287 \t| Accuracy: 93.958\n",
      "# Iteration  1843 -> Loss: 0.20048222609573346 \t| Accuracy: 93.958\n",
      "# Iteration  1844 -> Loss: 0.20044442323131684 \t| Accuracy: 93.958\n",
      "# Iteration  1845 -> Loss: 0.2004066726587533 \t| Accuracy: 93.958\n",
      "# Iteration  1846 -> Loss: 0.20036897427964265 \t| Accuracy: 93.958\n",
      "# Iteration  1847 -> Loss: 0.20033132799581377 \t| Accuracy: 93.958\n",
      "# Iteration  1848 -> Loss: 0.20029373370932366 \t| Accuracy: 93.958\n",
      "# Iteration  1849 -> Loss: 0.20025619132245695 \t| Accuracy: 93.958\n",
      "# Iteration  1850 -> Loss: 0.20021870073772535 \t| Accuracy: 93.958\n",
      "# Iteration  1851 -> Loss: 0.20018126185786683 \t| Accuracy: 93.958\n",
      "# Iteration  1852 -> Loss: 0.20014387458584518 \t| Accuracy: 93.958\n",
      "# Iteration  1853 -> Loss: 0.20010653882484905 \t| Accuracy: 93.958\n",
      "# Iteration  1854 -> Loss: 0.20006925447829182 \t| Accuracy: 93.958\n",
      "# Iteration  1855 -> Loss: 0.20003202144981055 \t| Accuracy: 93.958\n",
      "# Iteration  1856 -> Loss: 0.19999483964326553 \t| Accuracy: 93.958\n",
      "# Iteration  1857 -> Loss: 0.19995770896273973 \t| Accuracy: 93.958\n",
      "# Iteration  1858 -> Loss: 0.1999206293125379 \t| Accuracy: 93.958\n",
      "# Iteration  1859 -> Loss: 0.19988360059718632 \t| Accuracy: 93.958\n",
      "# Iteration  1860 -> Loss: 0.1998466227214319 \t| Accuracy: 93.958\n",
      "# Iteration  1861 -> Loss: 0.1998096955902417 \t| Accuracy: 93.958\n",
      "# Iteration  1862 -> Loss: 0.19977281910880226 \t| Accuracy: 93.958\n",
      "# Iteration  1863 -> Loss: 0.19973599318251906 \t| Accuracy: 93.958\n",
      "# Iteration  1864 -> Loss: 0.19969921771701582 \t| Accuracy: 93.958\n",
      "# Iteration  1865 -> Loss: 0.19966249261813399 \t| Accuracy: 93.958\n",
      "# Iteration  1866 -> Loss: 0.19962581779193211 \t| Accuracy: 93.958\n",
      "# Iteration  1867 -> Loss: 0.19958919314468512 \t| Accuracy: 93.958\n",
      "# Iteration  1868 -> Loss: 0.1995526185828839 \t| Accuracy: 93.958\n",
      "# Iteration  1869 -> Loss: 0.19951609401323456 \t| Accuracy: 93.958\n",
      "# Iteration  1870 -> Loss: 0.19947961934265793 \t| Accuracy: 93.958\n",
      "# Iteration  1871 -> Loss: 0.199443194478289 \t| Accuracy: 93.958\n",
      "# Iteration  1872 -> Loss: 0.199406819327476 \t| Accuracy: 93.958\n",
      "# Iteration  1873 -> Loss: 0.1993704937977805 \t| Accuracy: 93.958\n",
      "# Iteration  1874 -> Loss: 0.19933421779697594 \t| Accuracy: 93.958\n",
      "# Iteration  1875 -> Loss: 0.1992979912330477 \t| Accuracy: 93.958\n",
      "# Iteration  1876 -> Loss: 0.19926181401419238 \t| Accuracy: 93.958\n",
      "# Iteration  1877 -> Loss: 0.19922568604881705 \t| Accuracy: 93.958\n",
      "# Iteration  1878 -> Loss: 0.19918960724553875 \t| Accuracy: 93.958\n",
      "# Iteration  1879 -> Loss: 0.19915357751318394 \t| Accuracy: 93.958\n",
      "# Iteration  1880 -> Loss: 0.199117596760788 \t| Accuracy: 93.958\n",
      "# Iteration  1881 -> Loss: 0.19908166489759443 \t| Accuracy: 93.958\n",
      "# Iteration  1882 -> Loss: 0.19904578183305455 \t| Accuracy: 93.958\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Iteration  1883 -> Loss: 0.19900994747682663 \t| Accuracy: 93.958\n",
      "# Iteration  1884 -> Loss: 0.1989741617387756 \t| Accuracy: 93.958\n",
      "# Iteration  1885 -> Loss: 0.1989384245289724 \t| Accuracy: 93.958\n",
      "# Iteration  1886 -> Loss: 0.19890273575769335 \t| Accuracy: 93.958\n",
      "# Iteration  1887 -> Loss: 0.19886709533541946 \t| Accuracy: 93.958\n",
      "# Iteration  1888 -> Loss: 0.19883150317283635 \t| Accuracy: 93.958\n",
      "# Iteration  1889 -> Loss: 0.19879595918083298 \t| Accuracy: 93.958\n",
      "# Iteration  1890 -> Loss: 0.19876046327050176 \t| Accuracy: 93.958\n",
      "# Iteration  1891 -> Loss: 0.19872501535313764 \t| Accuracy: 93.958\n",
      "# Iteration  1892 -> Loss: 0.19868961534023763 \t| Accuracy: 93.958\n",
      "# Iteration  1893 -> Loss: 0.19865426314350015 \t| Accuracy: 93.958\n",
      "# Iteration  1894 -> Loss: 0.19861895867482476 \t| Accuracy: 93.958\n",
      "# Iteration  1895 -> Loss: 0.19858370184631124 \t| Accuracy: 93.958\n",
      "# Iteration  1896 -> Loss: 0.1985484925702594 \t| Accuracy: 93.958\n",
      "# Iteration  1897 -> Loss: 0.1985133307591682 \t| Accuracy: 93.958\n",
      "# Iteration  1898 -> Loss: 0.19847821632573545 \t| Accuracy: 93.958\n",
      "# Iteration  1899 -> Loss: 0.19844314918285716 \t| Accuracy: 93.958\n",
      "# Iteration  1900 -> Loss: 0.19840812924362713 \t| Accuracy: 93.958\n",
      "# Iteration  1901 -> Loss: 0.19837315642133618 \t| Accuracy: 93.958\n",
      "# Iteration  1902 -> Loss: 0.1983382306294718 \t| Accuracy: 93.958\n",
      "# Iteration  1903 -> Loss: 0.1983033517817176 \t| Accuracy: 93.958\n",
      "# Iteration  1904 -> Loss: 0.19826851979195256 \t| Accuracy: 93.958\n",
      "# Iteration  1905 -> Loss: 0.19823373457425095 \t| Accuracy: 93.958\n",
      "# Iteration  1906 -> Loss: 0.19819899604288124 \t| Accuracy: 93.958\n",
      "# Iteration  1907 -> Loss: 0.19816430411230612 \t| Accuracy: 93.958\n",
      "# Iteration  1908 -> Loss: 0.1981296586971815 \t| Accuracy: 93.958\n",
      "# Iteration  1909 -> Loss: 0.1980950597123563 \t| Accuracy: 93.958\n",
      "# Iteration  1910 -> Loss: 0.1980605070728717 \t| Accuracy: 93.958\n",
      "# Iteration  1911 -> Loss: 0.1980260006939611 \t| Accuracy: 93.958\n",
      "# Iteration  1912 -> Loss: 0.19799154049104875 \t| Accuracy: 93.958\n",
      "# Iteration  1913 -> Loss: 0.19795712637975002 \t| Accuracy: 93.958\n",
      "# Iteration  1914 -> Loss: 0.1979227582758706 \t| Accuracy: 93.958\n",
      "# Iteration  1915 -> Loss: 0.1978884360954058 \t| Accuracy: 93.958\n",
      "# Iteration  1916 -> Loss: 0.19785415975454032 \t| Accuracy: 93.958\n",
      "# Iteration  1917 -> Loss: 0.19781992916964775 \t| Accuracy: 93.958\n",
      "# Iteration  1918 -> Loss: 0.19778574425728965 \t| Accuracy: 93.958\n",
      "# Iteration  1919 -> Loss: 0.19775160493421556 \t| Accuracy: 93.958\n",
      "# Iteration  1920 -> Loss: 0.1977175111173622 \t| Accuracy: 93.958\n",
      "# Iteration  1921 -> Loss: 0.19768346272385304 \t| Accuracy: 93.958\n",
      "# Iteration  1922 -> Loss: 0.1976494596709978 \t| Accuracy: 93.958\n",
      "# Iteration  1923 -> Loss: 0.19761550187629187 \t| Accuracy: 93.958\n",
      "# Iteration  1924 -> Loss: 0.1975815892574159 \t| Accuracy: 93.958\n",
      "# Iteration  1925 -> Loss: 0.19754772173223542 \t| Accuracy: 93.958\n",
      "# Iteration  1926 -> Loss: 0.19751389921879992 \t| Accuracy: 93.958\n",
      "# Iteration  1927 -> Loss: 0.19748012163534293 \t| Accuracy: 93.958\n",
      "# Iteration  1928 -> Loss: 0.1974463889002811 \t| Accuracy: 93.958\n",
      "# Iteration  1929 -> Loss: 0.19741270093221383 \t| Accuracy: 93.958\n",
      "# Iteration  1930 -> Loss: 0.19737905764992272 \t| Accuracy: 93.958\n",
      "# Iteration  1931 -> Loss: 0.19734545897237138 \t| Accuracy: 93.958\n",
      "# Iteration  1932 -> Loss: 0.1973119048187045 \t| Accuracy: 93.958\n",
      "# Iteration  1933 -> Loss: 0.19727839510824768 \t| Accuracy: 93.958\n",
      "# Iteration  1934 -> Loss: 0.1972449297605068 \t| Accuracy: 93.958\n",
      "# Iteration  1935 -> Loss: 0.1972115086951677 \t| Accuracy: 93.958\n",
      "# Iteration  1936 -> Loss: 0.19717813183209537 \t| Accuracy: 93.958\n",
      "# Iteration  1937 -> Loss: 0.19714479909133387 \t| Accuracy: 93.958\n",
      "# Iteration  1938 -> Loss: 0.19711151039310557 \t| Accuracy: 93.958\n",
      "# Iteration  1939 -> Loss: 0.19707826565781084 \t| Accuracy: 93.958\n",
      "# Iteration  1940 -> Loss: 0.19704506480602743 \t| Accuracy: 93.958\n",
      "# Iteration  1941 -> Loss: 0.19701190775851007 \t| Accuracy: 93.958\n",
      "# Iteration  1942 -> Loss: 0.1969787944361901 \t| Accuracy: 93.958\n",
      "# Iteration  1943 -> Loss: 0.19694572476017477 \t| Accuracy: 93.958\n",
      "# Iteration  1944 -> Loss: 0.19691269865174696 \t| Accuracy: 93.958\n",
      "# Iteration  1945 -> Loss: 0.19687971603236468 \t| Accuracy: 93.958\n",
      "# Iteration  1946 -> Loss: 0.19684677682366053 \t| Accuracy: 93.958\n",
      "# Iteration  1947 -> Loss: 0.1968138809474413 \t| Accuracy: 93.958\n",
      "# Iteration  1948 -> Loss: 0.1967810283256874 \t| Accuracy: 93.958\n",
      "# Iteration  1949 -> Loss: 0.19674821888055266 \t| Accuracy: 93.958\n",
      "# Iteration  1950 -> Loss: 0.19671545253436357 \t| Accuracy: 93.958\n",
      "# Iteration  1951 -> Loss: 0.19668272920961896 \t| Accuracy: 93.958\n",
      "# Iteration  1952 -> Loss: 0.19665004882898962 \t| Accuracy: 93.958\n",
      "# Iteration  1953 -> Loss: 0.19661741131531757 \t| Accuracy: 93.958\n",
      "# Iteration  1954 -> Loss: 0.19658481659161595 \t| Accuracy: 93.958\n",
      "# Iteration  1955 -> Loss: 0.19655226458106842 \t| Accuracy: 93.958\n",
      "# Iteration  1956 -> Loss: 0.19651975520702852 \t| Accuracy: 94.375\n",
      "# Iteration  1957 -> Loss: 0.19648728839301963 \t| Accuracy: 94.375\n",
      "# Iteration  1958 -> Loss: 0.1964548640627341 \t| Accuracy: 94.375\n",
      "# Iteration  1959 -> Loss: 0.19642248214003302 \t| Accuracy: 94.375\n",
      "# Iteration  1960 -> Loss: 0.19639014254894582 \t| Accuracy: 94.375\n",
      "# Iteration  1961 -> Loss: 0.19635784521366975 \t| Accuracy: 94.375\n",
      "# Iteration  1962 -> Loss: 0.19632559005856928 \t| Accuracy: 94.375\n",
      "# Iteration  1963 -> Loss: 0.19629337700817603 \t| Accuracy: 94.375\n",
      "# Iteration  1964 -> Loss: 0.19626120598718788 \t| Accuracy: 94.375\n",
      "# Iteration  1965 -> Loss: 0.19622907692046898 \t| Accuracy: 94.375\n",
      "# Iteration  1966 -> Loss: 0.196196989733049 \t| Accuracy: 94.375\n",
      "# Iteration  1967 -> Loss: 0.19616494435012288 \t| Accuracy: 94.375\n",
      "# Iteration  1968 -> Loss: 0.19613294069705 \t| Accuracy: 94.375\n",
      "# Iteration  1969 -> Loss: 0.19610097869935447 \t| Accuracy: 94.375\n",
      "# Iteration  1970 -> Loss: 0.19606905828272403 \t| Accuracy: 94.375\n",
      "# Iteration  1971 -> Loss: 0.19603717937300985 \t| Accuracy: 94.375\n",
      "# Iteration  1972 -> Loss: 0.1960053418962263 \t| Accuracy: 94.375\n",
      "# Iteration  1973 -> Loss: 0.1959735457785502 \t| Accuracy: 94.375\n",
      "# Iteration  1974 -> Loss: 0.19594179094632047 \t| Accuracy: 94.375\n",
      "# Iteration  1975 -> Loss: 0.19591007732603802 \t| Accuracy: 94.375\n",
      "# Iteration  1976 -> Loss: 0.1958784048443648 \t| Accuracy: 94.375\n",
      "# Iteration  1977 -> Loss: 0.19584677342812384 \t| Accuracy: 94.375\n",
      "# Iteration  1978 -> Loss: 0.19581518300429857 \t| Accuracy: 94.375\n",
      "# Iteration  1979 -> Loss: 0.19578363350003242 \t| Accuracy: 94.375\n",
      "# Iteration  1980 -> Loss: 0.19575212484262852 \t| Accuracy: 94.375\n",
      "# Iteration  1981 -> Loss: 0.1957206569595492 \t| Accuracy: 94.375\n",
      "# Iteration  1982 -> Loss: 0.19568922977841563 \t| Accuracy: 94.375\n",
      "# Iteration  1983 -> Loss: 0.1956578432270072 \t| Accuracy: 94.375\n",
      "# Iteration  1984 -> Loss: 0.19562649723326153 \t| Accuracy: 94.375\n",
      "# Iteration  1985 -> Loss: 0.19559519172527348 \t| Accuracy: 94.375\n",
      "# Iteration  1986 -> Loss: 0.19556392663129535 \t| Accuracy: 94.375\n",
      "# Iteration  1987 -> Loss: 0.19553270187973598 \t| Accuracy: 94.375\n",
      "# Iteration  1988 -> Loss: 0.19550151739916058 \t| Accuracy: 94.375\n",
      "# Iteration  1989 -> Loss: 0.19547037311829035 \t| Accuracy: 94.375\n",
      "# Iteration  1990 -> Loss: 0.19543926896600194 \t| Accuracy: 94.375\n",
      "# Iteration  1991 -> Loss: 0.19540820487132707 \t| Accuracy: 94.375\n",
      "# Iteration  1992 -> Loss: 0.19537718076345226 \t| Accuracy: 94.375\n",
      "# Iteration  1993 -> Loss: 0.19534619657171823 \t| Accuracy: 94.375\n",
      "# Iteration  1994 -> Loss: 0.19531525222561963 \t| Accuracy: 94.375\n",
      "# Iteration  1995 -> Loss: 0.19528434765480468 \t| Accuracy: 94.375\n",
      "# Iteration  1996 -> Loss: 0.19525348278907464 \t| Accuracy: 94.375\n",
      "# Iteration  1997 -> Loss: 0.19522265755838344 \t| Accuracy: 94.375\n",
      "# Iteration  1998 -> Loss: 0.19519187189283732 \t| Accuracy: 94.375\n",
      "# Iteration  1999 -> Loss: 0.1951611257226946 \t| Accuracy: 94.375\n",
      "# Iteration  2000 -> Loss: 0.1951304189783648 \t| Accuracy: 94.375\n",
      "# Iteration  2001 -> Loss: 0.19509975159040888 \t| Accuracy: 94.375\n",
      "# Iteration  2002 -> Loss: 0.19506912348953834 \t| Accuracy: 94.375\n",
      "# Iteration  2003 -> Loss: 0.1950385346066151 \t| Accuracy: 94.375\n",
      "# Iteration  2004 -> Loss: 0.19500798487265095 \t| Accuracy: 94.375\n",
      "# Iteration  2005 -> Loss: 0.1949774742188074 \t| Accuracy: 94.375\n",
      "# Iteration  2006 -> Loss: 0.19494700257639497 \t| Accuracy: 94.375\n",
      "# Iteration  2007 -> Loss: 0.19491656987687306 \t| Accuracy: 94.375\n",
      "# Iteration  2008 -> Loss: 0.19488617605184952 \t| Accuracy: 94.375\n",
      "# Iteration  2009 -> Loss: 0.19485582103308013 \t| Accuracy: 94.375\n",
      "# Iteration  2010 -> Loss: 0.19482550475246838 \t| Accuracy: 94.375\n",
      "# Iteration  2011 -> Loss: 0.1947952271420649 \t| Accuracy: 94.375\n",
      "# Iteration  2012 -> Loss: 0.1947649881340674 \t| Accuracy: 94.375\n",
      "# Iteration  2013 -> Loss: 0.19473478766082 \t| Accuracy: 94.375\n",
      "# Iteration  2014 -> Loss: 0.1947046256548128 \t| Accuracy: 94.375\n",
      "# Iteration  2015 -> Loss: 0.19467450204868197 \t| Accuracy: 94.375\n",
      "# Iteration  2016 -> Loss: 0.1946444167752088 \t| Accuracy: 94.375\n",
      "# Iteration  2017 -> Loss: 0.19461436976731955 \t| Accuracy: 94.375\n",
      "# Iteration  2018 -> Loss: 0.1945843609580853 \t| Accuracy: 94.375\n",
      "# Iteration  2019 -> Loss: 0.19455439028072136 \t| Accuracy: 94.375\n",
      "# Iteration  2020 -> Loss: 0.19452445766858673 \t| Accuracy: 94.375\n",
      "# Iteration  2021 -> Loss: 0.19449456305518414 \t| Accuracy: 94.375\n",
      "# Iteration  2022 -> Loss: 0.19446470637415933 \t| Accuracy: 94.375\n",
      "# Iteration  2023 -> Loss: 0.19443488755930088 \t| Accuracy: 94.375\n",
      "# Iteration  2024 -> Loss: 0.19440510654453985 \t| Accuracy: 94.375\n",
      "# Iteration  2025 -> Loss: 0.19437536326394914 \t| Accuracy: 94.375\n",
      "# Iteration  2026 -> Loss: 0.19434565765174358 \t| Accuracy: 94.375\n",
      "# Iteration  2027 -> Loss: 0.19431598964227917 \t| Accuracy: 94.375\n",
      "# Iteration  2028 -> Loss: 0.19428635917005285 \t| Accuracy: 94.375\n",
      "# Iteration  2029 -> Loss: 0.19425676616970228 \t| Accuracy: 94.375\n",
      "# Iteration  2030 -> Loss: 0.19422721057600528 \t| Accuracy: 94.375\n",
      "# Iteration  2031 -> Loss: 0.19419769232387957 \t| Accuracy: 95.000\n",
      "# Iteration  2032 -> Loss: 0.1941682113483824 \t| Accuracy: 95.000\n",
      "# Iteration  2033 -> Loss: 0.19413876758471016 \t| Accuracy: 95.000\n",
      "# Iteration  2034 -> Loss: 0.194109360968198 \t| Accuracy: 95.000\n",
      "# Iteration  2035 -> Loss: 0.1940799914343197 \t| Accuracy: 95.000\n",
      "# Iteration  2036 -> Loss: 0.19405065891868703 \t| Accuracy: 95.000\n",
      "# Iteration  2037 -> Loss: 0.1940213633570494 \t| Accuracy: 95.000\n",
      "# Iteration  2038 -> Loss: 0.19399210468529382 \t| Accuracy: 95.000\n",
      "# Iteration  2039 -> Loss: 0.1939628828394443 \t| Accuracy: 95.000\n",
      "# Iteration  2040 -> Loss: 0.19393369775566144 \t| Accuracy: 95.000\n",
      "# Iteration  2041 -> Loss: 0.19390454937024232 \t| Accuracy: 95.000\n",
      "# Iteration  2042 -> Loss: 0.19387543761961992 \t| Accuracy: 95.000\n",
      "# Iteration  2043 -> Loss: 0.193846362440363 \t| Accuracy: 95.000\n",
      "# Iteration  2044 -> Loss: 0.19381732376917557 \t| Accuracy: 95.000\n",
      "# Iteration  2045 -> Loss: 0.19378832154289657 \t| Accuracy: 95.000\n",
      "# Iteration  2046 -> Loss: 0.19375935569849967 \t| Accuracy: 95.000\n",
      "# Iteration  2047 -> Loss: 0.19373042617309272 \t| Accuracy: 95.000\n",
      "# Iteration  2048 -> Loss: 0.19370153290391756 \t| Accuracy: 95.000\n",
      "# Iteration  2049 -> Loss: 0.19367267582834968 \t| Accuracy: 95.000\n",
      "# Iteration  2050 -> Loss: 0.1936438548838978 \t| Accuracy: 95.000\n",
      "# Iteration  2051 -> Loss: 0.19361507000820352 \t| Accuracy: 95.000\n",
      "# Iteration  2052 -> Loss: 0.1935863211390411 \t| Accuracy: 95.000\n",
      "# Iteration  2053 -> Loss: 0.19355760821431708 \t| Accuracy: 95.000\n",
      "# Iteration  2054 -> Loss: 0.19352893117206982 \t| Accuracy: 95.000\n",
      "# Iteration  2055 -> Loss: 0.19350028995046936 \t| Accuracy: 95.000\n",
      "# Iteration  2056 -> Loss: 0.19347168448781696 \t| Accuracy: 95.000\n",
      "# Iteration  2057 -> Loss: 0.19344311472254483 \t| Accuracy: 95.000\n",
      "# Iteration  2058 -> Loss: 0.1934145805932157 \t| Accuracy: 95.000\n",
      "# Iteration  2059 -> Loss: 0.19338608203852267 \t| Accuracy: 95.000\n",
      "# Iteration  2060 -> Loss: 0.19335761899728865 \t| Accuracy: 95.000\n",
      "# Iteration  2061 -> Loss: 0.19332919140846633 \t| Accuracy: 95.000\n",
      "# Iteration  2062 -> Loss: 0.1933007992111375 \t| Accuracy: 95.000\n",
      "# Iteration  2063 -> Loss: 0.1932724423445129 \t| Accuracy: 95.000\n",
      "# Iteration  2064 -> Loss: 0.19324412074793215 \t| Accuracy: 95.000\n",
      "# Iteration  2065 -> Loss: 0.1932158343608629 \t| Accuracy: 95.000\n",
      "# Iteration  2066 -> Loss: 0.19318758312290094 \t| Accuracy: 95.000\n",
      "# Iteration  2067 -> Loss: 0.19315936697376956 \t| Accuracy: 95.000\n",
      "# Iteration  2068 -> Loss: 0.1931311858533196 \t| Accuracy: 95.000\n",
      "# Iteration  2069 -> Loss: 0.19310303970152873 \t| Accuracy: 95.000\n",
      "# Iteration  2070 -> Loss: 0.19307492845850147 \t| Accuracy: 95.000\n",
      "# Iteration  2071 -> Loss: 0.1930468520644686 \t| Accuracy: 95.000\n",
      "# Iteration  2072 -> Loss: 0.19301881045978697 \t| Accuracy: 95.000\n",
      "# Iteration  2073 -> Loss: 0.19299080358493934 \t| Accuracy: 95.000\n",
      "# Iteration  2074 -> Loss: 0.19296283138053366 \t| Accuracy: 95.000\n",
      "# Iteration  2075 -> Loss: 0.1929348937873031 \t| Accuracy: 95.000\n",
      "# Iteration  2076 -> Loss: 0.1929069907461058 \t| Accuracy: 95.000\n",
      "# Iteration  2077 -> Loss: 0.19287912219792402 \t| Accuracy: 95.000\n",
      "# Iteration  2078 -> Loss: 0.1928512880838646 \t| Accuracy: 95.000\n",
      "# Iteration  2079 -> Loss: 0.19282348834515795 \t| Accuracy: 95.000\n",
      "# Iteration  2080 -> Loss: 0.19279572292315816 \t| Accuracy: 95.000\n",
      "# Iteration  2081 -> Loss: 0.19276799175934264 \t| Accuracy: 95.000\n",
      "# Iteration  2082 -> Loss: 0.19274029479531152 \t| Accuracy: 95.000\n",
      "# Iteration  2083 -> Loss: 0.19271263197278776 \t| Accuracy: 95.000\n",
      "# Iteration  2084 -> Loss: 0.19268500323361656 \t| Accuracy: 95.000\n",
      "# Iteration  2085 -> Loss: 0.19265740851976526 \t| Accuracy: 95.000\n",
      "# Iteration  2086 -> Loss: 0.19262984777332265 \t| Accuracy: 95.000\n",
      "# Iteration  2087 -> Loss: 0.19260232093649912 \t| Accuracy: 95.000\n",
      "# Iteration  2088 -> Loss: 0.19257482795162625 \t| Accuracy: 95.000\n",
      "# Iteration  2089 -> Loss: 0.1925473687611563 \t| Accuracy: 95.000\n",
      "# Iteration  2090 -> Loss: 0.19251994330766195 \t| Accuracy: 95.000\n",
      "# Iteration  2091 -> Loss: 0.19249255153383624 \t| Accuracy: 95.000\n",
      "# Iteration  2092 -> Loss: 0.1924651933824921 \t| Accuracy: 95.000\n",
      "# Iteration  2093 -> Loss: 0.19243786879656205 \t| Accuracy: 95.000\n",
      "# Iteration  2094 -> Loss: 0.19241057771909778 \t| Accuracy: 95.000\n",
      "# Iteration  2095 -> Loss: 0.19238332009327028 \t| Accuracy: 95.000\n",
      "# Iteration  2096 -> Loss: 0.19235609586236896 \t| Accuracy: 95.000\n",
      "# Iteration  2097 -> Loss: 0.1923289049698018 \t| Accuracy: 95.000\n",
      "# Iteration  2098 -> Loss: 0.19230174735909494 \t| Accuracy: 95.000\n",
      "# Iteration  2099 -> Loss: 0.19227462297389228 \t| Accuracy: 95.000\n",
      "# Iteration  2100 -> Loss: 0.19224753175795523 \t| Accuracy: 95.000\n",
      "# Iteration  2101 -> Loss: 0.1922204736551626 \t| Accuracy: 95.000\n",
      "# Iteration  2102 -> Loss: 0.19219344860951001 \t| Accuracy: 95.000\n",
      "# Iteration  2103 -> Loss: 0.19216645656510978 \t| Accuracy: 95.000\n",
      "# Iteration  2104 -> Loss: 0.19213949746619066 \t| Accuracy: 95.000\n",
      "# Iteration  2105 -> Loss: 0.19211257125709752 \t| Accuracy: 95.000\n",
      "# Iteration  2106 -> Loss: 0.19208567788229086 \t| Accuracy: 95.000\n",
      "# Iteration  2107 -> Loss: 0.19205881728634686 \t| Accuracy: 95.000\n",
      "# Iteration  2108 -> Loss: 0.19203198941395694 \t| Accuracy: 95.000\n",
      "# Iteration  2109 -> Loss: 0.1920051942099273 \t| Accuracy: 95.000\n",
      "# Iteration  2110 -> Loss: 0.191978431619179 \t| Accuracy: 95.000\n",
      "# Iteration  2111 -> Loss: 0.19195170158674738 \t| Accuracy: 95.000\n",
      "# Iteration  2112 -> Loss: 0.19192500405778187 \t| Accuracy: 95.000\n",
      "# Iteration  2113 -> Loss: 0.19189833897754577 \t| Accuracy: 95.000\n",
      "# Iteration  2114 -> Loss: 0.19187170629141578 \t| Accuracy: 95.000\n",
      "# Iteration  2115 -> Loss: 0.19184510594488202 \t| Accuracy: 95.000\n",
      "# Iteration  2116 -> Loss: 0.19181853788354755 \t| Accuracy: 95.000\n",
      "# Iteration  2117 -> Loss: 0.19179200205312802 \t| Accuracy: 95.000\n",
      "# Iteration  2118 -> Loss: 0.19176549839945164 \t| Accuracy: 95.000\n",
      "# Iteration  2119 -> Loss: 0.19173902686845862 \t| Accuracy: 95.000\n",
      "# Iteration  2120 -> Loss: 0.19171258740620106 \t| Accuracy: 95.000\n",
      "# Iteration  2121 -> Loss: 0.19168617995884282 \t| Accuracy: 95.000\n",
      "# Iteration  2122 -> Loss: 0.19165980447265885 \t| Accuracy: 95.000\n",
      "# Iteration  2123 -> Loss: 0.19163346089403532 \t| Accuracy: 95.000\n",
      "# Iteration  2124 -> Loss: 0.19160714916946897 \t| Accuracy: 95.000\n",
      "# Iteration  2125 -> Loss: 0.19158086924556728 \t| Accuracy: 95.000\n",
      "# Iteration  2126 -> Loss: 0.19155462106904775 \t| Accuracy: 95.000\n",
      "# Iteration  2127 -> Loss: 0.1915284045867379 \t| Accuracy: 95.000\n",
      "# Iteration  2128 -> Loss: 0.19150221974557496 \t| Accuracy: 95.000\n",
      "# Iteration  2129 -> Loss: 0.1914760664926056 \t| Accuracy: 95.000\n",
      "# Iteration  2130 -> Loss: 0.19144994477498556 \t| Accuracy: 95.000\n",
      "# Iteration  2131 -> Loss: 0.19142385453997945 \t| Accuracy: 95.000\n",
      "# Iteration  2132 -> Loss: 0.19139779573496069 \t| Accuracy: 95.000\n",
      "# Iteration  2133 -> Loss: 0.1913717683074107 \t| Accuracy: 95.000\n",
      "# Iteration  2134 -> Loss: 0.19134577220491925 \t| Accuracy: 95.000\n",
      "# Iteration  2135 -> Loss: 0.1913198073751839 \t| Accuracy: 95.000\n",
      "# Iteration  2136 -> Loss: 0.19129387376600968 \t| Accuracy: 95.000\n",
      "# Iteration  2137 -> Loss: 0.19126797132530884 \t| Accuracy: 95.000\n",
      "# Iteration  2138 -> Loss: 0.19124210000110087 \t| Accuracy: 95.000\n",
      "# Iteration  2139 -> Loss: 0.19121625974151196 \t| Accuracy: 95.000\n",
      "# Iteration  2140 -> Loss: 0.19119045049477462 \t| Accuracy: 95.000\n",
      "# Iteration  2141 -> Loss: 0.1911646722092278 \t| Accuracy: 95.000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Iteration  2142 -> Loss: 0.1911389248333164 \t| Accuracy: 95.000\n",
      "# Iteration  2143 -> Loss: 0.19111320831559103 \t| Accuracy: 95.000\n",
      "# Iteration  2144 -> Loss: 0.1910875226047077 \t| Accuracy: 95.000\n",
      "# Iteration  2145 -> Loss: 0.19106186764942776 \t| Accuracy: 95.000\n",
      "# Iteration  2146 -> Loss: 0.19103624339861738 \t| Accuracy: 95.000\n",
      "# Iteration  2147 -> Loss: 0.19101064980124757 \t| Accuracy: 95.000\n",
      "# Iteration  2148 -> Loss: 0.19098508680639367 \t| Accuracy: 95.000\n",
      "# Iteration  2149 -> Loss: 0.19095955436323528 \t| Accuracy: 95.000\n",
      "# Iteration  2150 -> Loss: 0.19093405242105585 \t| Accuracy: 95.000\n",
      "# Iteration  2151 -> Loss: 0.19090858092924268 \t| Accuracy: 95.000\n",
      "# Iteration  2152 -> Loss: 0.19088313983728622 \t| Accuracy: 95.000\n",
      "# Iteration  2153 -> Loss: 0.1908577290947805 \t| Accuracy: 95.000\n",
      "# Iteration  2154 -> Loss: 0.19083234865142207 \t| Accuracy: 95.000\n",
      "# Iteration  2155 -> Loss: 0.19080699845701043 \t| Accuracy: 95.000\n",
      "# Iteration  2156 -> Loss: 0.19078167846144745 \t| Accuracy: 95.000\n",
      "# Iteration  2157 -> Loss: 0.1907563886147371 \t| Accuracy: 95.000\n",
      "# Iteration  2158 -> Loss: 0.1907311288669853 \t| Accuracy: 95.000\n",
      "# Iteration  2159 -> Loss: 0.1907058991683998 \t| Accuracy: 95.000\n",
      "# Iteration  2160 -> Loss: 0.1906806994692896 \t| Accuracy: 95.000\n",
      "# Iteration  2161 -> Loss: 0.19065552972006508 \t| Accuracy: 95.000\n",
      "# Iteration  2162 -> Loss: 0.19063038987123734 \t| Accuracy: 95.000\n",
      "# Iteration  2163 -> Loss: 0.19060527987341847 \t| Accuracy: 95.000\n",
      "# Iteration  2164 -> Loss: 0.1905801996773207 \t| Accuracy: 95.000\n",
      "# Iteration  2165 -> Loss: 0.1905551492337568 \t| Accuracy: 95.000\n",
      "# Iteration  2166 -> Loss: 0.1905301284936393 \t| Accuracy: 95.000\n",
      "# Iteration  2167 -> Loss: 0.19050513740798047 \t| Accuracy: 95.000\n",
      "# Iteration  2168 -> Loss: 0.19048017592789224 \t| Accuracy: 95.000\n",
      "# Iteration  2169 -> Loss: 0.19045524400458558 \t| Accuracy: 95.000\n",
      "# Iteration  2170 -> Loss: 0.19043034158937064 \t| Accuracy: 95.000\n",
      "# Iteration  2171 -> Loss: 0.19040546863365626 \t| Accuracy: 95.000\n",
      "# Iteration  2172 -> Loss: 0.19038062508894985 \t| Accuracy: 95.000\n",
      "# Iteration  2173 -> Loss: 0.19035581090685702 \t| Accuracy: 95.000\n",
      "# Iteration  2174 -> Loss: 0.19033102603908164 \t| Accuracy: 95.000\n",
      "# Iteration  2175 -> Loss: 0.1903062704374252 \t| Accuracy: 95.000\n",
      "# Iteration  2176 -> Loss: 0.19028154405378697 \t| Accuracy: 95.000\n",
      "# Iteration  2177 -> Loss: 0.1902568468401634 \t| Accuracy: 95.000\n",
      "# Iteration  2178 -> Loss: 0.19023217874864806 \t| Accuracy: 95.000\n",
      "# Iteration  2179 -> Loss: 0.19020753973143165 \t| Accuracy: 95.000\n",
      "# Iteration  2180 -> Loss: 0.19018292974080125 \t| Accuracy: 95.000\n",
      "# Iteration  2181 -> Loss: 0.19015834872914042 \t| Accuracy: 95.000\n",
      "# Iteration  2182 -> Loss: 0.19013379664892904 \t| Accuracy: 95.000\n",
      "# Iteration  2183 -> Loss: 0.19010927345274287 \t| Accuracy: 95.000\n",
      "# Iteration  2184 -> Loss: 0.19008477909325325 \t| Accuracy: 95.000\n",
      "# Iteration  2185 -> Loss: 0.19006031352322728 \t| Accuracy: 95.000\n",
      "# Iteration  2186 -> Loss: 0.19003587669552716 \t| Accuracy: 95.000\n",
      "# Iteration  2187 -> Loss: 0.1900114685631102 \t| Accuracy: 95.000\n",
      "# Iteration  2188 -> Loss: 0.18998708907902842 \t| Accuracy: 95.000\n",
      "# Iteration  2189 -> Loss: 0.1899627381964286 \t| Accuracy: 95.000\n",
      "# Iteration  2190 -> Loss: 0.18993841586855167 \t| Accuracy: 95.000\n",
      "# Iteration  2191 -> Loss: 0.18991412204873287 \t| Accuracy: 95.000\n",
      "# Iteration  2192 -> Loss: 0.18988985669040126 \t| Accuracy: 95.000\n",
      "# Iteration  2193 -> Loss: 0.18986561974707955 \t| Accuracy: 95.000\n",
      "# Iteration  2194 -> Loss: 0.18984141117238398 \t| Accuracy: 95.000\n",
      "# Iteration  2195 -> Loss: 0.18981723092002406 \t| Accuracy: 95.000\n",
      "# Iteration  2196 -> Loss: 0.18979307894380212 \t| Accuracy: 95.000\n",
      "# Iteration  2197 -> Loss: 0.1897689551976135 \t| Accuracy: 95.000\n",
      "# Iteration  2198 -> Loss: 0.189744859635446 \t| Accuracy: 95.000\n",
      "# Iteration  2199 -> Loss: 0.1897207922113797 \t| Accuracy: 95.000\n",
      "# Iteration  2200 -> Loss: 0.18969675287958696 \t| Accuracy: 95.000\n",
      "# Iteration  2201 -> Loss: 0.18967274159433198 \t| Accuracy: 95.000\n",
      "# Iteration  2202 -> Loss: 0.18964875830997058 \t| Accuracy: 95.000\n",
      "# Iteration  2203 -> Loss: 0.18962480298095016 \t| Accuracy: 95.000\n",
      "# Iteration  2204 -> Loss: 0.18960087556180924 \t| Accuracy: 95.000\n",
      "# Iteration  2205 -> Loss: 0.18957697600717746 \t| Accuracy: 95.000\n",
      "# Iteration  2206 -> Loss: 0.1895531042717753 \t| Accuracy: 95.000\n",
      "# Iteration  2207 -> Loss: 0.18952926031041373 \t| Accuracy: 95.000\n",
      "# Iteration  2208 -> Loss: 0.1895054440779943 \t| Accuracy: 95.000\n",
      "# Iteration  2209 -> Loss: 0.1894816555295084 \t| Accuracy: 95.000\n",
      "# Iteration  2210 -> Loss: 0.1894578946200377 \t| Accuracy: 95.000\n",
      "# Iteration  2211 -> Loss: 0.18943416130475352 \t| Accuracy: 95.000\n",
      "# Iteration  2212 -> Loss: 0.18941045553891656 \t| Accuracy: 95.000\n",
      "# Iteration  2213 -> Loss: 0.189386777277877 \t| Accuracy: 95.000\n",
      "# Iteration  2214 -> Loss: 0.189363126477074 \t| Accuracy: 95.000\n",
      "# Iteration  2215 -> Loss: 0.18933950309203584 \t| Accuracy: 95.000\n",
      "# Iteration  2216 -> Loss: 0.1893159070783791 \t| Accuracy: 95.000\n",
      "# Iteration  2217 -> Loss: 0.18929233839180912 \t| Accuracy: 95.000\n",
      "# Iteration  2218 -> Loss: 0.18926879698811944 \t| Accuracy: 95.000\n",
      "# Iteration  2219 -> Loss: 0.1892452828231916 \t| Accuracy: 95.000\n",
      "# Iteration  2220 -> Loss: 0.1892217958529951 \t| Accuracy: 95.000\n",
      "# Iteration  2221 -> Loss: 0.18919833603358685 \t| Accuracy: 95.000\n",
      "# Iteration  2222 -> Loss: 0.18917490332111134 \t| Accuracy: 95.000\n",
      "# Iteration  2223 -> Loss: 0.18915149767180034 \t| Accuracy: 95.000\n",
      "# Iteration  2224 -> Loss: 0.18912811904197244 \t| Accuracy: 95.000\n",
      "# Iteration  2225 -> Loss: 0.1891047673880332 \t| Accuracy: 95.000\n",
      "# Iteration  2226 -> Loss: 0.18908144266647475 \t| Accuracy: 95.000\n",
      "# Iteration  2227 -> Loss: 0.18905814483387548 \t| Accuracy: 95.000\n",
      "# Iteration  2228 -> Loss: 0.18903487384690024 \t| Accuracy: 95.000\n",
      "# Iteration  2229 -> Loss: 0.1890116296622996 \t| Accuracy: 95.000\n",
      "# Iteration  2230 -> Loss: 0.18898841223691001 \t| Accuracy: 95.000\n",
      "# Iteration  2231 -> Loss: 0.18896522152765355 \t| Accuracy: 95.000\n",
      "# Iteration  2232 -> Loss: 0.18894205749153764 \t| Accuracy: 95.000\n",
      "# Iteration  2233 -> Loss: 0.1889189200856549 \t| Accuracy: 95.000\n",
      "# Iteration  2234 -> Loss: 0.18889580926718286 \t| Accuracy: 95.000\n",
      "# Iteration  2235 -> Loss: 0.188872724993384 \t| Accuracy: 95.000\n",
      "# Iteration  2236 -> Loss: 0.1888496672216051 \t| Accuracy: 95.000\n",
      "# Iteration  2237 -> Loss: 0.1888266359092776 \t| Accuracy: 95.000\n",
      "# Iteration  2238 -> Loss: 0.18880363101391695 \t| Accuracy: 95.000\n",
      "# Iteration  2239 -> Loss: 0.18878065249312265 \t| Accuracy: 95.000\n",
      "# Iteration  2240 -> Loss: 0.18875770030457795 \t| Accuracy: 95.000\n",
      "# Iteration  2241 -> Loss: 0.1887347744060497 \t| Accuracy: 95.000\n",
      "# Iteration  2242 -> Loss: 0.18871187475538817 \t| Accuracy: 95.000\n",
      "# Iteration  2243 -> Loss: 0.18868900131052674 \t| Accuracy: 95.000\n",
      "# Iteration  2244 -> Loss: 0.1886661540294819 \t| Accuracy: 95.000\n",
      "# Iteration  2245 -> Loss: 0.18864333287035287 \t| Accuracy: 95.000\n",
      "# Iteration  2246 -> Loss: 0.1886205377913215 \t| Accuracy: 95.000\n",
      "# Iteration  2247 -> Loss: 0.1885977687506521 \t| Accuracy: 95.000\n",
      "# Iteration  2248 -> Loss: 0.18857502570669113 \t| Accuracy: 95.000\n",
      "# Iteration  2249 -> Loss: 0.1885523086178671 \t| Accuracy: 95.000\n",
      "# Iteration  2250 -> Loss: 0.18852961744269037 \t| Accuracy: 95.000\n",
      "# Iteration  2251 -> Loss: 0.18850695213975296 \t| Accuracy: 95.000\n",
      "# Iteration  2252 -> Loss: 0.18848431266772833 \t| Accuracy: 95.000\n",
      "# Iteration  2253 -> Loss: 0.1884616989853712 \t| Accuracy: 95.000\n",
      "# Iteration  2254 -> Loss: 0.1884391110515174 \t| Accuracy: 95.000\n",
      "# Iteration  2255 -> Loss: 0.18841654882508352 \t| Accuracy: 95.000\n",
      "# Iteration  2256 -> Loss: 0.18839401226506702 \t| Accuracy: 95.000\n",
      "# Iteration  2257 -> Loss: 0.18837150133054575 \t| Accuracy: 95.000\n",
      "# Iteration  2258 -> Loss: 0.18834901598067788 \t| Accuracy: 95.000\n",
      "# Iteration  2259 -> Loss: 0.18832655617470184 \t| Accuracy: 95.000\n",
      "# Iteration  2260 -> Loss: 0.18830412187193574 \t| Accuracy: 95.000\n",
      "# Iteration  2261 -> Loss: 0.18828171303177776 \t| Accuracy: 95.000\n",
      "# Iteration  2262 -> Loss: 0.1882593296137054 \t| Accuracy: 95.000\n",
      "# Iteration  2263 -> Loss: 0.18823697157727573 \t| Accuracy: 95.000\n",
      "# Iteration  2264 -> Loss: 0.1882146388821248 \t| Accuracy: 95.000\n",
      "# Iteration  2265 -> Loss: 0.18819233148796796 \t| Accuracy: 95.000\n",
      "# Iteration  2266 -> Loss: 0.18817004935459908 \t| Accuracy: 95.000\n",
      "# Iteration  2267 -> Loss: 0.1881477924418909 \t| Accuracy: 95.000\n",
      "# Iteration  2268 -> Loss: 0.18812556070979453 \t| Accuracy: 95.000\n",
      "# Iteration  2269 -> Loss: 0.18810335411833948 \t| Accuracy: 95.000\n",
      "# Iteration  2270 -> Loss: 0.18808117262763313 \t| Accuracy: 95.000\n",
      "# Iteration  2271 -> Loss: 0.18805901619786095 \t| Accuracy: 95.000\n",
      "# Iteration  2272 -> Loss: 0.18803688478928618 \t| Accuracy: 95.000\n",
      "# Iteration  2273 -> Loss: 0.18801477836224942 \t| Accuracy: 95.000\n",
      "# Iteration  2274 -> Loss: 0.18799269687716882 \t| Accuracy: 95.000\n",
      "# Iteration  2275 -> Loss: 0.18797064029453966 \t| Accuracy: 95.000\n",
      "# Iteration  2276 -> Loss: 0.18794860857493423 \t| Accuracy: 95.000\n",
      "# Iteration  2277 -> Loss: 0.18792660167900171 \t| Accuracy: 95.000\n",
      "# Iteration  2278 -> Loss: 0.18790461956746776 \t| Accuracy: 95.000\n",
      "# Iteration  2279 -> Loss: 0.1878826622011348 \t| Accuracy: 95.000\n",
      "# Iteration  2280 -> Loss: 0.18786072954088137 \t| Accuracy: 95.000\n",
      "# Iteration  2281 -> Loss: 0.1878388215476621 \t| Accuracy: 95.000\n",
      "# Iteration  2282 -> Loss: 0.18781693818250772 \t| Accuracy: 95.000\n",
      "# Iteration  2283 -> Loss: 0.18779507940652457 \t| Accuracy: 95.000\n",
      "# Iteration  2284 -> Loss: 0.18777324518089475 \t| Accuracy: 95.000\n",
      "# Iteration  2285 -> Loss: 0.18775143546687564 \t| Accuracy: 95.000\n",
      "# Iteration  2286 -> Loss: 0.18772965022579996 \t| Accuracy: 95.000\n",
      "# Iteration  2287 -> Loss: 0.18770788941907549 \t| Accuracy: 95.000\n",
      "# Iteration  2288 -> Loss: 0.18768615300818486 \t| Accuracy: 95.000\n",
      "# Iteration  2289 -> Loss: 0.1876644409546854 \t| Accuracy: 95.000\n",
      "# Iteration  2290 -> Loss: 0.18764275322020915 \t| Accuracy: 95.000\n",
      "# Iteration  2291 -> Loss: 0.18762108976646238 \t| Accuracy: 95.000\n",
      "# Iteration  2292 -> Loss: 0.18759945055522564 \t| Accuracy: 95.000\n",
      "# Iteration  2293 -> Loss: 0.18757783554835347 \t| Accuracy: 95.000\n",
      "# Iteration  2294 -> Loss: 0.18755624470777432 \t| Accuracy: 95.000\n",
      "# Iteration  2295 -> Loss: 0.18753467799549034 \t| Accuracy: 95.000\n",
      "# Iteration  2296 -> Loss: 0.18751313537357722 \t| Accuracy: 95.000\n",
      "# Iteration  2297 -> Loss: 0.18749161680418386 \t| Accuracy: 95.000\n",
      "# Iteration  2298 -> Loss: 0.18747012224953258 \t| Accuracy: 95.000\n",
      "# Iteration  2299 -> Loss: 0.18744865167191857 \t| Accuracy: 95.000\n",
      "# Iteration  2300 -> Loss: 0.18742720503370988 \t| Accuracy: 95.000\n",
      "# Iteration  2301 -> Loss: 0.1874057822973473 \t| Accuracy: 95.000\n",
      "# Iteration  2302 -> Loss: 0.18738438342534405 \t| Accuracy: 95.000\n",
      "# Iteration  2303 -> Loss: 0.18736300838028583 \t| Accuracy: 95.000\n",
      "# Iteration  2304 -> Loss: 0.1873416571248304 \t| Accuracy: 95.000\n",
      "# Iteration  2305 -> Loss: 0.18732032962170753 \t| Accuracy: 95.000\n",
      "# Iteration  2306 -> Loss: 0.18729902583371902 \t| Accuracy: 95.000\n",
      "# Iteration  2307 -> Loss: 0.18727774572373812 \t| Accuracy: 95.000\n",
      "# Iteration  2308 -> Loss: 0.18725648925470975 \t| Accuracy: 95.000\n",
      "# Iteration  2309 -> Loss: 0.18723525638965016 \t| Accuracy: 95.000\n",
      "# Iteration  2310 -> Loss: 0.18721404709164677 \t| Accuracy: 95.000\n",
      "# Iteration  2311 -> Loss: 0.18719286132385798 \t| Accuracy: 95.000\n",
      "# Iteration  2312 -> Loss: 0.1871716990495132 \t| Accuracy: 95.000\n",
      "# Iteration  2313 -> Loss: 0.18715056023191243 \t| Accuracy: 95.000\n",
      "# Iteration  2314 -> Loss: 0.1871294448344262 \t| Accuracy: 95.000\n",
      "# Iteration  2315 -> Loss: 0.1871083528204955 \t| Accuracy: 95.000\n",
      "# Iteration  2316 -> Loss: 0.1870872841536315 \t| Accuracy: 95.000\n",
      "# Iteration  2317 -> Loss: 0.1870662387974154 \t| Accuracy: 95.000\n",
      "# Iteration  2318 -> Loss: 0.1870452167154984 \t| Accuracy: 95.000\n",
      "# Iteration  2319 -> Loss: 0.18702421787160126 \t| Accuracy: 95.000\n",
      "# Iteration  2320 -> Loss: 0.18700324222951448 \t| Accuracy: 95.000\n",
      "# Iteration  2321 -> Loss: 0.1869822897530979 \t| Accuracy: 95.000\n",
      "# Iteration  2322 -> Loss: 0.1869613604062806 \t| Accuracy: 95.000\n",
      "# Iteration  2323 -> Loss: 0.1869404541530609 \t| Accuracy: 95.000\n",
      "# Iteration  2324 -> Loss: 0.1869195709575059 \t| Accuracy: 95.000\n",
      "# Iteration  2325 -> Loss: 0.18689871078375153 \t| Accuracy: 95.000\n",
      "# Iteration  2326 -> Loss: 0.1868778735960024 \t| Accuracy: 95.000\n",
      "# Iteration  2327 -> Loss: 0.1868570593585316 \t| Accuracy: 95.000\n",
      "# Iteration  2328 -> Loss: 0.18683626803568043 \t| Accuracy: 95.000\n",
      "# Iteration  2329 -> Loss: 0.18681549959185842 \t| Accuracy: 95.000\n",
      "# Iteration  2330 -> Loss: 0.1867947539915432 \t| Accuracy: 95.000\n",
      "# Iteration  2331 -> Loss: 0.18677403119928002 \t| Accuracy: 95.000\n",
      "# Iteration  2332 -> Loss: 0.18675333117968204 \t| Accuracy: 95.000\n",
      "# Iteration  2333 -> Loss: 0.18673265389742985 \t| Accuracy: 95.000\n",
      "# Iteration  2334 -> Loss: 0.1867119993172715 \t| Accuracy: 95.000\n",
      "# Iteration  2335 -> Loss: 0.18669136740402217 \t| Accuracy: 95.000\n",
      "# Iteration  2336 -> Loss: 0.18667075812256423 \t| Accuracy: 95.000\n",
      "# Iteration  2337 -> Loss: 0.18665017143784693 \t| Accuracy: 95.000\n",
      "# Iteration  2338 -> Loss: 0.1866296073148863 \t| Accuracy: 95.000\n",
      "# Iteration  2339 -> Loss: 0.18660906571876487 \t| Accuracy: 95.000\n",
      "# Iteration  2340 -> Loss: 0.1865885466146319 \t| Accuracy: 95.000\n",
      "# Iteration  2341 -> Loss: 0.1865680499677028 \t| Accuracy: 95.000\n",
      "# Iteration  2342 -> Loss: 0.1865475757432592 \t| Accuracy: 95.000\n",
      "# Iteration  2343 -> Loss: 0.18652712390664866 \t| Accuracy: 95.000\n",
      "# Iteration  2344 -> Loss: 0.18650669442328477 \t| Accuracy: 95.000\n",
      "# Iteration  2345 -> Loss: 0.18648628725864672 \t| Accuracy: 95.000\n",
      "# Iteration  2346 -> Loss: 0.18646590237827929 \t| Accuracy: 95.000\n",
      "# Iteration  2347 -> Loss: 0.1864455397477927 \t| Accuracy: 95.000\n",
      "# Iteration  2348 -> Loss: 0.18642519933286253 \t| Accuracy: 95.000\n",
      "# Iteration  2349 -> Loss: 0.18640488109922934 \t| Accuracy: 95.000\n",
      "# Iteration  2350 -> Loss: 0.1863845850126987 \t| Accuracy: 95.000\n",
      "# Iteration  2351 -> Loss: 0.18636431103914117 \t| Accuracy: 95.000\n",
      "# Iteration  2352 -> Loss: 0.18634405914449173 \t| Accuracy: 95.000\n",
      "# Iteration  2353 -> Loss: 0.18632382929475005 \t| Accuracy: 95.000\n",
      "# Iteration  2354 -> Loss: 0.1863036214559803 \t| Accuracy: 95.000\n",
      "# Iteration  2355 -> Loss: 0.18628343559431074 \t| Accuracy: 95.000\n",
      "# Iteration  2356 -> Loss: 0.1862632716759337 \t| Accuracy: 95.000\n",
      "# Iteration  2357 -> Loss: 0.18624312966710566 \t| Accuracy: 95.000\n",
      "# Iteration  2358 -> Loss: 0.18622300953414667 \t| Accuracy: 95.000\n",
      "# Iteration  2359 -> Loss: 0.18620291124344068 \t| Accuracy: 95.000\n",
      "# Iteration  2360 -> Loss: 0.18618283476143502 \t| Accuracy: 95.000\n",
      "# Iteration  2361 -> Loss: 0.18616278005464043 \t| Accuracy: 95.000\n",
      "# Iteration  2362 -> Loss: 0.1861427470896309 \t| Accuracy: 95.000\n",
      "# Iteration  2363 -> Loss: 0.18612273583304356 \t| Accuracy: 95.000\n",
      "# Iteration  2364 -> Loss: 0.1861027462515784 \t| Accuracy: 95.000\n",
      "# Iteration  2365 -> Loss: 0.1860827783119983 \t| Accuracy: 95.000\n",
      "# Iteration  2366 -> Loss: 0.18606283198112875 \t| Accuracy: 95.000\n",
      "# Iteration  2367 -> Loss: 0.1860429072258579 \t| Accuracy: 95.000\n",
      "# Iteration  2368 -> Loss: 0.186023004013136 \t| Accuracy: 95.000\n",
      "# Iteration  2369 -> Loss: 0.1860031223099759 \t| Accuracy: 95.000\n",
      "# Iteration  2370 -> Loss: 0.18598326208345234 \t| Accuracy: 95.000\n",
      "# Iteration  2371 -> Loss: 0.1859634233007021 \t| Accuracy: 95.000\n",
      "# Iteration  2372 -> Loss: 0.18594360592892378 \t| Accuracy: 95.000\n",
      "# Iteration  2373 -> Loss: 0.18592380993537763 \t| Accuracy: 95.000\n",
      "# Iteration  2374 -> Loss: 0.18590403528738544 \t| Accuracy: 95.000\n",
      "# Iteration  2375 -> Loss: 0.18588428195233056 \t| Accuracy: 95.000\n",
      "# Iteration  2376 -> Loss: 0.1858645498976575 \t| Accuracy: 95.000\n",
      "# Iteration  2377 -> Loss: 0.18584483909087182 \t| Accuracy: 95.000\n",
      "# Iteration  2378 -> Loss: 0.1858251494995402 \t| Accuracy: 95.000\n",
      "# Iteration  2379 -> Loss: 0.18580548109129025 \t| Accuracy: 95.000\n",
      "# Iteration  2380 -> Loss: 0.18578583383381012 \t| Accuracy: 95.000\n",
      "# Iteration  2381 -> Loss: 0.18576620769484872 \t| Accuracy: 95.000\n",
      "# Iteration  2382 -> Loss: 0.18574660264221532 \t| Accuracy: 95.000\n",
      "# Iteration  2383 -> Loss: 0.18572701864377947 \t| Accuracy: 95.000\n",
      "# Iteration  2384 -> Loss: 0.18570745566747107 \t| Accuracy: 95.000\n",
      "# Iteration  2385 -> Loss: 0.1856879136812798 \t| Accuracy: 95.000\n",
      "# Iteration  2386 -> Loss: 0.18566839265325558 \t| Accuracy: 95.000\n",
      "# Iteration  2387 -> Loss: 0.1856488925515079 \t| Accuracy: 95.000\n",
      "# Iteration  2388 -> Loss: 0.18562941334420593 \t| Accuracy: 95.000\n",
      "# Iteration  2389 -> Loss: 0.18560995499957836 \t| Accuracy: 95.000\n",
      "# Iteration  2390 -> Loss: 0.1855905174859133 \t| Accuracy: 95.000\n",
      "# Iteration  2391 -> Loss: 0.18557110077155803 \t| Accuracy: 95.000\n",
      "# Iteration  2392 -> Loss: 0.18555170482491903 \t| Accuracy: 95.000\n",
      "# Iteration  2393 -> Loss: 0.1855323296144617 \t| Accuracy: 95.000\n",
      "# Iteration  2394 -> Loss: 0.1855129751087103 \t| Accuracy: 95.000\n",
      "# Iteration  2395 -> Loss: 0.18549364127624787 \t| Accuracy: 95.000\n",
      "# Iteration  2396 -> Loss: 0.185474328085716 \t| Accuracy: 95.000\n",
      "# Iteration  2397 -> Loss: 0.18545503550581466 \t| Accuracy: 95.000\n",
      "# Iteration  2398 -> Loss: 0.18543576350530233 \t| Accuracy: 95.000\n",
      "# Iteration  2399 -> Loss: 0.1854165120529955 \t| Accuracy: 95.000\n",
      "# Iteration  2400 -> Loss: 0.1853972811177688 \t| Accuracy: 95.000\n",
      "# Iteration  2401 -> Loss: 0.18537807066855483 \t| Accuracy: 95.000\n",
      "# Iteration  2402 -> Loss: 0.18535888067434403 \t| Accuracy: 95.000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Iteration  2403 -> Loss: 0.18533971110418435 \t| Accuracy: 95.000\n",
      "# Iteration  2404 -> Loss: 0.18532056192718147 \t| Accuracy: 95.000\n",
      "# Iteration  2405 -> Loss: 0.18530143311249844 \t| Accuracy: 95.000\n",
      "# Iteration  2406 -> Loss: 0.18528232462935565 \t| Accuracy: 95.000\n",
      "# Iteration  2407 -> Loss: 0.1852632364470305 \t| Accuracy: 95.000\n",
      "# Iteration  2408 -> Loss: 0.18524416853485762 \t| Accuracy: 95.000\n",
      "# Iteration  2409 -> Loss: 0.18522512086222842 \t| Accuracy: 95.000\n",
      "# Iteration  2410 -> Loss: 0.18520609339859123 \t| Accuracy: 95.000\n",
      "# Iteration  2411 -> Loss: 0.18518708611345092 \t| Accuracy: 95.000\n",
      "# Iteration  2412 -> Loss: 0.18516809897636893 \t| Accuracy: 95.000\n",
      "# Iteration  2413 -> Loss: 0.18514913195696317 \t| Accuracy: 95.000\n",
      "# Iteration  2414 -> Loss: 0.18513018502490777 \t| Accuracy: 95.000\n",
      "# Iteration  2415 -> Loss: 0.1851112581499331 \t| Accuracy: 95.000\n",
      "# Iteration  2416 -> Loss: 0.18509235130182544 \t| Accuracy: 95.000\n",
      "# Iteration  2417 -> Loss: 0.18507346445042708 \t| Accuracy: 95.000\n",
      "# Iteration  2418 -> Loss: 0.18505459756563608 \t| Accuracy: 95.000\n",
      "# Iteration  2419 -> Loss: 0.1850357506174062 \t| Accuracy: 95.000\n",
      "# Iteration  2420 -> Loss: 0.18501692357574664 \t| Accuracy: 95.000\n",
      "# Iteration  2421 -> Loss: 0.18499811641072217 \t| Accuracy: 95.000\n",
      "# Iteration  2422 -> Loss: 0.18497932909245265 \t| Accuracy: 95.000\n",
      "# Iteration  2423 -> Loss: 0.1849605615911133 \t| Accuracy: 95.000\n",
      "# Iteration  2424 -> Loss: 0.1849418138769343 \t| Accuracy: 95.000\n",
      "# Iteration  2425 -> Loss: 0.18492308592020087 \t| Accuracy: 95.000\n",
      "# Iteration  2426 -> Loss: 0.18490437769125287 \t| Accuracy: 95.000\n",
      "# Iteration  2427 -> Loss: 0.18488568916048495 \t| Accuracy: 95.000\n",
      "# Iteration  2428 -> Loss: 0.1848670202983463 \t| Accuracy: 95.000\n",
      "# Iteration  2429 -> Loss: 0.18484837107534066 \t| Accuracy: 95.000\n",
      "# Iteration  2430 -> Loss: 0.18482974146202583 \t| Accuracy: 95.000\n",
      "# Iteration  2431 -> Loss: 0.18481113142901415 \t| Accuracy: 95.000\n",
      "# Iteration  2432 -> Loss: 0.18479254094697187 \t| Accuracy: 95.000\n",
      "# Iteration  2433 -> Loss: 0.1847739699866191 \t| Accuracy: 95.000\n",
      "# Iteration  2434 -> Loss: 0.18475541851873 \t| Accuracy: 95.000\n",
      "# Iteration  2435 -> Loss: 0.18473688651413248 \t| Accuracy: 95.000\n",
      "# Iteration  2436 -> Loss: 0.1847183739437078 \t| Accuracy: 95.000\n",
      "# Iteration  2437 -> Loss: 0.184699880778391 \t| Accuracy: 95.000\n",
      "# Iteration  2438 -> Loss: 0.18468140698917038 \t| Accuracy: 95.000\n",
      "# Iteration  2439 -> Loss: 0.1846629525470874 \t| Accuracy: 95.000\n",
      "# Iteration  2440 -> Loss: 0.18464451742323681 \t| Accuracy: 95.000\n",
      "# Iteration  2441 -> Loss: 0.18462610158876636 \t| Accuracy: 95.000\n",
      "# Iteration  2442 -> Loss: 0.18460770501487658 \t| Accuracy: 95.000\n",
      "# Iteration  2443 -> Loss: 0.18458932767282096 \t| Accuracy: 95.000\n",
      "# Iteration  2444 -> Loss: 0.18457096953390556 \t| Accuracy: 95.000\n",
      "# Iteration  2445 -> Loss: 0.18455263056948915 \t| Accuracy: 95.000\n",
      "# Iteration  2446 -> Loss: 0.18453431075098267 \t| Accuracy: 95.000\n",
      "# Iteration  2447 -> Loss: 0.18451601004984966 \t| Accuracy: 95.000\n",
      "# Iteration  2448 -> Loss: 0.1844977284376057 \t| Accuracy: 95.000\n",
      "# Iteration  2449 -> Loss: 0.18447946588581857 \t| Accuracy: 95.000\n",
      "# Iteration  2450 -> Loss: 0.18446122236610793 \t| Accuracy: 95.000\n",
      "# Iteration  2451 -> Loss: 0.18444299785014542 \t| Accuracy: 95.000\n",
      "# Iteration  2452 -> Loss: 0.1844247923096544 \t| Accuracy: 95.000\n",
      "# Iteration  2453 -> Loss: 0.18440660571640988 \t| Accuracy: 95.000\n",
      "# Iteration  2454 -> Loss: 0.18438843804223837 \t| Accuracy: 95.000\n",
      "# Iteration  2455 -> Loss: 0.18437028925901786 \t| Accuracy: 95.000\n",
      "# Iteration  2456 -> Loss: 0.1843521593386775 \t| Accuracy: 95.000\n",
      "# Iteration  2457 -> Loss: 0.18433404825319782 \t| Accuracy: 95.000\n",
      "# Iteration  2458 -> Loss: 0.18431595597461037 \t| Accuracy: 95.000\n",
      "# Iteration  2459 -> Loss: 0.18429788247499757 \t| Accuracy: 95.000\n",
      "# Iteration  2460 -> Loss: 0.18427982772649287 \t| Accuracy: 95.000\n",
      "# Iteration  2461 -> Loss: 0.1842617917012803 \t| Accuracy: 95.000\n",
      "# Iteration  2462 -> Loss: 0.18424377437159462 \t| Accuracy: 95.000\n",
      "# Iteration  2463 -> Loss: 0.18422577570972104 \t| Accuracy: 95.000\n",
      "# Iteration  2464 -> Loss: 0.18420779568799533 \t| Accuracy: 95.000\n",
      "# Iteration  2465 -> Loss: 0.18418983427880348 \t| Accuracy: 95.000\n",
      "# Iteration  2466 -> Loss: 0.18417189145458165 \t| Accuracy: 95.000\n",
      "# Iteration  2467 -> Loss: 0.184153967187816 \t| Accuracy: 95.000\n",
      "# Iteration  2468 -> Loss: 0.18413606145104292 \t| Accuracy: 95.000\n",
      "# Iteration  2469 -> Loss: 0.18411817421684848 \t| Accuracy: 95.000\n",
      "# Iteration  2470 -> Loss: 0.18410030545786846 \t| Accuracy: 95.000\n",
      "# Iteration  2471 -> Loss: 0.18408245514678848 \t| Accuracy: 95.000\n",
      "# Iteration  2472 -> Loss: 0.1840646232563436 \t| Accuracy: 95.000\n",
      "# Iteration  2473 -> Loss: 0.18404680975931828 \t| Accuracy: 95.000\n",
      "# Iteration  2474 -> Loss: 0.18402901462854632 \t| Accuracy: 95.000\n",
      "# Iteration  2475 -> Loss: 0.1840112378369108 \t| Accuracy: 95.000\n",
      "# Iteration  2476 -> Loss: 0.18399347935734386 \t| Accuracy: 95.000\n",
      "# Iteration  2477 -> Loss: 0.18397573916282667 \t| Accuracy: 95.000\n",
      "# Iteration  2478 -> Loss: 0.18395801722638924 \t| Accuracy: 95.000\n",
      "# Iteration  2479 -> Loss: 0.1839403135211105 \t| Accuracy: 95.000\n",
      "# Iteration  2480 -> Loss: 0.18392262802011797 \t| Accuracy: 95.000\n",
      "# Iteration  2481 -> Loss: 0.1839049606965877 \t| Accuracy: 95.000\n",
      "# Iteration  2482 -> Loss: 0.18388731152374438 \t| Accuracy: 95.000\n",
      "# Iteration  2483 -> Loss: 0.18386968047486085 \t| Accuracy: 95.000\n",
      "# Iteration  2484 -> Loss: 0.18385206752325847 \t| Accuracy: 95.000\n",
      "# Iteration  2485 -> Loss: 0.18383447264230654 \t| Accuracy: 95.000\n",
      "# Iteration  2486 -> Loss: 0.18381689580542251 \t| Accuracy: 95.000\n",
      "# Iteration  2487 -> Loss: 0.18379933698607182 \t| Accuracy: 95.000\n",
      "# Iteration  2488 -> Loss: 0.18378179615776766 \t| Accuracy: 95.000\n",
      "# Iteration  2489 -> Loss: 0.18376427329407108 \t| Accuracy: 95.000\n",
      "# Iteration  2490 -> Loss: 0.18374676836859066 \t| Accuracy: 95.000\n",
      "# Iteration  2491 -> Loss: 0.18372928135498257 \t| Accuracy: 95.000\n",
      "# Iteration  2492 -> Loss: 0.18371181222695043 \t| Accuracy: 95.000\n",
      "# Iteration  2493 -> Loss: 0.18369436095824518 \t| Accuracy: 95.000\n",
      "# Iteration  2494 -> Loss: 0.18367692752266493 \t| Accuracy: 95.000\n",
      "# Iteration  2495 -> Loss: 0.18365951189405502 \t| Accuracy: 95.000\n",
      "# Iteration  2496 -> Loss: 0.18364211404630773 \t| Accuracy: 95.000\n",
      "# Iteration  2497 -> Loss: 0.1836247339533623 \t| Accuracy: 95.000\n",
      "# Iteration  2498 -> Loss: 0.18360737158920487 \t| Accuracy: 95.000\n",
      "# Iteration  2499 -> Loss: 0.18359002692786805 \t| Accuracy: 95.000\n",
      "# Iteration  2500 -> Loss: 0.18357269994343137 \t| Accuracy: 95.000\n",
      "# Iteration  2501 -> Loss: 0.18355539061002069 \t| Accuracy: 95.000\n",
      "# Iteration  2502 -> Loss: 0.18353809890180842 \t| Accuracy: 95.000\n",
      "# Iteration  2503 -> Loss: 0.18352082479301318 \t| Accuracy: 95.000\n",
      "# Iteration  2504 -> Loss: 0.18350356825789982 \t| Accuracy: 95.000\n",
      "# Iteration  2505 -> Loss: 0.18348632927077937 \t| Accuracy: 95.000\n",
      "# Iteration  2506 -> Loss: 0.1834691078060088 \t| Accuracy: 95.000\n",
      "# Iteration  2507 -> Loss: 0.18345190383799112 \t| Accuracy: 95.000\n",
      "# Iteration  2508 -> Loss: 0.18343471734117506 \t| Accuracy: 95.000\n",
      "# Iteration  2509 -> Loss: 0.18341754829005508 \t| Accuracy: 95.000\n",
      "# Iteration  2510 -> Loss: 0.1834003966591713 \t| Accuracy: 95.000\n",
      "# Iteration  2511 -> Loss: 0.18338326242310932 \t| Accuracy: 95.000\n",
      "# Iteration  2512 -> Loss: 0.18336614555650024 \t| Accuracy: 95.000\n",
      "# Iteration  2513 -> Loss: 0.18334904603402044 \t| Accuracy: 95.000\n",
      "# Iteration  2514 -> Loss: 0.18333196383039163 \t| Accuracy: 95.000\n",
      "# Iteration  2515 -> Loss: 0.18331489892038047 \t| Accuracy: 95.000\n",
      "# Iteration  2516 -> Loss: 0.1832978512787987 \t| Accuracy: 95.000\n",
      "# Iteration  2517 -> Loss: 0.1832808208805032 \t| Accuracy: 95.000\n",
      "# Iteration  2518 -> Loss: 0.18326380770039552 \t| Accuracy: 95.000\n",
      "# Iteration  2519 -> Loss: 0.18324681171342194 \t| Accuracy: 95.000\n",
      "# Iteration  2520 -> Loss: 0.18322983289457354 \t| Accuracy: 95.000\n",
      "# Iteration  2521 -> Loss: 0.1832128712188858 \t| Accuracy: 95.000\n",
      "# Iteration  2522 -> Loss: 0.1831959266614388 \t| Accuracy: 95.000\n",
      "# Iteration  2523 -> Loss: 0.1831789991973568 \t| Accuracy: 95.000\n",
      "# Iteration  2524 -> Loss: 0.1831620888018086 \t| Accuracy: 95.000\n",
      "# Iteration  2525 -> Loss: 0.18314519545000696 \t| Accuracy: 95.000\n",
      "# Iteration  2526 -> Loss: 0.1831283191172087 \t| Accuracy: 95.000\n",
      "# Iteration  2527 -> Loss: 0.18311145977871485 \t| Accuracy: 95.000\n",
      "# Iteration  2528 -> Loss: 0.18309461740987013 \t| Accuracy: 95.000\n",
      "# Iteration  2529 -> Loss: 0.18307779198606317 \t| Accuracy: 95.000\n",
      "# Iteration  2530 -> Loss: 0.18306098348272618 \t| Accuracy: 95.000\n",
      "# Iteration  2531 -> Loss: 0.18304419187533516 \t| Accuracy: 95.000\n",
      "# Iteration  2532 -> Loss: 0.18302741713940948 \t| Accuracy: 95.000\n",
      "# Iteration  2533 -> Loss: 0.18301065925051205 \t| Accuracy: 95.000\n",
      "# Iteration  2534 -> Loss: 0.18299391818424898 \t| Accuracy: 95.000\n",
      "# Iteration  2535 -> Loss: 0.18297719391626976 \t| Accuracy: 95.000\n",
      "# Iteration  2536 -> Loss: 0.1829604864222669 \t| Accuracy: 95.000\n",
      "# Iteration  2537 -> Loss: 0.1829437956779761 \t| Accuracy: 95.000\n",
      "# Iteration  2538 -> Loss: 0.18292712165917588 \t| Accuracy: 95.000\n",
      "# Iteration  2539 -> Loss: 0.18291046434168776 \t| Accuracy: 95.000\n",
      "# Iteration  2540 -> Loss: 0.18289382370137594 \t| Accuracy: 95.000\n",
      "# Iteration  2541 -> Loss: 0.18287719971414737 \t| Accuracy: 95.000\n",
      "# Iteration  2542 -> Loss: 0.1828605923559516 \t| Accuracy: 95.000\n",
      "# Iteration  2543 -> Loss: 0.18284400160278066 \t| Accuracy: 95.000\n",
      "# Iteration  2544 -> Loss: 0.18282742743066904 \t| Accuracy: 95.000\n",
      "# Iteration  2545 -> Loss: 0.18281086981569347 \t| Accuracy: 95.000\n",
      "# Iteration  2546 -> Loss: 0.182794328733973 \t| Accuracy: 95.000\n",
      "# Iteration  2547 -> Loss: 0.18277780416166878 \t| Accuracy: 95.000\n",
      "# Iteration  2548 -> Loss: 0.18276129607498415 \t| Accuracy: 95.000\n",
      "# Iteration  2549 -> Loss: 0.1827448044501641 \t| Accuracy: 95.000\n",
      "# Iteration  2550 -> Loss: 0.1827283292634959 \t| Accuracy: 95.000\n",
      "# Iteration  2551 -> Loss: 0.18271187049130821 \t| Accuracy: 95.000\n",
      "# Iteration  2552 -> Loss: 0.18269542810997175 \t| Accuracy: 95.000\n",
      "# Iteration  2553 -> Loss: 0.18267900209589868 \t| Accuracy: 95.000\n",
      "# Iteration  2554 -> Loss: 0.18266259242554264 \t| Accuracy: 95.000\n",
      "# Iteration  2555 -> Loss: 0.18264619907539875 \t| Accuracy: 95.000\n",
      "# Iteration  2556 -> Loss: 0.18262982202200356 \t| Accuracy: 95.000\n",
      "# Iteration  2557 -> Loss: 0.18261346124193473 \t| Accuracy: 95.000\n",
      "# Iteration  2558 -> Loss: 0.18259711671181122 \t| Accuracy: 95.000\n",
      "# Iteration  2559 -> Loss: 0.182580788408293 \t| Accuracy: 95.000\n",
      "# Iteration  2560 -> Loss: 0.1825644763080811 \t| Accuracy: 95.000\n",
      "# Iteration  2561 -> Loss: 0.1825481803879173 \t| Accuracy: 95.000\n",
      "# Iteration  2562 -> Loss: 0.18253190062458458 \t| Accuracy: 95.000\n",
      "# Iteration  2563 -> Loss: 0.18251563699490614 \t| Accuracy: 95.000\n",
      "# Iteration  2564 -> Loss: 0.1824993894757462 \t| Accuracy: 95.000\n",
      "# Iteration  2565 -> Loss: 0.18248315804400944 \t| Accuracy: 95.000\n",
      "# Iteration  2566 -> Loss: 0.18246694267664101 \t| Accuracy: 95.000\n",
      "# Iteration  2567 -> Loss: 0.1824507433506265 \t| Accuracy: 95.000\n",
      "# Iteration  2568 -> Loss: 0.18243456004299166 \t| Accuracy: 95.000\n",
      "# Iteration  2569 -> Loss: 0.18241839273080268 \t| Accuracy: 95.000\n",
      "# Iteration  2570 -> Loss: 0.18240224139116576 \t| Accuracy: 95.000\n",
      "# Iteration  2571 -> Loss: 0.1823861060012271 \t| Accuracy: 95.000\n",
      "# Iteration  2572 -> Loss: 0.18236998653817302 \t| Accuracy: 95.000\n",
      "# Iteration  2573 -> Loss: 0.1823538829792296 \t| Accuracy: 95.000\n",
      "# Iteration  2574 -> Loss: 0.18233779530166275 \t| Accuracy: 95.000\n",
      "# Iteration  2575 -> Loss: 0.18232172348277817 \t| Accuracy: 95.000\n",
      "# Iteration  2576 -> Loss: 0.18230566749992108 \t| Accuracy: 95.000\n",
      "# Iteration  2577 -> Loss: 0.1822896273304763 \t| Accuracy: 95.000\n",
      "# Iteration  2578 -> Loss: 0.18227360295186812 \t| Accuracy: 95.000\n",
      "# Iteration  2579 -> Loss: 0.1822575943415602 \t| Accuracy: 95.000\n",
      "# Iteration  2580 -> Loss: 0.18224160147705554 \t| Accuracy: 95.000\n",
      "# Iteration  2581 -> Loss: 0.18222562433589623 \t| Accuracy: 95.000\n",
      "# Iteration  2582 -> Loss: 0.18220966289566362 \t| Accuracy: 95.000\n",
      "# Iteration  2583 -> Loss: 0.1821937171339781 \t| Accuracy: 95.000\n",
      "# Iteration  2584 -> Loss: 0.182177787028499 \t| Accuracy: 95.000\n",
      "# Iteration  2585 -> Loss: 0.18216187255692445 \t| Accuracy: 95.000\n",
      "# Iteration  2586 -> Loss: 0.18214597369699154 \t| Accuracy: 95.000\n",
      "# Iteration  2587 -> Loss: 0.182130090426476 \t| Accuracy: 95.000\n",
      "# Iteration  2588 -> Loss: 0.1821142227231922 \t| Accuracy: 95.000\n",
      "# Iteration  2589 -> Loss: 0.1820983705649931 \t| Accuracy: 95.000\n",
      "# Iteration  2590 -> Loss: 0.18208253392977006 \t| Accuracy: 95.000\n",
      "# Iteration  2591 -> Loss: 0.18206671279545295 \t| Accuracy: 95.000\n",
      "# Iteration  2592 -> Loss: 0.18205090714000996 \t| Accuracy: 95.000\n",
      "# Iteration  2593 -> Loss: 0.1820351169414473 \t| Accuracy: 95.000\n",
      "# Iteration  2594 -> Loss: 0.18201934217780966 \t| Accuracy: 95.000\n",
      "# Iteration  2595 -> Loss: 0.18200358282717963 \t| Accuracy: 95.000\n",
      "# Iteration  2596 -> Loss: 0.18198783886767766 \t| Accuracy: 95.000\n",
      "# Iteration  2597 -> Loss: 0.18197211027746243 \t| Accuracy: 95.000\n",
      "# Iteration  2598 -> Loss: 0.18195639703473027 \t| Accuracy: 95.000\n",
      "# Iteration  2599 -> Loss: 0.18194069911771524 \t| Accuracy: 95.000\n",
      "# Iteration  2600 -> Loss: 0.18192501650468915 \t| Accuracy: 95.000\n",
      "# Iteration  2601 -> Loss: 0.18190934917396137 \t| Accuracy: 95.000\n",
      "# Iteration  2602 -> Loss: 0.1818936971038789 \t| Accuracy: 95.000\n",
      "# Iteration  2603 -> Loss: 0.181878060272826 \t| Accuracy: 95.000\n",
      "# Iteration  2604 -> Loss: 0.1818624386592244 \t| Accuracy: 95.000\n",
      "# Iteration  2605 -> Loss: 0.18184683224153322 \t| Accuracy: 95.000\n",
      "# Iteration  2606 -> Loss: 0.1818312409982486 \t| Accuracy: 95.000\n",
      "# Iteration  2607 -> Loss: 0.1818156649079038 \t| Accuracy: 95.000\n",
      "# Iteration  2608 -> Loss: 0.1818001039490693 \t| Accuracy: 95.000\n",
      "# Iteration  2609 -> Loss: 0.18178455810035254 \t| Accuracy: 95.000\n",
      "# Iteration  2610 -> Loss: 0.18176902734039768 \t| Accuracy: 95.000\n",
      "# Iteration  2611 -> Loss: 0.18175351164788586 \t| Accuracy: 95.000\n",
      "# Iteration  2612 -> Loss: 0.18173801100153492 \t| Accuracy: 95.000\n",
      "# Iteration  2613 -> Loss: 0.18172252538009934 \t| Accuracy: 95.000\n",
      "# Iteration  2614 -> Loss: 0.1817070547623702 \t| Accuracy: 95.000\n",
      "# Iteration  2615 -> Loss: 0.1816915991271752 \t| Accuracy: 95.000\n",
      "# Iteration  2616 -> Loss: 0.18167615845337828 \t| Accuracy: 95.000\n",
      "# Iteration  2617 -> Loss: 0.1816607327198799 \t| Accuracy: 95.000\n",
      "# Iteration  2618 -> Loss: 0.1816453219056168 \t| Accuracy: 95.000\n",
      "# Iteration  2619 -> Loss: 0.18162992598956174 \t| Accuracy: 95.000\n",
      "# Iteration  2620 -> Loss: 0.18161454495072393 \t| Accuracy: 95.000\n",
      "# Iteration  2621 -> Loss: 0.18159917876814835 \t| Accuracy: 95.000\n",
      "# Iteration  2622 -> Loss: 0.18158382742091614 \t| Accuracy: 95.000\n",
      "# Iteration  2623 -> Loss: 0.1815684908881443 \t| Accuracy: 95.000\n",
      "# Iteration  2624 -> Loss: 0.18155316914898567 \t| Accuracy: 95.000\n",
      "# Iteration  2625 -> Loss: 0.18153786218262877 \t| Accuracy: 95.000\n",
      "# Iteration  2626 -> Loss: 0.181522569968298 \t| Accuracy: 95.000\n",
      "# Iteration  2627 -> Loss: 0.18150729248525313 \t| Accuracy: 95.000\n",
      "# Iteration  2628 -> Loss: 0.18149202971278963 \t| Accuracy: 95.000\n",
      "# Iteration  2629 -> Loss: 0.18147678163023842 \t| Accuracy: 95.000\n",
      "# Iteration  2630 -> Loss: 0.1814615482169658 \t| Accuracy: 95.000\n",
      "# Iteration  2631 -> Loss: 0.1814463294523733 \t| Accuracy: 95.000\n",
      "# Iteration  2632 -> Loss: 0.18143112531589783 \t| Accuracy: 95.000\n",
      "# Iteration  2633 -> Loss: 0.18141593578701135 \t| Accuracy: 95.000\n",
      "# Iteration  2634 -> Loss: 0.18140076084522103 \t| Accuracy: 95.000\n",
      "# Iteration  2635 -> Loss: 0.181385600470069 \t| Accuracy: 95.000\n",
      "# Iteration  2636 -> Loss: 0.18137045464113233 \t| Accuracy: 95.000\n",
      "# Iteration  2637 -> Loss: 0.18135532333802296 \t| Accuracy: 95.000\n",
      "# Iteration  2638 -> Loss: 0.1813402065403877 \t| Accuracy: 95.000\n",
      "# Iteration  2639 -> Loss: 0.1813251042279081 \t| Accuracy: 95.000\n",
      "# Iteration  2640 -> Loss: 0.18131001638030028 \t| Accuracy: 95.000\n",
      "# Iteration  2641 -> Loss: 0.18129494297731502 \t| Accuracy: 95.000\n",
      "# Iteration  2642 -> Loss: 0.18127988399873765 \t| Accuracy: 95.000\n",
      "# Iteration  2643 -> Loss: 0.18126483942438784 \t| Accuracy: 95.000\n",
      "# Iteration  2644 -> Loss: 0.18124980923411976 \t| Accuracy: 95.000\n",
      "# Iteration  2645 -> Loss: 0.18123479340782184 \t| Accuracy: 95.000\n",
      "# Iteration  2646 -> Loss: 0.18121979192541668 \t| Accuracy: 95.000\n",
      "# Iteration  2647 -> Loss: 0.18120480476686113 \t| Accuracy: 95.000\n",
      "# Iteration  2648 -> Loss: 0.18118983191214608 \t| Accuracy: 95.000\n",
      "# Iteration  2649 -> Loss: 0.1811748733412965 \t| Accuracy: 95.000\n",
      "# Iteration  2650 -> Loss: 0.18115992903437128 \t| Accuracy: 95.000\n",
      "# Iteration  2651 -> Loss: 0.18114499897146316 \t| Accuracy: 95.000\n",
      "# Iteration  2652 -> Loss: 0.18113008313269882 \t| Accuracy: 95.000\n",
      "# Iteration  2653 -> Loss: 0.18111518149823844 \t| Accuracy: 95.000\n",
      "# Iteration  2654 -> Loss: 0.18110029404827616 \t| Accuracy: 95.000\n",
      "# Iteration  2655 -> Loss: 0.18108542076303946 \t| Accuracy: 95.000\n",
      "# Iteration  2656 -> Loss: 0.18107056162278962 \t| Accuracy: 95.000\n",
      "# Iteration  2657 -> Loss: 0.18105571660782116 \t| Accuracy: 95.000\n",
      "# Iteration  2658 -> Loss: 0.18104088569846213 \t| Accuracy: 95.000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Iteration  2659 -> Loss: 0.18102606887507383 \t| Accuracy: 95.000\n",
      "# Iteration  2660 -> Loss: 0.1810112661180509 \t| Accuracy: 95.000\n",
      "# Iteration  2661 -> Loss: 0.1809964774078211 \t| Accuracy: 95.000\n",
      "# Iteration  2662 -> Loss: 0.18098170272484526 \t| Accuracy: 95.000\n",
      "# Iteration  2663 -> Loss: 0.1809669420496175 \t| Accuracy: 95.000\n",
      "# Iteration  2664 -> Loss: 0.1809521953626647 \t| Accuracy: 95.000\n",
      "# Iteration  2665 -> Loss: 0.18093746264454666 \t| Accuracy: 95.000\n",
      "# Iteration  2666 -> Loss: 0.18092274387585616 \t| Accuracy: 95.000\n",
      "# Iteration  2667 -> Loss: 0.18090803903721872 \t| Accuracy: 95.000\n",
      "# Iteration  2668 -> Loss: 0.18089334810929247 \t| Accuracy: 95.000\n",
      "# Iteration  2669 -> Loss: 0.18087867107276842 \t| Accuracy: 95.000\n",
      "# Iteration  2670 -> Loss: 0.18086400790836984 \t| Accuracy: 95.000\n",
      "# Iteration  2671 -> Loss: 0.18084935859685292 \t| Accuracy: 95.000\n",
      "# Iteration  2672 -> Loss: 0.18083472311900586 \t| Accuracy: 95.000\n",
      "# Iteration  2673 -> Loss: 0.18082010145564958 \t| Accuracy: 95.000\n",
      "# Iteration  2674 -> Loss: 0.18080549358763717 \t| Accuracy: 95.000\n",
      "# Iteration  2675 -> Loss: 0.18079089949585392 \t| Accuracy: 95.000\n",
      "# Iteration  2676 -> Loss: 0.18077631916121745 \t| Accuracy: 95.000\n",
      "# Iteration  2677 -> Loss: 0.18076175256467736 \t| Accuracy: 95.000\n",
      "# Iteration  2678 -> Loss: 0.18074719968721542 \t| Accuracy: 95.000\n",
      "# Iteration  2679 -> Loss: 0.18073266050984532 \t| Accuracy: 95.000\n",
      "# Iteration  2680 -> Loss: 0.1807181350136126 \t| Accuracy: 95.000\n",
      "# Iteration  2681 -> Loss: 0.18070362317959485 \t| Accuracy: 95.000\n",
      "# Iteration  2682 -> Loss: 0.18068912498890122 \t| Accuracy: 95.000\n",
      "# Iteration  2683 -> Loss: 0.18067464042267273 \t| Accuracy: 95.000\n",
      "# Iteration  2684 -> Loss: 0.18066016946208202 \t| Accuracy: 95.000\n",
      "# Iteration  2685 -> Loss: 0.18064571208833335 \t| Accuracy: 95.000\n",
      "# Iteration  2686 -> Loss: 0.1806312682826625 \t| Accuracy: 95.000\n",
      "# Iteration  2687 -> Loss: 0.1806168380263367 \t| Accuracy: 95.000\n",
      "# Iteration  2688 -> Loss: 0.18060242130065451 \t| Accuracy: 95.000\n",
      "# Iteration  2689 -> Loss: 0.180588018086946 \t| Accuracy: 95.000\n",
      "# Iteration  2690 -> Loss: 0.1805736283665724 \t| Accuracy: 95.000\n",
      "# Iteration  2691 -> Loss: 0.18055925212092613 \t| Accuracy: 95.000\n",
      "# Iteration  2692 -> Loss: 0.1805448893314308 \t| Accuracy: 95.000\n",
      "# Iteration  2693 -> Loss: 0.18053053997954113 \t| Accuracy: 95.000\n",
      "# Iteration  2694 -> Loss: 0.1805162040467428 \t| Accuracy: 95.000\n",
      "# Iteration  2695 -> Loss: 0.18050188151455246 \t| Accuracy: 95.000\n",
      "# Iteration  2696 -> Loss: 0.18048757236451768 \t| Accuracy: 95.000\n",
      "# Iteration  2697 -> Loss: 0.18047327657821685 \t| Accuracy: 95.000\n",
      "# Iteration  2698 -> Loss: 0.1804589941372591 \t| Accuracy: 95.000\n",
      "# Iteration  2699 -> Loss: 0.1804447250232843 \t| Accuracy: 95.000\n",
      "# Iteration  2700 -> Loss: 0.18043046921796294 \t| Accuracy: 95.000\n",
      "# Iteration  2701 -> Loss: 0.1804162267029961 \t| Accuracy: 95.000\n",
      "# Iteration  2702 -> Loss: 0.18040199746011532 \t| Accuracy: 95.000\n",
      "# Iteration  2703 -> Loss: 0.18038778147108273 \t| Accuracy: 95.000\n",
      "# Iteration  2704 -> Loss: 0.1803735787176907 \t| Accuracy: 95.000\n",
      "# Iteration  2705 -> Loss: 0.1803593891817621 \t| Accuracy: 95.000\n",
      "# Iteration  2706 -> Loss: 0.1803452128451499 \t| Accuracy: 95.000\n",
      "# Iteration  2707 -> Loss: 0.18033104968973734 \t| Accuracy: 95.000\n",
      "# Iteration  2708 -> Loss: 0.1803168996974378 \t| Accuracy: 95.000\n",
      "# Iteration  2709 -> Loss: 0.1803027628501948 \t| Accuracy: 95.000\n",
      "# Iteration  2710 -> Loss: 0.18028863912998191 \t| Accuracy: 95.000\n",
      "# Iteration  2711 -> Loss: 0.18027452851880252 \t| Accuracy: 95.000\n",
      "# Iteration  2712 -> Loss: 0.18026043099869002 \t| Accuracy: 95.000\n",
      "# Iteration  2713 -> Loss: 0.1802463465517076 \t| Accuracy: 95.000\n",
      "# Iteration  2714 -> Loss: 0.18023227515994827 \t| Accuracy: 95.000\n",
      "# Iteration  2715 -> Loss: 0.1802182168055348 \t| Accuracy: 95.000\n",
      "# Iteration  2716 -> Loss: 0.1802041714706195 \t| Accuracy: 95.000\n",
      "# Iteration  2717 -> Loss: 0.18019013913738444 \t| Accuracy: 95.000\n",
      "# Iteration  2718 -> Loss: 0.18017611978804104 \t| Accuracy: 95.000\n",
      "# Iteration  2719 -> Loss: 0.18016211340483032 \t| Accuracy: 95.000\n",
      "# Iteration  2720 -> Loss: 0.1801481199700228 \t| Accuracy: 95.000\n",
      "# Iteration  2721 -> Loss: 0.1801341394659182 \t| Accuracy: 95.000\n",
      "# Iteration  2722 -> Loss: 0.18012017187484558 \t| Accuracy: 95.000\n",
      "# Iteration  2723 -> Loss: 0.18010621717916334 \t| Accuracy: 95.000\n",
      "# Iteration  2724 -> Loss: 0.1800922753612589 \t| Accuracy: 95.000\n",
      "# Iteration  2725 -> Loss: 0.18007834640354903 \t| Accuracy: 95.000\n",
      "# Iteration  2726 -> Loss: 0.1800644302884793 \t| Accuracy: 95.000\n",
      "# Iteration  2727 -> Loss: 0.18005052699852458 \t| Accuracy: 95.000\n",
      "# Iteration  2728 -> Loss: 0.18003663651618843 \t| Accuracy: 95.000\n",
      "# Iteration  2729 -> Loss: 0.18002275882400337 \t| Accuracy: 95.000\n",
      "# Iteration  2730 -> Loss: 0.1800088939045309 \t| Accuracy: 95.000\n",
      "# Iteration  2731 -> Loss: 0.17999504174036116 \t| Accuracy: 95.000\n",
      "# Iteration  2732 -> Loss: 0.17998120231411294 \t| Accuracy: 95.000\n",
      "# Iteration  2733 -> Loss: 0.17996737560843384 \t| Accuracy: 95.000\n",
      "# Iteration  2734 -> Loss: 0.17995356160600007 \t| Accuracy: 95.000\n",
      "# Iteration  2735 -> Loss: 0.17993976028951608 \t| Accuracy: 95.000\n",
      "# Iteration  2736 -> Loss: 0.17992597164171534 \t| Accuracy: 95.000\n",
      "# Iteration  2737 -> Loss: 0.1799121956453592 \t| Accuracy: 95.000\n",
      "# Iteration  2738 -> Loss: 0.17989843228323774 \t| Accuracy: 95.000\n",
      "# Iteration  2739 -> Loss: 0.17988468153816917 \t| Accuracy: 95.000\n",
      "# Iteration  2740 -> Loss: 0.1798709433930001 \t| Accuracy: 95.000\n",
      "# Iteration  2741 -> Loss: 0.17985721783060524 \t| Accuracy: 95.000\n",
      "# Iteration  2742 -> Loss: 0.17984350483388742 \t| Accuracy: 95.000\n",
      "# Iteration  2743 -> Loss: 0.17982980438577761 \t| Accuracy: 95.000\n",
      "# Iteration  2744 -> Loss: 0.17981611646923487 \t| Accuracy: 95.000\n",
      "# Iteration  2745 -> Loss: 0.17980244106724613 \t| Accuracy: 95.000\n",
      "# Iteration  2746 -> Loss: 0.17978877816282618 \t| Accuracy: 95.000\n",
      "# Iteration  2747 -> Loss: 0.17977512773901788 \t| Accuracy: 95.000\n",
      "# Iteration  2748 -> Loss: 0.17976148977889175 \t| Accuracy: 95.000\n",
      "# Iteration  2749 -> Loss: 0.1797478642655461 \t| Accuracy: 95.000\n",
      "# Iteration  2750 -> Loss: 0.17973425118210687 \t| Accuracy: 95.000\n",
      "# Iteration  2751 -> Loss: 0.17972065051172767 \t| Accuracy: 95.000\n",
      "# Iteration  2752 -> Loss: 0.17970706223758984 \t| Accuracy: 95.000\n",
      "# Iteration  2753 -> Loss: 0.17969348634290197 \t| Accuracy: 95.000\n",
      "# Iteration  2754 -> Loss: 0.17967992281090042 \t| Accuracy: 95.000\n",
      "# Iteration  2755 -> Loss: 0.1796663716248487 \t| Accuracy: 95.000\n",
      "# Iteration  2756 -> Loss: 0.17965283276803787 \t| Accuracy: 95.000\n",
      "# Iteration  2757 -> Loss: 0.17963930622378618 \t| Accuracy: 95.000\n",
      "# Iteration  2758 -> Loss: 0.17962579197543913 \t| Accuracy: 95.000\n",
      "# Iteration  2759 -> Loss: 0.17961229000636963 \t| Accuracy: 95.000\n",
      "# Iteration  2760 -> Loss: 0.1795988002999775 \t| Accuracy: 95.000\n",
      "# Iteration  2761 -> Loss: 0.17958532283968964 \t| Accuracy: 95.000\n",
      "# Iteration  2762 -> Loss: 0.17957185760896016 \t| Accuracy: 95.000\n",
      "# Iteration  2763 -> Loss: 0.17955840459127004 \t| Accuracy: 95.000\n",
      "# Iteration  2764 -> Loss: 0.17954496377012716 \t| Accuracy: 95.000\n",
      "# Iteration  2765 -> Loss: 0.17953153512906636 \t| Accuracy: 95.000\n",
      "# Iteration  2766 -> Loss: 0.1795181186516493 \t| Accuracy: 95.000\n",
      "# Iteration  2767 -> Loss: 0.17950471432146428 \t| Accuracy: 95.000\n",
      "# Iteration  2768 -> Loss: 0.17949132212212646 \t| Accuracy: 95.000\n",
      "# Iteration  2769 -> Loss: 0.17947794203727754 \t| Accuracy: 95.000\n",
      "# Iteration  2770 -> Loss: 0.1794645740505859 \t| Accuracy: 95.000\n",
      "# Iteration  2771 -> Loss: 0.17945121814574655 \t| Accuracy: 95.000\n",
      "# Iteration  2772 -> Loss: 0.17943787430648075 \t| Accuracy: 95.000\n",
      "# Iteration  2773 -> Loss: 0.1794245425165365 \t| Accuracy: 95.000\n",
      "# Iteration  2774 -> Loss: 0.17941122275968796 \t| Accuracy: 95.000\n",
      "# Iteration  2775 -> Loss: 0.17939791501973576 \t| Accuracy: 95.000\n",
      "# Iteration  2776 -> Loss: 0.1793846192805068 \t| Accuracy: 95.000\n",
      "# Iteration  2777 -> Loss: 0.17937133552585424 \t| Accuracy: 95.000\n",
      "# Iteration  2778 -> Loss: 0.17935806373965732 \t| Accuracy: 95.000\n",
      "# Iteration  2779 -> Loss: 0.1793448039058215 \t| Accuracy: 95.000\n",
      "# Iteration  2780 -> Loss: 0.17933155600827835 \t| Accuracy: 95.000\n",
      "# Iteration  2781 -> Loss: 0.17931832003098536 \t| Accuracy: 95.000\n",
      "# Iteration  2782 -> Loss: 0.1793050959579261 \t| Accuracy: 95.000\n",
      "# Iteration  2783 -> Loss: 0.17929188377311 \t| Accuracy: 95.000\n",
      "# Iteration  2784 -> Loss: 0.1792786834605725 \t| Accuracy: 95.000\n",
      "# Iteration  2785 -> Loss: 0.17926549500437455 \t| Accuracy: 95.000\n",
      "# Iteration  2786 -> Loss: 0.17925231838860323 \t| Accuracy: 95.000\n",
      "# Iteration  2787 -> Loss: 0.17923915359737108 \t| Accuracy: 95.000\n",
      "# Iteration  2788 -> Loss: 0.17922600061481653 \t| Accuracy: 95.000\n",
      "# Iteration  2789 -> Loss: 0.17921285942510337 \t| Accuracy: 95.000\n",
      "# Iteration  2790 -> Loss: 0.1791997300124212 \t| Accuracy: 95.000\n",
      "# Iteration  2791 -> Loss: 0.17918661236098501 \t| Accuracy: 95.000\n",
      "# Iteration  2792 -> Loss: 0.17917350645503524 \t| Accuracy: 95.000\n",
      "# Iteration  2793 -> Loss: 0.17916041227883783 \t| Accuracy: 95.000\n",
      "# Iteration  2794 -> Loss: 0.17914732981668405 \t| Accuracy: 95.000\n",
      "# Iteration  2795 -> Loss: 0.17913425905289035 \t| Accuracy: 95.000\n",
      "# Iteration  2796 -> Loss: 0.1791211999717987 \t| Accuracy: 95.000\n",
      "# Iteration  2797 -> Loss: 0.17910815255777612 \t| Accuracy: 95.000\n",
      "# Iteration  2798 -> Loss: 0.17909511679521478 \t| Accuracy: 95.000\n",
      "# Iteration  2799 -> Loss: 0.17908209266853212 \t| Accuracy: 95.000\n",
      "# Iteration  2800 -> Loss: 0.17906908016217038 \t| Accuracy: 95.000\n",
      "# Iteration  2801 -> Loss: 0.17905607926059705 \t| Accuracy: 95.000\n",
      "# Iteration  2802 -> Loss: 0.1790430899483045 \t| Accuracy: 95.000\n",
      "# Iteration  2803 -> Loss: 0.17903011220981008 \t| Accuracy: 95.000\n",
      "# Iteration  2804 -> Loss: 0.1790171460296559 \t| Accuracy: 95.000\n",
      "# Iteration  2805 -> Loss: 0.17900419139240892 \t| Accuracy: 95.000\n",
      "# Iteration  2806 -> Loss: 0.17899124828266086 \t| Accuracy: 95.000\n",
      "# Iteration  2807 -> Loss: 0.17897831668502825 \t| Accuracy: 95.000\n",
      "# Iteration  2808 -> Loss: 0.17896539658415223 \t| Accuracy: 95.000\n",
      "# Iteration  2809 -> Loss: 0.1789524879646985 \t| Accuracy: 95.000\n",
      "# Iteration  2810 -> Loss: 0.17893959081135746 \t| Accuracy: 95.000\n",
      "# Iteration  2811 -> Loss: 0.17892670510884393 \t| Accuracy: 95.000\n",
      "# Iteration  2812 -> Loss: 0.17891383084189727 \t| Accuracy: 95.000\n",
      "# Iteration  2813 -> Loss: 0.1789009679952813 \t| Accuracy: 95.000\n",
      "# Iteration  2814 -> Loss: 0.17888811655378403 \t| Accuracy: 95.000\n",
      "# Iteration  2815 -> Loss: 0.17887527650221802 \t| Accuracy: 95.000\n",
      "# Iteration  2816 -> Loss: 0.17886244782542007 \t| Accuracy: 95.000\n",
      "# Iteration  2817 -> Loss: 0.17884963050825112 \t| Accuracy: 95.000\n",
      "# Iteration  2818 -> Loss: 0.17883682453559638 \t| Accuracy: 95.000\n",
      "# Iteration  2819 -> Loss: 0.17882402989236526 \t| Accuracy: 95.000\n",
      "# Iteration  2820 -> Loss: 0.17881124656349107 \t| Accuracy: 95.000\n",
      "# Iteration  2821 -> Loss: 0.17879847453393136 \t| Accuracy: 95.000\n",
      "# Iteration  2822 -> Loss: 0.17878571378866748 \t| Accuracy: 95.000\n",
      "# Iteration  2823 -> Loss: 0.1787729643127051 \t| Accuracy: 95.000\n",
      "# Iteration  2824 -> Loss: 0.1787602260910733 \t| Accuracy: 95.000\n",
      "# Iteration  2825 -> Loss: 0.17874749910882534 \t| Accuracy: 95.000\n",
      "# Iteration  2826 -> Loss: 0.17873478335103832 \t| Accuracy: 95.000\n",
      "# Iteration  2827 -> Loss: 0.17872207880281293 \t| Accuracy: 95.000\n",
      "# Iteration  2828 -> Loss: 0.17870938544927367 \t| Accuracy: 95.000\n",
      "# Iteration  2829 -> Loss: 0.1786967032755686 \t| Accuracy: 95.000\n",
      "# Iteration  2830 -> Loss: 0.1786840322668697 \t| Accuracy: 95.000\n",
      "# Iteration  2831 -> Loss: 0.17867137240837216 \t| Accuracy: 95.000\n",
      "# Iteration  2832 -> Loss: 0.17865872368529492 \t| Accuracy: 95.000\n",
      "# Iteration  2833 -> Loss: 0.17864608608288038 \t| Accuracy: 95.000\n",
      "# Iteration  2834 -> Loss: 0.17863345958639432 \t| Accuracy: 95.000\n",
      "# Iteration  2835 -> Loss: 0.17862084418112592 \t| Accuracy: 95.000\n",
      "# Iteration  2836 -> Loss: 0.17860823985238786 \t| Accuracy: 95.000\n",
      "# Iteration  2837 -> Loss: 0.17859564658551588 \t| Accuracy: 95.000\n",
      "# Iteration  2838 -> Loss: 0.17858306436586915 \t| Accuracy: 95.000\n",
      "# Iteration  2839 -> Loss: 0.17857049317883 \t| Accuracy: 95.000\n",
      "# Iteration  2840 -> Loss: 0.17855793300980394 \t| Accuracy: 95.000\n",
      "# Iteration  2841 -> Loss: 0.17854538384421959 \t| Accuracy: 95.000\n",
      "# Iteration  2842 -> Loss: 0.17853284566752858 \t| Accuracy: 95.000\n",
      "# Iteration  2843 -> Loss: 0.1785203184652057 \t| Accuracy: 95.000\n",
      "# Iteration  2844 -> Loss: 0.1785078022227486 \t| Accuracy: 95.000\n",
      "# Iteration  2845 -> Loss: 0.178495296925678 \t| Accuracy: 95.000\n",
      "# Iteration  2846 -> Loss: 0.17848280255953738 \t| Accuracy: 95.000\n",
      "# Iteration  2847 -> Loss: 0.1784703191098932 \t| Accuracy: 95.000\n",
      "# Iteration  2848 -> Loss: 0.1784578465623346 \t| Accuracy: 95.000\n",
      "# Iteration  2849 -> Loss: 0.17844538490247358 \t| Accuracy: 95.000\n",
      "# Iteration  2850 -> Loss: 0.17843293411594485 \t| Accuracy: 95.000\n",
      "# Iteration  2851 -> Loss: 0.17842049418840572 \t| Accuracy: 95.000\n",
      "# Iteration  2852 -> Loss: 0.1784080651055362 \t| Accuracy: 95.000\n",
      "# Iteration  2853 -> Loss: 0.17839564685303877 \t| Accuracy: 95.000\n",
      "# Iteration  2854 -> Loss: 0.1783832394166387 \t| Accuracy: 95.000\n",
      "# Iteration  2855 -> Loss: 0.17837084278208346 \t| Accuracy: 95.000\n",
      "# Iteration  2856 -> Loss: 0.17835845693514318 \t| Accuracy: 95.000\n",
      "# Iteration  2857 -> Loss: 0.17834608186161033 \t| Accuracy: 95.000\n",
      "# Iteration  2858 -> Loss: 0.17833371754729968 \t| Accuracy: 95.000\n",
      "# Iteration  2859 -> Loss: 0.1783213639780485 \t| Accuracy: 95.000\n",
      "# Iteration  2860 -> Loss: 0.17830902113971614 \t| Accuracy: 95.000\n",
      "# Iteration  2861 -> Loss: 0.17829668901818427 \t| Accuracy: 95.000\n",
      "# Iteration  2862 -> Loss: 0.17828436759935679 \t| Accuracy: 95.000\n",
      "# Iteration  2863 -> Loss: 0.17827205686915976 \t| Accuracy: 95.000\n",
      "# Iteration  2864 -> Loss: 0.17825975681354117 \t| Accuracy: 95.000\n",
      "# Iteration  2865 -> Loss: 0.17824746741847128 \t| Accuracy: 95.000\n",
      "# Iteration  2866 -> Loss: 0.1782351886699423 \t| Accuracy: 95.000\n",
      "# Iteration  2867 -> Loss: 0.1782229205539684 \t| Accuracy: 95.000\n",
      "# Iteration  2868 -> Loss: 0.17821066305658567 \t| Accuracy: 95.000\n",
      "# Iteration  2869 -> Loss: 0.1781984161638522 \t| Accuracy: 95.000\n",
      "# Iteration  2870 -> Loss: 0.17818617986184773 \t| Accuracy: 95.000\n",
      "# Iteration  2871 -> Loss: 0.17817395413667403 \t| Accuracy: 95.000\n",
      "# Iteration  2872 -> Loss: 0.17816173897445442 \t| Accuracy: 95.000\n",
      "# Iteration  2873 -> Loss: 0.17814953436133416 \t| Accuracy: 95.000\n",
      "# Iteration  2874 -> Loss: 0.17813734028348002 \t| Accuracy: 95.000\n",
      "# Iteration  2875 -> Loss: 0.17812515672708049 \t| Accuracy: 95.000\n",
      "# Iteration  2876 -> Loss: 0.17811298367834566 \t| Accuracy: 95.000\n",
      "# Iteration  2877 -> Loss: 0.1781008211235071 \t| Accuracy: 95.000\n",
      "# Iteration  2878 -> Loss: 0.17808866904881804 \t| Accuracy: 95.000\n",
      "# Iteration  2879 -> Loss: 0.17807652744055297 \t| Accuracy: 95.000\n",
      "# Iteration  2880 -> Loss: 0.17806439628500803 \t| Accuracy: 95.000\n",
      "# Iteration  2881 -> Loss: 0.17805227556850062 \t| Accuracy: 95.000\n",
      "# Iteration  2882 -> Loss: 0.1780401652773695 \t| Accuracy: 95.000\n",
      "# Iteration  2883 -> Loss: 0.17802806539797475 \t| Accuracy: 95.000\n",
      "# Iteration  2884 -> Loss: 0.17801597591669777 \t| Accuracy: 95.000\n",
      "# Iteration  2885 -> Loss: 0.17800389681994103 \t| Accuracy: 95.000\n",
      "# Iteration  2886 -> Loss: 0.17799182809412836 \t| Accuracy: 95.000\n",
      "# Iteration  2887 -> Loss: 0.17797976972570467 \t| Accuracy: 95.000\n",
      "# Iteration  2888 -> Loss: 0.17796772170113592 \t| Accuracy: 95.000\n",
      "# Iteration  2889 -> Loss: 0.17795568400690923 \t| Accuracy: 95.000\n",
      "# Iteration  2890 -> Loss: 0.17794365662953257 \t| Accuracy: 95.000\n",
      "# Iteration  2891 -> Loss: 0.17793163955553504 \t| Accuracy: 95.000\n",
      "# Iteration  2892 -> Loss: 0.17791963277146672 \t| Accuracy: 95.000\n",
      "# Iteration  2893 -> Loss: 0.1779076362638984 \t| Accuracy: 95.000\n",
      "# Iteration  2894 -> Loss: 0.1778956500194219 \t| Accuracy: 95.000\n",
      "# Iteration  2895 -> Loss: 0.17788367402464975 \t| Accuracy: 95.000\n",
      "# Iteration  2896 -> Loss: 0.1778717082662153 \t| Accuracy: 95.000\n",
      "# Iteration  2897 -> Loss: 0.17785975273077265 \t| Accuracy: 95.000\n",
      "# Iteration  2898 -> Loss: 0.17784780740499664 \t| Accuracy: 95.000\n",
      "# Iteration  2899 -> Loss: 0.17783587227558256 \t| Accuracy: 95.000\n",
      "# Iteration  2900 -> Loss: 0.1778239473292467 \t| Accuracy: 95.000\n",
      "# Iteration  2901 -> Loss: 0.17781203255272554 \t| Accuracy: 95.000\n",
      "# Iteration  2902 -> Loss: 0.17780012793277628 \t| Accuracy: 95.000\n",
      "# Iteration  2903 -> Loss: 0.17778823345617661 \t| Accuracy: 95.000\n",
      "# Iteration  2904 -> Loss: 0.17777634910972478 \t| Accuracy: 95.000\n",
      "# Iteration  2905 -> Loss: 0.17776447488023922 \t| Accuracy: 95.000\n",
      "# Iteration  2906 -> Loss: 0.17775261075455887 \t| Accuracy: 95.000\n",
      "# Iteration  2907 -> Loss: 0.1777407567195431 \t| Accuracy: 95.000\n",
      "# Iteration  2908 -> Loss: 0.17772891276207142 \t| Accuracy: 95.000\n",
      "# Iteration  2909 -> Loss: 0.17771707886904378 \t| Accuracy: 95.000\n",
      "# Iteration  2910 -> Loss: 0.17770525502738022 \t| Accuracy: 95.000\n",
      "# Iteration  2911 -> Loss: 0.17769344122402103 \t| Accuracy: 95.000\n",
      "# Iteration  2912 -> Loss: 0.17768163744592655 \t| Accuracy: 95.000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Iteration  2913 -> Loss: 0.17766984368007743 \t| Accuracy: 95.000\n",
      "# Iteration  2914 -> Loss: 0.1776580599134742 \t| Accuracy: 95.000\n",
      "# Iteration  2915 -> Loss: 0.17764628613313746 \t| Accuracy: 95.000\n",
      "# Iteration  2916 -> Loss: 0.17763452232610794 \t| Accuracy: 95.000\n",
      "# Iteration  2917 -> Loss: 0.17762276847944614 \t| Accuracy: 95.000\n",
      "# Iteration  2918 -> Loss: 0.17761102458023256 \t| Accuracy: 95.000\n",
      "# Iteration  2919 -> Loss: 0.17759929061556765 \t| Accuracy: 95.000\n",
      "# Iteration  2920 -> Loss: 0.1775875665725716 \t| Accuracy: 95.000\n",
      "# Iteration  2921 -> Loss: 0.1775758524383844 \t| Accuracy: 95.000\n",
      "# Iteration  2922 -> Loss: 0.17756414820016594 \t| Accuracy: 95.000\n",
      "# Iteration  2923 -> Loss: 0.1775524538450957 \t| Accuracy: 95.000\n",
      "# Iteration  2924 -> Loss: 0.17754076936037286 \t| Accuracy: 95.000\n",
      "# Iteration  2925 -> Loss: 0.17752909473321632 \t| Accuracy: 95.000\n",
      "# Iteration  2926 -> Loss: 0.17751742995086453 \t| Accuracy: 95.000\n",
      "# Iteration  2927 -> Loss: 0.17750577500057566 \t| Accuracy: 95.000\n",
      "# Iteration  2928 -> Loss: 0.17749412986962715 \t| Accuracy: 95.000\n",
      "# Iteration  2929 -> Loss: 0.17748249454531623 \t| Accuracy: 95.000\n",
      "# Iteration  2930 -> Loss: 0.17747086901495948 \t| Accuracy: 95.000\n",
      "# Iteration  2931 -> Loss: 0.17745925326589285 \t| Accuracy: 95.000\n",
      "# Iteration  2932 -> Loss: 0.17744764728547172 \t| Accuracy: 95.000\n",
      "# Iteration  2933 -> Loss: 0.1774360510610709 \t| Accuracy: 95.000\n",
      "# Iteration  2934 -> Loss: 0.17742446458008443 \t| Accuracy: 95.000\n",
      "# Iteration  2935 -> Loss: 0.17741288782992565 \t| Accuracy: 95.000\n",
      "# Iteration  2936 -> Loss: 0.17740132079802723 \t| Accuracy: 95.000\n",
      "# Iteration  2937 -> Loss: 0.17738976347184085 \t| Accuracy: 95.000\n",
      "# Iteration  2938 -> Loss: 0.17737821583883762 \t| Accuracy: 95.000\n",
      "# Iteration  2939 -> Loss: 0.17736667788650753 \t| Accuracy: 95.000\n",
      "# Iteration  2940 -> Loss: 0.1773551496023599 \t| Accuracy: 95.000\n",
      "# Iteration  2941 -> Loss: 0.17734363097392297 \t| Accuracy: 95.000\n",
      "# Iteration  2942 -> Loss: 0.177332121988744 \t| Accuracy: 95.000\n",
      "# Iteration  2943 -> Loss: 0.17732062263438936 \t| Accuracy: 95.000\n",
      "# Iteration  2944 -> Loss: 0.17730913289844424 \t| Accuracy: 95.000\n",
      "# Iteration  2945 -> Loss: 0.1772976527685128 \t| Accuracy: 95.000\n",
      "# Iteration  2946 -> Loss: 0.17728618223221815 \t| Accuracy: 95.000\n",
      "# Iteration  2947 -> Loss: 0.17727472127720212 \t| Accuracy: 95.000\n",
      "# Iteration  2948 -> Loss: 0.17726326989112545 \t| Accuracy: 95.000\n",
      "# Iteration  2949 -> Loss: 0.17725182806166762 \t| Accuracy: 95.000\n",
      "# Iteration  2950 -> Loss: 0.17724039577652684 \t| Accuracy: 95.000\n",
      "# Iteration  2951 -> Loss: 0.17722897302342003 \t| Accuracy: 95.000\n",
      "# Iteration  2952 -> Loss: 0.17721755979008283 \t| Accuracy: 95.000\n",
      "# Iteration  2953 -> Loss: 0.1772061560642694 \t| Accuracy: 95.000\n",
      "# Iteration  2954 -> Loss: 0.1771947618337526 \t| Accuracy: 95.000\n",
      "# Iteration  2955 -> Loss: 0.17718337708632378 \t| Accuracy: 95.000\n",
      "# Iteration  2956 -> Loss: 0.17717200180979298 \t| Accuracy: 95.000\n",
      "# Iteration  2957 -> Loss: 0.1771606359919885 \t| Accuracy: 95.000\n",
      "# Iteration  2958 -> Loss: 0.17714927962075724 \t| Accuracy: 95.000\n",
      "# Iteration  2959 -> Loss: 0.17713793268396447 \t| Accuracy: 95.000\n",
      "# Iteration  2960 -> Loss: 0.17712659516949392 \t| Accuracy: 95.000\n",
      "# Iteration  2961 -> Loss: 0.17711526706524752 \t| Accuracy: 95.000\n",
      "# Iteration  2962 -> Loss: 0.17710394835914578 \t| Accuracy: 95.000\n",
      "# Iteration  2963 -> Loss: 0.17709263903912717 \t| Accuracy: 95.000\n",
      "# Iteration  2964 -> Loss: 0.17708133909314872 \t| Accuracy: 95.000\n",
      "# Iteration  2965 -> Loss: 0.17707004850918545 \t| Accuracy: 95.000\n",
      "# Iteration  2966 -> Loss: 0.17705876727523062 \t| Accuracy: 95.000\n",
      "# Iteration  2967 -> Loss: 0.1770474953792957 \t| Accuracy: 95.000\n",
      "# Iteration  2968 -> Loss: 0.17703623280941022 \t| Accuracy: 95.000\n",
      "# Iteration  2969 -> Loss: 0.17702497955362181 \t| Accuracy: 95.000\n",
      "# Iteration  2970 -> Loss: 0.17701373559999617 \t| Accuracy: 95.000\n",
      "# Iteration  2971 -> Loss: 0.1770025009366169 \t| Accuracy: 95.000\n",
      "# Iteration  2972 -> Loss: 0.17699127555158567 \t| Accuracy: 95.000\n",
      "# Iteration  2973 -> Loss: 0.1769800594330221 \t| Accuracy: 95.000\n",
      "# Iteration  2974 -> Loss: 0.17696885256906364 \t| Accuracy: 95.000\n",
      "# Iteration  2975 -> Loss: 0.17695765494786572 \t| Accuracy: 95.000\n",
      "# Iteration  2976 -> Loss: 0.1769464665576015 \t| Accuracy: 95.000\n",
      "# Iteration  2977 -> Loss: 0.1769352873864621 \t| Accuracy: 95.000\n",
      "# Iteration  2978 -> Loss: 0.17692411742265624 \t| Accuracy: 95.000\n",
      "# Iteration  2979 -> Loss: 0.17691295665441048 \t| Accuracy: 95.000\n",
      "# Iteration  2980 -> Loss: 0.17690180506996905 \t| Accuracy: 95.000\n",
      "# Iteration  2981 -> Loss: 0.1768906626575939 \t| Accuracy: 95.000\n",
      "# Iteration  2982 -> Loss: 0.17687952940556462 \t| Accuracy: 95.000\n",
      "# Iteration  2983 -> Loss: 0.17686840530217834 \t| Accuracy: 95.000\n",
      "# Iteration  2984 -> Loss: 0.1768572903357498 \t| Accuracy: 95.000\n",
      "# Iteration  2985 -> Loss: 0.17684618449461129 \t| Accuracy: 95.000\n",
      "# Iteration  2986 -> Loss: 0.17683508776711268 \t| Accuracy: 95.000\n",
      "# Iteration  2987 -> Loss: 0.17682400014162114 \t| Accuracy: 95.000\n",
      "# Iteration  2988 -> Loss: 0.17681292160652146 \t| Accuracy: 95.000\n",
      "# Iteration  2989 -> Loss: 0.17680185215021577 \t| Accuracy: 95.000\n",
      "# Iteration  2990 -> Loss: 0.17679079176112353 \t| Accuracy: 95.000\n",
      "# Iteration  2991 -> Loss: 0.17677974042768163 \t| Accuracy: 95.000\n",
      "# Iteration  2992 -> Loss: 0.17676869813834428 \t| Accuracy: 95.000\n",
      "# Iteration  2993 -> Loss: 0.17675766488158287 \t| Accuracy: 95.000\n",
      "# Iteration  2994 -> Loss: 0.17674664064588608 \t| Accuracy: 95.000\n",
      "# Iteration  2995 -> Loss: 0.17673562541975987 \t| Accuracy: 95.000\n",
      "# Iteration  2996 -> Loss: 0.17672461919172733 \t| Accuracy: 95.000\n",
      "# Iteration  2997 -> Loss: 0.17671362195032866 \t| Accuracy: 95.000\n",
      "# Iteration  2998 -> Loss: 0.1767026336841213 \t| Accuracy: 95.000\n",
      "# Iteration  2999 -> Loss: 0.17669165438167975 \t| Accuracy: 95.000\n",
      "# Iteration  3000 -> Loss: 0.1766806840315954 \t| Accuracy: 95.000\n",
      "# Iteration  3001 -> Loss: 0.17666972262247685 \t| Accuracy: 95.000\n",
      "# Iteration  3002 -> Loss: 0.17665877014294964 \t| Accuracy: 95.000\n",
      "# Iteration  3003 -> Loss: 0.17664782658165631 \t| Accuracy: 95.000\n",
      "# Iteration  3004 -> Loss: 0.1766368919272562 \t| Accuracy: 95.000\n",
      "# Iteration  3005 -> Loss: 0.17662596616842569 \t| Accuracy: 95.000\n",
      "# Iteration  3006 -> Loss: 0.1766150492938579 \t| Accuracy: 95.000\n",
      "# Iteration  3007 -> Loss: 0.17660414129226293 \t| Accuracy: 95.000\n",
      "# Iteration  3008 -> Loss: 0.17659324215236757 \t| Accuracy: 95.000\n",
      "# Iteration  3009 -> Loss: 0.1765823518629154 \t| Accuracy: 95.000\n",
      "# Iteration  3010 -> Loss: 0.17657147041266677 \t| Accuracy: 95.000\n",
      "# Iteration  3011 -> Loss: 0.1765605977903988 \t| Accuracy: 95.000\n",
      "# Iteration  3012 -> Loss: 0.17654973398490512 \t| Accuracy: 95.000\n",
      "# Iteration  3013 -> Loss: 0.17653887898499618 \t| Accuracy: 95.000\n",
      "# Iteration  3014 -> Loss: 0.1765280327794989 \t| Accuracy: 95.000\n",
      "# Iteration  3015 -> Loss: 0.17651719535725696 \t| Accuracy: 95.000\n",
      "# Iteration  3016 -> Loss: 0.17650636670713038 \t| Accuracy: 95.000\n",
      "# Iteration  3017 -> Loss: 0.17649554681799592 \t| Accuracy: 95.000\n",
      "# Iteration  3018 -> Loss: 0.17648473567874667 \t| Accuracy: 95.000\n",
      "# Iteration  3019 -> Loss: 0.17647393327829228 \t| Accuracy: 95.000\n",
      "# Iteration  3020 -> Loss: 0.17646313960555882 \t| Accuracy: 95.000\n",
      "# Iteration  3021 -> Loss: 0.17645235464948866 \t| Accuracy: 95.000\n",
      "# Iteration  3022 -> Loss: 0.1764415783990407 \t| Accuracy: 95.000\n",
      "# Iteration  3023 -> Loss: 0.17643081084319012 \t| Accuracy: 95.000\n",
      "# Iteration  3024 -> Loss: 0.1764200519709283 \t| Accuracy: 95.000\n",
      "# Iteration  3025 -> Loss: 0.1764093017712631 \t| Accuracy: 95.000\n",
      "# Iteration  3026 -> Loss: 0.17639856023321845 \t| Accuracy: 95.000\n",
      "# Iteration  3027 -> Loss: 0.17638782734583464 \t| Accuracy: 95.000\n",
      "# Iteration  3028 -> Loss: 0.1763771030981681 \t| Accuracy: 95.000\n",
      "# Iteration  3029 -> Loss: 0.1763663874792913 \t| Accuracy: 95.000\n",
      "# Iteration  3030 -> Loss: 0.1763556804782931 \t| Accuracy: 95.000\n",
      "# Iteration  3031 -> Loss: 0.17634498208427823 \t| Accuracy: 95.000\n",
      "# Iteration  3032 -> Loss: 0.1763342922863676 \t| Accuracy: 95.000\n",
      "# Iteration  3033 -> Loss: 0.1763236110736981 \t| Accuracy: 95.000\n",
      "# Iteration  3034 -> Loss: 0.1763129384354227 \t| Accuracy: 95.000\n",
      "# Iteration  3035 -> Loss: 0.17630227436071036 \t| Accuracy: 95.000\n",
      "# Iteration  3036 -> Loss: 0.1762916188387459 \t| Accuracy: 95.000\n",
      "# Iteration  3037 -> Loss: 0.17628097185873007 \t| Accuracy: 95.000\n",
      "# Iteration  3038 -> Loss: 0.17627033340987971 \t| Accuracy: 95.000\n",
      "# Iteration  3039 -> Loss: 0.17625970348142728 \t| Accuracy: 95.000\n",
      "# Iteration  3040 -> Loss: 0.17624908206262113 \t| Accuracy: 95.000\n",
      "# Iteration  3041 -> Loss: 0.17623846914272562 \t| Accuracy: 95.000\n",
      "# Iteration  3042 -> Loss: 0.17622786471102056 \t| Accuracy: 95.000\n",
      "# Iteration  3043 -> Loss: 0.17621726875680171 \t| Accuracy: 95.000\n",
      "# Iteration  3044 -> Loss: 0.17620668126938063 \t| Accuracy: 95.000\n",
      "# Iteration  3045 -> Loss: 0.17619610223808432 \t| Accuracy: 95.000\n",
      "# Iteration  3046 -> Loss: 0.1761855316522557 \t| Accuracy: 95.000\n",
      "# Iteration  3047 -> Loss: 0.17617496950125305 \t| Accuracy: 95.000\n",
      "# Iteration  3048 -> Loss: 0.17616441577445055 \t| Accuracy: 95.000\n",
      "# Iteration  3049 -> Loss: 0.17615387046123773 \t| Accuracy: 95.000\n",
      "# Iteration  3050 -> Loss: 0.17614333355101977 \t| Accuracy: 95.000\n",
      "# Iteration  3051 -> Loss: 0.1761328050332173 \t| Accuracy: 95.000\n",
      "# Iteration  3052 -> Loss: 0.17612228489726658 \t| Accuracy: 95.000\n",
      "# Iteration  3053 -> Loss: 0.17611177313261922 \t| Accuracy: 95.000\n",
      "# Iteration  3054 -> Loss: 0.17610126972874218 \t| Accuracy: 95.000\n",
      "# Iteration  3055 -> Loss: 0.17609077467511802 \t| Accuracy: 95.000\n",
      "# Iteration  3056 -> Loss: 0.17608028796124456 \t| Accuracy: 95.000\n",
      "# Iteration  3057 -> Loss: 0.17606980957663498 \t| Accuracy: 95.000\n",
      "# Iteration  3058 -> Loss: 0.17605933951081773 \t| Accuracy: 95.000\n",
      "# Iteration  3059 -> Loss: 0.17604887775333675 \t| Accuracy: 95.000\n",
      "# Iteration  3060 -> Loss: 0.17603842429375097 \t| Accuracy: 95.000\n",
      "# Iteration  3061 -> Loss: 0.17602797912163481 \t| Accuracy: 95.000\n",
      "# Iteration  3062 -> Loss: 0.17601754222657767 \t| Accuracy: 95.000\n",
      "# Iteration  3063 -> Loss: 0.17600711359818433 \t| Accuracy: 95.000\n",
      "# Iteration  3064 -> Loss: 0.17599669322607453 \t| Accuracy: 95.000\n",
      "# Iteration  3065 -> Loss: 0.17598628109988337 \t| Accuracy: 95.000\n",
      "# Iteration  3066 -> Loss: 0.17597587720926086 \t| Accuracy: 95.000\n",
      "# Iteration  3067 -> Loss: 0.1759654815438721 \t| Accuracy: 95.000\n",
      "# Iteration  3068 -> Loss: 0.17595509409339732 \t| Accuracy: 95.000\n",
      "# Iteration  3069 -> Loss: 0.17594471484753169 \t| Accuracy: 95.000\n",
      "# Iteration  3070 -> Loss: 0.17593434379598535 \t| Accuracy: 95.000\n",
      "# Iteration  3071 -> Loss: 0.17592398092848352 \t| Accuracy: 95.000\n",
      "# Iteration  3072 -> Loss: 0.17591362623476622 \t| Accuracy: 95.000\n",
      "# Iteration  3073 -> Loss: 0.1759032797045885 \t| Accuracy: 95.000\n",
      "# Iteration  3074 -> Loss: 0.1758929413277201 \t| Accuracy: 95.000\n",
      "# Iteration  3075 -> Loss: 0.17588261109394582 \t| Accuracy: 95.000\n",
      "# Iteration  3076 -> Loss: 0.17587228899306523 \t| Accuracy: 95.000\n",
      "# Iteration  3077 -> Loss: 0.17586197501489256 \t| Accuracy: 95.000\n",
      "# Iteration  3078 -> Loss: 0.17585166914925698 \t| Accuracy: 95.000\n",
      "# Iteration  3079 -> Loss: 0.17584137138600228 \t| Accuracy: 95.000\n",
      "# Iteration  3080 -> Loss: 0.17583108171498713 \t| Accuracy: 95.000\n",
      "# Iteration  3081 -> Loss: 0.17582080012608464 \t| Accuracy: 95.000\n",
      "# Iteration  3082 -> Loss: 0.1758105266091828 \t| Accuracy: 95.000\n",
      "# Iteration  3083 -> Loss: 0.17580026115418418 \t| Accuracy: 95.000\n",
      "# Iteration  3084 -> Loss: 0.1757900037510059 \t| Accuracy: 95.000\n",
      "# Iteration  3085 -> Loss: 0.17577975438957977 \t| Accuracy: 95.000\n",
      "# Iteration  3086 -> Loss: 0.17576951305985206 \t| Accuracy: 95.000\n",
      "# Iteration  3087 -> Loss: 0.17575927975178351 \t| Accuracy: 95.000\n",
      "# Iteration  3088 -> Loss: 0.1757490544553497 \t| Accuracy: 95.000\n",
      "# Iteration  3089 -> Loss: 0.17573883716054023 \t| Accuracy: 95.000\n",
      "# Iteration  3090 -> Loss: 0.1757286278573594 \t| Accuracy: 95.000\n",
      "# Iteration  3091 -> Loss: 0.17571842653582598 \t| Accuracy: 95.000\n",
      "# Iteration  3092 -> Loss: 0.175708233185973 \t| Accuracy: 95.000\n",
      "# Iteration  3093 -> Loss: 0.17569804779784803 \t| Accuracy: 95.000\n",
      "# Iteration  3094 -> Loss: 0.17568787036151276 \t| Accuracy: 95.000\n",
      "# Iteration  3095 -> Loss: 0.1756777008670434 \t| Accuracy: 95.000\n",
      "# Iteration  3096 -> Loss: 0.17566753930453036 \t| Accuracy: 95.000\n",
      "# Iteration  3097 -> Loss: 0.1756573856640784 \t| Accuracy: 95.000\n",
      "# Iteration  3098 -> Loss: 0.1756472399358064 \t| Accuracy: 95.000\n",
      "# Iteration  3099 -> Loss: 0.1756371021098475 \t| Accuracy: 95.000\n",
      "# Iteration  3100 -> Loss: 0.17562697217634915 \t| Accuracy: 95.000\n",
      "# Iteration  3101 -> Loss: 0.17561685012547285 \t| Accuracy: 95.000\n",
      "# Iteration  3102 -> Loss: 0.17560673594739423 \t| Accuracy: 95.000\n",
      "# Iteration  3103 -> Loss: 0.1755966296323031 \t| Accuracy: 95.000\n",
      "# Iteration  3104 -> Loss: 0.17558653117040332 \t| Accuracy: 95.000\n",
      "# Iteration  3105 -> Loss: 0.17557644055191285 \t| Accuracy: 95.000\n",
      "# Iteration  3106 -> Loss: 0.1755663577670637 \t| Accuracy: 95.000\n",
      "# Iteration  3107 -> Loss: 0.17555628280610178 \t| Accuracy: 95.000\n",
      "# Iteration  3108 -> Loss: 0.17554621565928713 \t| Accuracy: 95.000\n",
      "# Iteration  3109 -> Loss: 0.17553615631689373 \t| Accuracy: 95.000\n",
      "# Iteration  3110 -> Loss: 0.1755261047692094 \t| Accuracy: 95.000\n",
      "# Iteration  3111 -> Loss: 0.17551606100653594 \t| Accuracy: 95.000\n",
      "# Iteration  3112 -> Loss: 0.17550602501918908 \t| Accuracy: 95.000\n",
      "# Iteration  3113 -> Loss: 0.17549599679749844 \t| Accuracy: 95.000\n",
      "# Iteration  3114 -> Loss: 0.17548597633180726 \t| Accuracy: 95.000\n",
      "# Iteration  3115 -> Loss: 0.17547596361247286 \t| Accuracy: 95.000\n",
      "# Iteration  3116 -> Loss: 0.1754659586298662 \t| Accuracy: 95.000\n",
      "# Iteration  3117 -> Loss: 0.17545596137437203 \t| Accuracy: 95.000\n",
      "\n",
      "!!! Convergence reached !!!\n",
      "# Iteration 3116 #\n",
      "Cross-Entropy Loss:      0.17545596137437203\n",
      "Accuracy (Training Set): 95.000%\n",
      "Weights\n",
      "S -> H:\n",
      " [[-2.78551104 -2.01736768  1.37555724  2.49924298 -3.10351227 -1.94271747\n",
      "  -0.16011587 -3.23236854 -1.85791356  3.02390194]\n",
      " [-0.98394269  0.60303809 -1.04059287 -0.67026191  1.19045703  0.59344061\n",
      "  -0.36054921 -1.51375542  0.80099959  1.00543563]\n",
      " [ 0.93782892  0.69160844 -0.60462288 -0.78255775 -0.91666401  0.67485878\n",
      "  -0.80722616 -1.20451783 -0.76823463 -0.99048563]] \n",
      "H -> O:\n",
      " [[ 0.82969976]\n",
      " [ 3.14086138]\n",
      " [-1.97547496]\n",
      " [ 0.86595786]\n",
      " [ 2.15244485]\n",
      " [ 4.06671875]\n",
      " [-1.90172778]\n",
      " [-0.43641572]\n",
      " [-4.72281037]\n",
      " [ 1.80427269]\n",
      " [-4.02834417]]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Creates the Neural Network with 2 Input Neurons, 10 Hidden Neurons and 1 Output Neuron\n",
    "# (The weights are randomnly initiated)\n",
    "brain = MultilayerPerceptron(n_neurons=[2,10,1])\n",
    "\n",
    "# Run the Gradient Descent and returns the Error History for the training.\n",
    "errorHist = brain.train(X_train, y_train, alpha=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then can classify the entire training data space in order to see the pattern that our Neural Network learned _(I recommend you to change the Neural Network architecture and the training parameters in the previous cell in order to see different patterns that the Network can fit)_:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Classify and plot the results for the entire dataset space using the trained Network\n",
    "tl.plotClassContour(X_train, y_train, brain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Training a MLP for Multiclass Classification\n",
    "\n",
    "The training process in the casse of multiclass classification is exactly the same for the binary case. In this case, however, the outputs in the Training data must follow the [One-Hot Encoding](https://www.kaggle.com/dansbecker/using-categorical-data-with-one-hot-encoding) in order to be compared to the network's activation in the output layer (that is now a vector).\n",
    "\n",
    "Consider our already known Multilayer Perceptron for multiclass classification:\n",
    "\n",
    "<img src=\"../imgs/mlp_02.png\" alt=\"binary multilayer perceptron\" width=\"350px\"/>\n",
    "\n",
    "**We will train this network using the dataset from the previous notebook:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## CARREGANDO E VISUALIZANDO OS DADOS ##\n",
    "X_train, X_test, y_train, y_test = tl.loadData(\"../data/toy_data_02.csv\")\n",
    "tl.plotData(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we create the Neural Network with the wanted architecture, all we need to do is run the train() method passing the desired training data. Remember to set the number of output neurons to be equal to 4:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Iteration     1 -> Loss: 3.3045442255951247 \t| Accuracy: 42.917\n",
      "# Iteration     2 -> Loss: 3.189861744291028 \t| Accuracy: 43.125\n",
      "# Iteration     3 -> Loss: 3.0858655280091476 \t| Accuracy: 43.750\n",
      "# Iteration     4 -> Loss: 2.9932522582527303 \t| Accuracy: 60.833\n",
      "# Iteration     5 -> Loss: 2.911819605983686 \t| Accuracy: 81.667\n",
      "# Iteration     6 -> Loss: 2.8403441026698184 \t| Accuracy: 93.542\n",
      "# Iteration     7 -> Loss: 2.7770438484149813 \t| Accuracy: 99.375\n",
      "# Iteration     8 -> Loss: 2.7201603513091057 \t| Accuracy: 107.917\n",
      "# Iteration     9 -> Loss: 2.668299676463272 \t| Accuracy: 115.833\n",
      "# Iteration    10 -> Loss: 2.620497332953709 \t| Accuracy: 124.583\n",
      "# Iteration    11 -> Loss: 2.5761258289112563 \t| Accuracy: 129.792\n",
      "# Iteration    12 -> Loss: 2.53475986472217 \t| Accuracy: 136.458\n",
      "# Iteration    13 -> Loss: 2.496071082338638 \t| Accuracy: 146.458\n",
      "# Iteration    14 -> Loss: 2.45977694972295 \t| Accuracy: 148.333\n",
      "# Iteration    15 -> Loss: 2.42562990861672 \t| Accuracy: 150.417\n",
      "# Iteration    16 -> Loss: 2.3934206355896457 \t| Accuracy: 151.667\n",
      "# Iteration    17 -> Loss: 2.362978829735624 \t| Accuracy: 152.083\n",
      "# Iteration    18 -> Loss: 2.3341679448759605 \t| Accuracy: 153.542\n",
      "# Iteration    19 -> Loss: 2.306876635309905 \t| Accuracy: 154.375\n",
      "# Iteration    20 -> Loss: 2.2810102301514807 \t| Accuracy: 156.875\n",
      "# Iteration    21 -> Loss: 2.2564840390306937 \t| Accuracy: 158.542\n",
      "# Iteration    22 -> Loss: 2.233218906905486 \t| Accuracy: 159.792\n",
      "# Iteration    23 -> Loss: 2.211138705625341 \t| Accuracy: 161.042\n",
      "# Iteration    24 -> Loss: 2.190169220465596 \t| Accuracy: 162.917\n",
      "# Iteration    25 -> Loss: 2.17023791521338 \t| Accuracy: 161.042\n",
      "# Iteration    26 -> Loss: 2.1512741758661904 \t| Accuracy: 160.833\n",
      "# Iteration    27 -> Loss: 2.1332097597174857 \t| Accuracy: 162.917\n",
      "# Iteration    28 -> Loss: 2.115979281978592 \t| Accuracy: 156.875\n",
      "# Iteration    29 -> Loss: 2.0995206484879483 \t| Accuracy: 153.333\n",
      "# Iteration    30 -> Loss: 2.0837753930100917 \t| Accuracy: 149.375\n",
      "# Iteration    31 -> Loss: 2.0686889071375805 \t| Accuracy: 148.750\n",
      "# Iteration    32 -> Loss: 2.054210566261091 \t| Accuracy: 147.500\n",
      "# Iteration    33 -> Loss: 2.040293761864129 \t| Accuracy: 146.667\n",
      "# Iteration    34 -> Loss: 2.0268958525165304 \t| Accuracy: 148.333\n",
      "# Iteration    35 -> Loss: 2.01397804601993 \t| Accuracy: 148.542\n",
      "# Iteration    36 -> Loss: 2.001505224741721 \t| Accuracy: 148.333\n",
      "# Iteration    37 -> Loss: 1.9894457260060296 \t| Accuracy: 147.917\n",
      "# Iteration    38 -> Loss: 1.9777710896861658 \t| Accuracy: 147.500\n",
      "# Iteration    39 -> Loss: 1.9664557857118596 \t| Accuracy: 148.333\n",
      "# Iteration    40 -> Loss: 1.9554769347452103 \t| Accuracy: 149.375\n",
      "# Iteration    41 -> Loss: 1.9448140354617758 \t| Accuracy: 150.208\n",
      "# Iteration    42 -> Loss: 1.934448711478872 \t| Accuracy: 150.833\n",
      "# Iteration    43 -> Loss: 1.9243644899290884 \t| Accuracy: 151.458\n",
      "# Iteration    44 -> Loss: 1.9145466219843243 \t| Accuracy: 152.500\n",
      "# Iteration    45 -> Loss: 1.9049819532338461 \t| Accuracy: 153.125\n",
      "# Iteration    46 -> Loss: 1.8956588484967638 \t| Accuracy: 155.833\n",
      "# Iteration    47 -> Loss: 1.8865671711014789 \t| Accuracy: 155.208\n",
      "# Iteration    48 -> Loss: 1.877698310724712 \t| Accuracy: 155.625\n",
      "# Iteration    49 -> Loss: 1.8690452467937284 \t| Accuracy: 157.083\n",
      "# Iteration    50 -> Loss: 1.8606026270087472 \t| Accuracy: 156.667\n",
      "# Iteration    51 -> Loss: 1.8523668339747217 \t| Accuracy: 157.500\n",
      "# Iteration    52 -> Loss: 1.8443360086559484 \t| Accuracy: 158.750\n",
      "# Iteration    53 -> Loss: 1.8365099987397129 \t| Accuracy: 159.583\n",
      "# Iteration    54 -> Loss: 1.8288902042245387 \t| Accuracy: 160.208\n",
      "# Iteration    55 -> Loss: 1.8214793025908993 \t| Accuracy: 161.042\n",
      "# Iteration    56 -> Loss: 1.8142808520299756 \t| Accuracy: 158.958\n",
      "# Iteration    57 -> Loss: 1.8072987920977237 \t| Accuracy: 157.083\n",
      "# Iteration    58 -> Loss: 1.8005368831805744 \t| Accuracy: 156.042\n",
      "# Iteration    59 -> Loss: 1.7939981435706096 \t| Accuracy: 153.125\n",
      "# Iteration    60 -> Loss: 1.787684349776461 \t| Accuracy: 152.917\n",
      "# Iteration    61 -> Loss: 1.781595658514667 \t| Accuracy: 151.458\n",
      "# Iteration    62 -> Loss: 1.7757303888075242 \t| Accuracy: 151.667\n",
      "# Iteration    63 -> Loss: 1.7700849753738723 \t| Accuracy: 148.333\n",
      "# Iteration    64 -> Loss: 1.7646540777139703 \t| Accuracy: 147.917\n",
      "# Iteration    65 -> Loss: 1.7594308095262936 \t| Accuracy: 146.250\n",
      "# Iteration    66 -> Loss: 1.7544070436248689 \t| Accuracy: 146.250\n",
      "# Iteration    67 -> Loss: 1.749573747979484 \t| Accuracy: 144.583\n",
      "# Iteration    68 -> Loss: 1.7449213160144381 \t| Accuracy: 144.583\n",
      "# Iteration    69 -> Loss: 1.740439865194422 \t| Accuracy: 143.750\n",
      "# Iteration    70 -> Loss: 1.7361194890384046 \t| Accuracy: 143.750\n",
      "# Iteration    71 -> Loss: 1.7319504570288724 \t| Accuracy: 143.542\n",
      "# Iteration    72 -> Loss: 1.7279233635903744 \t| Accuracy: 142.083\n",
      "# Iteration    73 -> Loss: 1.7240292314154342 \t| Accuracy: 142.083\n",
      "# Iteration    74 -> Loss: 1.720259576393455 \t| Accuracy: 138.333\n",
      "# Iteration    75 -> Loss: 1.716606441870116 \t| Accuracy: 138.333\n",
      "# Iteration    76 -> Loss: 1.7130624095072733 \t| Accuracy: 138.333\n",
      "# Iteration    77 -> Loss: 1.7096205930755421 \t| Accuracy: 138.333\n",
      "# Iteration    78 -> Loss: 1.7062746204017836 \t| Accuracy: 138.542\n",
      "# Iteration    79 -> Loss: 1.7030186075996003 \t| Accuracy: 137.292\n",
      "# Iteration    80 -> Loss: 1.6998471287317676 \t| Accuracy: 137.292\n",
      "# Iteration    81 -> Loss: 1.696755183229678 \t| Accuracy: 137.292\n",
      "# Iteration    82 -> Loss: 1.693738162731458 \t| Accuracy: 137.292\n",
      "# Iteration    83 -> Loss: 1.6907918184839972 \t| Accuracy: 137.292\n",
      "# Iteration    84 -> Loss: 1.6879122300632161 \t| Accuracy: 137.292\n",
      "# Iteration    85 -> Loss: 1.6850957758782144 \t| Accuracy: 137.292\n",
      "# Iteration    86 -> Loss: 1.6823391057167536 \t| Accuracy: 135.833\n",
      "# Iteration    87 -> Loss: 1.679639115442977 \t| Accuracy: 135.833\n",
      "# Iteration    88 -> Loss: 1.6769929238579417 \t| Accuracy: 136.042\n",
      "# Iteration    89 -> Loss: 1.674397851667113 \t| Accuracy: 136.250\n",
      "# Iteration    90 -> Loss: 1.6718514024570117 \t| Accuracy: 136.250\n",
      "# Iteration    91 -> Loss: 1.6693512455585484 \t| Accuracy: 136.250\n",
      "# Iteration    92 -> Loss: 1.6668952006618998 \t| Accuracy: 136.250\n",
      "# Iteration    93 -> Loss: 1.6644812240432358 \t| Accuracy: 136.250\n",
      "# Iteration    94 -> Loss: 1.66210739626441 \t| Accuracy: 136.250\n",
      "# Iteration    95 -> Loss: 1.6597719112109535 \t| Accuracy: 135.833\n",
      "# Iteration    96 -> Loss: 1.6574730663399733 \t| Accuracy: 135.833\n",
      "# Iteration    97 -> Loss: 1.6552092540169951 \t| Accuracy: 135.833\n",
      "# Iteration    98 -> Loss: 1.6529789538287778 \t| Accuracy: 135.833\n",
      "# Iteration    99 -> Loss: 1.6507807257672826 \t| Accuracy: 135.833\n",
      "# Iteration   100 -> Loss: 1.6486132041880792 \t| Accuracy: 135.833\n",
      "# Iteration   101 -> Loss: 1.6464750924543679 \t| Accuracy: 135.833\n",
      "# Iteration   102 -> Loss: 1.6443651581854126 \t| Accuracy: 135.833\n",
      "# Iteration   103 -> Loss: 1.6422822290354857 \t| Accuracy: 135.833\n",
      "# Iteration   104 -> Loss: 1.6402251889363781 \t| Accuracy: 133.958\n",
      "# Iteration   105 -> Loss: 1.6381929747431447 \t| Accuracy: 133.958\n",
      "# Iteration   106 -> Loss: 1.636184573229009 \t| Accuracy: 133.958\n",
      "# Iteration   107 -> Loss: 1.6341990183812567 \t| Accuracy: 133.333\n",
      "# Iteration   108 -> Loss: 1.6322353889554622 \t| Accuracy: 133.542\n",
      "# Iteration   109 -> Loss: 1.6302928062505768 \t| Accuracy: 133.542\n",
      "# Iteration   110 -> Loss: 1.628370432072179 \t| Accuracy: 133.542\n",
      "# Iteration   111 -> Loss: 1.626467466855604 \t| Accuracy: 133.542\n",
      "# Iteration   112 -> Loss: 1.6245831479247002 \t| Accuracy: 133.542\n",
      "# Iteration   113 -> Loss: 1.622716747865617 \t| Accuracy: 133.542\n",
      "# Iteration   114 -> Loss: 1.6208675729983029 \t| Accuracy: 133.542\n",
      "# Iteration   115 -> Loss: 1.6190349619313285 \t| Accuracy: 133.542\n",
      "# Iteration   116 -> Loss: 1.617218284188215 \t| Accuracy: 133.542\n",
      "# Iteration   117 -> Loss: 1.6154169388956905 \t| Accuracy: 133.542\n",
      "# Iteration   118 -> Loss: 1.6136303535262324 \t| Accuracy: 133.542\n",
      "# Iteration   119 -> Loss: 1.6118579826888986 \t| Accuracy: 133.542\n",
      "# Iteration   120 -> Loss: 1.6100993069638203 \t| Accuracy: 133.542\n",
      "# Iteration   121 -> Loss: 1.608353831776871 \t| Accuracy: 133.542\n",
      "# Iteration   122 -> Loss: 1.6066210863119461 \t| Accuracy: 133.542\n",
      "# Iteration   123 -> Loss: 1.6049006224590263 \t| Accuracy: 132.292\n",
      "# Iteration   124 -> Loss: 1.6031920137967521 \t| Accuracy: 132.292\n",
      "# Iteration   125 -> Loss: 1.6014948546086751 \t| Accuracy: 132.292\n",
      "# Iteration   126 -> Loss: 1.5998087589326493 \t| Accuracy: 132.292\n",
      "# Iteration   127 -> Loss: 1.598133359643036 \t| Accuracy: 132.292\n",
      "# Iteration   128 -> Loss: 1.5964683075655153 \t| Accuracy: 132.292\n",
      "# Iteration   129 -> Loss: 1.5948132706243616 \t| Accuracy: 132.292\n",
      "# Iteration   130 -> Loss: 1.593167933022048 \t| Accuracy: 132.292\n",
      "# Iteration   131 -> Loss: 1.591531994451009 \t| Accuracy: 131.667\n",
      "# Iteration   132 -> Loss: 1.589905169337349 \t| Accuracy: 131.667\n",
      "# Iteration   133 -> Loss: 1.5882871861161927 \t| Accuracy: 131.667\n",
      "# Iteration   134 -> Loss: 1.5866777865382973 \t| Accuracy: 132.292\n",
      "# Iteration   135 -> Loss: 1.5850767250074576 \t| Accuracy: 132.292\n",
      "# Iteration   136 -> Loss: 1.5834837679481317 \t| Accuracy: 132.292\n",
      "# Iteration   137 -> Loss: 1.5818986932026498 \t| Accuracy: 132.292\n",
      "# Iteration   138 -> Loss: 1.5803212894572587 \t| Accuracy: 132.292\n",
      "# Iteration   139 -> Loss: 1.5787513556962096 \t| Accuracy: 132.292\n",
      "# Iteration   140 -> Loss: 1.5771887006830165 \t| Accuracy: 132.292\n",
      "# Iteration   141 -> Loss: 1.5756331424679615 \t| Accuracy: 131.667\n",
      "# Iteration   142 -> Loss: 1.574084507920883 \t| Accuracy: 131.667\n",
      "# Iteration   143 -> Loss: 1.5725426322882494 \t| Accuracy: 131.667\n",
      "# Iteration   144 -> Loss: 1.5710073587735005 \t| Accuracy: 131.667\n",
      "# Iteration   145 -> Loss: 1.5694785381396128 \t| Accuracy: 131.667\n",
      "# Iteration   146 -> Loss: 1.567956028332857 \t| Accuracy: 131.667\n",
      "# Iteration   147 -> Loss: 1.5664396941267085 \t| Accuracy: 131.667\n",
      "# Iteration   148 -> Loss: 1.5649294067848805 \t| Accuracy: 131.667\n",
      "# Iteration   149 -> Loss: 1.5634250437424697 \t| Accuracy: 131.667\n",
      "# Iteration   150 -> Loss: 1.5619264883042296 \t| Accuracy: 131.667\n",
      "# Iteration   151 -> Loss: 1.560433629358998 \t| Accuracy: 131.875\n",
      "# Iteration   152 -> Loss: 1.558946361109354 \t| Accuracy: 131.875\n",
      "# Iteration   153 -> Loss: 1.557464582815595 \t| Accuracy: 131.875\n",
      "# Iteration   154 -> Loss: 1.5559881985531792 \t| Accuracy: 131.875\n",
      "# Iteration   155 -> Loss: 1.5545171169827896 \t| Accuracy: 131.875\n",
      "# Iteration   156 -> Loss: 1.5530512511322485 \t| Accuracy: 131.875\n",
      "# Iteration   157 -> Loss: 1.5515905181895135 \t| Accuracy: 131.875\n",
      "# Iteration   158 -> Loss: 1.550134839306061 \t| Accuracy: 131.875\n",
      "# Iteration   159 -> Loss: 1.548684139409973 \t| Accuracy: 131.875\n",
      "# Iteration   160 -> Loss: 1.5472383470281073 \t| Accuracy: 131.875\n",
      "# Iteration   161 -> Loss: 1.54579739411675 \t| Accuracy: 131.458\n",
      "# Iteration   162 -> Loss: 1.5443612159002018 \t| Accuracy: 131.042\n",
      "# Iteration   163 -> Loss: 1.5429297507167798 \t| Accuracy: 131.042\n",
      "# Iteration   164 -> Loss: 1.5415029398717521 \t| Accuracy: 131.042\n",
      "# Iteration   165 -> Loss: 1.54008072749676 \t| Accuracy: 131.042\n",
      "# Iteration   166 -> Loss: 1.5386630604153144 \t| Accuracy: 131.042\n",
      "# Iteration   167 -> Loss: 1.5372498880139847 \t| Accuracy: 131.042\n",
      "# Iteration   168 -> Loss: 1.535841162118927 \t| Accuracy: 131.042\n",
      "# Iteration   169 -> Loss: 1.5344368368774306 \t| Accuracy: 131.042\n",
      "# Iteration   170 -> Loss: 1.5330368686441862 \t| Accuracy: 131.042\n",
      "# Iteration   171 -> Loss: 1.5316412158720012 \t| Accuracy: 131.042\n",
      "# Iteration   172 -> Loss: 1.5302498390067207 \t| Accuracy: 131.042\n",
      "# Iteration   173 -> Loss: 1.5288627003861226 \t| Accuracy: 131.042\n",
      "# Iteration   174 -> Loss: 1.5274797641425886 \t| Accuracy: 131.042\n",
      "# Iteration   175 -> Loss: 1.5261009961093588 \t| Accuracy: 131.042\n",
      "# Iteration   176 -> Loss: 1.5247263637302044 \t| Accuracy: 131.042\n",
      "# Iteration   177 -> Loss: 1.5233558359723713 \t| Accuracy: 131.042\n",
      "# Iteration   178 -> Loss: 1.521989383242644 \t| Accuracy: 131.042\n",
      "# Iteration   179 -> Loss: 1.5206269773064207 \t| Accuracy: 131.042\n",
      "# Iteration   180 -> Loss: 1.5192685912096797 \t| Accuracy: 130.625\n",
      "# Iteration   181 -> Loss: 1.517914199203732 \t| Accuracy: 130.625\n",
      "# Iteration   182 -> Loss: 1.5165637766726814 \t| Accuracy: 130.625\n",
      "# Iteration   183 -> Loss: 1.5152173000634954 \t| Accuracy: 130.625\n",
      "# Iteration   184 -> Loss: 1.5138747468186224 \t| Accuracy: 130.625\n",
      "# Iteration   185 -> Loss: 1.5125360953110811 \t| Accuracy: 130.625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Iteration   186 -> Loss: 1.5112013247819633 \t| Accuracy: 130.625\n",
      "# Iteration   187 -> Loss: 1.5098704152802898 \t| Accuracy: 130.625\n",
      "# Iteration   188 -> Loss: 1.5085433476051702 \t| Accuracy: 130.625\n",
      "# Iteration   189 -> Loss: 1.5072201032502148 \t| Accuracy: 130.625\n",
      "# Iteration   190 -> Loss: 1.5059006643501505 \t| Accuracy: 130.625\n",
      "# Iteration   191 -> Loss: 1.504585013629604 \t| Accuracy: 130.625\n",
      "# Iteration   192 -> Loss: 1.5032731343540024 \t| Accuracy: 130.625\n",
      "# Iteration   193 -> Loss: 1.5019650102825555 \t| Accuracy: 130.625\n",
      "# Iteration   194 -> Loss: 1.5006606256232806 \t| Accuracy: 130.625\n",
      "# Iteration   195 -> Loss: 1.4993599649900269 \t| Accuracy: 130.625\n",
      "# Iteration   196 -> Loss: 1.4980630133614683 \t| Accuracy: 130.208\n",
      "# Iteration   197 -> Loss: 1.4967697560420214 \t| Accuracy: 130.208\n",
      "# Iteration   198 -> Loss: 1.495480178624651 \t| Accuracy: 130.208\n",
      "# Iteration   199 -> Loss: 1.4941942669555275 \t| Accuracy: 130.208\n",
      "# Iteration   200 -> Loss: 1.4929120071004962 \t| Accuracy: 130.208\n",
      "# Iteration   201 -> Loss: 1.4916333853133177 \t| Accuracy: 130.208\n",
      "# Iteration   202 -> Loss: 1.4903583880056421 \t| Accuracy: 130.208\n",
      "# Iteration   203 -> Loss: 1.489087001718677 \t| Accuracy: 130.208\n",
      "# Iteration   204 -> Loss: 1.4878192130965031 \t| Accuracy: 130.208\n",
      "# Iteration   205 -> Loss: 1.4865550088610022 \t| Accuracy: 130.208\n",
      "# Iteration   206 -> Loss: 1.485294375788353 \t| Accuracy: 130.208\n",
      "# Iteration   207 -> Loss: 1.484037300687049 \t| Accuracy: 130.208\n",
      "# Iteration   208 -> Loss: 1.4827837703773978 \t| Accuracy: 130.208\n",
      "# Iteration   209 -> Loss: 1.4815337716724566 \t| Accuracy: 130.208\n",
      "# Iteration   210 -> Loss: 1.4802872913603602 \t| Accuracy: 130.208\n",
      "# Iteration   211 -> Loss: 1.479044316187995 \t| Accuracy: 130.208\n",
      "# Iteration   212 -> Loss: 1.4778048328459747 \t| Accuracy: 130.208\n",
      "# Iteration   213 -> Loss: 1.4765688279548725 \t| Accuracy: 130.208\n",
      "# Iteration   214 -> Loss: 1.4753362880526637 \t| Accuracy: 129.792\n",
      "# Iteration   215 -> Loss: 1.4741071995833324 \t| Accuracy: 129.792\n",
      "# Iteration   216 -> Loss: 1.472881548886598 \t| Accuracy: 129.792\n",
      "# Iteration   217 -> Loss: 1.471659322188718 \t| Accuracy: 129.792\n",
      "# Iteration   218 -> Loss: 1.4704405055943142 \t| Accuracy: 129.792\n",
      "# Iteration   219 -> Loss: 1.4692250850791904 \t| Accuracy: 129.792\n",
      "# Iteration   220 -> Loss: 1.468013046484084 \t| Accuracy: 129.792\n",
      "# Iteration   221 -> Loss: 1.466804375509318 \t| Accuracy: 129.792\n",
      "# Iteration   222 -> Loss: 1.4655990577103049 \t| Accuracy: 130.000\n",
      "# Iteration   223 -> Loss: 1.4643970784938616 \t| Accuracy: 130.000\n",
      "# Iteration   224 -> Loss: 1.4631984231152937 \t| Accuracy: 130.208\n",
      "# Iteration   225 -> Loss: 1.4620030766762078 \t| Accuracy: 130.208\n",
      "# Iteration   226 -> Loss: 1.4608110241230166 \t| Accuracy: 130.208\n",
      "# Iteration   227 -> Loss: 1.4596222502460876 \t| Accuracy: 130.208\n",
      "# Iteration   228 -> Loss: 1.45843673967951 \t| Accuracy: 130.208\n",
      "# Iteration   229 -> Loss: 1.4572544769014324 \t| Accuracy: 130.208\n",
      "# Iteration   230 -> Loss: 1.4560754462349446 \t| Accuracy: 130.208\n",
      "# Iteration   231 -> Loss: 1.4548996318494578 \t| Accuracy: 130.208\n",
      "# Iteration   232 -> Loss: 1.4537270177625647 \t| Accuracy: 130.208\n",
      "# Iteration   233 -> Loss: 1.4525575878423322 \t| Accuracy: 130.208\n",
      "# Iteration   234 -> Loss: 1.4513913258100029 \t| Accuracy: 130.208\n",
      "# Iteration   235 -> Loss: 1.4502282152430794 \t| Accuracy: 130.208\n",
      "# Iteration   236 -> Loss: 1.449068239578752 \t| Accuracy: 130.208\n",
      "# Iteration   237 -> Loss: 1.4479113821176521 \t| Accuracy: 130.208\n",
      "# Iteration   238 -> Loss: 1.4467576260278978 \t| Accuracy: 130.208\n",
      "# Iteration   239 -> Loss: 1.4456069543494148 \t| Accuracy: 130.208\n",
      "# Iteration   240 -> Loss: 1.4444593499984957 \t| Accuracy: 130.208\n",
      "# Iteration   241 -> Loss: 1.4433147957725894 \t| Accuracy: 130.208\n",
      "# Iteration   242 -> Loss: 1.4421732743552889 \t| Accuracy: 130.208\n",
      "# Iteration   243 -> Loss: 1.4410347683215006 \t| Accuracy: 130.208\n",
      "# Iteration   244 -> Loss: 1.4398992601427736 \t| Accuracy: 130.208\n",
      "# Iteration   245 -> Loss: 1.4387667321927737 \t| Accuracy: 130.208\n",
      "# Iteration   246 -> Loss: 1.437637166752882 \t| Accuracy: 130.208\n",
      "# Iteration   247 -> Loss: 1.4365105460179022 \t| Accuracy: 130.208\n",
      "# Iteration   248 -> Loss: 1.435386852101861 \t| Accuracy: 130.208\n",
      "# Iteration   249 -> Loss: 1.4342660670438894 \t| Accuracy: 130.208\n",
      "# Iteration   250 -> Loss: 1.433148172814167 \t| Accuracy: 130.208\n",
      "# Iteration   251 -> Loss: 1.4320331513199214 \t| Accuracy: 130.208\n",
      "# Iteration   252 -> Loss: 1.430920984411466 \t| Accuracy: 130.208\n",
      "# Iteration   253 -> Loss: 1.429811653888271 \t| Accuracy: 130.208\n",
      "# Iteration   254 -> Loss: 1.4287051415050491 \t| Accuracy: 130.208\n",
      "# Iteration   255 -> Loss: 1.4276014289778578 \t| Accuracy: 130.208\n",
      "# Iteration   256 -> Loss: 1.4265004979901954 \t| Accuracy: 130.208\n",
      "# Iteration   257 -> Loss: 1.4254023301990981 \t| Accuracy: 130.208\n",
      "# Iteration   258 -> Loss: 1.4243069072412158 \t| Accuracy: 130.208\n",
      "# Iteration   259 -> Loss: 1.4232142107388706 \t| Accuracy: 130.208\n",
      "# Iteration   260 -> Loss: 1.4221242223060875 \t| Accuracy: 130.417\n",
      "# Iteration   261 -> Loss: 1.4210369235545874 \t| Accuracy: 130.417\n",
      "# Iteration   262 -> Loss: 1.4199522960997493 \t| Accuracy: 130.417\n",
      "# Iteration   263 -> Loss: 1.4188703215665204 \t| Accuracy: 130.417\n",
      "# Iteration   264 -> Loss: 1.417790981595283 \t| Accuracy: 130.417\n",
      "# Iteration   265 -> Loss: 1.4167142578476692 \t| Accuracy: 130.417\n",
      "# Iteration   266 -> Loss: 1.4156401320123173 \t| Accuracy: 130.417\n",
      "# Iteration   267 -> Loss: 1.4145685858105717 \t| Accuracy: 130.417\n",
      "# Iteration   268 -> Loss: 1.4134996010021192 \t| Accuracy: 130.417\n",
      "# Iteration   269 -> Loss: 1.4124331593905612 \t| Accuracy: 130.417\n",
      "# Iteration   270 -> Loss: 1.4113692428289197 \t| Accuracy: 130.417\n",
      "# Iteration   271 -> Loss: 1.4103078332250694 \t| Accuracy: 130.417\n",
      "# Iteration   272 -> Loss: 1.4092489125471057 \t| Accuracy: 130.417\n",
      "# Iteration   273 -> Loss: 1.408192462828633 \t| Accuracy: 130.417\n",
      "# Iteration   274 -> Loss: 1.4071384661739805 \t| Accuracy: 130.417\n",
      "# Iteration   275 -> Loss: 1.4060869047633426 \t| Accuracy: 130.417\n",
      "# Iteration   276 -> Loss: 1.405037760857839 \t| Accuracy: 130.417\n",
      "# Iteration   277 -> Loss: 1.403991016804497 \t| Accuracy: 130.417\n",
      "# Iteration   278 -> Loss: 1.4029466550411542 \t| Accuracy: 130.417\n",
      "# Iteration   279 -> Loss: 1.4019046581012775 \t| Accuracy: 130.417\n",
      "# Iteration   280 -> Loss: 1.4008650086187022 \t| Accuracy: 130.417\n",
      "# Iteration   281 -> Loss: 1.399827689332283 \t| Accuracy: 130.417\n",
      "# Iteration   282 -> Loss: 1.398792683090466 \t| Accuracy: 130.417\n",
      "# Iteration   283 -> Loss: 1.3977599728557697 \t| Accuracy: 130.417\n",
      "# Iteration   284 -> Loss: 1.3967295417091814 \t| Accuracy: 130.417\n",
      "# Iteration   285 -> Loss: 1.395701372854466 \t| Accuracy: 130.417\n",
      "# Iteration   286 -> Loss: 1.3946754496223863 \t| Accuracy: 130.417\n",
      "# Iteration   287 -> Loss: 1.3936517554748282 \t| Accuracy: 130.417\n",
      "# Iteration   288 -> Loss: 1.392630274008841 \t| Accuracy: 130.417\n",
      "# Iteration   289 -> Loss: 1.3916109889605823 \t| Accuracy: 130.417\n",
      "# Iteration   290 -> Loss: 1.3905938842091685 \t| Accuracy: 130.417\n",
      "# Iteration   291 -> Loss: 1.3895789437804311 \t| Accuracy: 130.417\n",
      "# Iteration   292 -> Loss: 1.3885661518505747 \t| Accuracy: 130.417\n",
      "# Iteration   293 -> Loss: 1.3875554927497398 \t| Accuracy: 130.417\n",
      "# Iteration   294 -> Loss: 1.3865469509654644 \t| Accuracy: 130.417\n",
      "# Iteration   295 -> Loss: 1.3855405111460437 \t| Accuracy: 130.417\n",
      "# Iteration   296 -> Loss: 1.3845361581037883 \t| Accuracy: 130.417\n",
      "# Iteration   297 -> Loss: 1.3835338768181784 \t| Accuracy: 130.417\n",
      "# Iteration   298 -> Loss: 1.3825336524389096 \t| Accuracy: 130.417\n",
      "# Iteration   299 -> Loss: 1.381535470288831 \t| Accuracy: 130.417\n",
      "# Iteration   300 -> Loss: 1.3805393158667763 \t| Accuracy: 130.417\n",
      "# Iteration   301 -> Loss: 1.379545174850278 \t| Accuracy: 130.417\n",
      "# Iteration   302 -> Loss: 1.3785530330981721 \t| Accuracy: 130.417\n",
      "# Iteration   303 -> Loss: 1.377562876653083 \t| Accuracy: 130.417\n",
      "# Iteration   304 -> Loss: 1.3765746917437924 \t| Accuracy: 130.417\n",
      "# Iteration   305 -> Loss: 1.3755884647874879 \t| Accuracy: 130.417\n",
      "# Iteration   306 -> Loss: 1.3746041823918873 \t| Accuracy: 130.417\n",
      "# Iteration   307 -> Loss: 1.3736218313572417 \t| Accuracy: 130.417\n",
      "# Iteration   308 -> Loss: 1.3726413986782073 \t| Accuracy: 130.417\n",
      "# Iteration   309 -> Loss: 1.371662871545593 \t| Accuracy: 130.417\n",
      "# Iteration   310 -> Loss: 1.3706862373479738 \t| Accuracy: 130.417\n",
      "# Iteration   311 -> Loss: 1.3697114836731705 \t| Accuracy: 130.417\n",
      "# Iteration   312 -> Loss: 1.3687385983096003 \t| Accuracy: 130.417\n",
      "# Iteration   313 -> Loss: 1.367767569247481 \t| Accuracy: 130.417\n",
      "# Iteration   314 -> Loss: 1.366798384679904 \t| Accuracy: 130.417\n",
      "# Iteration   315 -> Loss: 1.3658310330037615 \t| Accuracy: 130.417\n",
      "# Iteration   316 -> Loss: 1.364865502820533 \t| Accuracy: 130.417\n",
      "# Iteration   317 -> Loss: 1.3639017829369287 \t| Accuracy: 130.417\n",
      "# Iteration   318 -> Loss: 1.3629398623653817 \t| Accuracy: 130.417\n",
      "# Iteration   319 -> Loss: 1.3619797303244 \t| Accuracy: 130.417\n",
      "# Iteration   320 -> Loss: 1.361021376238762 \t| Accuracy: 130.417\n",
      "# Iteration   321 -> Loss: 1.3600647897395701 \t| Accuracy: 130.417\n",
      "# Iteration   322 -> Loss: 1.3591099606641446 \t| Accuracy: 130.417\n",
      "# Iteration   323 -> Loss: 1.3581568790557725 \t| Accuracy: 130.417\n",
      "# Iteration   324 -> Loss: 1.3572055351633021 \t| Accuracy: 130.417\n",
      "# Iteration   325 -> Loss: 1.3562559194405817 \t| Accuracy: 130.417\n",
      "# Iteration   326 -> Loss: 1.3553080225457472 \t| Accuracy: 130.417\n",
      "# Iteration   327 -> Loss: 1.3543618353403588 \t| Accuracy: 130.417\n",
      "# Iteration   328 -> Loss: 1.353417348888377 \t| Accuracy: 129.792\n",
      "# Iteration   329 -> Loss: 1.352474554454994 \t| Accuracy: 129.792\n",
      "# Iteration   330 -> Loss: 1.351533443505308 \t| Accuracy: 129.792\n",
      "# Iteration   331 -> Loss: 1.3505940077028449 \t| Accuracy: 129.792\n",
      "# Iteration   332 -> Loss: 1.349656238907933 \t| Accuracy: 129.792\n",
      "# Iteration   333 -> Loss: 1.3487201291759277 \t| Accuracy: 129.792\n",
      "# Iteration   334 -> Loss: 1.347785670755284 \t| Accuracy: 129.792\n",
      "# Iteration   335 -> Loss: 1.3468528560854907 \t| Accuracy: 129.792\n",
      "# Iteration   336 -> Loss: 1.3459216777948526 \t| Accuracy: 129.792\n",
      "# Iteration   337 -> Loss: 1.344992128698138 \t| Accuracy: 129.792\n",
      "# Iteration   338 -> Loss: 1.3440642017940823 \t| Accuracy: 129.792\n",
      "# Iteration   339 -> Loss: 1.3431378902627582 \t| Accuracy: 129.792\n",
      "# Iteration   340 -> Loss: 1.3422131874628118 \t| Accuracy: 129.792\n",
      "# Iteration   341 -> Loss: 1.3412900869285673 \t| Accuracy: 129.792\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Iteration   342 -> Loss: 1.3403685823670077 \t| Accuracy: 129.792\n",
      "# Iteration   343 -> Loss: 1.3394486676546327 \t| Accuracy: 129.792\n",
      "# Iteration   344 -> Loss: 1.3385303368341945 \t| Accuracy: 129.792\n",
      "# Iteration   345 -> Loss: 1.3376135841113248 \t| Accuracy: 129.792\n",
      "# Iteration   346 -> Loss: 1.3366984038510468 \t| Accuracy: 129.792\n",
      "# Iteration   347 -> Loss: 1.3357847905741853 \t| Accuracy: 129.792\n",
      "# Iteration   348 -> Loss: 1.3348727389536739 \t| Accuracy: 129.792\n",
      "# Iteration   349 -> Loss: 1.3339622438107686 \t| Accuracy: 129.792\n",
      "# Iteration   350 -> Loss: 1.3330533001111704 \t| Accuracy: 129.792\n",
      "# Iteration   351 -> Loss: 1.3321459029610612 \t| Accuracy: 129.792\n",
      "# Iteration   352 -> Loss: 1.3312400476030664 \t| Accuracy: 129.792\n",
      "# Iteration   353 -> Loss: 1.3303357294121336 \t| Accuracy: 129.792\n",
      "# Iteration   354 -> Loss: 1.3294329438913546 \t| Accuracy: 129.792\n",
      "# Iteration   355 -> Loss: 1.328531686667716 \t| Accuracy: 129.792\n",
      "# Iteration   356 -> Loss: 1.3276319534878 \t| Accuracy: 129.792\n",
      "# Iteration   357 -> Loss: 1.326733740213432 \t| Accuracy: 129.792\n",
      "# Iteration   358 -> Loss: 1.3258370428172848 \t| Accuracy: 129.792\n",
      "# Iteration   359 -> Loss: 1.324941857378446 \t| Accuracy: 129.792\n",
      "# Iteration   360 -> Loss: 1.3240481800779524 \t| Accuracy: 129.792\n",
      "# Iteration   361 -> Loss: 1.323156007194302 \t| Accuracy: 129.792\n",
      "# Iteration   362 -> Loss: 1.3222653350989428 \t| Accuracy: 129.792\n",
      "# Iteration   363 -> Loss: 1.3213761602517526 \t| Accuracy: 129.792\n",
      "# Iteration   364 -> Loss: 1.3204884791965124 \t| Accuracy: 129.792\n",
      "# Iteration   365 -> Loss: 1.3196022885563752 \t| Accuracy: 129.792\n",
      "# Iteration   366 -> Loss: 1.318717585029347 \t| Accuracy: 129.792\n",
      "# Iteration   367 -> Loss: 1.3178343653837754 \t| Accuracy: 129.792\n",
      "# Iteration   368 -> Loss: 1.3169526264538576 \t| Accuracy: 129.792\n",
      "# Iteration   369 -> Loss: 1.3160723651351731 \t| Accuracy: 129.792\n",
      "# Iteration   370 -> Loss: 1.3151935783802466 \t| Accuracy: 129.792\n",
      "# Iteration   371 -> Loss: 1.314316263194146 \t| Accuracy: 129.792\n",
      "# Iteration   372 -> Loss: 1.3134404166301215 \t| Accuracy: 129.792\n",
      "# Iteration   373 -> Loss: 1.3125660357852937 \t| Accuracy: 129.792\n",
      "# Iteration   374 -> Loss: 1.3116931177963906 \t| Accuracy: 129.792\n",
      "# Iteration   375 -> Loss: 1.3108216598355473 \t| Accuracy: 129.792\n",
      "# Iteration   376 -> Loss: 1.3099516591061637 \t| Accuracy: 129.792\n",
      "# Iteration   377 -> Loss: 1.3090831128388318 \t| Accuracy: 129.792\n",
      "# Iteration   378 -> Loss: 1.3082160182873372 \t| Accuracy: 129.792\n",
      "# Iteration   379 -> Loss: 1.3073503727247346 \t| Accuracy: 129.792\n",
      "# Iteration   380 -> Loss: 1.306486173439507 \t| Accuracy: 129.792\n",
      "# Iteration   381 -> Loss: 1.3056234177318096 \t| Accuracy: 129.792\n",
      "# Iteration   382 -> Loss: 1.3047621029098044 \t| Accuracy: 129.792\n",
      "# Iteration   383 -> Loss: 1.3039022262860858 \t| Accuracy: 129.792\n",
      "# Iteration   384 -> Loss: 1.3030437851742087 \t| Accuracy: 129.792\n",
      "# Iteration   385 -> Loss: 1.3021867768853104 \t| Accuracy: 129.792\n",
      "# Iteration   386 -> Loss: 1.301331198724841 \t| Accuracy: 129.792\n",
      "# Iteration   387 -> Loss: 1.3004770479893988 \t| Accuracy: 129.792\n",
      "# Iteration   388 -> Loss: 1.2996243219636754 \t| Accuracy: 129.792\n",
      "# Iteration   389 -> Loss: 1.298773017917513 \t| Accuracy: 129.792\n",
      "# Iteration   390 -> Loss: 1.297923133103075 \t| Accuracy: 129.792\n",
      "# Iteration   391 -> Loss: 1.2970746647521327 \t| Accuracy: 129.792\n",
      "# Iteration   392 -> Loss: 1.2962276100734738 \t| Accuracy: 129.792\n",
      "# Iteration   393 -> Loss: 1.2953819662504236 \t| Accuracy: 129.792\n",
      "# Iteration   394 -> Loss: 1.2945377304384944 \t| Accuracy: 129.792\n",
      "# Iteration   395 -> Loss: 1.2936948997631519 \t| Accuracy: 129.792\n",
      "# Iteration   396 -> Loss: 1.2928534713177082 \t| Accuracy: 129.792\n",
      "# Iteration   397 -> Loss: 1.292013442161337 \t| Accuracy: 129.792\n",
      "# Iteration   398 -> Loss: 1.2911748093172108 \t| Accuracy: 129.792\n",
      "# Iteration   399 -> Loss: 1.2903375697707684 \t| Accuracy: 129.792\n",
      "# Iteration   400 -> Loss: 1.2895017204681003 \t| Accuracy: 129.792\n",
      "# Iteration   401 -> Loss: 1.2886672583144623 \t| Accuracy: 129.792\n",
      "# Iteration   402 -> Loss: 1.287834180172912 \t| Accuracy: 129.792\n",
      "# Iteration   403 -> Loss: 1.2870024828630688 \t| Accuracy: 129.792\n",
      "# Iteration   404 -> Loss: 1.2861721631599934 \t| Accuracy: 129.792\n",
      "# Iteration   405 -> Loss: 1.2853432177931936 \t| Accuracy: 129.792\n",
      "# Iteration   406 -> Loss: 1.284515643445746 \t| Accuracy: 129.792\n",
      "# Iteration   407 -> Loss: 1.2836894367535368 \t| Accuracy: 129.792\n",
      "# Iteration   408 -> Loss: 1.2828645943046215 \t| Accuracy: 129.792\n",
      "# Iteration   409 -> Loss: 1.282041112638699 \t| Accuracy: 129.792\n",
      "# Iteration   410 -> Loss: 1.2812189882466962 \t| Accuracy: 129.792\n",
      "# Iteration   411 -> Loss: 1.280398217570468 \t| Accuracy: 129.792\n",
      "# Iteration   412 -> Loss: 1.279578797002603 \t| Accuracy: 129.792\n",
      "# Iteration   413 -> Loss: 1.278760722886335 \t| Accuracy: 129.792\n",
      "# Iteration   414 -> Loss: 1.277943991515563 \t| Accuracy: 129.792\n",
      "# Iteration   415 -> Loss: 1.2771285991349657 \t| Accuracy: 129.792\n",
      "# Iteration   416 -> Loss: 1.276314541940219 \t| Accuracy: 129.792\n",
      "# Iteration   417 -> Loss: 1.2755018160783056 \t| Accuracy: 129.792\n",
      "# Iteration   418 -> Loss: 1.2746904176479192 \t| Accuracy: 129.792\n",
      "# Iteration   419 -> Loss: 1.2738803426999572 \t| Accuracy: 129.792\n",
      "# Iteration   420 -> Loss: 1.2730715872380975 \t| Accuracy: 129.792\n",
      "# Iteration   421 -> Loss: 1.27226414721946 \t| Accuracy: 129.792\n",
      "# Iteration   422 -> Loss: 1.271458018555346 \t| Accuracy: 129.792\n",
      "# Iteration   423 -> Loss: 1.270653197112052 \t| Accuracy: 129.792\n",
      "# Iteration   424 -> Loss: 1.269849678711762 \t| Accuracy: 129.792\n",
      "# Iteration   425 -> Loss: 1.2690474591334975 \t| Accuracy: 129.792\n",
      "# Iteration   426 -> Loss: 1.2682465341141438 \t| Accuracy: 129.792\n",
      "# Iteration   427 -> Loss: 1.26744689934953 \t| Accuracy: 129.792\n",
      "# Iteration   428 -> Loss: 1.2666485504955736 \t| Accuracy: 129.792\n",
      "# Iteration   429 -> Loss: 1.2658514831694712 \t| Accuracy: 129.792\n",
      "# Iteration   430 -> Loss: 1.265055692950949 \t| Accuracy: 129.792\n",
      "# Iteration   431 -> Loss: 1.2642611753835558 \t| Accuracy: 129.792\n",
      "# Iteration   432 -> Loss: 1.2634679259759973 \t| Accuracy: 129.792\n",
      "# Iteration   433 -> Loss: 1.2626759402035195 \t| Accuracy: 129.792\n",
      "# Iteration   434 -> Loss: 1.2618852135093188 \t| Accuracy: 129.792\n",
      "# Iteration   435 -> Loss: 1.2610957413059924 \t| Accuracy: 129.792\n",
      "# Iteration   436 -> Loss: 1.2603075189770176 \t| Accuracy: 129.792\n",
      "# Iteration   437 -> Loss: 1.2595205418782542 \t| Accuracy: 129.792\n",
      "# Iteration   438 -> Loss: 1.2587348053394771 \t| Accuracy: 129.792\n",
      "# Iteration   439 -> Loss: 1.2579503046659226 \t| Accuracy: 129.792\n",
      "# Iteration   440 -> Loss: 1.2571670351398574 \t| Accuracy: 129.792\n",
      "# Iteration   441 -> Loss: 1.2563849920221593 \t| Accuracy: 129.792\n",
      "# Iteration   442 -> Loss: 1.2556041705539105 \t| Accuracy: 129.792\n",
      "# Iteration   443 -> Loss: 1.2548245659579995 \t| Accuracy: 129.792\n",
      "# Iteration   444 -> Loss: 1.254046173440729 \t| Accuracy: 129.792\n",
      "# Iteration   445 -> Loss: 1.2532689881934276 \t| Accuracy: 129.792\n",
      "# Iteration   446 -> Loss: 1.2524930053940626 \t| Accuracy: 129.792\n",
      "# Iteration   447 -> Loss: 1.2517182202088504 \t| Accuracy: 129.792\n",
      "# Iteration   448 -> Loss: 1.2509446277938658 \t| Accuracy: 129.792\n",
      "# Iteration   449 -> Loss: 1.2501722232966423 \t| Accuracy: 129.792\n",
      "# Iteration   450 -> Loss: 1.2494010018577677 \t| Accuracy: 129.792\n",
      "# Iteration   451 -> Loss: 1.248630958612467 \t| Accuracy: 129.792\n",
      "# Iteration   452 -> Loss: 1.2478620886921756 \t| Accuracy: 129.792\n",
      "# Iteration   453 -> Loss: 1.2470943872260993 \t| Accuracy: 129.792\n",
      "# Iteration   454 -> Loss: 1.246327849342758 \t| Accuracy: 129.792\n",
      "# Iteration   455 -> Loss: 1.2455624701715133 \t| Accuracy: 129.792\n",
      "# Iteration   456 -> Loss: 1.2447982448440793 \t| Accuracy: 129.792\n",
      "# Iteration   457 -> Loss: 1.244035168496013 \t| Accuracy: 129.792\n",
      "# Iteration   458 -> Loss: 1.243273236268185 \t| Accuracy: 129.792\n",
      "# Iteration   459 -> Loss: 1.2425124433082293 \t| Accuracy: 129.792\n",
      "# Iteration   460 -> Loss: 1.241752784771968 \t| Accuracy: 129.792\n",
      "# Iteration   461 -> Loss: 1.2409942558248164 \t| Accuracy: 129.792\n",
      "# Iteration   462 -> Loss: 1.240236851643162 \t| Accuracy: 129.792\n",
      "# Iteration   463 -> Loss: 1.2394805674157188 \t| Accuracy: 129.792\n",
      "# Iteration   464 -> Loss: 1.2387253983448554 \t| Accuracy: 129.792\n",
      "# Iteration   465 -> Loss: 1.2379713396478995 \t| Accuracy: 129.792\n",
      "# Iteration   466 -> Loss: 1.2372183865584139 \t| Accuracy: 129.792\n",
      "# Iteration   467 -> Loss: 1.2364665343274461 \t| Accuracy: 129.792\n",
      "# Iteration   468 -> Loss: 1.2357157782247523 \t| Accuracy: 129.792\n",
      "# Iteration   469 -> Loss: 1.2349661135399916 \t| Accuracy: 129.792\n",
      "# Iteration   470 -> Loss: 1.234217535583896 \t| Accuracy: 129.792\n",
      "# Iteration   471 -> Loss: 1.2334700396894085 \t| Accuracy: 129.792\n",
      "# Iteration   472 -> Loss: 1.2327236212127994 \t| Accuracy: 129.792\n",
      "# Iteration   473 -> Loss: 1.23197827553475 \t| Accuracy: 129.792\n",
      "# Iteration   474 -> Loss: 1.2312339980614107 \t| Accuracy: 129.792\n",
      "# Iteration   475 -> Loss: 1.230490784225434 \t| Accuracy: 129.792\n",
      "# Iteration   476 -> Loss: 1.2297486294869766 \t| Accuracy: 129.792\n",
      "# Iteration   477 -> Loss: 1.2290075293346767 \t| Accuracy: 129.792\n",
      "# Iteration   478 -> Loss: 1.228267479286605 \t| Accuracy: 129.792\n",
      "# Iteration   479 -> Loss: 1.227528474891189 \t| Accuracy: 129.792\n",
      "# Iteration   480 -> Loss: 1.2267905117281104 \t| Accuracy: 129.792\n",
      "# Iteration   481 -> Loss: 1.226053585409177 \t| Accuracy: 129.792\n",
      "# Iteration   482 -> Loss: 1.2253176915791717 \t| Accuracy: 129.792\n",
      "# Iteration   483 -> Loss: 1.2245828259166733 \t| Accuracy: 129.792\n",
      "# Iteration   484 -> Loss: 1.223848984134854 \t| Accuracy: 129.792\n",
      "# Iteration   485 -> Loss: 1.2231161619822548 \t| Accuracy: 129.792\n",
      "# Iteration   486 -> Loss: 1.2223843552435332 \t| Accuracy: 129.792\n",
      "# Iteration   487 -> Loss: 1.2216535597401919 \t| Accuracy: 129.792\n",
      "# Iteration   488 -> Loss: 1.2209237713312826 \t| Accuracy: 129.792\n",
      "# Iteration   489 -> Loss: 1.2201949859140868 \t| Accuracy: 129.792\n",
      "# Iteration   490 -> Loss: 1.219467199424777 \t| Accuracy: 129.792\n",
      "# Iteration   491 -> Loss: 1.2187404078390554 \t| Accuracy: 129.792\n",
      "# Iteration   492 -> Loss: 1.2180146071727698 \t| Accuracy: 129.792\n",
      "# Iteration   493 -> Loss: 1.2172897934825135 \t| Accuracy: 129.792\n",
      "# Iteration   494 -> Loss: 1.2165659628661996 \t| Accuracy: 129.792\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Iteration   495 -> Loss: 1.215843111463621 \t| Accuracy: 129.792\n",
      "# Iteration   496 -> Loss: 1.2151212354569876 \t| Accuracy: 129.792\n",
      "# Iteration   497 -> Loss: 1.2144003310714448 \t| Accuracy: 129.792\n",
      "# Iteration   498 -> Loss: 1.213680394575576 \t| Accuracy: 129.792\n",
      "# Iteration   499 -> Loss: 1.212961422281885 \t| Accuracy: 129.792\n",
      "# Iteration   500 -> Loss: 1.2122434105472606 \t| Accuracy: 129.792\n",
      "# Iteration   501 -> Loss: 1.2115263557734255 \t| Accuracy: 129.792\n",
      "# Iteration   502 -> Loss: 1.210810254407365 \t| Accuracy: 129.792\n",
      "# Iteration   503 -> Loss: 1.210095102941744 \t| Accuracy: 129.792\n",
      "# Iteration   504 -> Loss: 1.2093808979153002 \t| Accuracy: 129.792\n",
      "# Iteration   505 -> Loss: 1.2086676359132293 \t| Accuracy: 129.792\n",
      "# Iteration   506 -> Loss: 1.2079553135675472 \t| Accuracy: 129.792\n",
      "# Iteration   507 -> Loss: 1.20724392755744 \t| Accuracy: 129.792\n",
      "# Iteration   508 -> Loss: 1.2065334746095975 \t| Accuracy: 129.792\n",
      "# Iteration   509 -> Loss: 1.2058239514985303 \t| Accuracy: 129.792\n",
      "# Iteration   510 -> Loss: 1.2051153550468747 \t| Accuracy: 129.792\n",
      "# Iteration   511 -> Loss: 1.204407682125677 \t| Accuracy: 129.792\n",
      "# Iteration   512 -> Loss: 1.2037009296546668 \t| Accuracy: 129.792\n",
      "# Iteration   513 -> Loss: 1.2029950946025147 \t| Accuracy: 129.792\n",
      "# Iteration   514 -> Loss: 1.2022901739870737 \t| Accuracy: 129.792\n",
      "# Iteration   515 -> Loss: 1.2015861648756043 \t| Accuracy: 129.792\n",
      "# Iteration   516 -> Loss: 1.200883064384989 \t| Accuracy: 129.792\n",
      "# Iteration   517 -> Loss: 1.2001808696819272 \t| Accuracy: 129.792\n",
      "# Iteration   518 -> Loss: 1.1994795779831169 \t| Accuracy: 129.792\n",
      "# Iteration   519 -> Loss: 1.1987791865554218 \t| Accuracy: 129.792\n",
      "# Iteration   520 -> Loss: 1.1980796927160209 \t| Accuracy: 129.792\n",
      "# Iteration   521 -> Loss: 1.197381093832545 \t| Accuracy: 129.792\n",
      "# Iteration   522 -> Loss: 1.1966833873231977 \t| Accuracy: 129.792\n",
      "# Iteration   523 -> Loss: 1.1959865706568575 \t| Accuracy: 129.792\n",
      "# Iteration   524 -> Loss: 1.1952906413531688 \t| Accuracy: 129.792\n",
      "# Iteration   525 -> Loss: 1.1945955969826132 \t| Accuracy: 129.792\n",
      "# Iteration   526 -> Loss: 1.1939014351665662 \t| Accuracy: 129.792\n",
      "# Iteration   527 -> Loss: 1.1932081535773382 \t| Accuracy: 129.792\n",
      "# Iteration   528 -> Loss: 1.1925157499381966 \t| Accuracy: 129.792\n",
      "# Iteration   529 -> Loss: 1.1918242220233746 \t| Accuracy: 129.792\n",
      "# Iteration   530 -> Loss: 1.1911335676580592 \t| Accuracy: 129.792\n",
      "# Iteration   531 -> Loss: 1.1904437847183644 \t| Accuracy: 129.792\n",
      "# Iteration   532 -> Loss: 1.1897548711312866 \t| Accuracy: 129.792\n",
      "# Iteration   533 -> Loss: 1.1890668248746428 \t| Accuracy: 129.792\n",
      "# Iteration   534 -> Loss: 1.1883796439769871 \t| Accuracy: 129.792\n",
      "# Iteration   535 -> Loss: 1.1876933265175156 \t| Accuracy: 129.792\n",
      "# Iteration   536 -> Loss: 1.1870078706259468 \t| Accuracy: 129.792\n",
      "# Iteration   537 -> Loss: 1.1863232744823857 \t| Accuracy: 129.792\n",
      "# Iteration   538 -> Loss: 1.18563953631717 \t| Accuracy: 129.792\n",
      "# Iteration   539 -> Loss: 1.1849566544106938 \t| Accuracy: 129.792\n",
      "# Iteration   540 -> Loss: 1.1842746270932163 \t| Accuracy: 129.792\n",
      "# Iteration   541 -> Loss: 1.1835934527446468 \t| Accuracy: 129.792\n",
      "# Iteration   542 -> Loss: 1.1829131297943116 \t| Accuracy: 129.792\n",
      "# Iteration   543 -> Loss: 1.1822336567207006 \t| Accuracy: 129.792\n",
      "# Iteration   544 -> Loss: 1.1815550320511936 \t| Accuracy: 129.792\n",
      "# Iteration   545 -> Loss: 1.1808772543617656 \t| Accuracy: 129.792\n",
      "# Iteration   546 -> Loss: 1.180200322276671 \t| Accuracy: 129.792\n",
      "# Iteration   547 -> Loss: 1.1795242344681085 \t| Accuracy: 129.792\n",
      "# Iteration   548 -> Loss: 1.178848989655863 \t| Accuracy: 129.792\n",
      "# Iteration   549 -> Loss: 1.1781745866069278 \t| Accuracy: 129.792\n",
      "# Iteration   550 -> Loss: 1.177501024135104 \t| Accuracy: 129.792\n",
      "# Iteration   551 -> Loss: 1.1768283011005796 \t| Accuracy: 129.792\n",
      "# Iteration   552 -> Loss: 1.1761564164094869 \t| Accuracy: 129.792\n",
      "# Iteration   553 -> Loss: 1.1754853690134381 \t| Accuracy: 129.792\n",
      "# Iteration   554 -> Loss: 1.1748151579090387 \t| Accuracy: 129.792\n",
      "# Iteration   555 -> Loss: 1.174145782137381 \t| Accuracy: 129.792\n",
      "# Iteration   556 -> Loss: 1.1734772407835137 \t| Accuracy: 129.792\n",
      "# Iteration   557 -> Loss: 1.1728095329758914 \t| Accuracy: 129.792\n",
      "# Iteration   558 -> Loss: 1.1721426578858032 \t| Accuracy: 129.792\n",
      "# Iteration   559 -> Loss: 1.1714766147267772 \t| Accuracy: 129.792\n",
      "# Iteration   560 -> Loss: 1.1708114027539667 \t| Accuracy: 129.792\n",
      "# Iteration   561 -> Loss: 1.1701470212635143 \t| Accuracy: 129.792\n",
      "# Iteration   562 -> Loss: 1.1694834695918934 \t| Accuracy: 129.792\n",
      "# Iteration   563 -> Loss: 1.1688207471152312 \t| Accuracy: 129.792\n",
      "# Iteration   564 -> Loss: 1.1681588532486098 \t| Accuracy: 129.792\n",
      "# Iteration   565 -> Loss: 1.1674977874453472 \t| Accuracy: 129.792\n",
      "# Iteration   566 -> Loss: 1.16683754919626 \t| Accuracy: 129.792\n",
      "# Iteration   567 -> Loss: 1.1661781380289025 \t| Accuracy: 129.792\n",
      "# Iteration   568 -> Loss: 1.1655195535067915 \t| Accuracy: 129.792\n",
      "# Iteration   569 -> Loss: 1.1648617952286078 \t| Accuracy: 129.792\n",
      "# Iteration   570 -> Loss: 1.1642048628273827 \t| Accuracy: 129.792\n",
      "# Iteration   571 -> Loss: 1.1635487559696638 \t| Accuracy: 129.792\n",
      "# Iteration   572 -> Loss: 1.162893474354665 \t| Accuracy: 129.792\n",
      "# Iteration   573 -> Loss: 1.1622390177133983 \t| Accuracy: 129.792\n",
      "# Iteration   574 -> Loss: 1.1615853858077902 \t| Accuracy: 129.792\n",
      "# Iteration   575 -> Loss: 1.1609325784297813 \t| Accuracy: 129.792\n",
      "# Iteration   576 -> Loss: 1.16028059540041 \t| Accuracy: 129.792\n",
      "# Iteration   577 -> Loss: 1.159629436568884 \t| Accuracy: 129.792\n",
      "# Iteration   578 -> Loss: 1.1589791018116349 \t| Accuracy: 129.792\n",
      "# Iteration   579 -> Loss: 1.15832959103136 \t| Accuracy: 129.792\n",
      "# Iteration   580 -> Loss: 1.1576809041560532 \t| Accuracy: 129.792\n",
      "# Iteration   581 -> Loss: 1.1570330411380216 \t| Accuracy: 129.792\n",
      "# Iteration   582 -> Loss: 1.1563860019528922 \t| Accuracy: 129.792\n",
      "# Iteration   583 -> Loss: 1.155739786598608 \t| Accuracy: 129.792\n",
      "# Iteration   584 -> Loss: 1.1550943950944128 \t| Accuracy: 129.792\n",
      "# Iteration   585 -> Loss: 1.1544498274798305 \t| Accuracy: 129.792\n",
      "# Iteration   586 -> Loss: 1.1538060838136324 \t| Accuracy: 129.792\n",
      "# Iteration   587 -> Loss: 1.1531631641728006 \t| Accuracy: 131.042\n",
      "# Iteration   588 -> Loss: 1.152521068651482 \t| Accuracy: 131.042\n",
      "# Iteration   589 -> Loss: 1.1518797973599388 \t| Accuracy: 131.042\n",
      "# Iteration   590 -> Loss: 1.151239350423495 \t| Accuracy: 131.042\n",
      "# Iteration   591 -> Loss: 1.1505997279814757 \t| Accuracy: 131.042\n",
      "# Iteration   592 -> Loss: 1.1499609301861469 \t| Accuracy: 131.042\n",
      "# Iteration   593 -> Loss: 1.1493229572016526 \t| Accuracy: 131.042\n",
      "# Iteration   594 -> Loss: 1.1486858092029493 \t| Accuracy: 131.042\n",
      "# Iteration   595 -> Loss: 1.148049486374742 \t| Accuracy: 131.042\n",
      "# Iteration   596 -> Loss: 1.1474139889104191 \t| Accuracy: 131.042\n",
      "# Iteration   597 -> Loss: 1.1467793170109923 \t| Accuracy: 131.042\n",
      "# Iteration   598 -> Loss: 1.1461454708840344 \t| Accuracy: 131.042\n",
      "# Iteration   599 -> Loss: 1.1455124507426249 \t| Accuracy: 131.042\n",
      "# Iteration   600 -> Loss: 1.144880256804296 \t| Accuracy: 131.042\n",
      "# Iteration   601 -> Loss: 1.144248889289989 \t| Accuracy: 131.042\n",
      "# Iteration   602 -> Loss: 1.14361834842301 \t| Accuracy: 131.042\n",
      "# Iteration   603 -> Loss: 1.1429886344279994 \t| Accuracy: 131.042\n",
      "# Iteration   604 -> Loss: 1.1423597475299037 \t| Accuracy: 131.042\n",
      "# Iteration   605 -> Loss: 1.1417316879529609 \t| Accuracy: 131.042\n",
      "# Iteration   606 -> Loss: 1.1411044559196908 \t| Accuracy: 131.042\n",
      "# Iteration   607 -> Loss: 1.1404780516498996 \t| Accuracy: 131.042\n",
      "# Iteration   608 -> Loss: 1.139852475359693 \t| Accuracy: 131.042\n",
      "# Iteration   609 -> Loss: 1.1392277272605031 \t| Accuracy: 131.042\n",
      "# Iteration   610 -> Loss: 1.1386038075581268 \t| Accuracy: 131.042\n",
      "# Iteration   611 -> Loss: 1.137980716451778 \t| Accuracy: 131.042\n",
      "# Iteration   612 -> Loss: 1.1373584541331538 \t| Accuracy: 131.042\n",
      "# Iteration   613 -> Loss: 1.136737020785518 \t| Accuracy: 131.042\n",
      "# Iteration   614 -> Loss: 1.1361164165827957 \t| Accuracy: 131.042\n",
      "# Iteration   615 -> Loss: 1.1354966416886882 \t| Accuracy: 131.042\n",
      "# Iteration   616 -> Loss: 1.134877696255802 \t| Accuracy: 131.042\n",
      "# Iteration   617 -> Loss: 1.1342595804247977 \t| Accuracy: 131.042\n",
      "# Iteration   618 -> Loss: 1.1336422943235562 \t| Accuracy: 131.042\n",
      "# Iteration   619 -> Loss: 1.1330258380663603 \t| Accuracy: 131.042\n",
      "# Iteration   620 -> Loss: 1.1324102117531016 \t| Accuracy: 131.042\n",
      "# Iteration   621 -> Loss: 1.1317954154685026 \t| Accuracy: 131.042\n",
      "# Iteration   622 -> Loss: 1.131181449281358 \t| Accuracy: 131.042\n",
      "# Iteration   623 -> Loss: 1.1305683132438014 \t| Accuracy: 131.042\n",
      "# Iteration   624 -> Loss: 1.129956007390587 \t| Accuracy: 131.042\n",
      "# Iteration   625 -> Loss: 1.1293445317383988 \t| Accuracy: 131.042\n",
      "# Iteration   626 -> Loss: 1.128733886285175 \t| Accuracy: 132.292\n",
      "# Iteration   627 -> Loss: 1.128124071009459 \t| Accuracy: 132.292\n",
      "# Iteration   628 -> Loss: 1.1275150858697713 \t| Accuracy: 132.292\n",
      "# Iteration   629 -> Loss: 1.1269069308040023 \t| Accuracy: 132.292\n",
      "# Iteration   630 -> Loss: 1.1262996057288306 \t| Accuracy: 132.292\n",
      "# Iteration   631 -> Loss: 1.125693110539161 \t| Accuracy: 132.292\n",
      "# Iteration   632 -> Loss: 1.1250874451075885 \t| Accuracy: 132.292\n",
      "# Iteration   633 -> Loss: 1.124482609283883 \t| Accuracy: 132.292\n",
      "# Iteration   634 -> Loss: 1.123878602894499 \t| Accuracy: 132.292\n",
      "# Iteration   635 -> Loss: 1.1232754257421058 \t| Accuracy: 132.292\n",
      "# Iteration   636 -> Loss: 1.122673077605147 \t| Accuracy: 132.292\n",
      "# Iteration   637 -> Loss: 1.1220715582374146 \t| Accuracy: 132.292\n",
      "# Iteration   638 -> Loss: 1.1214708673676543 \t| Accuracy: 132.292\n",
      "# Iteration   639 -> Loss: 1.1208710046991897 \t| Accuracy: 132.292\n",
      "# Iteration   640 -> Loss: 1.120271969909571 \t| Accuracy: 132.292\n",
      "# Iteration   641 -> Loss: 1.119673762650247 \t| Accuracy: 132.292\n",
      "# Iteration   642 -> Loss: 1.1190763825462589 \t| Accuracy: 132.292\n",
      "# Iteration   643 -> Loss: 1.1184798291959588 \t| Accuracy: 132.292\n",
      "# Iteration   644 -> Loss: 1.1178841021707484 \t| Accuracy: 132.292\n",
      "# Iteration   645 -> Loss: 1.1172892010148427 \t| Accuracy: 132.292\n",
      "# Iteration   646 -> Loss: 1.1166951252450543 \t| Accuracy: 132.292\n",
      "# Iteration   647 -> Loss: 1.1161018743506 \t| Accuracy: 132.292\n",
      "# Iteration   648 -> Loss: 1.1155094477929288 \t| Accuracy: 132.292\n",
      "# Iteration   649 -> Loss: 1.114917845005573 \t| Accuracy: 132.292\n",
      "# Iteration   650 -> Loss: 1.114327065394019 \t| Accuracy: 132.292\n",
      "# Iteration   651 -> Loss: 1.1137371083355974 \t| Accuracy: 132.292\n",
      "# Iteration   652 -> Loss: 1.1131479731793983 \t| Accuracy: 132.292\n",
      "# Iteration   653 -> Loss: 1.1125596592462017 \t| Accuracy: 132.292\n",
      "# Iteration   654 -> Loss: 1.1119721658284312 \t| Accuracy: 132.292\n",
      "# Iteration   655 -> Loss: 1.1113854921901254 \t| Accuracy: 132.292\n",
      "# Iteration   656 -> Loss: 1.1107996375669287 \t| Accuracy: 132.292\n",
      "# Iteration   657 -> Loss: 1.110214601166102 \t| Accuracy: 132.292\n",
      "# Iteration   658 -> Loss: 1.1096303821665487 \t| Accuracy: 132.292\n",
      "# Iteration   659 -> Loss: 1.109046979718862 \t| Accuracy: 132.292\n",
      "# Iteration   660 -> Loss: 1.1084643929453877 \t| Accuracy: 132.292\n",
      "# Iteration   661 -> Loss: 1.1078826209403048 \t| Accuracy: 132.292\n",
      "# Iteration   662 -> Loss: 1.107301662769721 \t| Accuracy: 132.292\n",
      "# Iteration   663 -> Loss: 1.1067215174717855 \t| Accuracy: 132.292\n",
      "# Iteration   664 -> Loss: 1.106142184056819 \t| Accuracy: 132.292\n",
      "# Iteration   665 -> Loss: 1.1055636615074569 \t| Accuracy: 132.292\n",
      "# Iteration   666 -> Loss: 1.1049859487788056 \t| Accuracy: 132.292\n",
      "# Iteration   667 -> Loss: 1.1044090447986175 \t| Accuracy: 132.292\n",
      "# Iteration   668 -> Loss: 1.1038329484674756 \t| Accuracy: 132.292\n",
      "# Iteration   669 -> Loss: 1.1032576586589955 \t| Accuracy: 132.292\n",
      "# Iteration   670 -> Loss: 1.1026831742200347 \t| Accuracy: 132.292\n",
      "# Iteration   671 -> Loss: 1.1021094939709204 \t| Accuracy: 132.292\n",
      "# Iteration   672 -> Loss: 1.101536616705684 \t| Accuracy: 132.292\n",
      "# Iteration   673 -> Loss: 1.1009645411923117 \t| Accuracy: 132.292\n",
      "# Iteration   674 -> Loss: 1.1003932661730014 \t| Accuracy: 132.292\n",
      "# Iteration   675 -> Loss: 1.0998227903644342 \t| Accuracy: 132.292\n",
      "# Iteration   676 -> Loss: 1.0992531124580522 \t| Accuracy: 132.292\n",
      "# Iteration   677 -> Loss: 1.0986842311203509 \t| Accuracy: 132.292\n",
      "# Iteration   678 -> Loss: 1.0981161449931738 \t| Accuracy: 132.292\n",
      "# Iteration   679 -> Loss: 1.0975488526940222 \t| Accuracy: 132.292\n",
      "# Iteration   680 -> Loss: 1.09698235281637 \t| Accuracy: 132.292\n",
      "# Iteration   681 -> Loss: 1.096416643929987 \t| Accuracy: 132.292\n",
      "# Iteration   682 -> Loss: 1.0958517245812671 \t| Accuracy: 132.292\n",
      "# Iteration   683 -> Loss: 1.0952875932935697 \t| Accuracy: 132.292\n",
      "# Iteration   684 -> Loss: 1.0947242485675606 \t| Accuracy: 132.292\n",
      "# Iteration   685 -> Loss: 1.0941616888815628 \t| Accuracy: 132.292\n",
      "# Iteration   686 -> Loss: 1.0935999126919118 \t| Accuracy: 132.292\n",
      "# Iteration   687 -> Loss: 1.0930389184333176 \t| Accuracy: 132.292\n",
      "# Iteration   688 -> Loss: 1.0924787045192292 \t| Accuracy: 132.292\n",
      "# Iteration   689 -> Loss: 1.0919192693422055 \t| Accuracy: 132.292\n",
      "# Iteration   690 -> Loss: 1.09136061127429 \t| Accuracy: 132.292\n",
      "# Iteration   691 -> Loss: 1.090802728667388 \t| Accuracy: 132.292\n",
      "# Iteration   692 -> Loss: 1.0902456198536483 \t| Accuracy: 132.292\n",
      "# Iteration   693 -> Loss: 1.0896892831458491 \t| Accuracy: 132.292\n",
      "# Iteration   694 -> Loss: 1.0891337168377828 \t| Accuracy: 132.292\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Iteration   695 -> Loss: 1.0885789192046478 \t| Accuracy: 132.292\n",
      "# Iteration   696 -> Loss: 1.088024888503438 \t| Accuracy: 132.292\n",
      "# Iteration   697 -> Loss: 1.087471622973338 \t| Accuracy: 132.292\n",
      "# Iteration   698 -> Loss: 1.0869191208361155 \t| Accuracy: 132.292\n",
      "# Iteration   699 -> Loss: 1.0863673802965195 \t| Accuracy: 132.292\n",
      "# Iteration   700 -> Loss: 1.0858163995426735 \t| Accuracy: 132.292\n",
      "# Iteration   701 -> Loss: 1.0852661767464766 \t| Accuracy: 132.292\n",
      "# Iteration   702 -> Loss: 1.0847167100639967 \t| Accuracy: 132.292\n",
      "# Iteration   703 -> Loss: 1.08416799763587 \t| Accuracy: 132.292\n",
      "# Iteration   704 -> Loss: 1.0836200375876972 \t| Accuracy: 132.292\n",
      "# Iteration   705 -> Loss: 1.08307282803044 \t| Accuracy: 132.292\n",
      "# Iteration   706 -> Loss: 1.0825263670608158 \t| Accuracy: 132.292\n",
      "# Iteration   707 -> Loss: 1.0819806527616933 \t| Accuracy: 132.292\n",
      "# Iteration   708 -> Loss: 1.0814356832024854 \t| Accuracy: 132.292\n",
      "# Iteration   709 -> Loss: 1.0808914564395404 \t| Accuracy: 132.292\n",
      "# Iteration   710 -> Loss: 1.080347970516533 \t| Accuracy: 132.292\n",
      "# Iteration   711 -> Loss: 1.0798052234648525 \t| Accuracy: 132.292\n",
      "# Iteration   712 -> Loss: 1.0792632133039888 \t| Accuracy: 132.292\n",
      "# Iteration   713 -> Loss: 1.0787219380419164 \t| Accuracy: 132.292\n",
      "# Iteration   714 -> Loss: 1.0781813956754764 \t| Accuracy: 132.292\n",
      "# Iteration   715 -> Loss: 1.0776415841907558 \t| Accuracy: 132.292\n",
      "# Iteration   716 -> Loss: 1.0771025015634637 \t| Accuracy: 132.292\n",
      "# Iteration   717 -> Loss: 1.0765641457593043 \t| Accuracy: 132.292\n",
      "# Iteration   718 -> Loss: 1.076026514734348 \t| Accuracy: 132.292\n",
      "# Iteration   719 -> Loss: 1.0754896064353996 \t| Accuracy: 132.292\n",
      "# Iteration   720 -> Loss: 1.074953418800361 \t| Accuracy: 132.292\n",
      "# Iteration   721 -> Loss: 1.0744179497585937 \t| Accuracy: 132.292\n",
      "# Iteration   722 -> Loss: 1.0738831972312748 \t| Accuracy: 132.292\n",
      "# Iteration   723 -> Loss: 1.0733491591317503 \t| Accuracy: 132.292\n",
      "# Iteration   724 -> Loss: 1.0728158333658877 \t| Accuracy: 132.292\n",
      "# Iteration   725 -> Loss: 1.0722832178324189 \t| Accuracy: 132.292\n",
      "# Iteration   726 -> Loss: 1.0717513104232856 \t| Accuracy: 132.292\n",
      "# Iteration   727 -> Loss: 1.0712201090239764 \t| Accuracy: 132.292\n",
      "# Iteration   728 -> Loss: 1.0706896115138598 \t| Accuracy: 132.292\n",
      "# Iteration   729 -> Loss: 1.0701598157665173 \t| Accuracy: 132.292\n",
      "# Iteration   730 -> Loss: 1.0696307196500678 \t| Accuracy: 132.292\n",
      "# Iteration   731 -> Loss: 1.0691023210274897 \t| Accuracy: 132.292\n",
      "# Iteration   732 -> Loss: 1.0685746177569395 \t| Accuracy: 132.292\n",
      "# Iteration   733 -> Loss: 1.0680476076920633 \t| Accuracy: 132.292\n",
      "# Iteration   734 -> Loss: 1.0675212886823082 \t| Accuracy: 132.292\n",
      "# Iteration   735 -> Loss: 1.0669956585732243 \t| Accuracy: 132.292\n",
      "# Iteration   736 -> Loss: 1.0664707152067674 \t| Accuracy: 132.292\n",
      "# Iteration   737 -> Loss: 1.0659464564215924 \t| Accuracy: 132.292\n",
      "# Iteration   738 -> Loss: 1.065422880053346 \t| Accuracy: 132.292\n",
      "# Iteration   739 -> Loss: 1.0648999839349533 \t| Accuracy: 132.292\n",
      "# Iteration   740 -> Loss: 1.064377765896899 \t| Accuracy: 132.292\n",
      "# Iteration   741 -> Loss: 1.0638562237675055 \t| Accuracy: 132.292\n",
      "# Iteration   742 -> Loss: 1.0633353553732072 \t| Accuracy: 132.292\n",
      "# Iteration   743 -> Loss: 1.0628151585388168 \t| Accuracy: 132.292\n",
      "# Iteration   744 -> Loss: 1.062295631087791 \t| Accuracy: 132.292\n",
      "# Iteration   745 -> Loss: 1.061776770842489 \t| Accuracy: 132.292\n",
      "# Iteration   746 -> Loss: 1.0612585756244282 \t| Accuracy: 132.292\n",
      "# Iteration   747 -> Loss: 1.0607410432545339 \t| Accuracy: 132.292\n",
      "# Iteration   748 -> Loss: 1.0602241715533853 \t| Accuracy: 132.292\n",
      "# Iteration   749 -> Loss: 1.0597079583414561 \t| Accuracy: 132.292\n",
      "# Iteration   750 -> Loss: 1.0591924014393537 \t| Accuracy: 132.292\n",
      "# Iteration   751 -> Loss: 1.0586774986680478 \t| Accuracy: 132.292\n",
      "# Iteration   752 -> Loss: 1.0581632478491012 \t| Accuracy: 132.292\n",
      "# Iteration   753 -> Loss: 1.0576496468048926 \t| Accuracy: 132.292\n",
      "# Iteration   754 -> Loss: 1.057136693358834 \t| Accuracy: 132.292\n",
      "# Iteration   755 -> Loss: 1.0566243853355874 \t| Accuracy: 132.292\n",
      "# Iteration   756 -> Loss: 1.0561127205612735 \t| Accuracy: 132.292\n",
      "# Iteration   757 -> Loss: 1.0556016968636788 \t| Accuracy: 132.292\n",
      "# Iteration   758 -> Loss: 1.055091312072456 \t| Accuracy: 132.292\n",
      "# Iteration   759 -> Loss: 1.0545815640193223 \t| Accuracy: 132.292\n",
      "# Iteration   760 -> Loss: 1.0540724505382522 \t| Accuracy: 132.292\n",
      "# Iteration   761 -> Loss: 1.053563969465665 \t| Accuracy: 132.292\n",
      "# Iteration   762 -> Loss: 1.0530561186406129 \t| Accuracy: 132.292\n",
      "# Iteration   763 -> Loss: 1.0525488959049585 \t| Accuracy: 132.292\n",
      "# Iteration   764 -> Loss: 1.0520422991035518 \t| Accuracy: 132.292\n",
      "# Iteration   765 -> Loss: 1.0515363260844053 \t| Accuracy: 132.292\n",
      "# Iteration   766 -> Loss: 1.051030974698858 \t| Accuracy: 132.292\n",
      "# Iteration   767 -> Loss: 1.0505262428017452 \t| Accuracy: 132.292\n",
      "# Iteration   768 -> Loss: 1.050022128251555 \t| Accuracy: 132.292\n",
      "# Iteration   769 -> Loss: 1.0495186289105871 \t| Accuracy: 132.292\n",
      "# Iteration   770 -> Loss: 1.0490157426451059 \t| Accuracy: 132.292\n",
      "# Iteration   771 -> Loss: 1.0485134673254886 \t| Accuracy: 132.292\n",
      "# Iteration   772 -> Loss: 1.048011800826372 \t| Accuracy: 132.292\n",
      "# Iteration   773 -> Loss: 1.0475107410267932 \t| Accuracy: 132.292\n",
      "# Iteration   774 -> Loss: 1.0470102858103292 \t| Accuracy: 132.292\n",
      "# Iteration   775 -> Loss: 1.0465104330652308 \t| Accuracy: 132.292\n",
      "# Iteration   776 -> Loss: 1.0460111806845531 \t| Accuracy: 132.292\n",
      "# Iteration   777 -> Loss: 1.0455125265662848 \t| Accuracy: 132.292\n",
      "# Iteration   778 -> Loss: 1.0450144686134708 \t| Accuracy: 132.292\n",
      "# Iteration   779 -> Loss: 1.0445170047343342 \t| Accuracy: 132.292\n",
      "# Iteration   780 -> Loss: 1.0440201328423937 \t| Accuracy: 132.292\n",
      "# Iteration   781 -> Loss: 1.0435238508565772 \t| Accuracy: 132.292\n",
      "# Iteration   782 -> Loss: 1.0430281567013338 \t| Accuracy: 132.292\n",
      "# Iteration   783 -> Loss: 1.042533048306741 \t| Accuracy: 132.292\n",
      "# Iteration   784 -> Loss: 1.04203852360861 \t| Accuracy: 132.292\n",
      "# Iteration   785 -> Loss: 1.0415445805485875 \t| Accuracy: 132.292\n",
      "# Iteration   786 -> Loss: 1.0410512170742525 \t| Accuracy: 132.292\n",
      "# Iteration   787 -> Loss: 1.0405584311392155 \t| Accuracy: 132.292\n",
      "# Iteration   788 -> Loss: 1.0400662207032085 \t| Accuracy: 132.292\n",
      "# Iteration   789 -> Loss: 1.0395745837321753 \t| Accuracy: 132.292\n",
      "# Iteration   790 -> Loss: 1.0390835181983595 \t| Accuracy: 132.292\n",
      "# Iteration   791 -> Loss: 1.0385930220803887 \t| Accuracy: 132.292\n",
      "# Iteration   792 -> Loss: 1.038103093363355 \t| Accuracy: 132.292\n",
      "# Iteration   793 -> Loss: 1.0376137300388955 \t| Accuracy: 132.292\n",
      "# Iteration   794 -> Loss: 1.0371249301052674 \t| Accuracy: 132.292\n",
      "# Iteration   795 -> Loss: 1.0366366915674223 \t| Accuracy: 132.292\n",
      "# Iteration   796 -> Loss: 1.0361490124370771 \t| Accuracy: 132.292\n",
      "# Iteration   797 -> Loss: 1.0356618907327835 \t| Accuracy: 132.292\n",
      "# Iteration   798 -> Loss: 1.0351753244799924 \t| Accuracy: 132.292\n",
      "# Iteration   799 -> Loss: 1.0346893117111204 \t| Accuracy: 132.292\n",
      "# Iteration   800 -> Loss: 1.0342038504656095 \t| Accuracy: 132.292\n",
      "# Iteration   801 -> Loss: 1.0337189387899863 \t| Accuracy: 132.292\n",
      "# Iteration   802 -> Loss: 1.0332345747379208 \t| Accuracy: 132.292\n",
      "# Iteration   803 -> Loss: 1.0327507563702791 \t| Accuracy: 132.292\n",
      "# Iteration   804 -> Loss: 1.0322674817551774 \t| Accuracy: 132.292\n",
      "# Iteration   805 -> Loss: 1.031784748968032 \t| Accuracy: 132.292\n",
      "# Iteration   806 -> Loss: 1.0313025560916083 \t| Accuracy: 132.292\n",
      "# Iteration   807 -> Loss: 1.0308209012160663 \t| Accuracy: 132.292\n",
      "# Iteration   808 -> Loss: 1.0303397824390057 \t| Accuracy: 132.292\n",
      "# Iteration   809 -> Loss: 1.029859197865509 \t| Accuracy: 132.292\n",
      "# Iteration   810 -> Loss: 1.0293791456081811 \t| Accuracy: 132.292\n",
      "# Iteration   811 -> Loss: 1.0288996237871877 \t| Accuracy: 132.292\n",
      "# Iteration   812 -> Loss: 1.0284206305302943 \t| Accuracy: 132.292\n",
      "# Iteration   813 -> Loss: 1.0279421639728996 \t| Accuracy: 132.292\n",
      "# Iteration   814 -> Loss: 1.027464222258069 \t| Accuracy: 132.292\n",
      "# Iteration   815 -> Loss: 1.0269868035365675 \t| Accuracy: 132.292\n",
      "# Iteration   816 -> Loss: 1.0265099059668885 \t| Accuracy: 132.292\n",
      "# Iteration   817 -> Loss: 1.026033527715283 \t| Accuracy: 132.292\n",
      "# Iteration   818 -> Loss: 1.0255576669557869 \t| Accuracy: 132.292\n",
      "# Iteration   819 -> Loss: 1.0250823218702443 \t| Accuracy: 132.292\n",
      "# Iteration   820 -> Loss: 1.024607490648334 \t| Accuracy: 132.292\n",
      "# Iteration   821 -> Loss: 1.0241331714875905 \t| Accuracy: 132.292\n",
      "# Iteration   822 -> Loss: 1.0236593625934247 \t| Accuracy: 132.292\n",
      "# Iteration   823 -> Loss: 1.0231860621791435 \t| Accuracy: 132.292\n",
      "# Iteration   824 -> Loss: 1.0227132684659692 \t| Accuracy: 132.292\n",
      "# Iteration   825 -> Loss: 1.022240979683054 \t| Accuracy: 132.292\n",
      "# Iteration   826 -> Loss: 1.021769194067498 \t| Accuracy: 132.292\n",
      "# Iteration   827 -> Loss: 1.021297909864361 \t| Accuracy: 132.292\n",
      "# Iteration   828 -> Loss: 1.0208271253266783 \t| Accuracy: 132.292\n",
      "# Iteration   829 -> Loss: 1.0203568387154691 \t| Accuracy: 132.292\n",
      "# Iteration   830 -> Loss: 1.0198870482997509 \t| Accuracy: 132.292\n",
      "# Iteration   831 -> Loss: 1.0194177523565455 \t| Accuracy: 132.292\n",
      "# Iteration   832 -> Loss: 1.0189489491708894 \t| Accuracy: 132.292\n",
      "# Iteration   833 -> Loss: 1.0184806370358412 \t| Accuracy: 132.292\n",
      "# Iteration   834 -> Loss: 1.0180128142524865 \t| Accuracy: 132.292\n",
      "# Iteration   835 -> Loss: 1.0175454791299439 \t| Accuracy: 132.292\n",
      "# Iteration   836 -> Loss: 1.0170786299853694 \t| Accuracy: 132.292\n",
      "# Iteration   837 -> Loss: 1.0166122651439593 \t| Accuracy: 132.292\n",
      "# Iteration   838 -> Loss: 1.016146382938952 \t| Accuracy: 132.292\n",
      "# Iteration   839 -> Loss: 1.0156809817116306 \t| Accuracy: 132.292\n",
      "# Iteration   840 -> Loss: 1.0152160598113231 \t| Accuracy: 132.292\n",
      "# Iteration   841 -> Loss: 1.0147516155954 \t| Accuracy: 132.292\n",
      "# Iteration   842 -> Loss: 1.0142876474292757 \t| Accuracy: 132.292\n",
      "# Iteration   843 -> Loss: 1.013824153686406 \t| Accuracy: 132.292\n",
      "# Iteration   844 -> Loss: 1.0133611327482834 \t| Accuracy: 132.292\n",
      "# Iteration   845 -> Loss: 1.0128985830044355 \t| Accuracy: 132.917\n",
      "# Iteration   846 -> Loss: 1.0124365028524198 \t| Accuracy: 132.917\n",
      "# Iteration   847 -> Loss: 1.0119748906978185 \t| Accuracy: 132.917\n",
      "# Iteration   848 -> Loss: 1.0115137449542333 \t| Accuracy: 132.917\n",
      "# Iteration   849 -> Loss: 1.0110530640432784 \t| Accuracy: 132.917\n",
      "# Iteration   850 -> Loss: 1.010592846394573 \t| Accuracy: 132.917\n",
      "# Iteration   851 -> Loss: 1.0101330904457353 \t| Accuracy: 132.917\n",
      "# Iteration   852 -> Loss: 1.0096737946423715 \t| Accuracy: 132.917\n",
      "# Iteration   853 -> Loss: 1.0092149574380689 \t| Accuracy: 132.917\n",
      "# Iteration   854 -> Loss: 1.0087565772943858 \t| Accuracy: 132.917\n",
      "# Iteration   855 -> Loss: 1.0082986526808408 \t| Accuracy: 132.917\n",
      "# Iteration   856 -> Loss: 1.0078411820749025 \t| Accuracy: 132.917\n",
      "# Iteration   857 -> Loss: 1.007384163961979 \t| Accuracy: 132.917\n",
      "# Iteration   858 -> Loss: 1.0069275968354043 \t| Accuracy: 132.917\n",
      "# Iteration   859 -> Loss: 1.0064714791964287 \t| Accuracy: 132.917\n",
      "# Iteration   860 -> Loss: 1.0060158095542036 \t| Accuracy: 132.917\n",
      "# Iteration   861 -> Loss: 1.0055605864257706 \t| Accuracy: 132.917\n",
      "# Iteration   862 -> Loss: 1.0051058083360465 \t| Accuracy: 132.917\n",
      "# Iteration   863 -> Loss: 1.004651473817809 \t| Accuracy: 132.917\n",
      "# Iteration   864 -> Loss: 1.0041975814116841 \t| Accuracy: 132.917\n",
      "# Iteration   865 -> Loss: 1.0037441296661296 \t| Accuracy: 132.917\n",
      "# Iteration   866 -> Loss: 1.003291117137421 \t| Accuracy: 132.917\n",
      "# Iteration   867 -> Loss: 1.002838542389635 \t| Accuracy: 132.917\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Iteration   868 -> Loss: 1.0023864039946346 \t| Accuracy: 132.917\n",
      "# Iteration   869 -> Loss: 1.0019347005320522 \t| Accuracy: 132.917\n",
      "# Iteration   870 -> Loss: 1.0014834305892737 \t| Accuracy: 132.917\n",
      "# Iteration   871 -> Loss: 1.00103259276142 \t| Accuracy: 132.917\n",
      "# Iteration   872 -> Loss: 1.0005821856513328 \t| Accuracy: 132.917\n",
      "# Iteration   873 -> Loss: 1.000132207869554 \t| Accuracy: 132.917\n",
      "# Iteration   874 -> Loss: 0.9996826580343101 \t| Accuracy: 132.917\n",
      "# Iteration   875 -> Loss: 0.9992335347714932 \t| Accuracy: 132.917\n",
      "# Iteration   876 -> Loss: 0.9987848367146435 \t| Accuracy: 132.917\n",
      "# Iteration   877 -> Loss: 0.9983365625049297 \t| Accuracy: 132.917\n",
      "# Iteration   878 -> Loss: 0.9978887107911317 \t| Accuracy: 132.917\n",
      "# Iteration   879 -> Loss: 0.9974412802296203 \t| Accuracy: 132.917\n",
      "# Iteration   880 -> Loss: 0.9969942694843394 \t| Accuracy: 132.917\n",
      "# Iteration   881 -> Loss: 0.9965476772267851 \t| Accuracy: 132.917\n",
      "# Iteration   882 -> Loss: 0.9961015021359877 \t| Accuracy: 132.917\n",
      "# Iteration   883 -> Loss: 0.9956557428984911 \t| Accuracy: 132.917\n",
      "# Iteration   884 -> Loss: 0.9952103982083329 \t| Accuracy: 132.917\n",
      "# Iteration   885 -> Loss: 0.9947654667670249 \t| Accuracy: 132.917\n",
      "# Iteration   886 -> Loss: 0.9943209472835323 \t| Accuracy: 132.917\n",
      "# Iteration   887 -> Loss: 0.9938768384742538 \t| Accuracy: 132.917\n",
      "# Iteration   888 -> Loss: 0.9934331390630005 \t| Accuracy: 132.917\n",
      "# Iteration   889 -> Loss: 0.9929898477809757 \t| Accuracy: 132.917\n",
      "# Iteration   890 -> Loss: 0.9925469633667545 \t| Accuracy: 132.917\n",
      "# Iteration   891 -> Loss: 0.9921044845662623 \t| Accuracy: 132.917\n",
      "# Iteration   892 -> Loss: 0.9916624101327538 \t| Accuracy: 132.917\n",
      "# Iteration   893 -> Loss: 0.9912207388267921 \t| Accuracy: 132.917\n",
      "# Iteration   894 -> Loss: 0.9907794694162281 \t| Accuracy: 132.917\n",
      "# Iteration   895 -> Loss: 0.990338600676178 \t| Accuracy: 132.917\n",
      "# Iteration   896 -> Loss: 0.9898981313890026 \t| Accuracy: 132.917\n",
      "# Iteration   897 -> Loss: 0.9894580603442861 \t| Accuracy: 132.917\n",
      "# Iteration   898 -> Loss: 0.9890183863388148 \t| Accuracy: 132.917\n",
      "# Iteration   899 -> Loss: 0.9885791081765538 \t| Accuracy: 132.917\n",
      "# Iteration   900 -> Loss: 0.9881402246686274 \t| Accuracy: 132.917\n",
      "# Iteration   901 -> Loss: 0.9877017346332962 \t| Accuracy: 132.917\n",
      "# Iteration   902 -> Loss: 0.987263636895936 \t| Accuracy: 132.917\n",
      "# Iteration   903 -> Loss: 0.9868259302890154 \t| Accuracy: 132.917\n",
      "# Iteration   904 -> Loss: 0.9863886136520745 \t| Accuracy: 132.917\n",
      "# Iteration   905 -> Loss: 0.985951685831702 \t| Accuracy: 132.917\n",
      "# Iteration   906 -> Loss: 0.9855151456815147 \t| Accuracy: 132.917\n",
      "# Iteration   907 -> Loss: 0.9850789920621345 \t| Accuracy: 132.917\n",
      "# Iteration   908 -> Loss: 0.9846432238411666 \t| Accuracy: 132.917\n",
      "# Iteration   909 -> Loss: 0.9842078398931777 \t| Accuracy: 132.917\n",
      "# Iteration   910 -> Loss: 0.9837728390996742 \t| Accuracy: 132.917\n",
      "# Iteration   911 -> Loss: 0.983338220349079 \t| Accuracy: 132.917\n",
      "# Iteration   912 -> Loss: 0.9829039825367106 \t| Accuracy: 132.917\n",
      "# Iteration   913 -> Loss: 0.9824701245647608 \t| Accuracy: 132.917\n",
      "# Iteration   914 -> Loss: 0.9820366453422721 \t| Accuracy: 133.542\n",
      "# Iteration   915 -> Loss: 0.9816035437851164 \t| Accuracy: 133.542\n",
      "# Iteration   916 -> Loss: 0.9811708188159718 \t| Accuracy: 133.542\n",
      "# Iteration   917 -> Loss: 0.9807384693643012 \t| Accuracy: 133.542\n",
      "# Iteration   918 -> Loss: 0.9803064943663308 \t| Accuracy: 133.542\n",
      "# Iteration   919 -> Loss: 0.9798748927650268 \t| Accuracy: 133.542\n",
      "# Iteration   920 -> Loss: 0.9794436635100738 \t| Accuracy: 133.542\n",
      "# Iteration   921 -> Loss: 0.9790128055578529 \t| Accuracy: 133.542\n",
      "# Iteration   922 -> Loss: 0.9785823178714194 \t| Accuracy: 133.542\n",
      "# Iteration   923 -> Loss: 0.9781521994204811 \t| Accuracy: 133.542\n",
      "# Iteration   924 -> Loss: 0.9777224491813754 \t| Accuracy: 133.542\n",
      "# Iteration   925 -> Loss: 0.9772930661370484 \t| Accuracy: 133.542\n",
      "# Iteration   926 -> Loss: 0.9768640492770321 \t| Accuracy: 133.542\n",
      "# Iteration   927 -> Loss: 0.9764353975974228 \t| Accuracy: 133.542\n",
      "# Iteration   928 -> Loss: 0.9760071101008589 \t| Accuracy: 133.542\n",
      "# Iteration   929 -> Loss: 0.9755791857964984 \t| Accuracy: 133.542\n",
      "# Iteration   930 -> Loss: 0.9751516236999987 \t| Accuracy: 133.542\n",
      "# Iteration   931 -> Loss: 0.9747244228334926 \t| Accuracy: 133.542\n",
      "# Iteration   932 -> Loss: 0.9742975822255677 \t| Accuracy: 133.542\n",
      "# Iteration   933 -> Loss: 0.9738711009112444 \t| Accuracy: 133.542\n",
      "# Iteration   934 -> Loss: 0.9734449779319534 \t| Accuracy: 133.542\n",
      "# Iteration   935 -> Loss: 0.9730192123355148 \t| Accuracy: 133.542\n",
      "# Iteration   936 -> Loss: 0.9725938031761154 \t| Accuracy: 133.542\n",
      "# Iteration   937 -> Loss: 0.9721687495142877 \t| Accuracy: 133.542\n",
      "# Iteration   938 -> Loss: 0.9717440504168877 \t| Accuracy: 133.542\n",
      "# Iteration   939 -> Loss: 0.9713197049570733 \t| Accuracy: 133.542\n",
      "# Iteration   940 -> Loss: 0.9708957122142827 \t| Accuracy: 133.542\n",
      "# Iteration   941 -> Loss: 0.9704720712742128 \t| Accuracy: 133.542\n",
      "# Iteration   942 -> Loss: 0.9700487812287967 \t| Accuracy: 133.542\n",
      "# Iteration   943 -> Loss: 0.9696258411761841 \t| Accuracy: 133.542\n",
      "# Iteration   944 -> Loss: 0.9692032502207177 \t| Accuracy: 133.542\n",
      "# Iteration   945 -> Loss: 0.9687810074729124 \t| Accuracy: 133.542\n",
      "# Iteration   946 -> Loss: 0.9683591120494339 \t| Accuracy: 133.542\n",
      "# Iteration   947 -> Loss: 0.967937563073077 \t| Accuracy: 133.542\n",
      "# Iteration   948 -> Loss: 0.9675163596727446 \t| Accuracy: 133.542\n",
      "# Iteration   949 -> Loss: 0.9670955009834254 \t| Accuracy: 133.542\n",
      "# Iteration   950 -> Loss: 0.9666749861461736 \t| Accuracy: 133.542\n",
      "# Iteration   951 -> Loss: 0.9662548143080861 \t| Accuracy: 133.542\n",
      "# Iteration   952 -> Loss: 0.9658349846222829 \t| Accuracy: 133.542\n",
      "# Iteration   953 -> Loss: 0.9654154962478835 \t| Accuracy: 133.542\n",
      "# Iteration   954 -> Loss: 0.9649963483499883 \t| Accuracy: 133.542\n",
      "# Iteration   955 -> Loss: 0.9645775400996548 \t| Accuracy: 133.542\n",
      "# Iteration   956 -> Loss: 0.964159070673878 \t| Accuracy: 133.542\n",
      "# Iteration   957 -> Loss: 0.9637409392555681 \t| Accuracy: 133.542\n",
      "# Iteration   958 -> Loss: 0.9633231450335307 \t| Accuracy: 133.542\n",
      "# Iteration   959 -> Loss: 0.9629056872024433 \t| Accuracy: 133.542\n",
      "# Iteration   960 -> Loss: 0.9624885649628362 \t| Accuracy: 133.542\n",
      "# Iteration   961 -> Loss: 0.9620717775210712 \t| Accuracy: 133.542\n",
      "# Iteration   962 -> Loss: 0.9616553240893185 \t| Accuracy: 133.542\n",
      "# Iteration   963 -> Loss: 0.9612392038855383 \t| Accuracy: 133.542\n",
      "# Iteration   964 -> Loss: 0.9608234161334575 \t| Accuracy: 133.542\n",
      "# Iteration   965 -> Loss: 0.9604079600625496 \t| Accuracy: 133.542\n",
      "# Iteration   966 -> Loss: 0.9599928349080137 \t| Accuracy: 133.542\n",
      "# Iteration   967 -> Loss: 0.9595780399107532 \t| Accuracy: 133.542\n",
      "# Iteration   968 -> Loss: 0.9591635743173546 \t| Accuracy: 133.542\n",
      "# Iteration   969 -> Loss: 0.9587494373800666 \t| Accuracy: 133.542\n",
      "# Iteration   970 -> Loss: 0.9583356283567799 \t| Accuracy: 133.542\n",
      "# Iteration   971 -> Loss: 0.9579221465110043 \t| Accuracy: 133.542\n",
      "# Iteration   972 -> Loss: 0.9575089911118495 \t| Accuracy: 133.542\n",
      "# Iteration   973 -> Loss: 0.9570961614340034 \t| Accuracy: 133.542\n",
      "# Iteration   974 -> Loss: 0.9566836567577113 \t| Accuracy: 133.542\n",
      "# Iteration   975 -> Loss: 0.9562714763687544 \t| Accuracy: 133.542\n",
      "# Iteration   976 -> Loss: 0.9558596195584298 \t| Accuracy: 133.542\n",
      "# Iteration   977 -> Loss: 0.9554480856235285 \t| Accuracy: 133.542\n",
      "# Iteration   978 -> Loss: 0.9550368738663152 \t| Accuracy: 133.542\n",
      "# Iteration   979 -> Loss: 0.9546259835945072 \t| Accuracy: 133.542\n",
      "# Iteration   980 -> Loss: 0.9542154141212532 \t| Accuracy: 133.542\n",
      "# Iteration   981 -> Loss: 0.9538051647651123 \t| Accuracy: 133.542\n",
      "# Iteration   982 -> Loss: 0.953395234850034 \t| Accuracy: 133.542\n",
      "# Iteration   983 -> Loss: 0.9529856237053357 \t| Accuracy: 133.542\n",
      "# Iteration   984 -> Loss: 0.9525763306656829 \t| Accuracy: 133.542\n",
      "# Iteration   985 -> Loss: 0.9521673550710683 \t| Accuracy: 133.542\n",
      "# Iteration   986 -> Loss: 0.9517586962667898 \t| Accuracy: 133.542\n",
      "# Iteration   987 -> Loss: 0.9513503536034309 \t| Accuracy: 133.542\n",
      "# Iteration   988 -> Loss: 0.9509423264368386 \t| Accuracy: 133.542\n",
      "# Iteration   989 -> Loss: 0.9505346141281034 \t| Accuracy: 133.542\n",
      "# Iteration   990 -> Loss: 0.9501272160435374 \t| Accuracy: 133.542\n",
      "# Iteration   991 -> Loss: 0.9497201315546541 \t| Accuracy: 133.542\n",
      "# Iteration   992 -> Loss: 0.9493133600381467 \t| Accuracy: 133.542\n",
      "# Iteration   993 -> Loss: 0.9489069008758676 \t| Accuracy: 133.542\n",
      "# Iteration   994 -> Loss: 0.9485007534548078 \t| Accuracy: 133.542\n",
      "# Iteration   995 -> Loss: 0.9480949171670746 \t| Accuracy: 133.542\n",
      "# Iteration   996 -> Loss: 0.947689391409872 \t| Accuracy: 133.542\n",
      "# Iteration   997 -> Loss: 0.9472841755854783 \t| Accuracy: 133.542\n",
      "# Iteration   998 -> Loss: 0.9468792691012261 \t| Accuracy: 133.542\n",
      "# Iteration   999 -> Loss: 0.9464746713694808 \t| Accuracy: 133.542\n",
      "# Iteration  1000 -> Loss: 0.9460703818076195 \t| Accuracy: 133.542\n",
      "# Iteration  1001 -> Loss: 0.9456663998380098 \t| Accuracy: 133.542\n",
      "# Iteration  1002 -> Loss: 0.9452627248879892 \t| Accuracy: 133.542\n",
      "# Iteration  1003 -> Loss: 0.9448593563898432 \t| Accuracy: 133.542\n",
      "# Iteration  1004 -> Loss: 0.9444562937807842 \t| Accuracy: 133.542\n",
      "# Iteration  1005 -> Loss: 0.9440535365029313 \t| Accuracy: 133.542\n",
      "# Iteration  1006 -> Loss: 0.9436510840032879 \t| Accuracy: 133.542\n",
      "# Iteration  1007 -> Loss: 0.9432489357337211 \t| Accuracy: 133.542\n",
      "# Iteration  1008 -> Loss: 0.9428470911509402 \t| Accuracy: 133.542\n",
      "# Iteration  1009 -> Loss: 0.9424455497164759 \t| Accuracy: 133.542\n",
      "# Iteration  1010 -> Loss: 0.942044310896658 \t| Accuracy: 133.542\n",
      "# Iteration  1011 -> Loss: 0.9416433741625949 \t| Accuracy: 133.542\n",
      "# Iteration  1012 -> Loss: 0.9412427389901525 \t| Accuracy: 133.542\n",
      "# Iteration  1013 -> Loss: 0.9408424048599322 \t| Accuracy: 133.542\n",
      "# Iteration  1014 -> Loss: 0.9404423712572488 \t| Accuracy: 133.542\n",
      "# Iteration  1015 -> Loss: 0.9400426376721112 \t| Accuracy: 133.542\n",
      "# Iteration  1016 -> Loss: 0.9396432035991987 \t| Accuracy: 133.542\n",
      "# Iteration  1017 -> Loss: 0.9392440685378414 \t| Accuracy: 133.542\n",
      "# Iteration  1018 -> Loss: 0.9388452319919967 \t| Accuracy: 133.542\n",
      "# Iteration  1019 -> Loss: 0.9384466934702296 \t| Accuracy: 133.542\n",
      "# Iteration  1020 -> Loss: 0.9380484524856902 \t| Accuracy: 133.542\n",
      "# Iteration  1021 -> Loss: 0.9376505085560919 \t| Accuracy: 133.542\n",
      "# Iteration  1022 -> Loss: 0.9372528612036909 \t| Accuracy: 133.542\n",
      "# Iteration  1023 -> Loss: 0.9368555099552628 \t| Accuracy: 133.542\n",
      "# Iteration  1024 -> Loss: 0.9364584543420821 \t| Accuracy: 133.542\n",
      "# Iteration  1025 -> Loss: 0.9360616938999006 \t| Accuracy: 133.542\n",
      "# Iteration  1026 -> Loss: 0.935665228168925 \t| Accuracy: 133.542\n",
      "# Iteration  1027 -> Loss: 0.9352690566937953 \t| Accuracy: 133.542\n",
      "# Iteration  1028 -> Loss: 0.934873179023563 \t| Accuracy: 133.542\n",
      "# Iteration  1029 -> Loss: 0.9344775947116695 \t| Accuracy: 133.542\n",
      "# Iteration  1030 -> Loss: 0.9340823033159237 \t| Accuracy: 133.542\n",
      "# Iteration  1031 -> Loss: 0.9336873043984802 \t| Accuracy: 133.542\n",
      "# Iteration  1032 -> Loss: 0.9332925975258178 \t| Accuracy: 133.542\n",
      "# Iteration  1033 -> Loss: 0.9328981822687171 \t| Accuracy: 133.542\n",
      "# Iteration  1034 -> Loss: 0.9325040582022384 \t| Accuracy: 133.542\n",
      "# Iteration  1035 -> Loss: 0.9321102249057 \t| Accuracy: 133.542\n",
      "# Iteration  1036 -> Loss: 0.9317166819626554 \t| Accuracy: 133.542\n",
      "# Iteration  1037 -> Loss: 0.9313234289608724 \t| Accuracy: 133.542\n",
      "# Iteration  1038 -> Loss: 0.9309304654923092 \t| Accuracy: 133.542\n",
      "# Iteration  1039 -> Loss: 0.9305377911530938 \t| Accuracy: 133.542\n",
      "# Iteration  1040 -> Loss: 0.9301454055435011 \t| Accuracy: 133.542\n",
      "# Iteration  1041 -> Loss: 0.9297533082679307 \t| Accuracy: 133.542\n",
      "# Iteration  1042 -> Loss: 0.9293614989348838 \t| Accuracy: 133.542\n",
      "# Iteration  1043 -> Loss: 0.9289699771569427 \t| Accuracy: 133.542\n",
      "# Iteration  1044 -> Loss: 0.9285787425507465 \t| Accuracy: 133.542\n",
      "# Iteration  1045 -> Loss: 0.9281877947369698 \t| Accuracy: 133.542\n",
      "# Iteration  1046 -> Loss: 0.9277971333403 \t| Accuracy: 133.542\n",
      "# Iteration  1047 -> Loss: 0.9274067579894146 \t| Accuracy: 133.542\n",
      "# Iteration  1048 -> Loss: 0.9270166683169592 \t| Accuracy: 133.542\n",
      "# Iteration  1049 -> Loss: 0.9266268639595239 \t| Accuracy: 133.542\n",
      "# Iteration  1050 -> Loss: 0.9262373445576223 \t| Accuracy: 133.542\n",
      "# Iteration  1051 -> Loss: 0.9258481097556677 \t| Accuracy: 133.542\n",
      "# Iteration  1052 -> Loss: 0.9254591592019503 \t| Accuracy: 133.542\n",
      "# Iteration  1053 -> Loss: 0.9250704925486158 \t| Accuracy: 133.542\n",
      "# Iteration  1054 -> Loss: 0.9246821094516413 \t| Accuracy: 133.542\n",
      "# Iteration  1055 -> Loss: 0.9242940095708133 \t| Accuracy: 133.542\n",
      "# Iteration  1056 -> Loss: 0.9239061925697046 \t| Accuracy: 133.542\n",
      "# Iteration  1057 -> Loss: 0.9235186581156524 \t| Accuracy: 133.542\n",
      "# Iteration  1058 -> Loss: 0.9231314058797334 \t| Accuracy: 133.542\n",
      "# Iteration  1059 -> Loss: 0.9227444355367439 \t| Accuracy: 133.542\n",
      "# Iteration  1060 -> Loss: 0.9223577467651735 \t| Accuracy: 133.542\n",
      "# Iteration  1061 -> Loss: 0.9219713392471851 \t| Accuracy: 133.542\n",
      "# Iteration  1062 -> Loss: 0.9215852126685905 \t| Accuracy: 133.542\n",
      "# Iteration  1063 -> Loss: 0.9211993667188279 \t| Accuracy: 133.542\n",
      "# Iteration  1064 -> Loss: 0.920813801090938 \t| Accuracy: 133.542\n",
      "# Iteration  1065 -> Loss: 0.920428515481542 \t| Accuracy: 133.542\n",
      "# Iteration  1066 -> Loss: 0.9200435095908188 \t| Accuracy: 133.542\n",
      "# Iteration  1067 -> Loss: 0.9196587831224802 \t| Accuracy: 133.542\n",
      "# Iteration  1068 -> Loss: 0.9192743357837495 \t| Accuracy: 133.542\n",
      "# Iteration  1069 -> Loss: 0.9188901672853376 \t| Accuracy: 133.542\n",
      "# Iteration  1070 -> Loss: 0.9185062773414203 \t| Accuracy: 133.542\n",
      "# Iteration  1071 -> Loss: 0.918122665669614 \t| Accuracy: 133.542\n",
      "# Iteration  1072 -> Loss: 0.9177393319909538 \t| Accuracy: 133.542\n",
      "# Iteration  1073 -> Loss: 0.9173562760298698 \t| Accuracy: 133.542\n",
      "# Iteration  1074 -> Loss: 0.9169734975141637 \t| Accuracy: 133.542\n",
      "# Iteration  1075 -> Loss: 0.916590996174985 \t| Accuracy: 133.542\n",
      "# Iteration  1076 -> Loss: 0.9162087717468093 \t| Accuracy: 133.542\n",
      "# Iteration  1077 -> Loss: 0.9158268239674132 \t| Accuracy: 133.542\n",
      "# Iteration  1078 -> Loss: 0.9154451525778524 \t| Accuracy: 133.542\n",
      "# Iteration  1079 -> Loss: 0.915063757322437 \t| Accuracy: 133.542\n",
      "# Iteration  1080 -> Loss: 0.9146826379487095 \t| Accuracy: 133.542\n",
      "# Iteration  1081 -> Loss: 0.9143017942074204 \t| Accuracy: 133.542\n",
      "# Iteration  1082 -> Loss: 0.9139212258525051 \t| Accuracy: 133.542\n",
      "# Iteration  1083 -> Loss: 0.9135409326410612 \t| Accuracy: 133.542\n",
      "# Iteration  1084 -> Loss: 0.9131609143333236 \t| Accuracy: 133.542\n",
      "# Iteration  1085 -> Loss: 0.9127811706926422 \t| Accuracy: 133.542\n",
      "# Iteration  1086 -> Loss: 0.9124017014854587 \t| Accuracy: 133.542\n",
      "# Iteration  1087 -> Loss: 0.9120225064812818 \t| Accuracy: 133.542\n",
      "# Iteration  1088 -> Loss: 0.9116435854526651 \t| Accuracy: 133.542\n",
      "# Iteration  1089 -> Loss: 0.9112649381751827 \t| Accuracy: 133.542\n",
      "# Iteration  1090 -> Loss: 0.9108865644274066 \t| Accuracy: 133.542\n",
      "# Iteration  1091 -> Loss: 0.9105084639908821 \t| Accuracy: 133.542\n",
      "# Iteration  1092 -> Loss: 0.9101306366501056 \t| Accuracy: 133.542\n",
      "# Iteration  1093 -> Loss: 0.9097530821925001 \t| Accuracy: 133.542\n",
      "# Iteration  1094 -> Loss: 0.9093758004083916 \t| Accuracy: 133.542\n",
      "# Iteration  1095 -> Loss: 0.9089987910909869 \t| Accuracy: 133.542\n",
      "# Iteration  1096 -> Loss: 0.9086220540363487 \t| Accuracy: 133.542\n",
      "# Iteration  1097 -> Loss: 0.9082455890433724 \t| Accuracy: 133.542\n",
      "# Iteration  1098 -> Loss: 0.9078693959137637 \t| Accuracy: 133.542\n",
      "# Iteration  1099 -> Loss: 0.9074934744520134 \t| Accuracy: 133.542\n",
      "# Iteration  1100 -> Loss: 0.9071178244653756 \t| Accuracy: 133.542\n",
      "# Iteration  1101 -> Loss: 0.9067424457638422 \t| Accuracy: 133.542\n",
      "# Iteration  1102 -> Loss: 0.9063673381601218 \t| Accuracy: 133.542\n",
      "# Iteration  1103 -> Loss: 0.9059925014696143 \t| Accuracy: 133.542\n",
      "# Iteration  1104 -> Loss: 0.9056179355103883 \t| Accuracy: 133.542\n",
      "# Iteration  1105 -> Loss: 0.9052436401031574 \t| Accuracy: 133.542\n",
      "# Iteration  1106 -> Loss: 0.9048696150712573 \t| Accuracy: 133.542\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Iteration  1107 -> Loss: 0.9044958602406211 \t| Accuracy: 133.542\n",
      "# Iteration  1108 -> Loss: 0.9041223754397577 \t| Accuracy: 133.542\n",
      "# Iteration  1109 -> Loss: 0.9037491604997265 \t| Accuracy: 132.917\n",
      "# Iteration  1110 -> Loss: 0.9033762152541153 \t| Accuracy: 132.917\n",
      "# Iteration  1111 -> Loss: 0.9030035395390162 \t| Accuracy: 132.917\n",
      "# Iteration  1112 -> Loss: 0.9026311331930031 \t| Accuracy: 132.917\n",
      "# Iteration  1113 -> Loss: 0.9022589960571074 \t| Accuracy: 132.917\n",
      "# Iteration  1114 -> Loss: 0.9018871279747948 \t| Accuracy: 132.917\n",
      "# Iteration  1115 -> Loss: 0.9015155287919425 \t| Accuracy: 132.917\n",
      "# Iteration  1116 -> Loss: 0.9011441983568164 \t| Accuracy: 132.917\n",
      "# Iteration  1117 -> Loss: 0.9007731365200458 \t| Accuracy: 132.917\n",
      "# Iteration  1118 -> Loss: 0.9004023431346025 \t| Accuracy: 132.917\n",
      "# Iteration  1119 -> Loss: 0.9000318180557765 \t| Accuracy: 132.917\n",
      "# Iteration  1120 -> Loss: 0.899661561141152 \t| Accuracy: 132.917\n",
      "# Iteration  1121 -> Loss: 0.8992915722505865 \t| Accuracy: 132.917\n",
      "# Iteration  1122 -> Loss: 0.8989218512461853 \t| Accuracy: 132.917\n",
      "# Iteration  1123 -> Loss: 0.8985523979922798 \t| Accuracy: 132.917\n",
      "# Iteration  1124 -> Loss: 0.8981832123554042 \t| Accuracy: 132.917\n",
      "# Iteration  1125 -> Loss: 0.8978142942042723 \t| Accuracy: 132.917\n",
      "# Iteration  1126 -> Loss: 0.8974456434097541 \t| Accuracy: 132.917\n",
      "# Iteration  1127 -> Loss: 0.8970772598448539 \t| Accuracy: 132.917\n",
      "# Iteration  1128 -> Loss: 0.8967091433846864 \t| Accuracy: 132.917\n",
      "# Iteration  1129 -> Loss: 0.8963412939064545 \t| Accuracy: 132.917\n",
      "# Iteration  1130 -> Loss: 0.8959737112894255 \t| Accuracy: 132.917\n",
      "# Iteration  1131 -> Loss: 0.8956063954149099 \t| Accuracy: 132.917\n",
      "# Iteration  1132 -> Loss: 0.8952393461662367 \t| Accuracy: 132.917\n",
      "# Iteration  1133 -> Loss: 0.894872563428732 \t| Accuracy: 132.917\n",
      "# Iteration  1134 -> Loss: 0.8945060470896964 \t| Accuracy: 132.917\n",
      "# Iteration  1135 -> Loss: 0.8941397970383812 \t| Accuracy: 132.917\n",
      "# Iteration  1136 -> Loss: 0.893773813165967 \t| Accuracy: 132.917\n",
      "# Iteration  1137 -> Loss: 0.8934080953655406 \t| Accuracy: 132.917\n",
      "# Iteration  1138 -> Loss: 0.8930426435320724 \t| Accuracy: 132.917\n",
      "# Iteration  1139 -> Loss: 0.8926774575623945 \t| Accuracy: 132.917\n",
      "# Iteration  1140 -> Loss: 0.8923125373551777 \t| Accuracy: 132.917\n",
      "# Iteration  1141 -> Loss: 0.8919478828109094 \t| Accuracy: 132.917\n",
      "# Iteration  1142 -> Loss: 0.8915834938318715 \t| Accuracy: 132.917\n",
      "# Iteration  1143 -> Loss: 0.8912193703221182 \t| Accuracy: 132.917\n",
      "# Iteration  1144 -> Loss: 0.8908555121874531 \t| Accuracy: 132.917\n",
      "# Iteration  1145 -> Loss: 0.8904919193354077 \t| Accuracy: 132.917\n",
      "# Iteration  1146 -> Loss: 0.89012859167522 \t| Accuracy: 132.917\n",
      "# Iteration  1147 -> Loss: 0.8897655291178106 \t| Accuracy: 132.917\n",
      "# Iteration  1148 -> Loss: 0.8894027315757628 \t| Accuracy: 132.917\n",
      "# Iteration  1149 -> Loss: 0.8890401989632994 \t| Accuracy: 132.917\n",
      "# Iteration  1150 -> Loss: 0.8886779311962613 \t| Accuracy: 132.917\n",
      "# Iteration  1151 -> Loss: 0.8883159281920856 \t| Accuracy: 132.917\n",
      "# Iteration  1152 -> Loss: 0.8879541898697844 \t| Accuracy: 132.917\n",
      "# Iteration  1153 -> Loss: 0.8875927161499226 \t| Accuracy: 132.917\n",
      "# Iteration  1154 -> Loss: 0.8872315069545964 \t| Accuracy: 132.917\n",
      "# Iteration  1155 -> Loss: 0.8868705622074126 \t| Accuracy: 132.917\n",
      "# Iteration  1156 -> Loss: 0.8865098818334654 \t| Accuracy: 132.917\n",
      "# Iteration  1157 -> Loss: 0.8861494657593171 \t| Accuracy: 132.917\n",
      "# Iteration  1158 -> Loss: 0.8857893139129754 \t| Accuracy: 132.917\n",
      "# Iteration  1159 -> Loss: 0.8854294262238733 \t| Accuracy: 132.917\n",
      "# Iteration  1160 -> Loss: 0.8850698026228465 \t| Accuracy: 132.917\n",
      "# Iteration  1161 -> Loss: 0.8847104430421135 \t| Accuracy: 132.917\n",
      "# Iteration  1162 -> Loss: 0.8843513474152547 \t| Accuracy: 132.917\n",
      "# Iteration  1163 -> Loss: 0.8839925156771905 \t| Accuracy: 132.917\n",
      "# Iteration  1164 -> Loss: 0.8836339477641608 \t| Accuracy: 132.917\n",
      "# Iteration  1165 -> Loss: 0.8832756436137055 \t| Accuracy: 132.917\n",
      "# Iteration  1166 -> Loss: 0.8829176031646416 \t| Accuracy: 132.917\n",
      "# Iteration  1167 -> Loss: 0.8825598263570446 \t| Accuracy: 132.917\n",
      "# Iteration  1168 -> Loss: 0.882202313132227 \t| Accuracy: 132.917\n",
      "# Iteration  1169 -> Loss: 0.8818450634327175 \t| Accuracy: 132.917\n",
      "# Iteration  1170 -> Loss: 0.8814880772022418 \t| Accuracy: 131.042\n",
      "# Iteration  1171 -> Loss: 0.8811313543857011 \t| Accuracy: 131.042\n",
      "# Iteration  1172 -> Loss: 0.8807748949291532 \t| Accuracy: 131.042\n",
      "# Iteration  1173 -> Loss: 0.880418698779791 \t| Accuracy: 131.042\n",
      "# Iteration  1174 -> Loss: 0.8800627658859236 \t| Accuracy: 131.042\n",
      "# Iteration  1175 -> Loss: 0.8797070961969558 \t| Accuracy: 131.042\n",
      "# Iteration  1176 -> Loss: 0.8793516896633681 \t| Accuracy: 131.042\n",
      "# Iteration  1177 -> Loss: 0.8789965462366981 \t| Accuracy: 131.042\n",
      "# Iteration  1178 -> Loss: 0.878641665869519 \t| Accuracy: 131.042\n",
      "# Iteration  1179 -> Loss: 0.8782870485154214 \t| Accuracy: 131.042\n",
      "# Iteration  1180 -> Loss: 0.8779326941289937 \t| Accuracy: 131.042\n",
      "# Iteration  1181 -> Loss: 0.877578602665802 \t| Accuracy: 131.042\n",
      "# Iteration  1182 -> Loss: 0.8772247740823715 \t| Accuracy: 130.417\n",
      "# Iteration  1183 -> Loss: 0.8768712083361669 \t| Accuracy: 130.417\n",
      "# Iteration  1184 -> Loss: 0.8765179053855734 \t| Accuracy: 130.417\n",
      "# Iteration  1185 -> Loss: 0.8761648651898779 \t| Accuracy: 130.417\n",
      "# Iteration  1186 -> Loss: 0.8758120877092498 \t| Accuracy: 130.417\n",
      "# Iteration  1187 -> Loss: 0.875459572904722 \t| Accuracy: 130.417\n",
      "# Iteration  1188 -> Loss: 0.8751073207381724 \t| Accuracy: 130.417\n",
      "# Iteration  1189 -> Loss: 0.8747553311723063 \t| Accuracy: 130.417\n",
      "# Iteration  1190 -> Loss: 0.8744036041706357 \t| Accuracy: 130.417\n",
      "# Iteration  1191 -> Loss: 0.8740521396974621 \t| Accuracy: 130.417\n",
      "# Iteration  1192 -> Loss: 0.8737009377178591 \t| Accuracy: 130.417\n",
      "# Iteration  1193 -> Loss: 0.8733499981976522 \t| Accuracy: 130.417\n",
      "# Iteration  1194 -> Loss: 0.8729993211034022 \t| Accuracy: 130.417\n",
      "# Iteration  1195 -> Loss: 0.8726489064023866 \t| Accuracy: 130.417\n",
      "# Iteration  1196 -> Loss: 0.8722987540625814 \t| Accuracy: 130.417\n",
      "# Iteration  1197 -> Loss: 0.8719488640526435 \t| Accuracy: 130.417\n",
      "# Iteration  1198 -> Loss: 0.8715992363418938 \t| Accuracy: 130.417\n",
      "# Iteration  1199 -> Loss: 0.8712498709002982 \t| Accuracy: 130.417\n",
      "# Iteration  1200 -> Loss: 0.8709007676984508 \t| Accuracy: 130.417\n",
      "# Iteration  1201 -> Loss: 0.8705519267075565 \t| Accuracy: 129.167\n",
      "# Iteration  1202 -> Loss: 0.8702033478994141 \t| Accuracy: 129.167\n",
      "# Iteration  1203 -> Loss: 0.8698550312463982 \t| Accuracy: 129.167\n",
      "# Iteration  1204 -> Loss: 0.8695069767214423 \t| Accuracy: 129.167\n",
      "# Iteration  1205 -> Loss: 0.8691591842980223 \t| Accuracy: 129.167\n",
      "# Iteration  1206 -> Loss: 0.8688116539501399 \t| Accuracy: 129.167\n",
      "# Iteration  1207 -> Loss: 0.8684643856523048 \t| Accuracy: 129.167\n",
      "# Iteration  1208 -> Loss: 0.8681173793795187 \t| Accuracy: 129.167\n",
      "# Iteration  1209 -> Loss: 0.8677706351072589 \t| Accuracy: 129.167\n",
      "# Iteration  1210 -> Loss: 0.8674241528114611 \t| Accuracy: 129.167\n",
      "# Iteration  1211 -> Loss: 0.8670779324685038 \t| Accuracy: 129.167\n",
      "# Iteration  1212 -> Loss: 0.8667319740551924 \t| Accuracy: 129.167\n",
      "# Iteration  1213 -> Loss: 0.8663862775487419 \t| Accuracy: 129.167\n",
      "# Iteration  1214 -> Loss: 0.8660408429267614 \t| Accuracy: 129.167\n",
      "# Iteration  1215 -> Loss: 0.8656956701672393 \t| Accuracy: 129.167\n",
      "# Iteration  1216 -> Loss: 0.8653507592485254 \t| Accuracy: 129.167\n",
      "# Iteration  1217 -> Loss: 0.8650061101493177 \t| Accuracy: 129.167\n",
      "# Iteration  1218 -> Loss: 0.8646617228486444 \t| Accuracy: 129.167\n",
      "# Iteration  1219 -> Loss: 0.8643175973258508 \t| Accuracy: 129.167\n",
      "# Iteration  1220 -> Loss: 0.8639737335605815 \t| Accuracy: 129.167\n",
      "# Iteration  1221 -> Loss: 0.8636301315327678 \t| Accuracy: 129.167\n",
      "# Iteration  1222 -> Loss: 0.8632867912226109 \t| Accuracy: 129.167\n",
      "# Iteration  1223 -> Loss: 0.8629437126105663 \t| Accuracy: 129.167\n",
      "# Iteration  1224 -> Loss: 0.8626008956773317 \t| Accuracy: 129.167\n",
      "# Iteration  1225 -> Loss: 0.8622583404038292 \t| Accuracy: 129.167\n",
      "# Iteration  1226 -> Loss: 0.8619160467711924 \t| Accuracy: 129.167\n",
      "# Iteration  1227 -> Loss: 0.8615740147607513 \t| Accuracy: 129.167\n",
      "# Iteration  1228 -> Loss: 0.8612322443540181 \t| Accuracy: 129.167\n",
      "# Iteration  1229 -> Loss: 0.860890735532673 \t| Accuracy: 129.167\n",
      "# Iteration  1230 -> Loss: 0.8605494882785493 \t| Accuracy: 129.167\n",
      "# Iteration  1231 -> Loss: 0.8602085025736201 \t| Accuracy: 129.167\n",
      "# Iteration  1232 -> Loss: 0.8598677783999842 \t| Accuracy: 129.167\n",
      "# Iteration  1233 -> Loss: 0.8595273157398517 \t| Accuracy: 129.167\n",
      "# Iteration  1234 -> Loss: 0.8591871145755313 \t| Accuracy: 129.167\n",
      "# Iteration  1235 -> Loss: 0.8588471748894158 \t| Accuracy: 129.167\n",
      "# Iteration  1236 -> Loss: 0.8585074966639685 \t| Accuracy: 129.167\n",
      "# Iteration  1237 -> Loss: 0.8581680798817106 \t| Accuracy: 129.167\n",
      "# Iteration  1238 -> Loss: 0.8578289245252079 \t| Accuracy: 129.167\n",
      "# Iteration  1239 -> Loss: 0.8574900305770563 \t| Accuracy: 129.167\n",
      "# Iteration  1240 -> Loss: 0.8571513980198707 \t| Accuracy: 129.167\n",
      "# Iteration  1241 -> Loss: 0.8568130268362705 \t| Accuracy: 129.167\n",
      "# Iteration  1242 -> Loss: 0.8564749170088675 \t| Accuracy: 129.167\n",
      "# Iteration  1243 -> Loss: 0.8561370685202534 \t| Accuracy: 129.167\n",
      "# Iteration  1244 -> Loss: 0.8557994813529867 \t| Accuracy: 129.167\n",
      "# Iteration  1245 -> Loss: 0.8554621554895803 \t| Accuracy: 129.167\n",
      "# Iteration  1246 -> Loss: 0.8551250909124898 \t| Accuracy: 129.167\n",
      "# Iteration  1247 -> Loss: 0.8547882876041003 \t| Accuracy: 129.167\n",
      "# Iteration  1248 -> Loss: 0.8544517455467151 \t| Accuracy: 129.167\n",
      "# Iteration  1249 -> Loss: 0.8541154647225431 \t| Accuracy: 129.167\n",
      "# Iteration  1250 -> Loss: 0.8537794451136875 \t| Accuracy: 129.167\n",
      "# Iteration  1251 -> Loss: 0.8534436867021333 \t| Accuracy: 129.167\n",
      "# Iteration  1252 -> Loss: 0.8531081894697363 \t| Accuracy: 129.167\n",
      "# Iteration  1253 -> Loss: 0.8527729533982115 \t| Accuracy: 129.167\n",
      "# Iteration  1254 -> Loss: 0.8524379784691208 \t| Accuracy: 129.167\n",
      "# Iteration  1255 -> Loss: 0.8521032646638629 \t| Accuracy: 129.167\n",
      "# Iteration  1256 -> Loss: 0.8517688119636615 \t| Accuracy: 129.167\n",
      "# Iteration  1257 -> Loss: 0.8514346203495542 \t| Accuracy: 129.167\n",
      "# Iteration  1258 -> Loss: 0.8511006898023812 \t| Accuracy: 129.167\n",
      "# Iteration  1259 -> Loss: 0.8507670203027755 \t| Accuracy: 129.167\n",
      "# Iteration  1260 -> Loss: 0.8504336118311507 \t| Accuracy: 129.167\n",
      "# Iteration  1261 -> Loss: 0.8501004643676919 \t| Accuracy: 129.167\n",
      "# Iteration  1262 -> Loss: 0.8497675778923438 \t| Accuracy: 129.167\n",
      "# Iteration  1263 -> Loss: 0.8494349523848013 \t| Accuracy: 129.167\n",
      "# Iteration  1264 -> Loss: 0.8491025878244983 \t| Accuracy: 129.167\n",
      "# Iteration  1265 -> Loss: 0.8487704841905985 \t| Accuracy: 129.167\n",
      "# Iteration  1266 -> Loss: 0.8484386414619846 \t| Accuracy: 129.167\n",
      "# Iteration  1267 -> Loss: 0.8481070596172487 \t| Accuracy: 129.167\n",
      "# Iteration  1268 -> Loss: 0.8477757386346821 \t| Accuracy: 129.167\n",
      "# Iteration  1269 -> Loss: 0.8474446784922658 \t| Accuracy: 129.167\n",
      "# Iteration  1270 -> Loss: 0.8471138791676611 \t| Accuracy: 129.167\n",
      "# Iteration  1271 -> Loss: 0.8467833406381994 \t| Accuracy: 129.167\n",
      "# Iteration  1272 -> Loss: 0.8464530628808736 \t| Accuracy: 129.167\n",
      "# Iteration  1273 -> Loss: 0.8461230458723282 \t| Accuracy: 129.167\n",
      "# Iteration  1274 -> Loss: 0.8457932895888501 \t| Accuracy: 129.167\n",
      "# Iteration  1275 -> Loss: 0.8454637940063605 \t| Accuracy: 129.167\n",
      "# Iteration  1276 -> Loss: 0.845134559100404 \t| Accuracy: 129.167\n",
      "# Iteration  1277 -> Loss: 0.8448055848461422 \t| Accuracy: 129.167\n",
      "# Iteration  1278 -> Loss: 0.8444768712183423 \t| Accuracy: 129.167\n",
      "# Iteration  1279 -> Loss: 0.8441484181913705 \t| Accuracy: 129.167\n",
      "# Iteration  1280 -> Loss: 0.8438202257391829 \t| Accuracy: 129.167\n",
      "# Iteration  1281 -> Loss: 0.8434922938353168 \t| Accuracy: 129.167\n",
      "# Iteration  1282 -> Loss: 0.8431646224528817 \t| Accuracy: 129.167\n",
      "# Iteration  1283 -> Loss: 0.8428372115645532 \t| Accuracy: 129.167\n",
      "# Iteration  1284 -> Loss: 0.842510061142562 \t| Accuracy: 129.167\n",
      "# Iteration  1285 -> Loss: 0.8421831711586888 \t| Accuracy: 129.167\n",
      "# Iteration  1286 -> Loss: 0.8418565415842543 \t| Accuracy: 129.167\n",
      "# Iteration  1287 -> Loss: 0.8415301723901121 \t| Accuracy: 129.167\n",
      "# Iteration  1288 -> Loss: 0.8412040635466409 \t| Accuracy: 129.167\n",
      "# Iteration  1289 -> Loss: 0.8408782150237377 \t| Accuracy: 129.167\n",
      "# Iteration  1290 -> Loss: 0.8405526267908089 \t| Accuracy: 129.167\n",
      "# Iteration  1291 -> Loss: 0.8402272988167638 \t| Accuracy: 129.167\n",
      "# Iteration  1292 -> Loss: 0.8399022310700074 \t| Accuracy: 129.167\n",
      "# Iteration  1293 -> Loss: 0.8395774235184327 \t| Accuracy: 129.167\n",
      "# Iteration  1294 -> Loss: 0.8392528761294137 \t| Accuracy: 129.167\n",
      "# Iteration  1295 -> Loss: 0.8389285888697993 \t| Accuracy: 129.167\n",
      "# Iteration  1296 -> Loss: 0.8386045617059049 \t| Accuracy: 129.167\n",
      "# Iteration  1297 -> Loss: 0.8382807946035067 \t| Accuracy: 129.167\n",
      "# Iteration  1298 -> Loss: 0.8379572875278352 \t| Accuracy: 128.542\n",
      "# Iteration  1299 -> Loss: 0.8376340404435672 \t| Accuracy: 128.542\n",
      "# Iteration  1300 -> Loss: 0.8373110533148211 \t| Accuracy: 128.542\n",
      "# Iteration  1301 -> Loss: 0.8369883261051494 \t| Accuracy: 128.542\n",
      "# Iteration  1302 -> Loss: 0.8366658587775329 \t| Accuracy: 128.542\n",
      "# Iteration  1303 -> Loss: 0.8363436512943737 \t| Accuracy: 128.542\n",
      "# Iteration  1304 -> Loss: 0.8360217036174904 \t| Accuracy: 128.542\n",
      "# Iteration  1305 -> Loss: 0.835700015708111 \t| Accuracy: 128.542\n",
      "# Iteration  1306 -> Loss: 0.8353785875268676 \t| Accuracy: 128.542\n",
      "# Iteration  1307 -> Loss: 0.8350574190337902 \t| Accuracy: 128.542\n",
      "# Iteration  1308 -> Loss: 0.8347365101883014 \t| Accuracy: 128.542\n",
      "# Iteration  1309 -> Loss: 0.8344158609492103 \t| Accuracy: 128.542\n",
      "# Iteration  1310 -> Loss: 0.8340954712747075 \t| Accuracy: 128.542\n",
      "# Iteration  1311 -> Loss: 0.8337753411223593 \t| Accuracy: 128.542\n",
      "# Iteration  1312 -> Loss: 0.8334554704491022 \t| Accuracy: 128.542\n",
      "# Iteration  1313 -> Loss: 0.8331358592112382 \t| Accuracy: 128.542\n",
      "# Iteration  1314 -> Loss: 0.8328165073644296 \t| Accuracy: 128.542\n",
      "# Iteration  1315 -> Loss: 0.8324974148636928 \t| Accuracy: 128.542\n",
      "# Iteration  1316 -> Loss: 0.8321785816633951 \t| Accuracy: 128.542\n",
      "# Iteration  1317 -> Loss: 0.8318600077172483 \t| Accuracy: 128.542\n",
      "# Iteration  1318 -> Loss: 0.831541692978305 \t| Accuracy: 128.542\n",
      "# Iteration  1319 -> Loss: 0.8312236373989531 \t| Accuracy: 128.542\n",
      "# Iteration  1320 -> Loss: 0.8309058409309111 \t| Accuracy: 128.542\n",
      "# Iteration  1321 -> Loss: 0.8305883035252253 \t| Accuracy: 128.542\n",
      "# Iteration  1322 -> Loss: 0.830271025132262 \t| Accuracy: 128.542\n",
      "# Iteration  1323 -> Loss: 0.8299540057017067 \t| Accuracy: 128.542\n",
      "# Iteration  1324 -> Loss: 0.8296372451825578 \t| Accuracy: 128.542\n",
      "# Iteration  1325 -> Loss: 0.8293207435231229 \t| Accuracy: 128.542\n",
      "# Iteration  1326 -> Loss: 0.8290045006710139 \t| Accuracy: 128.542\n",
      "# Iteration  1327 -> Loss: 0.8286885165731452 \t| Accuracy: 128.542\n",
      "# Iteration  1328 -> Loss: 0.8283727911757272 \t| Accuracy: 128.542\n",
      "# Iteration  1329 -> Loss: 0.8280573244242636 \t| Accuracy: 128.542\n",
      "# Iteration  1330 -> Loss: 0.827742116263548 \t| Accuracy: 128.542\n",
      "# Iteration  1331 -> Loss: 0.8274271666376597 \t| Accuracy: 128.542\n",
      "# Iteration  1332 -> Loss: 0.8271124754899597 \t| Accuracy: 128.542\n",
      "# Iteration  1333 -> Loss: 0.8267980427630881 \t| Accuracy: 128.542\n",
      "# Iteration  1334 -> Loss: 0.8264838683989602 \t| Accuracy: 128.542\n",
      "# Iteration  1335 -> Loss: 0.8261699523387629 \t| Accuracy: 128.542\n",
      "# Iteration  1336 -> Loss: 0.8258562945229513 \t| Accuracy: 128.542\n",
      "# Iteration  1337 -> Loss: 0.8255428948912464 \t| Accuracy: 128.542\n",
      "# Iteration  1338 -> Loss: 0.8252297533826315 \t| Accuracy: 128.542\n",
      "# Iteration  1339 -> Loss: 0.8249168699353483 \t| Accuracy: 128.542\n",
      "# Iteration  1340 -> Loss: 0.824604244486895 \t| Accuracy: 128.542\n",
      "# Iteration  1341 -> Loss: 0.8242918769740228 \t| Accuracy: 128.542\n",
      "# Iteration  1342 -> Loss: 0.8239797673327339 \t| Accuracy: 128.542\n",
      "# Iteration  1343 -> Loss: 0.8236679154982773 \t| Accuracy: 128.542\n",
      "# Iteration  1344 -> Loss: 0.8233563214051479 \t| Accuracy: 128.542\n",
      "# Iteration  1345 -> Loss: 0.8230449849870819 \t| Accuracy: 128.542\n",
      "# Iteration  1346 -> Loss: 0.8227339061770559 \t| Accuracy: 128.542\n",
      "# Iteration  1347 -> Loss: 0.8224230849072837 \t| Accuracy: 128.542\n",
      "# Iteration  1348 -> Loss: 0.8221125211092143 \t| Accuracy: 128.542\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Iteration  1349 -> Loss: 0.8218022147135289 \t| Accuracy: 128.542\n",
      "# Iteration  1350 -> Loss: 0.8214921656501392 \t| Accuracy: 128.542\n",
      "# Iteration  1351 -> Loss: 0.8211823738481854 \t| Accuracy: 128.542\n",
      "# Iteration  1352 -> Loss: 0.8208728392360336 \t| Accuracy: 128.542\n",
      "# Iteration  1353 -> Loss: 0.8205635617412738 \t| Accuracy: 128.542\n",
      "# Iteration  1354 -> Loss: 0.8202545412907178 \t| Accuracy: 128.542\n",
      "# Iteration  1355 -> Loss: 0.8199457778103986 \t| Accuracy: 128.542\n",
      "# Iteration  1356 -> Loss: 0.8196372712255663 \t| Accuracy: 128.542\n",
      "# Iteration  1357 -> Loss: 0.8193290214606882 \t| Accuracy: 128.542\n",
      "# Iteration  1358 -> Loss: 0.8190210284394465 \t| Accuracy: 128.542\n",
      "# Iteration  1359 -> Loss: 0.8187132920847363 \t| Accuracy: 128.542\n",
      "# Iteration  1360 -> Loss: 0.8184058123186645 \t| Accuracy: 128.542\n",
      "# Iteration  1361 -> Loss: 0.8180985890625481 \t| Accuracy: 128.542\n",
      "# Iteration  1362 -> Loss: 0.8177916222369125 \t| Accuracy: 128.542\n",
      "# Iteration  1363 -> Loss: 0.8174849117614906 \t| Accuracy: 128.542\n",
      "# Iteration  1364 -> Loss: 0.8171784575552214 \t| Accuracy: 128.542\n",
      "# Iteration  1365 -> Loss: 0.8168722595362486 \t| Accuracy: 128.542\n",
      "# Iteration  1366 -> Loss: 0.8165663176219188 \t| Accuracy: 128.542\n",
      "# Iteration  1367 -> Loss: 0.8162606317287817 \t| Accuracy: 128.542\n",
      "# Iteration  1368 -> Loss: 0.8159552017725876 \t| Accuracy: 128.542\n",
      "# Iteration  1369 -> Loss: 0.8156500276682876 \t| Accuracy: 128.542\n",
      "# Iteration  1370 -> Loss: 0.8153451093300319 \t| Accuracy: 128.542\n",
      "# Iteration  1371 -> Loss: 0.8150404466711688 \t| Accuracy: 128.542\n",
      "# Iteration  1372 -> Loss: 0.8147360396042443 \t| Accuracy: 128.542\n",
      "# Iteration  1373 -> Loss: 0.8144318880410016 \t| Accuracy: 128.542\n",
      "# Iteration  1374 -> Loss: 0.8141279918923792 \t| Accuracy: 128.542\n",
      "# Iteration  1375 -> Loss: 0.8138243510685113 \t| Accuracy: 127.292\n",
      "# Iteration  1376 -> Loss: 0.8135209654787269 \t| Accuracy: 127.292\n",
      "# Iteration  1377 -> Loss: 0.813217835031549 \t| Accuracy: 127.292\n",
      "# Iteration  1378 -> Loss: 0.8129149596346941 \t| Accuracy: 127.292\n",
      "# Iteration  1379 -> Loss: 0.812612339195072 \t| Accuracy: 127.292\n",
      "# Iteration  1380 -> Loss: 0.8123099736187859 \t| Accuracy: 127.292\n",
      "# Iteration  1381 -> Loss: 0.8120078628111301 \t| Accuracy: 127.292\n",
      "# Iteration  1382 -> Loss: 0.8117060066765923 \t| Accuracy: 127.292\n",
      "# Iteration  1383 -> Loss: 0.8114044051188515 \t| Accuracy: 127.083\n",
      "# Iteration  1384 -> Loss: 0.8111030580407783 \t| Accuracy: 127.083\n",
      "# Iteration  1385 -> Loss: 0.8108019653444352 \t| Accuracy: 127.083\n",
      "# Iteration  1386 -> Loss: 0.8105011269310758 \t| Accuracy: 127.083\n",
      "# Iteration  1387 -> Loss: 0.8102005427011455 \t| Accuracy: 127.083\n",
      "# Iteration  1388 -> Loss: 0.8099002125542806 \t| Accuracy: 127.083\n",
      "# Iteration  1389 -> Loss: 0.8096001363893094 \t| Accuracy: 127.083\n",
      "# Iteration  1390 -> Loss: 0.8093003141042513 \t| Accuracy: 127.083\n",
      "# Iteration  1391 -> Loss: 0.8090007455963177 \t| Accuracy: 127.083\n",
      "# Iteration  1392 -> Loss: 0.8087014307619116 \t| Accuracy: 127.083\n",
      "# Iteration  1393 -> Loss: 0.8084023694966286 \t| Accuracy: 127.083\n",
      "# Iteration  1394 -> Loss: 0.8081035616952562 \t| Accuracy: 127.083\n",
      "# Iteration  1395 -> Loss: 0.807805007251775 \t| Accuracy: 127.083\n",
      "# Iteration  1396 -> Loss: 0.8075067060593586 \t| Accuracy: 127.083\n",
      "# Iteration  1397 -> Loss: 0.8072086580103742 \t| Accuracy: 127.083\n",
      "# Iteration  1398 -> Loss: 0.806910862996383 \t| Accuracy: 126.458\n",
      "# Iteration  1399 -> Loss: 0.8066133209081405 \t| Accuracy: 126.458\n",
      "# Iteration  1400 -> Loss: 0.8063160316355981 \t| Accuracy: 126.458\n",
      "# Iteration  1401 -> Loss: 0.8060189950679013 \t| Accuracy: 126.458\n",
      "# Iteration  1402 -> Loss: 0.8057222110933933 \t| Accuracy: 125.833\n",
      "# Iteration  1403 -> Loss: 0.8054256795996139 \t| Accuracy: 125.833\n",
      "# Iteration  1404 -> Loss: 0.8051294004733003 \t| Accuracy: 125.833\n",
      "# Iteration  1405 -> Loss: 0.8048333736003883 \t| Accuracy: 125.208\n",
      "# Iteration  1406 -> Loss: 0.8045375988660128 \t| Accuracy: 125.208\n",
      "# Iteration  1407 -> Loss: 0.804242076154509 \t| Accuracy: 125.208\n",
      "# Iteration  1408 -> Loss: 0.8039468053494131 \t| Accuracy: 125.208\n",
      "# Iteration  1409 -> Loss: 0.8036517863334626 \t| Accuracy: 125.208\n",
      "# Iteration  1410 -> Loss: 0.8033570189885982 \t| Accuracy: 125.208\n",
      "# Iteration  1411 -> Loss: 0.8030625031959646 \t| Accuracy: 125.208\n",
      "# Iteration  1412 -> Loss: 0.802768238835911 \t| Accuracy: 125.208\n",
      "# Iteration  1413 -> Loss: 0.8024742257879921 \t| Accuracy: 125.208\n",
      "# Iteration  1414 -> Loss: 0.8021804639309703 \t| Accuracy: 125.208\n",
      "# Iteration  1415 -> Loss: 0.8018869531428157 \t| Accuracy: 125.208\n",
      "# Iteration  1416 -> Loss: 0.8015936933007074 \t| Accuracy: 125.208\n",
      "# Iteration  1417 -> Loss: 0.8013006842810353 \t| Accuracy: 125.208\n",
      "# Iteration  1418 -> Loss: 0.801007925959401 \t| Accuracy: 125.208\n",
      "# Iteration  1419 -> Loss: 0.800715418210619 \t| Accuracy: 125.208\n",
      "# Iteration  1420 -> Loss: 0.8004231609087178 \t| Accuracy: 125.208\n",
      "# Iteration  1421 -> Loss: 0.8001311539269415 \t| Accuracy: 125.208\n",
      "# Iteration  1422 -> Loss: 0.7998393971377514 \t| Accuracy: 125.208\n",
      "# Iteration  1423 -> Loss: 0.7995478904128271 \t| Accuracy: 125.208\n",
      "# Iteration  1424 -> Loss: 0.799256633623068 \t| Accuracy: 125.208\n",
      "# Iteration  1425 -> Loss: 0.798965626638594 \t| Accuracy: 125.208\n",
      "# Iteration  1426 -> Loss: 0.7986748693287489 \t| Accuracy: 124.583\n",
      "# Iteration  1427 -> Loss: 0.7983843615620998 \t| Accuracy: 124.583\n",
      "# Iteration  1428 -> Loss: 0.7980941032064397 \t| Accuracy: 124.583\n",
      "# Iteration  1429 -> Loss: 0.7978040941287894 \t| Accuracy: 124.583\n",
      "# Iteration  1430 -> Loss: 0.7975143341953982 \t| Accuracy: 124.583\n",
      "# Iteration  1431 -> Loss: 0.7972248232717463 \t| Accuracy: 124.583\n",
      "# Iteration  1432 -> Loss: 0.7969355612225457 \t| Accuracy: 124.583\n",
      "# Iteration  1433 -> Loss: 0.7966465479117429 \t| Accuracy: 124.583\n",
      "# Iteration  1434 -> Loss: 0.7963577832025197 \t| Accuracy: 124.583\n",
      "# Iteration  1435 -> Loss: 0.796069266957295 \t| Accuracy: 124.583\n",
      "# Iteration  1436 -> Loss: 0.7957809990377276 \t| Accuracy: 124.583\n",
      "# Iteration  1437 -> Loss: 0.7954929793047164 \t| Accuracy: 124.583\n",
      "# Iteration  1438 -> Loss: 0.7952052076184041 \t| Accuracy: 124.583\n",
      "# Iteration  1439 -> Loss: 0.7949176838381767 \t| Accuracy: 124.583\n",
      "# Iteration  1440 -> Loss: 0.7946304078226681 \t| Accuracy: 124.583\n",
      "# Iteration  1441 -> Loss: 0.7943433794297594 \t| Accuracy: 124.583\n",
      "# Iteration  1442 -> Loss: 0.7940565985165824 \t| Accuracy: 124.583\n",
      "# Iteration  1443 -> Loss: 0.7937700649395213 \t| Accuracy: 123.958\n",
      "# Iteration  1444 -> Loss: 0.7934837785542146 \t| Accuracy: 123.958\n",
      "# Iteration  1445 -> Loss: 0.7931977392155561 \t| Accuracy: 123.958\n",
      "# Iteration  1446 -> Loss: 0.7929119467776988 \t| Accuracy: 123.958\n",
      "# Iteration  1447 -> Loss: 0.7926264010940557 \t| Accuracy: 123.958\n",
      "# Iteration  1448 -> Loss: 0.7923411020173016 \t| Accuracy: 123.958\n",
      "# Iteration  1449 -> Loss: 0.7920560493993757 \t| Accuracy: 123.958\n",
      "# Iteration  1450 -> Loss: 0.791771243091484 \t| Accuracy: 123.958\n",
      "# Iteration  1451 -> Loss: 0.791486682944101 \t| Accuracy: 123.958\n",
      "# Iteration  1452 -> Loss: 0.7912023688069714 \t| Accuracy: 123.958\n",
      "# Iteration  1453 -> Loss: 0.7909183005291134 \t| Accuracy: 123.958\n",
      "# Iteration  1454 -> Loss: 0.7906344779588194 \t| Accuracy: 123.958\n",
      "# Iteration  1455 -> Loss: 0.7903509009436595 \t| Accuracy: 123.958\n",
      "# Iteration  1456 -> Loss: 0.7900675693304835 \t| Accuracy: 123.958\n",
      "# Iteration  1457 -> Loss: 0.7897844829654219 \t| Accuracy: 123.958\n",
      "# Iteration  1458 -> Loss: 0.7895016416938898 \t| Accuracy: 123.958\n",
      "# Iteration  1459 -> Loss: 0.7892190453605881 \t| Accuracy: 123.958\n",
      "# Iteration  1460 -> Loss: 0.7889366938095063 \t| Accuracy: 123.958\n",
      "# Iteration  1461 -> Loss: 0.7886545868839244 \t| Accuracy: 123.958\n",
      "# Iteration  1462 -> Loss: 0.7883727244264158 \t| Accuracy: 123.958\n",
      "# Iteration  1463 -> Loss: 0.7880911062788484 \t| Accuracy: 123.958\n",
      "# Iteration  1464 -> Loss: 0.7878097322823888 \t| Accuracy: 123.958\n",
      "# Iteration  1465 -> Loss: 0.7875286022775031 \t| Accuracy: 123.958\n",
      "# Iteration  1466 -> Loss: 0.7872477161039599 \t| Accuracy: 123.958\n",
      "# Iteration  1467 -> Loss: 0.7869670736008328 \t| Accuracy: 123.958\n",
      "# Iteration  1468 -> Loss: 0.7866866746065022 \t| Accuracy: 123.958\n",
      "# Iteration  1469 -> Loss: 0.7864065189586588 \t| Accuracy: 123.958\n",
      "# Iteration  1470 -> Loss: 0.7861266064943045 \t| Accuracy: 123.958\n",
      "# Iteration  1471 -> Loss: 0.7858469370497566 \t| Accuracy: 123.958\n",
      "# Iteration  1472 -> Loss: 0.7855675104606492 \t| Accuracy: 123.958\n",
      "# Iteration  1473 -> Loss: 0.7852883265619353 \t| Accuracy: 123.958\n",
      "# Iteration  1474 -> Loss: 0.7850093851878907 \t| Accuracy: 123.958\n",
      "# Iteration  1475 -> Loss: 0.7847306861721146 \t| Accuracy: 123.958\n",
      "# Iteration  1476 -> Loss: 0.7844522293475344 \t| Accuracy: 123.958\n",
      "# Iteration  1477 -> Loss: 0.784174014546406 \t| Accuracy: 123.333\n",
      "# Iteration  1478 -> Loss: 0.7838960416003178 \t| Accuracy: 123.333\n",
      "# Iteration  1479 -> Loss: 0.7836183103401928 \t| Accuracy: 123.333\n",
      "# Iteration  1480 -> Loss: 0.7833408205962908 \t| Accuracy: 123.333\n",
      "# Iteration  1481 -> Loss: 0.7830635721982117 \t| Accuracy: 123.333\n",
      "# Iteration  1482 -> Loss: 0.7827865649748975 \t| Accuracy: 123.333\n",
      "# Iteration  1483 -> Loss: 0.782509798754635 \t| Accuracy: 123.333\n",
      "# Iteration  1484 -> Loss: 0.7822332733650585 \t| Accuracy: 123.333\n",
      "# Iteration  1485 -> Loss: 0.7819569886331527 \t| Accuracy: 123.333\n",
      "# Iteration  1486 -> Loss: 0.7816809443852544 \t| Accuracy: 123.333\n",
      "# Iteration  1487 -> Loss: 0.7814051404470566 \t| Accuracy: 123.333\n",
      "# Iteration  1488 -> Loss: 0.7811295766436092 \t| Accuracy: 123.333\n",
      "# Iteration  1489 -> Loss: 0.7808542527993234 \t| Accuracy: 123.333\n",
      "# Iteration  1490 -> Loss: 0.7805791687379738 \t| Accuracy: 123.333\n",
      "# Iteration  1491 -> Loss: 0.7803043242827007 \t| Accuracy: 123.333\n",
      "# Iteration  1492 -> Loss: 0.7800297192560126 \t| Accuracy: 123.333\n",
      "# Iteration  1493 -> Loss: 0.7797553534797901 \t| Accuracy: 123.333\n",
      "# Iteration  1494 -> Loss: 0.7794812267752872 \t| Accuracy: 123.333\n",
      "# Iteration  1495 -> Loss: 0.7792073389631349 \t| Accuracy: 123.333\n",
      "# Iteration  1496 -> Loss: 0.7789336898633432 \t| Accuracy: 123.333\n",
      "# Iteration  1497 -> Loss: 0.7786602792953045 \t| Accuracy: 123.333\n",
      "# Iteration  1498 -> Loss: 0.7783871070777959 \t| Accuracy: 123.125\n",
      "# Iteration  1499 -> Loss: 0.7781141730289819 \t| Accuracy: 123.125\n",
      "# Iteration  1500 -> Loss: 0.7778414769664174 \t| Accuracy: 123.125\n",
      "# Iteration  1501 -> Loss: 0.7775690187070502 \t| Accuracy: 123.125\n",
      "# Iteration  1502 -> Loss: 0.777296798067224 \t| Accuracy: 123.125\n",
      "# Iteration  1503 -> Loss: 0.7770248148626804 \t| Accuracy: 123.125\n",
      "# Iteration  1504 -> Loss: 0.776753068908563 \t| Accuracy: 123.125\n",
      "# Iteration  1505 -> Loss: 0.7764815600194186 \t| Accuracy: 123.125\n",
      "# Iteration  1506 -> Loss: 0.7762102880092012 \t| Accuracy: 123.125\n",
      "# Iteration  1507 -> Loss: 0.7759392526912741 \t| Accuracy: 123.125\n",
      "# Iteration  1508 -> Loss: 0.7756684538784131 \t| Accuracy: 123.125\n",
      "# Iteration  1509 -> Loss: 0.7753978913828083 \t| Accuracy: 122.500\n",
      "# Iteration  1510 -> Loss: 0.7751275650160688 \t| Accuracy: 122.500\n",
      "# Iteration  1511 -> Loss: 0.7748574745892232 \t| Accuracy: 122.500\n",
      "# Iteration  1512 -> Loss: 0.7745876199127237 \t| Accuracy: 122.500\n",
      "# Iteration  1513 -> Loss: 0.7743180007964487 \t| Accuracy: 121.875\n",
      "# Iteration  1514 -> Loss: 0.7740486170497062 \t| Accuracy: 121.875\n",
      "# Iteration  1515 -> Loss: 0.7737794684812349 \t| Accuracy: 121.875\n",
      "# Iteration  1516 -> Loss: 0.7735105548992086 \t| Accuracy: 121.875\n",
      "# Iteration  1517 -> Loss: 0.773241876111238 \t| Accuracy: 121.875\n",
      "# Iteration  1518 -> Loss: 0.7729734319243746 \t| Accuracy: 121.875\n",
      "# Iteration  1519 -> Loss: 0.7727052221451125 \t| Accuracy: 121.875\n",
      "# Iteration  1520 -> Loss: 0.772437246579391 \t| Accuracy: 121.875\n",
      "# Iteration  1521 -> Loss: 0.7721695050325987 \t| Accuracy: 121.875\n",
      "# Iteration  1522 -> Loss: 0.7719019973095753 \t| Accuracy: 121.875\n",
      "# Iteration  1523 -> Loss: 0.7716347232146143 \t| Accuracy: 121.875\n",
      "# Iteration  1524 -> Loss: 0.7713676825514671 \t| Accuracy: 121.875\n",
      "# Iteration  1525 -> Loss: 0.7711008751233436 \t| Accuracy: 121.875\n",
      "# Iteration  1526 -> Loss: 0.7708343007329171 \t| Accuracy: 121.875\n",
      "# Iteration  1527 -> Loss: 0.7705679591823265 \t| Accuracy: 121.875\n",
      "# Iteration  1528 -> Loss: 0.7703018502731782 \t| Accuracy: 121.875\n",
      "# Iteration  1529 -> Loss: 0.7700359738065502 \t| Accuracy: 121.875\n",
      "# Iteration  1530 -> Loss: 0.7697703295829941 \t| Accuracy: 121.875\n",
      "# Iteration  1531 -> Loss: 0.7695049174025382 \t| Accuracy: 121.875\n",
      "# Iteration  1532 -> Loss: 0.7692397370646906 \t| Accuracy: 121.875\n",
      "# Iteration  1533 -> Loss: 0.7689747883684411 \t| Accuracy: 121.875\n",
      "# Iteration  1534 -> Loss: 0.7687100711122651 \t| Accuracy: 121.875\n",
      "# Iteration  1535 -> Loss: 0.7684455850941259 \t| Accuracy: 121.875\n",
      "# Iteration  1536 -> Loss: 0.7681813301114773 \t| Accuracy: 121.875\n",
      "# Iteration  1537 -> Loss: 0.7679173059612667 \t| Accuracy: 121.875\n",
      "# Iteration  1538 -> Loss: 0.7676535124399385 \t| Accuracy: 121.875\n",
      "# Iteration  1539 -> Loss: 0.7673899493434355 \t| Accuracy: 121.875\n",
      "# Iteration  1540 -> Loss: 0.7671266164672026 \t| Accuracy: 121.875\n",
      "# Iteration  1541 -> Loss: 0.7668635136061901 \t| Accuracy: 121.875\n",
      "# Iteration  1542 -> Loss: 0.7666006405548554 \t| Accuracy: 121.875\n",
      "# Iteration  1543 -> Loss: 0.7663379971071665 \t| Accuracy: 121.875\n",
      "# Iteration  1544 -> Loss: 0.7660755830566046 \t| Accuracy: 121.875\n",
      "# Iteration  1545 -> Loss: 0.7658133981961671 \t| Accuracy: 121.875\n",
      "# Iteration  1546 -> Loss: 0.7655514423183701 \t| Accuracy: 121.875\n",
      "# Iteration  1547 -> Loss: 0.7652897152152512 \t| Accuracy: 121.875\n",
      "# Iteration  1548 -> Loss: 0.7650282166783727 \t| Accuracy: 121.875\n",
      "# Iteration  1549 -> Loss: 0.7647669464988245 \t| Accuracy: 121.875\n",
      "# Iteration  1550 -> Loss: 0.7645059044672257 \t| Accuracy: 121.875\n",
      "# Iteration  1551 -> Loss: 0.7642450903737286 \t| Accuracy: 121.875\n",
      "# Iteration  1552 -> Loss: 0.7639845040080215 \t| Accuracy: 121.875\n",
      "# Iteration  1553 -> Loss: 0.7637241451593304 \t| Accuracy: 121.875\n",
      "# Iteration  1554 -> Loss: 0.7634640136164231 \t| Accuracy: 121.875\n",
      "# Iteration  1555 -> Loss: 0.7632041091676114 \t| Accuracy: 121.875\n",
      "# Iteration  1556 -> Loss: 0.7629444316007531 \t| Accuracy: 121.875\n",
      "# Iteration  1557 -> Loss: 0.7626849807032563 \t| Accuracy: 121.875\n",
      "# Iteration  1558 -> Loss: 0.7624257562620811 \t| Accuracy: 121.875\n",
      "# Iteration  1559 -> Loss: 0.7621667580637426 \t| Accuracy: 121.875\n",
      "# Iteration  1560 -> Loss: 0.761907985894314 \t| Accuracy: 121.875\n",
      "# Iteration  1561 -> Loss: 0.7616494395394287 \t| Accuracy: 121.875\n",
      "# Iteration  1562 -> Loss: 0.7613911187842836 \t| Accuracy: 121.875\n",
      "# Iteration  1563 -> Loss: 0.7611330234136418 \t| Accuracy: 121.875\n",
      "# Iteration  1564 -> Loss: 0.7608751532118352 \t| Accuracy: 121.875\n",
      "# Iteration  1565 -> Loss: 0.7606175079627674 \t| Accuracy: 121.875\n",
      "# Iteration  1566 -> Loss: 0.7603600874499161 \t| Accuracy: 121.875\n",
      "# Iteration  1567 -> Loss: 0.7601028914563362 \t| Accuracy: 121.875\n",
      "# Iteration  1568 -> Loss: 0.7598459197646624 \t| Accuracy: 121.875\n",
      "# Iteration  1569 -> Loss: 0.759589172157112 \t| Accuracy: 121.875\n",
      "# Iteration  1570 -> Loss: 0.7593326484154874 \t| Accuracy: 121.875\n",
      "# Iteration  1571 -> Loss: 0.759076348321179 \t| Accuracy: 121.875\n",
      "# Iteration  1572 -> Loss: 0.7588202716551682 \t| Accuracy: 121.875\n",
      "# Iteration  1573 -> Loss: 0.7585644181980293 \t| Accuracy: 121.875\n",
      "# Iteration  1574 -> Loss: 0.7583087877299332 \t| Accuracy: 121.875\n",
      "# Iteration  1575 -> Loss: 0.7580533800306493 \t| Accuracy: 121.875\n",
      "# Iteration  1576 -> Loss: 0.7577981948795485 \t| Accuracy: 121.875\n",
      "# Iteration  1577 -> Loss: 0.7575432320556061 \t| Accuracy: 121.875\n",
      "# Iteration  1578 -> Loss: 0.7572884913374042 \t| Accuracy: 121.875\n",
      "# Iteration  1579 -> Loss: 0.7570339725031345 \t| Accuracy: 121.875\n",
      "# Iteration  1580 -> Loss: 0.756779675330601 \t| Accuracy: 121.875\n",
      "# Iteration  1581 -> Loss: 0.7565255995972222 \t| Accuracy: 121.875\n",
      "# Iteration  1582 -> Loss: 0.7562717450800348 \t| Accuracy: 121.875\n",
      "# Iteration  1583 -> Loss: 0.7560181115556952 \t| Accuracy: 121.875\n",
      "# Iteration  1584 -> Loss: 0.755764698800483 \t| Accuracy: 121.875\n",
      "# Iteration  1585 -> Loss: 0.7555115065903033 \t| Accuracy: 121.875\n",
      "# Iteration  1586 -> Loss: 0.7552585347006894 \t| Accuracy: 121.875\n",
      "# Iteration  1587 -> Loss: 0.7550057829068054 \t| Accuracy: 121.875\n",
      "# Iteration  1588 -> Loss: 0.7547532509834487 \t| Accuracy: 121.875\n",
      "# Iteration  1589 -> Loss: 0.7545009387050532 \t| Accuracy: 121.875\n",
      "# Iteration  1590 -> Loss: 0.7542488458456909 \t| Accuracy: 120.625\n",
      "# Iteration  1591 -> Loss: 0.7539969721790755 \t| Accuracy: 120.625\n",
      "# Iteration  1592 -> Loss: 0.7537453174785647 \t| Accuracy: 120.625\n",
      "# Iteration  1593 -> Loss: 0.7534938815171623 \t| Accuracy: 120.625\n",
      "# Iteration  1594 -> Loss: 0.7532426640675219 \t| Accuracy: 120.625\n",
      "# Iteration  1595 -> Loss: 0.7529916649019484 \t| Accuracy: 120.625\n",
      "# Iteration  1596 -> Loss: 0.7527408837924007 \t| Accuracy: 120.625\n",
      "# Iteration  1597 -> Loss: 0.752490320510495 \t| Accuracy: 120.625\n",
      "# Iteration  1598 -> Loss: 0.752239974827507 \t| Accuracy: 120.625\n",
      "# Iteration  1599 -> Loss: 0.7519898465143742 \t| Accuracy: 120.625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Iteration  1600 -> Loss: 0.7517399353416986 \t| Accuracy: 120.625\n",
      "# Iteration  1601 -> Loss: 0.7514902410797495 \t| Accuracy: 120.625\n",
      "# Iteration  1602 -> Loss: 0.7512407634984659 \t| Accuracy: 120.625\n",
      "# Iteration  1603 -> Loss: 0.7509915023674589 \t| Accuracy: 120.625\n",
      "# Iteration  1604 -> Loss: 0.7507424574560142 \t| Accuracy: 120.625\n",
      "# Iteration  1605 -> Loss: 0.7504936285330951 \t| Accuracy: 120.625\n",
      "# Iteration  1606 -> Loss: 0.7502450153673444 \t| Accuracy: 120.625\n",
      "# Iteration  1607 -> Loss: 0.7499966177270874 \t| Accuracy: 120.000\n",
      "# Iteration  1608 -> Loss: 0.7497484353803339 \t| Accuracy: 120.000\n",
      "# Iteration  1609 -> Loss: 0.7495004680947815 \t| Accuracy: 120.000\n",
      "# Iteration  1610 -> Loss: 0.749252715637817 \t| Accuracy: 120.000\n",
      "# Iteration  1611 -> Loss: 0.7490051777765199 \t| Accuracy: 118.750\n",
      "# Iteration  1612 -> Loss: 0.7487578542776644 \t| Accuracy: 118.750\n",
      "# Iteration  1613 -> Loss: 0.7485107449077216 \t| Accuracy: 118.750\n",
      "# Iteration  1614 -> Loss: 0.748263849432863 \t| Accuracy: 118.750\n",
      "# Iteration  1615 -> Loss: 0.7480171676189613 \t| Accuracy: 118.750\n",
      "# Iteration  1616 -> Loss: 0.7477706992315946 \t| Accuracy: 118.750\n",
      "# Iteration  1617 -> Loss: 0.7475244440360477 \t| Accuracy: 118.750\n",
      "# Iteration  1618 -> Loss: 0.747278401797315 \t| Accuracy: 118.750\n",
      "# Iteration  1619 -> Loss: 0.7470325722801026 \t| Accuracy: 118.750\n",
      "# Iteration  1620 -> Loss: 0.746786955248831 \t| Accuracy: 118.750\n",
      "# Iteration  1621 -> Loss: 0.7465415504676377 \t| Accuracy: 118.750\n",
      "# Iteration  1622 -> Loss: 0.7462963577003789 \t| Accuracy: 118.750\n",
      "# Iteration  1623 -> Loss: 0.7460513767106329 \t| Accuracy: 118.750\n",
      "# Iteration  1624 -> Loss: 0.7458066072617017 \t| Accuracy: 118.750\n",
      "# Iteration  1625 -> Loss: 0.7455620491166132 \t| Accuracy: 118.750\n",
      "# Iteration  1626 -> Loss: 0.745317702038125 \t| Accuracy: 118.750\n",
      "# Iteration  1627 -> Loss: 0.7450735657887245 \t| Accuracy: 118.750\n",
      "# Iteration  1628 -> Loss: 0.7448296401306339 \t| Accuracy: 118.750\n",
      "# Iteration  1629 -> Loss: 0.7445859248258101 \t| Accuracy: 118.750\n",
      "# Iteration  1630 -> Loss: 0.7443424196359489 \t| Accuracy: 118.750\n",
      "# Iteration  1631 -> Loss: 0.7440991243224861 \t| Accuracy: 117.500\n",
      "# Iteration  1632 -> Loss: 0.7438560386466005 \t| Accuracy: 117.500\n",
      "# Iteration  1633 -> Loss: 0.7436131623692163 \t| Accuracy: 117.500\n",
      "# Iteration  1634 -> Loss: 0.7433704952510048 \t| Accuracy: 117.500\n",
      "# Iteration  1635 -> Loss: 0.7431280370523872 \t| Accuracy: 117.500\n",
      "# Iteration  1636 -> Loss: 0.742885787533537 \t| Accuracy: 117.500\n",
      "# Iteration  1637 -> Loss: 0.7426437464543819 \t| Accuracy: 117.500\n",
      "# Iteration  1638 -> Loss: 0.7424019135746059 \t| Accuracy: 117.500\n",
      "# Iteration  1639 -> Loss: 0.742160288653653 \t| Accuracy: 117.500\n",
      "# Iteration  1640 -> Loss: 0.7419188714507275 \t| Accuracy: 117.500\n",
      "# Iteration  1641 -> Loss: 0.7416776617247975 \t| Accuracy: 117.500\n",
      "# Iteration  1642 -> Loss: 0.7414366592345971 \t| Accuracy: 117.500\n",
      "# Iteration  1643 -> Loss: 0.7411958637386278 \t| Accuracy: 117.500\n",
      "# Iteration  1644 -> Loss: 0.7409552749951618 \t| Accuracy: 117.500\n",
      "# Iteration  1645 -> Loss: 0.7407148927622439 \t| Accuracy: 117.500\n",
      "# Iteration  1646 -> Loss: 0.7404747167976936 \t| Accuracy: 117.500\n",
      "# Iteration  1647 -> Loss: 0.7402347468591068 \t| Accuracy: 117.500\n",
      "# Iteration  1648 -> Loss: 0.7399949827038594 \t| Accuracy: 117.500\n",
      "# Iteration  1649 -> Loss: 0.739755424089108 \t| Accuracy: 117.500\n",
      "# Iteration  1650 -> Loss: 0.7395160707717929 \t| Accuracy: 117.500\n",
      "# Iteration  1651 -> Loss: 0.7392769225086404 \t| Accuracy: 117.500\n",
      "# Iteration  1652 -> Loss: 0.7390379790561648 \t| Accuracy: 117.500\n",
      "# Iteration  1653 -> Loss: 0.7387992401706697 \t| Accuracy: 117.500\n",
      "# Iteration  1654 -> Loss: 0.7385607056082523 \t| Accuracy: 117.500\n",
      "# Iteration  1655 -> Loss: 0.7383223751248029 \t| Accuracy: 117.500\n",
      "# Iteration  1656 -> Loss: 0.7380842484760097 \t| Accuracy: 117.500\n",
      "# Iteration  1657 -> Loss: 0.7378463254173584 \t| Accuracy: 117.500\n",
      "# Iteration  1658 -> Loss: 0.7376086057041364 \t| Accuracy: 117.500\n",
      "# Iteration  1659 -> Loss: 0.7373710890914339 \t| Accuracy: 117.500\n",
      "# Iteration  1660 -> Loss: 0.7371337753341461 \t| Accuracy: 117.500\n",
      "# Iteration  1661 -> Loss: 0.7368966641869757 \t| Accuracy: 117.500\n",
      "# Iteration  1662 -> Loss: 0.7366597554044344 \t| Accuracy: 117.500\n",
      "# Iteration  1663 -> Loss: 0.7364230487408459 \t| Accuracy: 117.500\n",
      "# Iteration  1664 -> Loss: 0.7361865439503467 \t| Accuracy: 117.500\n",
      "# Iteration  1665 -> Loss: 0.7359502407868895 \t| Accuracy: 117.500\n",
      "# Iteration  1666 -> Loss: 0.7357141390042448 \t| Accuracy: 117.500\n",
      "# Iteration  1667 -> Loss: 0.7354782383560021 \t| Accuracy: 117.500\n",
      "# Iteration  1668 -> Loss: 0.7352425385955738 \t| Accuracy: 117.500\n",
      "# Iteration  1669 -> Loss: 0.7350070394761955 \t| Accuracy: 117.500\n",
      "# Iteration  1670 -> Loss: 0.7347717407509288 \t| Accuracy: 117.500\n",
      "# Iteration  1671 -> Loss: 0.7345366421726633 \t| Accuracy: 117.500\n",
      "# Iteration  1672 -> Loss: 0.734301743494119 \t| Accuracy: 117.500\n",
      "# Iteration  1673 -> Loss: 0.7340670444678475 \t| Accuracy: 117.500\n",
      "# Iteration  1674 -> Loss: 0.7338325448462346 \t| Accuracy: 117.500\n",
      "# Iteration  1675 -> Loss: 0.733598244381502 \t| Accuracy: 117.500\n",
      "# Iteration  1676 -> Loss: 0.7333641428257099 \t| Accuracy: 117.500\n",
      "# Iteration  1677 -> Loss: 0.7331302399307579 \t| Accuracy: 117.500\n",
      "# Iteration  1678 -> Loss: 0.7328965354483885 \t| Accuracy: 117.500\n",
      "# Iteration  1679 -> Loss: 0.7326630291301872 \t| Accuracy: 117.500\n",
      "# Iteration  1680 -> Loss: 0.7324297207275865 \t| Accuracy: 117.500\n",
      "# Iteration  1681 -> Loss: 0.7321966099918662 \t| Accuracy: 117.500\n",
      "# Iteration  1682 -> Loss: 0.731963696674156 \t| Accuracy: 117.500\n",
      "# Iteration  1683 -> Loss: 0.731730980525438 \t| Accuracy: 117.500\n",
      "# Iteration  1684 -> Loss: 0.7314984612965475 \t| Accuracy: 117.500\n",
      "# Iteration  1685 -> Loss: 0.7312661387381761 \t| Accuracy: 117.500\n",
      "# Iteration  1686 -> Loss: 0.7310340126008725 \t| Accuracy: 117.500\n",
      "# Iteration  1687 -> Loss: 0.7308020826350455 \t| Accuracy: 116.875\n",
      "# Iteration  1688 -> Loss: 0.730570348590965 \t| Accuracy: 116.875\n",
      "# Iteration  1689 -> Loss: 0.7303388102187647 \t| Accuracy: 116.875\n",
      "# Iteration  1690 -> Loss: 0.7301074672684431 \t| Accuracy: 116.875\n",
      "# Iteration  1691 -> Loss: 0.7298763194898664 \t| Accuracy: 116.875\n",
      "# Iteration  1692 -> Loss: 0.7296453666327695 \t| Accuracy: 116.875\n",
      "# Iteration  1693 -> Loss: 0.7294146084467582 \t| Accuracy: 116.875\n",
      "# Iteration  1694 -> Loss: 0.7291840446813119 \t| Accuracy: 116.875\n",
      "# Iteration  1695 -> Loss: 0.7289536750857837 \t| Accuracy: 116.875\n",
      "# Iteration  1696 -> Loss: 0.7287234994094036 \t| Accuracy: 116.875\n",
      "# Iteration  1697 -> Loss: 0.7284935174012801 \t| Accuracy: 116.875\n",
      "# Iteration  1698 -> Loss: 0.728263728810402 \t| Accuracy: 116.875\n",
      "# Iteration  1699 -> Loss: 0.7280341333856399 \t| Accuracy: 116.875\n",
      "# Iteration  1700 -> Loss: 0.7278047308757484 \t| Accuracy: 116.875\n",
      "# Iteration  1701 -> Loss: 0.7275755210293676 \t| Accuracy: 116.875\n",
      "# Iteration  1702 -> Loss: 0.7273465035950257 \t| Accuracy: 116.875\n",
      "# Iteration  1703 -> Loss: 0.7271176783211393 \t| Accuracy: 116.875\n",
      "# Iteration  1704 -> Loss: 0.7268890449560172 \t| Accuracy: 116.875\n",
      "# Iteration  1705 -> Loss: 0.7266606032478601 \t| Accuracy: 116.875\n",
      "# Iteration  1706 -> Loss: 0.7264323529447639 \t| Accuracy: 116.875\n",
      "# Iteration  1707 -> Loss: 0.7262042937947212 \t| Accuracy: 116.875\n",
      "# Iteration  1708 -> Loss: 0.7259764255456224 \t| Accuracy: 116.875\n",
      "# Iteration  1709 -> Loss: 0.7257487479452577 \t| Accuracy: 116.875\n",
      "# Iteration  1710 -> Loss: 0.7255212607413196 \t| Accuracy: 116.875\n",
      "# Iteration  1711 -> Loss: 0.7252939636814041 \t| Accuracy: 116.875\n",
      "# Iteration  1712 -> Loss: 0.725066856513012 \t| Accuracy: 116.875\n",
      "# Iteration  1713 -> Loss: 0.7248399389835513 \t| Accuracy: 116.875\n",
      "# Iteration  1714 -> Loss: 0.7246132108403387 \t| Accuracy: 116.875\n",
      "# Iteration  1715 -> Loss: 0.7243866718306016 \t| Accuracy: 116.875\n",
      "# Iteration  1716 -> Loss: 0.7241603217014793 \t| Accuracy: 116.875\n",
      "# Iteration  1717 -> Loss: 0.7239341602000245 \t| Accuracy: 116.875\n",
      "# Iteration  1718 -> Loss: 0.7237081870732064 \t| Accuracy: 116.875\n",
      "# Iteration  1719 -> Loss: 0.7234824020679108 \t| Accuracy: 116.875\n",
      "# Iteration  1720 -> Loss: 0.7232568049309426 \t| Accuracy: 116.875\n",
      "# Iteration  1721 -> Loss: 0.7230313954090271 \t| Accuracy: 116.875\n",
      "# Iteration  1722 -> Loss: 0.7228061732488127 \t| Accuracy: 116.875\n",
      "# Iteration  1723 -> Loss: 0.7225811381968706 \t| Accuracy: 116.875\n",
      "# Iteration  1724 -> Loss: 0.7223562899996987 \t| Accuracy: 116.250\n",
      "# Iteration  1725 -> Loss: 0.7221316284037215 \t| Accuracy: 116.250\n",
      "# Iteration  1726 -> Loss: 0.7219071531552925 \t| Accuracy: 116.250\n",
      "# Iteration  1727 -> Loss: 0.7216828640006963 \t| Accuracy: 116.250\n",
      "# Iteration  1728 -> Loss: 0.721458760686149 \t| Accuracy: 116.250\n",
      "# Iteration  1729 -> Loss: 0.7212348429578012 \t| Accuracy: 116.250\n",
      "# Iteration  1730 -> Loss: 0.7210111105617385 \t| Accuracy: 116.250\n",
      "# Iteration  1731 -> Loss: 0.7207875632439839 \t| Accuracy: 116.250\n",
      "# Iteration  1732 -> Loss: 0.7205642007504985 \t| Accuracy: 116.250\n",
      "# Iteration  1733 -> Loss: 0.7203410228271848 \t| Accuracy: 116.250\n",
      "# Iteration  1734 -> Loss: 0.7201180292198862 \t| Accuracy: 116.250\n",
      "# Iteration  1735 -> Loss: 0.7198952196743901 \t| Accuracy: 116.250\n",
      "# Iteration  1736 -> Loss: 0.7196725939364288 \t| Accuracy: 116.250\n",
      "# Iteration  1737 -> Loss: 0.7194501517516815 \t| Accuracy: 116.250\n",
      "# Iteration  1738 -> Loss: 0.7192278928657749 \t| Accuracy: 116.250\n",
      "# Iteration  1739 -> Loss: 0.7190058170242871 \t| Accuracy: 116.250\n",
      "# Iteration  1740 -> Loss: 0.7187839239727456 \t| Accuracy: 116.250\n",
      "# Iteration  1741 -> Loss: 0.7185622134566323 \t| Accuracy: 116.250\n",
      "# Iteration  1742 -> Loss: 0.7183406852213828 \t| Accuracy: 116.250\n",
      "# Iteration  1743 -> Loss: 0.7181193390123892 \t| Accuracy: 116.250\n",
      "# Iteration  1744 -> Loss: 0.7178981745750009 \t| Accuracy: 116.250\n",
      "# Iteration  1745 -> Loss: 0.7176771916545266 \t| Accuracy: 116.250\n",
      "# Iteration  1746 -> Loss: 0.7174563899962352 \t| Accuracy: 116.250\n",
      "# Iteration  1747 -> Loss: 0.7172357693453584 \t| Accuracy: 116.250\n",
      "# Iteration  1748 -> Loss: 0.7170153294470909 \t| Accuracy: 116.250\n",
      "# Iteration  1749 -> Loss: 0.7167950700465934 \t| Accuracy: 116.250\n",
      "# Iteration  1750 -> Loss: 0.7165749908889921 \t| Accuracy: 116.250\n",
      "# Iteration  1751 -> Loss: 0.7163550917193827 \t| Accuracy: 116.250\n",
      "# Iteration  1752 -> Loss: 0.7161353722828296 \t| Accuracy: 116.250\n",
      "# Iteration  1753 -> Loss: 0.715915832324369 \t| Accuracy: 116.250\n",
      "# Iteration  1754 -> Loss: 0.7156964715890091 \t| Accuracy: 116.250\n",
      "# Iteration  1755 -> Loss: 0.7154772898217331 \t| Accuracy: 116.250\n",
      "# Iteration  1756 -> Loss: 0.7152582867674989 \t| Accuracy: 116.250\n",
      "# Iteration  1757 -> Loss: 0.715039462171242 \t| Accuracy: 116.250\n",
      "# Iteration  1758 -> Loss: 0.7148208157778765 \t| Accuracy: 116.250\n",
      "# Iteration  1759 -> Loss: 0.714602347332296 \t| Accuracy: 116.250\n",
      "# Iteration  1760 -> Loss: 0.7143840565793761 \t| Accuracy: 116.250\n",
      "# Iteration  1761 -> Loss: 0.7141659432639749 \t| Accuracy: 116.250\n",
      "# Iteration  1762 -> Loss: 0.7139480071309348 \t| Accuracy: 115.000\n",
      "# Iteration  1763 -> Loss: 0.7137302479250844 \t| Accuracy: 115.000\n",
      "# Iteration  1764 -> Loss: 0.7135126653912391 \t| Accuracy: 115.000\n",
      "# Iteration  1765 -> Loss: 0.7132952592742031 \t| Accuracy: 115.000\n",
      "# Iteration  1766 -> Loss: 0.7130780293187707 \t| Accuracy: 115.000\n",
      "# Iteration  1767 -> Loss: 0.7128609752697272 \t| Accuracy: 115.000\n",
      "# Iteration  1768 -> Loss: 0.7126440968718513 \t| Accuracy: 114.375\n",
      "# Iteration  1769 -> Loss: 0.7124273938699157 \t| Accuracy: 114.375\n",
      "# Iteration  1770 -> Loss: 0.7122108660086889 \t| Accuracy: 114.375\n",
      "# Iteration  1771 -> Loss: 0.7119945130329363 \t| Accuracy: 114.375\n",
      "# Iteration  1772 -> Loss: 0.7117783346874219 \t| Accuracy: 114.375\n",
      "# Iteration  1773 -> Loss: 0.71156233071691 \t| Accuracy: 114.375\n",
      "# Iteration  1774 -> Loss: 0.7113465008661649 \t| Accuracy: 114.375\n",
      "# Iteration  1775 -> Loss: 0.7111308448799549 \t| Accuracy: 113.750\n",
      "# Iteration  1776 -> Loss: 0.7109153625030514 \t| Accuracy: 113.750\n",
      "# Iteration  1777 -> Loss: 0.7107000534802315 \t| Accuracy: 113.750\n",
      "# Iteration  1778 -> Loss: 0.7104849175562788 \t| Accuracy: 113.750\n",
      "# Iteration  1779 -> Loss: 0.7102699544759853 \t| Accuracy: 113.750\n",
      "# Iteration  1780 -> Loss: 0.7100551639841524 \t| Accuracy: 113.750\n",
      "# Iteration  1781 -> Loss: 0.7098405458255915 \t| Accuracy: 113.750\n",
      "# Iteration  1782 -> Loss: 0.7096260997451277 \t| Accuracy: 113.750\n",
      "# Iteration  1783 -> Loss: 0.7094118254875977 \t| Accuracy: 113.750\n",
      "# Iteration  1784 -> Loss: 0.7091977227978549 \t| Accuracy: 113.750\n",
      "# Iteration  1785 -> Loss: 0.7089837914207671 \t| Accuracy: 113.750\n",
      "# Iteration  1786 -> Loss: 0.7087700311012208 \t| Accuracy: 113.750\n",
      "# Iteration  1787 -> Loss: 0.7085564415841207 \t| Accuracy: 113.750\n",
      "# Iteration  1788 -> Loss: 0.7083430226143921 \t| Accuracy: 113.750\n",
      "# Iteration  1789 -> Loss: 0.7081297739369813 \t| Accuracy: 113.750\n",
      "# Iteration  1790 -> Loss: 0.7079166952968574 \t| Accuracy: 113.750\n",
      "# Iteration  1791 -> Loss: 0.7077037864390139 \t| Accuracy: 113.750\n",
      "# Iteration  1792 -> Loss: 0.7074910471084693 \t| Accuracy: 113.750\n",
      "# Iteration  1793 -> Loss: 0.707278477050269 \t| Accuracy: 113.750\n",
      "# Iteration  1794 -> Loss: 0.7070660760094865 \t| Accuracy: 113.750\n",
      "# Iteration  1795 -> Loss: 0.7068538437312242 \t| Accuracy: 113.750\n",
      "# Iteration  1796 -> Loss: 0.7066417799606152 \t| Accuracy: 113.750\n",
      "# Iteration  1797 -> Loss: 0.7064298844428246 \t| Accuracy: 113.750\n",
      "# Iteration  1798 -> Loss: 0.7062181569230506 \t| Accuracy: 113.750\n",
      "# Iteration  1799 -> Loss: 0.7060065971465257 \t| Accuracy: 113.750\n",
      "# Iteration  1800 -> Loss: 0.7057952048585181 \t| Accuracy: 113.750\n",
      "# Iteration  1801 -> Loss: 0.7055839798043332 \t| Accuracy: 113.750\n",
      "# Iteration  1802 -> Loss: 0.7053729217293143 \t| Accuracy: 113.750\n",
      "# Iteration  1803 -> Loss: 0.7051620303788444 \t| Accuracy: 113.750\n",
      "# Iteration  1804 -> Loss: 0.7049513054983473 \t| Accuracy: 113.750\n",
      "# Iteration  1805 -> Loss: 0.7047407468332887 \t| Accuracy: 113.750\n",
      "# Iteration  1806 -> Loss: 0.7045303541291776 \t| Accuracy: 113.750\n",
      "# Iteration  1807 -> Loss: 0.7043201271315678 \t| Accuracy: 113.750\n",
      "# Iteration  1808 -> Loss: 0.7041100655860584 \t| Accuracy: 113.750\n",
      "# Iteration  1809 -> Loss: 0.703900169238296 \t| Accuracy: 113.750\n",
      "# Iteration  1810 -> Loss: 0.7036904378339756 \t| Accuracy: 113.750\n",
      "# Iteration  1811 -> Loss: 0.7034808711188408 \t| Accuracy: 113.750\n",
      "# Iteration  1812 -> Loss: 0.7032714688386873 \t| Accuracy: 113.750\n",
      "# Iteration  1813 -> Loss: 0.7030622307393615 \t| Accuracy: 113.750\n",
      "# Iteration  1814 -> Loss: 0.702853156566764 \t| Accuracy: 113.750\n",
      "# Iteration  1815 -> Loss: 0.7026442460668496 \t| Accuracy: 113.750\n",
      "# Iteration  1816 -> Loss: 0.7024354989856287 \t| Accuracy: 113.750\n",
      "# Iteration  1817 -> Loss: 0.7022269150691685 \t| Accuracy: 113.750\n",
      "# Iteration  1818 -> Loss: 0.7020184940635944 \t| Accuracy: 113.750\n",
      "# Iteration  1819 -> Loss: 0.7018102357150916 \t| Accuracy: 113.750\n",
      "# Iteration  1820 -> Loss: 0.7016021397699054 \t| Accuracy: 113.750\n",
      "# Iteration  1821 -> Loss: 0.701394205974343 \t| Accuracy: 113.750\n",
      "# Iteration  1822 -> Loss: 0.7011864340747748 \t| Accuracy: 113.750\n",
      "# Iteration  1823 -> Loss: 0.7009788238176353 \t| Accuracy: 113.750\n",
      "# Iteration  1824 -> Loss: 0.7007713749494243 \t| Accuracy: 113.750\n",
      "# Iteration  1825 -> Loss: 0.7005640872167085 \t| Accuracy: 113.750\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Iteration  1826 -> Loss: 0.7003569603661222 \t| Accuracy: 113.750\n",
      "# Iteration  1827 -> Loss: 0.700149994144369 \t| Accuracy: 113.750\n",
      "# Iteration  1828 -> Loss: 0.6999431882982226 \t| Accuracy: 113.750\n",
      "# Iteration  1829 -> Loss: 0.6997365425745281 \t| Accuracy: 113.750\n",
      "# Iteration  1830 -> Loss: 0.6995300567202034 \t| Accuracy: 113.750\n",
      "# Iteration  1831 -> Loss: 0.6993237304822401 \t| Accuracy: 113.750\n",
      "# Iteration  1832 -> Loss: 0.6991175636077049 \t| Accuracy: 113.750\n",
      "# Iteration  1833 -> Loss: 0.6989115558437405 \t| Accuracy: 113.750\n",
      "# Iteration  1834 -> Loss: 0.6987057069375675 \t| Accuracy: 113.750\n",
      "# Iteration  1835 -> Loss: 0.698500016636485 \t| Accuracy: 113.750\n",
      "# Iteration  1836 -> Loss: 0.6982944846878711 \t| Accuracy: 113.542\n",
      "# Iteration  1837 -> Loss: 0.6980891108391859 \t| Accuracy: 113.542\n",
      "# Iteration  1838 -> Loss: 0.6978838948379709 \t| Accuracy: 113.542\n",
      "# Iteration  1839 -> Loss: 0.6976788364318514 \t| Accuracy: 113.542\n",
      "# Iteration  1840 -> Loss: 0.6974739353685371 \t| Accuracy: 113.542\n",
      "# Iteration  1841 -> Loss: 0.6972691913958231 \t| Accuracy: 113.542\n",
      "# Iteration  1842 -> Loss: 0.6970646042615919 \t| Accuracy: 113.542\n",
      "# Iteration  1843 -> Loss: 0.6968601737138137 \t| Accuracy: 113.542\n",
      "# Iteration  1844 -> Loss: 0.6966558995005475 \t| Accuracy: 113.542\n",
      "# Iteration  1845 -> Loss: 0.6964517813699438 \t| Accuracy: 112.917\n",
      "# Iteration  1846 -> Loss: 0.6962478190702435 \t| Accuracy: 112.917\n",
      "# Iteration  1847 -> Loss: 0.6960440123497807 \t| Accuracy: 112.917\n",
      "# Iteration  1848 -> Loss: 0.695840360956984 \t| Accuracy: 112.917\n",
      "# Iteration  1849 -> Loss: 0.6956368646403758 \t| Accuracy: 112.708\n",
      "# Iteration  1850 -> Loss: 0.6954335231485758 \t| Accuracy: 112.708\n",
      "# Iteration  1851 -> Loss: 0.6952303362303007 \t| Accuracy: 112.708\n",
      "# Iteration  1852 -> Loss: 0.6950273036343657 \t| Accuracy: 112.708\n",
      "# Iteration  1853 -> Loss: 0.694824425109686 \t| Accuracy: 112.708\n",
      "# Iteration  1854 -> Loss: 0.6946217004052774 \t| Accuracy: 112.708\n",
      "# Iteration  1855 -> Loss: 0.6944191292702582 \t| Accuracy: 112.708\n",
      "# Iteration  1856 -> Loss: 0.6942167114538493 \t| Accuracy: 112.708\n",
      "# Iteration  1857 -> Loss: 0.6940144467053765 \t| Accuracy: 112.708\n",
      "# Iteration  1858 -> Loss: 0.6938123347742712 \t| Accuracy: 112.708\n",
      "# Iteration  1859 -> Loss: 0.693610375410071 \t| Accuracy: 112.708\n",
      "# Iteration  1860 -> Loss: 0.693408568362422 \t| Accuracy: 112.708\n",
      "# Iteration  1861 -> Loss: 0.693206913381079 \t| Accuracy: 112.708\n",
      "# Iteration  1862 -> Loss: 0.6930054102159069 \t| Accuracy: 112.708\n",
      "# Iteration  1863 -> Loss: 0.6928040586168822 \t| Accuracy: 110.833\n",
      "# Iteration  1864 -> Loss: 0.6926028583340941 \t| Accuracy: 110.833\n",
      "# Iteration  1865 -> Loss: 0.6924018091177452 \t| Accuracy: 110.833\n",
      "# Iteration  1866 -> Loss: 0.6922009107181529 \t| Accuracy: 110.833\n",
      "# Iteration  1867 -> Loss: 0.6920001628857511 \t| Accuracy: 110.833\n",
      "# Iteration  1868 -> Loss: 0.69179956537109 \t| Accuracy: 110.833\n",
      "# Iteration  1869 -> Loss: 0.6915991179248392 \t| Accuracy: 110.833\n",
      "# Iteration  1870 -> Loss: 0.6913988202977871 \t| Accuracy: 110.833\n",
      "# Iteration  1871 -> Loss: 0.691198672240843 \t| Accuracy: 110.833\n",
      "# Iteration  1872 -> Loss: 0.6909986735050382 \t| Accuracy: 110.833\n",
      "# Iteration  1873 -> Loss: 0.6907988238415262 \t| Accuracy: 110.833\n",
      "# Iteration  1874 -> Loss: 0.690599123001586 \t| Accuracy: 110.833\n",
      "# Iteration  1875 -> Loss: 0.6903995707366208 \t| Accuracy: 110.833\n",
      "# Iteration  1876 -> Loss: 0.6902001667981612 \t| Accuracy: 110.833\n",
      "# Iteration  1877 -> Loss: 0.6900009109378641 \t| Accuracy: 110.833\n",
      "# Iteration  1878 -> Loss: 0.689801802907517 \t| Accuracy: 110.833\n",
      "# Iteration  1879 -> Loss: 0.6896028424590357 \t| Accuracy: 110.833\n",
      "# Iteration  1880 -> Loss: 0.6894040293444685 \t| Accuracy: 110.833\n",
      "# Iteration  1881 -> Loss: 0.6892053633159955 \t| Accuracy: 110.833\n",
      "# Iteration  1882 -> Loss: 0.6890068441259299 \t| Accuracy: 110.833\n",
      "# Iteration  1883 -> Loss: 0.6888084715267203 \t| Accuracy: 110.833\n",
      "# Iteration  1884 -> Loss: 0.6886102452709508 \t| Accuracy: 110.833\n",
      "# Iteration  1885 -> Loss: 0.6884121651113425 \t| Accuracy: 110.833\n",
      "# Iteration  1886 -> Loss: 0.688214230800755 \t| Accuracy: 110.833\n",
      "# Iteration  1887 -> Loss: 0.6880164420921865 \t| Accuracy: 110.833\n",
      "# Iteration  1888 -> Loss: 0.6878187987387768 \t| Accuracy: 110.833\n",
      "# Iteration  1889 -> Loss: 0.6876213004938067 \t| Accuracy: 110.833\n",
      "# Iteration  1890 -> Loss: 0.6874239471107003 \t| Accuracy: 110.833\n",
      "# Iteration  1891 -> Loss: 0.6872267383430255 \t| Accuracy: 110.833\n",
      "# Iteration  1892 -> Loss: 0.6870296739444958 \t| Accuracy: 110.833\n",
      "# Iteration  1893 -> Loss: 0.6868327536689707 \t| Accuracy: 110.833\n",
      "# Iteration  1894 -> Loss: 0.6866359772704581 \t| Accuracy: 110.833\n",
      "# Iteration  1895 -> Loss: 0.6864393445031141 \t| Accuracy: 110.833\n",
      "# Iteration  1896 -> Loss: 0.6862428551212448 \t| Accuracy: 110.833\n",
      "# Iteration  1897 -> Loss: 0.6860465088793084 \t| Accuracy: 110.833\n",
      "# Iteration  1898 -> Loss: 0.6858503055319143 \t| Accuracy: 110.833\n",
      "# Iteration  1899 -> Loss: 0.6856542448338264 \t| Accuracy: 110.833\n",
      "# Iteration  1900 -> Loss: 0.6854583265399632 \t| Accuracy: 110.833\n",
      "# Iteration  1901 -> Loss: 0.685262550405399 \t| Accuracy: 110.833\n",
      "# Iteration  1902 -> Loss: 0.6850669161853656 \t| Accuracy: 110.833\n",
      "# Iteration  1903 -> Loss: 0.6848714236352532 \t| Accuracy: 110.833\n",
      "# Iteration  1904 -> Loss: 0.6846760725106118 \t| Accuracy: 110.833\n",
      "# Iteration  1905 -> Loss: 0.6844808625671513 \t| Accuracy: 110.833\n",
      "# Iteration  1906 -> Loss: 0.6842857935607453 \t| Accuracy: 110.833\n",
      "# Iteration  1907 -> Loss: 0.6840908652474291 \t| Accuracy: 110.833\n",
      "# Iteration  1908 -> Loss: 0.6838960773834034 \t| Accuracy: 110.833\n",
      "# Iteration  1909 -> Loss: 0.6837014297250347 \t| Accuracy: 110.833\n",
      "# Iteration  1910 -> Loss: 0.6835069220288558 \t| Accuracy: 110.833\n",
      "# Iteration  1911 -> Loss: 0.6833125540515681 \t| Accuracy: 110.833\n",
      "# Iteration  1912 -> Loss: 0.6831183255500423 \t| Accuracy: 110.833\n",
      "# Iteration  1913 -> Loss: 0.6829242362813199 \t| Accuracy: 110.833\n",
      "# Iteration  1914 -> Loss: 0.6827302860026142 \t| Accuracy: 110.833\n",
      "# Iteration  1915 -> Loss: 0.682536474471311 \t| Accuracy: 110.833\n",
      "# Iteration  1916 -> Loss: 0.6823428014449715 \t| Accuracy: 110.833\n",
      "# Iteration  1917 -> Loss: 0.682149266681332 \t| Accuracy: 110.833\n",
      "# Iteration  1918 -> Loss: 0.6819558699383054 \t| Accuracy: 110.833\n",
      "# Iteration  1919 -> Loss: 0.6817626109739827 \t| Accuracy: 110.833\n",
      "# Iteration  1920 -> Loss: 0.6815694895466349 \t| Accuracy: 110.833\n",
      "# Iteration  1921 -> Loss: 0.6813765054147128 \t| Accuracy: 110.833\n",
      "# Iteration  1922 -> Loss: 0.6811836583368498 \t| Accuracy: 110.833\n",
      "# Iteration  1923 -> Loss: 0.6809909480718617 \t| Accuracy: 110.833\n",
      "# Iteration  1924 -> Loss: 0.6807983743787491 \t| Accuracy: 110.833\n",
      "# Iteration  1925 -> Loss: 0.6806059370166985 \t| Accuracy: 110.833\n",
      "# Iteration  1926 -> Loss: 0.6804136357450827 \t| Accuracy: 110.833\n",
      "# Iteration  1927 -> Loss: 0.6802214703234634 \t| Accuracy: 110.833\n",
      "# Iteration  1928 -> Loss: 0.6800294405115913 \t| Accuracy: 110.833\n",
      "# Iteration  1929 -> Loss: 0.6798375460694084 \t| Accuracy: 110.833\n",
      "# Iteration  1930 -> Loss: 0.6796457867570481 \t| Accuracy: 110.833\n",
      "# Iteration  1931 -> Loss: 0.6794541623348378 \t| Accuracy: 110.833\n",
      "# Iteration  1932 -> Loss: 0.6792626725632994 \t| Accuracy: 110.833\n",
      "# Iteration  1933 -> Loss: 0.6790713172031507 \t| Accuracy: 110.833\n",
      "# Iteration  1934 -> Loss: 0.6788800960153071 \t| Accuracy: 110.833\n",
      "# Iteration  1935 -> Loss: 0.6786890087608819 \t| Accuracy: 110.833\n",
      "# Iteration  1936 -> Loss: 0.6784980552011892 \t| Accuracy: 110.833\n",
      "# Iteration  1937 -> Loss: 0.6783072350977438 \t| Accuracy: 110.833\n",
      "# Iteration  1938 -> Loss: 0.6781165482122632 \t| Accuracy: 110.833\n",
      "# Iteration  1939 -> Loss: 0.6779259943066689 \t| Accuracy: 110.833\n",
      "# Iteration  1940 -> Loss: 0.6777355731430875 \t| Accuracy: 110.833\n",
      "# Iteration  1941 -> Loss: 0.6775452844838525 \t| Accuracy: 110.833\n",
      "# Iteration  1942 -> Loss: 0.6773551280915048 \t| Accuracy: 110.833\n",
      "# Iteration  1943 -> Loss: 0.6771651037287948 \t| Accuracy: 110.833\n",
      "# Iteration  1944 -> Loss: 0.676975211158684 \t| Accuracy: 110.833\n",
      "# Iteration  1945 -> Loss: 0.6767854501443452 \t| Accuracy: 110.833\n",
      "# Iteration  1946 -> Loss: 0.6765958204491647 \t| Accuracy: 110.833\n",
      "# Iteration  1947 -> Loss: 0.6764063218367438 \t| Accuracy: 110.833\n",
      "# Iteration  1948 -> Loss: 0.6762169540709 \t| Accuracy: 110.833\n",
      "# Iteration  1949 -> Loss: 0.6760277169156675 \t| Accuracy: 110.833\n",
      "# Iteration  1950 -> Loss: 0.6758386101353004 \t| Accuracy: 110.833\n",
      "# Iteration  1951 -> Loss: 0.6756496334942722 \t| Accuracy: 110.833\n",
      "# Iteration  1952 -> Loss: 0.6754607867572785 \t| Accuracy: 110.833\n",
      "# Iteration  1953 -> Loss: 0.6752720696892379 \t| Accuracy: 110.833\n",
      "# Iteration  1954 -> Loss: 0.6750834820552937 \t| Accuracy: 110.833\n",
      "# Iteration  1955 -> Loss: 0.6748950236208139 \t| Accuracy: 110.833\n",
      "# Iteration  1956 -> Loss: 0.6747066941513954 \t| Accuracy: 110.833\n",
      "# Iteration  1957 -> Loss: 0.674518493412863 \t| Accuracy: 110.833\n",
      "# Iteration  1958 -> Loss: 0.6743304211712714 \t| Accuracy: 110.833\n",
      "# Iteration  1959 -> Loss: 0.6741424771929075 \t| Accuracy: 110.833\n",
      "# Iteration  1960 -> Loss: 0.6739546612442908 \t| Accuracy: 110.833\n",
      "# Iteration  1961 -> Loss: 0.6737669730921751 \t| Accuracy: 110.833\n",
      "# Iteration  1962 -> Loss: 0.6735794125035509 \t| Accuracy: 110.833\n",
      "# Iteration  1963 -> Loss: 0.6733919792456451 \t| Accuracy: 110.833\n",
      "# Iteration  1964 -> Loss: 0.673204673085924 \t| Accuracy: 110.833\n",
      "# Iteration  1965 -> Loss: 0.6730174937920945 \t| Accuracy: 110.833\n",
      "# Iteration  1966 -> Loss: 0.6728304411321043 \t| Accuracy: 110.833\n",
      "# Iteration  1967 -> Loss: 0.6726435148741455 \t| Accuracy: 110.833\n",
      "# Iteration  1968 -> Loss: 0.6724567147866541 \t| Accuracy: 110.833\n",
      "# Iteration  1969 -> Loss: 0.672270040638313 \t| Accuracy: 110.833\n",
      "# Iteration  1970 -> Loss: 0.6720834921980523 \t| Accuracy: 110.833\n",
      "# Iteration  1971 -> Loss: 0.6718970692350519 \t| Accuracy: 110.833\n",
      "# Iteration  1972 -> Loss: 0.6717107715187421 \t| Accuracy: 110.833\n",
      "# Iteration  1973 -> Loss: 0.6715245988188059 \t| Accuracy: 110.833\n",
      "# Iteration  1974 -> Loss: 0.6713385509051797 \t| Accuracy: 110.833\n",
      "# Iteration  1975 -> Loss: 0.6711526275480554 \t| Accuracy: 110.833\n",
      "# Iteration  1976 -> Loss: 0.6709668285178823 \t| Accuracy: 110.833\n",
      "# Iteration  1977 -> Loss: 0.6707811535853674 \t| Accuracy: 110.833\n",
      "# Iteration  1978 -> Loss: 0.6705956025214785 \t| Accuracy: 110.833\n",
      "# Iteration  1979 -> Loss: 0.6704101750974447 \t| Accuracy: 110.833\n",
      "# Iteration  1980 -> Loss: 0.6702248710847577 \t| Accuracy: 110.833\n",
      "# Iteration  1981 -> Loss: 0.6700396902551751 \t| Accuracy: 110.833\n",
      "# Iteration  1982 -> Loss: 0.6698546323807197 \t| Accuracy: 110.833\n",
      "# Iteration  1983 -> Loss: 0.6696696972336831 \t| Accuracy: 110.833\n",
      "# Iteration  1984 -> Loss: 0.6694848845866259 \t| Accuracy: 110.833\n",
      "# Iteration  1985 -> Loss: 0.6693001942123802 \t| Accuracy: 110.833\n",
      "# Iteration  1986 -> Loss: 0.6691156258840506 \t| Accuracy: 110.833\n",
      "# Iteration  1987 -> Loss: 0.6689311793750162 \t| Accuracy: 110.833\n",
      "# Iteration  1988 -> Loss: 0.668746854458932 \t| Accuracy: 110.833\n",
      "# Iteration  1989 -> Loss: 0.6685626509097307 \t| Accuracy: 110.833\n",
      "# Iteration  1990 -> Loss: 0.6683785685016244 \t| Accuracy: 110.833\n",
      "# Iteration  1991 -> Loss: 0.6681946070091058 \t| Accuracy: 110.833\n",
      "# Iteration  1992 -> Loss: 0.668010766206951 \t| Accuracy: 110.833\n",
      "# Iteration  1993 -> Loss: 0.6678270458702193 \t| Accuracy: 110.833\n",
      "# Iteration  1994 -> Loss: 0.667643445774257 \t| Accuracy: 110.833\n",
      "# Iteration  1995 -> Loss: 0.6674599656946971 \t| Accuracy: 110.833\n",
      "# Iteration  1996 -> Loss: 0.6672766054074628 \t| Accuracy: 110.833\n",
      "# Iteration  1997 -> Loss: 0.6670933646887676 \t| Accuracy: 110.833\n",
      "# Iteration  1998 -> Loss: 0.6669102433151182 \t| Accuracy: 110.833\n",
      "# Iteration  1999 -> Loss: 0.6667272410633155 \t| Accuracy: 110.833\n",
      "# Iteration  2000 -> Loss: 0.6665443577104568 \t| Accuracy: 110.833\n",
      "# Iteration  2001 -> Loss: 0.666361593033937 \t| Accuracy: 110.833\n",
      "# Iteration  2002 -> Loss: 0.6661789468114513 \t| Accuracy: 110.833\n",
      "# Iteration  2003 -> Loss: 0.6659964188209958 \t| Accuracy: 110.833\n",
      "# Iteration  2004 -> Loss: 0.6658140088408695 \t| Accuracy: 110.833\n",
      "# Iteration  2005 -> Loss: 0.665631716649677 \t| Accuracy: 110.833\n",
      "# Iteration  2006 -> Loss: 0.665449542026329 \t| Accuracy: 110.833\n",
      "# Iteration  2007 -> Loss: 0.6652674847500454 \t| Accuracy: 110.833\n",
      "# Iteration  2008 -> Loss: 0.6650855446003558 \t| Accuracy: 110.833\n",
      "# Iteration  2009 -> Loss: 0.6649037213571021 \t| Accuracy: 110.833\n",
      "# Iteration  2010 -> Loss: 0.6647220148004397 \t| Accuracy: 110.833\n",
      "# Iteration  2011 -> Loss: 0.6645404247108406 \t| Accuracy: 110.833\n",
      "# Iteration  2012 -> Loss: 0.6643589508690935 \t| Accuracy: 110.833\n",
      "# Iteration  2013 -> Loss: 0.6641775930563067 \t| Accuracy: 110.833\n",
      "# Iteration  2014 -> Loss: 0.6639963510539102 \t| Accuracy: 110.833\n",
      "# Iteration  2015 -> Loss: 0.6638152246436564 \t| Accuracy: 110.833\n",
      "# Iteration  2016 -> Loss: 0.6636342136076228 \t| Accuracy: 110.833\n",
      "# Iteration  2017 -> Loss: 0.6634533177282141 \t| Accuracy: 110.833\n",
      "# Iteration  2018 -> Loss: 0.6632725367881631 \t| Accuracy: 110.833\n",
      "# Iteration  2019 -> Loss: 0.6630918705705335 \t| Accuracy: 110.833\n",
      "# Iteration  2020 -> Loss: 0.6629113188587219 \t| Accuracy: 110.833\n",
      "# Iteration  2021 -> Loss: 0.6627308814364582 \t| Accuracy: 110.833\n",
      "# Iteration  2022 -> Loss: 0.6625505580878097 \t| Accuracy: 110.833\n",
      "# Iteration  2023 -> Loss: 0.662370348597181 \t| Accuracy: 110.833\n",
      "# Iteration  2024 -> Loss: 0.6621902527493175 \t| Accuracy: 110.833\n",
      "# Iteration  2025 -> Loss: 0.6620102703293066 \t| Accuracy: 110.833\n",
      "# Iteration  2026 -> Loss: 0.6618304011225795 \t| Accuracy: 110.833\n",
      "# Iteration  2027 -> Loss: 0.6616506449149133 \t| Accuracy: 110.833\n",
      "# Iteration  2028 -> Loss: 0.6614710014924339 \t| Accuracy: 110.833\n",
      "# Iteration  2029 -> Loss: 0.661291470641616 \t| Accuracy: 110.833\n",
      "# Iteration  2030 -> Loss: 0.6611120521492869 \t| Accuracy: 110.833\n",
      "# Iteration  2031 -> Loss: 0.6609327458026281 \t| Accuracy: 110.833\n",
      "# Iteration  2032 -> Loss: 0.6607535513891765 \t| Accuracy: 110.833\n",
      "# Iteration  2033 -> Loss: 0.6605744686968272 \t| Accuracy: 110.833\n",
      "# Iteration  2034 -> Loss: 0.6603954975138354 \t| Accuracy: 110.833\n",
      "# Iteration  2035 -> Loss: 0.6602166376288182 \t| Accuracy: 110.833\n",
      "# Iteration  2036 -> Loss: 0.660037888830757 \t| Accuracy: 110.833\n",
      "# Iteration  2037 -> Loss: 0.659859250908999 \t| Accuracy: 110.833\n",
      "# Iteration  2038 -> Loss: 0.6596807236532598 \t| Accuracy: 110.833\n",
      "# Iteration  2039 -> Loss: 0.6595023068536254 \t| Accuracy: 110.833\n",
      "# Iteration  2040 -> Loss: 0.6593240003005538 \t| Accuracy: 110.833\n",
      "# Iteration  2041 -> Loss: 0.659145803784878 \t| Accuracy: 110.833\n",
      "# Iteration  2042 -> Loss: 0.6589677170978071 \t| Accuracy: 110.833\n",
      "# Iteration  2043 -> Loss: 0.6587897400309293 \t| Accuracy: 110.833\n",
      "# Iteration  2044 -> Loss: 0.6586118723762128 \t| Accuracy: 110.833\n",
      "# Iteration  2045 -> Loss: 0.6584341139260097 \t| Accuracy: 110.833\n",
      "# Iteration  2046 -> Loss: 0.6582564644730567 \t| Accuracy: 110.833\n",
      "# Iteration  2047 -> Loss: 0.6580789238104778 \t| Accuracy: 110.833\n",
      "# Iteration  2048 -> Loss: 0.6579014917317862 \t| Accuracy: 110.833\n",
      "# Iteration  2049 -> Loss: 0.6577241680308871 \t| Accuracy: 110.833\n",
      "# Iteration  2050 -> Loss: 0.657546952502079 \t| Accuracy: 110.833\n",
      "# Iteration  2051 -> Loss: 0.6573698449400565 \t| Accuracy: 110.833\n",
      "# Iteration  2052 -> Loss: 0.6571928451399122 \t| Accuracy: 110.833\n",
      "# Iteration  2053 -> Loss: 0.6570159528971395 \t| Accuracy: 110.833\n",
      "# Iteration  2054 -> Loss: 0.6568391680076339 \t| Accuracy: 110.833\n",
      "# Iteration  2055 -> Loss: 0.6566624902676956 \t| Accuracy: 110.833\n",
      "# Iteration  2056 -> Loss: 0.6564859194740327 \t| Accuracy: 110.833\n",
      "# Iteration  2057 -> Loss: 0.6563094554237613 \t| Accuracy: 110.833\n",
      "# Iteration  2058 -> Loss: 0.6561330979144098 \t| Accuracy: 110.833\n",
      "# Iteration  2059 -> Loss: 0.6559568467439205 \t| Accuracy: 110.833\n",
      "# Iteration  2060 -> Loss: 0.6557807017106513 \t| Accuracy: 110.833\n",
      "# Iteration  2061 -> Loss: 0.655604662613379 \t| Accuracy: 110.833\n",
      "# Iteration  2062 -> Loss: 0.6554287292513007 \t| Accuracy: 110.833\n",
      "# Iteration  2063 -> Loss: 0.6552529014240365 \t| Accuracy: 110.833\n",
      "# Iteration  2064 -> Loss: 0.655077178931632 \t| Accuracy: 110.833\n",
      "# Iteration  2065 -> Loss: 0.65490156157456 \t| Accuracy: 110.833\n",
      "# Iteration  2066 -> Loss: 0.6547260491537239 \t| Accuracy: 110.833\n",
      "# Iteration  2067 -> Loss: 0.654550641470459 \t| Accuracy: 110.833\n",
      "# Iteration  2068 -> Loss: 0.6543753383265353 \t| Accuracy: 110.833\n",
      "# Iteration  2069 -> Loss: 0.6542001395241595 \t| Accuracy: 110.833\n",
      "# Iteration  2070 -> Loss: 0.6540250448659783 \t| Accuracy: 110.833\n",
      "# Iteration  2071 -> Loss: 0.6538500541550798 \t| Accuracy: 110.833\n",
      "# Iteration  2072 -> Loss: 0.6536751671949963 \t| Accuracy: 110.833\n",
      "# Iteration  2073 -> Loss: 0.653500383789707 \t| Accuracy: 110.833\n",
      "# Iteration  2074 -> Loss: 0.6533257037436393 \t| Accuracy: 110.833\n",
      "# Iteration  2075 -> Loss: 0.653151126861673 \t| Accuracy: 110.833\n",
      "# Iteration  2076 -> Loss: 0.6529766529491409 \t| Accuracy: 110.833\n",
      "# Iteration  2077 -> Loss: 0.6528022818118325 \t| Accuracy: 110.833\n",
      "# Iteration  2078 -> Loss: 0.6526280132559961 \t| Accuracy: 110.833\n",
      "# Iteration  2079 -> Loss: 0.6524538470883411 \t| Accuracy: 110.833\n",
      "# Iteration  2080 -> Loss: 0.6522797831160401 \t| Accuracy: 110.833\n",
      "# Iteration  2081 -> Loss: 0.6521058211467324 \t| Accuracy: 110.833\n",
      "# Iteration  2082 -> Loss: 0.6519319609885258 \t| Accuracy: 110.833\n",
      "# Iteration  2083 -> Loss: 0.6517582024499993 \t| Accuracy: 110.833\n",
      "# Iteration  2084 -> Loss: 0.6515845453402049 \t| Accuracy: 110.833\n",
      "# Iteration  2085 -> Loss: 0.6514109894686715 \t| Accuracy: 110.833\n",
      "# Iteration  2086 -> Loss: 0.6512375346454061 \t| Accuracy: 110.833\n",
      "# Iteration  2087 -> Loss: 0.6510641806808974 \t| Accuracy: 110.833\n",
      "# Iteration  2088 -> Loss: 0.6508909273861173 \t| Accuracy: 110.833\n",
      "# Iteration  2089 -> Loss: 0.6507177745725242 \t| Accuracy: 110.833\n",
      "# Iteration  2090 -> Loss: 0.6505447220520655 \t| Accuracy: 110.833\n",
      "# Iteration  2091 -> Loss: 0.6503717696371798 \t| Accuracy: 110.833\n",
      "# Iteration  2092 -> Loss: 0.6501989171407994 \t| Accuracy: 110.833\n",
      "# Iteration  2093 -> Loss: 0.6500261643763535 \t| Accuracy: 110.833\n",
      "# Iteration  2094 -> Loss: 0.6498535111577708 \t| Accuracy: 110.833\n",
      "# Iteration  2095 -> Loss: 0.6496809572994809 \t| Accuracy: 110.833\n",
      "# Iteration  2096 -> Loss: 0.6495085026164183 \t| Accuracy: 110.833\n",
      "# Iteration  2097 -> Loss: 0.6493361469240246 \t| Accuracy: 110.833\n",
      "# Iteration  2098 -> Loss: 0.6491638900382503 \t| Accuracy: 110.833\n",
      "# Iteration  2099 -> Loss: 0.6489917317755588 \t| Accuracy: 110.833\n",
      "# Iteration  2100 -> Loss: 0.6488196719529281 \t| Accuracy: 110.833\n",
      "# Iteration  2101 -> Loss: 0.648647710387854 \t| Accuracy: 110.833\n",
      "# Iteration  2102 -> Loss: 0.6484758468983517 \t| Accuracy: 110.833\n",
      "# Iteration  2103 -> Loss: 0.6483040813029602 \t| Accuracy: 110.833\n",
      "# Iteration  2104 -> Loss: 0.6481324134207435 \t| Accuracy: 110.833\n",
      "# Iteration  2105 -> Loss: 0.6479608430712939 \t| Accuracy: 110.833\n",
      "# Iteration  2106 -> Loss: 0.6477893700747342 \t| Accuracy: 110.833\n",
      "# Iteration  2107 -> Loss: 0.6476179942517215 \t| Accuracy: 110.833\n",
      "# Iteration  2108 -> Loss: 0.6474467154234489 \t| Accuracy: 110.833\n",
      "# Iteration  2109 -> Loss: 0.6472755334116481 \t| Accuracy: 110.833\n",
      "# Iteration  2110 -> Loss: 0.647104448038593 \t| Accuracy: 110.833\n",
      "# Iteration  2111 -> Loss: 0.6469334591271014 \t| Accuracy: 110.833\n",
      "# Iteration  2112 -> Loss: 0.6467625665005392 \t| Accuracy: 110.833\n",
      "# Iteration  2113 -> Loss: 0.6465917699828209 \t| Accuracy: 110.833\n",
      "# Iteration  2114 -> Loss: 0.6464210693984147 \t| Accuracy: 110.833\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Iteration  2115 -> Loss: 0.6462504645723437 \t| Accuracy: 110.833\n",
      "# Iteration  2116 -> Loss: 0.6460799553301891 \t| Accuracy: 110.833\n",
      "# Iteration  2117 -> Loss: 0.6459095414980934 \t| Accuracy: 110.833\n",
      "# Iteration  2118 -> Loss: 0.6457392229027624 \t| Accuracy: 110.833\n",
      "# Iteration  2119 -> Loss: 0.6455689993714687 \t| Accuracy: 110.833\n",
      "# Iteration  2120 -> Loss: 0.6453988707320536 \t| Accuracy: 110.833\n",
      "# Iteration  2121 -> Loss: 0.6452288368129309 \t| Accuracy: 110.833\n",
      "# Iteration  2122 -> Loss: 0.645058897443089 \t| Accuracy: 110.833\n",
      "# Iteration  2123 -> Loss: 0.6448890524520938 \t| Accuracy: 110.833\n",
      "# Iteration  2124 -> Loss: 0.6447193016700923 \t| Accuracy: 110.833\n",
      "# Iteration  2125 -> Loss: 0.6445496449278139 \t| Accuracy: 110.833\n",
      "# Iteration  2126 -> Loss: 0.6443800820565744 \t| Accuracy: 110.833\n",
      "# Iteration  2127 -> Loss: 0.6442106128882782 \t| Accuracy: 110.833\n",
      "# Iteration  2128 -> Loss: 0.6440412372554218 \t| Accuracy: 110.833\n",
      "# Iteration  2129 -> Loss: 0.6438719549910962 \t| Accuracy: 110.833\n",
      "# Iteration  2130 -> Loss: 0.6437027659289891 \t| Accuracy: 110.833\n",
      "# Iteration  2131 -> Loss: 0.6435336699033892 \t| Accuracy: 110.833\n",
      "# Iteration  2132 -> Loss: 0.6433646667491878 \t| Accuracy: 110.833\n",
      "# Iteration  2133 -> Loss: 0.6431957563018821 \t| Accuracy: 110.833\n",
      "# Iteration  2134 -> Loss: 0.643026938397578 \t| Accuracy: 110.833\n",
      "# Iteration  2135 -> Loss: 0.642858212872993 \t| Accuracy: 110.833\n",
      "# Iteration  2136 -> Loss: 0.6426895795654592 \t| Accuracy: 110.833\n",
      "# Iteration  2137 -> Loss: 0.642521038312926 \t| Accuracy: 110.833\n",
      "# Iteration  2138 -> Loss: 0.6423525889539623 \t| Accuracy: 110.833\n",
      "# Iteration  2139 -> Loss: 0.6421842313277609 \t| Accuracy: 110.833\n",
      "# Iteration  2140 -> Loss: 0.64201596527414 \t| Accuracy: 110.833\n",
      "# Iteration  2141 -> Loss: 0.6418477906335467 \t| Accuracy: 110.208\n",
      "# Iteration  2142 -> Loss: 0.6416797072470596 \t| Accuracy: 110.208\n",
      "# Iteration  2143 -> Loss: 0.641511714956392 \t| Accuracy: 110.208\n",
      "# Iteration  2144 -> Loss: 0.6413438136038946 \t| Accuracy: 110.208\n",
      "# Iteration  2145 -> Loss: 0.6411760030325578 \t| Accuracy: 110.208\n",
      "# Iteration  2146 -> Loss: 0.6410082830860162 \t| Accuracy: 110.208\n",
      "# Iteration  2147 -> Loss: 0.6408406536085496 \t| Accuracy: 110.208\n",
      "# Iteration  2148 -> Loss: 0.6406731144450868 \t| Accuracy: 110.208\n",
      "# Iteration  2149 -> Loss: 0.6405056654412089 \t| Accuracy: 110.208\n",
      "# Iteration  2150 -> Loss: 0.6403383064431512 \t| Accuracy: 110.208\n",
      "# Iteration  2151 -> Loss: 0.6401710372978064 \t| Accuracy: 110.208\n",
      "# Iteration  2152 -> Loss: 0.6400038578527285 \t| Accuracy: 110.208\n",
      "# Iteration  2153 -> Loss: 0.6398367679561341 \t| Accuracy: 110.208\n",
      "# Iteration  2154 -> Loss: 0.6396697674569064 \t| Accuracy: 110.208\n",
      "# Iteration  2155 -> Loss: 0.6395028562045972 \t| Accuracy: 110.208\n",
      "# Iteration  2156 -> Loss: 0.6393360340494314 \t| Accuracy: 110.208\n",
      "# Iteration  2157 -> Loss: 0.6391693008423073 \t| Accuracy: 110.208\n",
      "# Iteration  2158 -> Loss: 0.6390026564348025 \t| Accuracy: 110.208\n",
      "# Iteration  2159 -> Loss: 0.6388361006791738 \t| Accuracy: 110.208\n",
      "# Iteration  2160 -> Loss: 0.638669633428363 \t| Accuracy: 110.208\n",
      "# Iteration  2161 -> Loss: 0.6385032545359973 \t| Accuracy: 110.208\n",
      "# Iteration  2162 -> Loss: 0.6383369638563934 \t| Accuracy: 110.208\n",
      "# Iteration  2163 -> Loss: 0.6381707612445603 \t| Accuracy: 110.208\n",
      "# Iteration  2164 -> Loss: 0.6380046465562023 \t| Accuracy: 110.208\n",
      "# Iteration  2165 -> Loss: 0.6378386196477209 \t| Accuracy: 110.208\n",
      "# Iteration  2166 -> Loss: 0.6376726803762193 \t| Accuracy: 110.208\n",
      "# Iteration  2167 -> Loss: 0.6375068285995036 \t| Accuracy: 110.208\n",
      "# Iteration  2168 -> Loss: 0.6373410641760867 \t| Accuracy: 110.208\n",
      "# Iteration  2169 -> Loss: 0.6371753869651909 \t| Accuracy: 110.208\n",
      "# Iteration  2170 -> Loss: 0.6370097968267509 \t| Accuracy: 110.208\n",
      "# Iteration  2171 -> Loss: 0.6368442936214157 \t| Accuracy: 110.208\n",
      "# Iteration  2172 -> Loss: 0.6366788772105532 \t| Accuracy: 110.208\n",
      "# Iteration  2173 -> Loss: 0.6365135474562517 \t| Accuracy: 110.208\n",
      "# Iteration  2174 -> Loss: 0.6363483042213225 \t| Accuracy: 110.208\n",
      "# Iteration  2175 -> Loss: 0.6361831473693041 \t| Accuracy: 110.208\n",
      "# Iteration  2176 -> Loss: 0.6360180767644638 \t| Accuracy: 110.208\n",
      "# Iteration  2177 -> Loss: 0.6358530922718013 \t| Accuracy: 110.208\n",
      "# Iteration  2178 -> Loss: 0.6356881937570507 \t| Accuracy: 110.208\n",
      "# Iteration  2179 -> Loss: 0.6355233810866837 \t| Accuracy: 110.208\n",
      "# Iteration  2180 -> Loss: 0.6353586541279133 \t| Accuracy: 110.208\n",
      "# Iteration  2181 -> Loss: 0.6351940127486947 \t| Accuracy: 110.208\n",
      "# Iteration  2182 -> Loss: 0.6350294568177297 \t| Accuracy: 110.208\n",
      "# Iteration  2183 -> Loss: 0.634864986204469 \t| Accuracy: 110.208\n",
      "# Iteration  2184 -> Loss: 0.6347006007791142 \t| Accuracy: 110.208\n",
      "# Iteration  2185 -> Loss: 0.6345363004126219 \t| Accuracy: 110.208\n",
      "# Iteration  2186 -> Loss: 0.6343720849767055 \t| Accuracy: 110.208\n",
      "# Iteration  2187 -> Loss: 0.6342079543438378 \t| Accuracy: 110.208\n",
      "# Iteration  2188 -> Loss: 0.6340439083872549 \t| Accuracy: 110.208\n",
      "# Iteration  2189 -> Loss: 0.6338799469809575 \t| Accuracy: 110.208\n",
      "# Iteration  2190 -> Loss: 0.6337160699997146 \t| Accuracy: 110.208\n",
      "# Iteration  2191 -> Loss: 0.6335522773190656 \t| Accuracy: 110.208\n",
      "# Iteration  2192 -> Loss: 0.6333885688153233 \t| Accuracy: 110.208\n",
      "# Iteration  2193 -> Loss: 0.6332249443655766 \t| Accuracy: 110.208\n",
      "# Iteration  2194 -> Loss: 0.6330614038476932 \t| Accuracy: 110.208\n",
      "# Iteration  2195 -> Loss: 0.6328979471403215 \t| Accuracy: 110.208\n",
      "# Iteration  2196 -> Loss: 0.6327345741228944 \t| Accuracy: 110.208\n",
      "# Iteration  2197 -> Loss: 0.6325712846756314 \t| Accuracy: 110.208\n",
      "# Iteration  2198 -> Loss: 0.6324080786795409 \t| Accuracy: 110.208\n",
      "# Iteration  2199 -> Loss: 0.632244956016423 \t| Accuracy: 110.208\n",
      "# Iteration  2200 -> Loss: 0.6320819165688727 \t| Accuracy: 110.208\n",
      "# Iteration  2201 -> Loss: 0.6319189602202815 \t| Accuracy: 110.208\n",
      "# Iteration  2202 -> Loss: 0.6317560868548405 \t| Accuracy: 110.208\n",
      "# Iteration  2203 -> Loss: 0.6315932963575431 \t| Accuracy: 110.208\n",
      "# Iteration  2204 -> Loss: 0.6314305886141867 \t| Accuracy: 110.208\n",
      "# Iteration  2205 -> Loss: 0.6312679635113763 \t| Accuracy: 110.208\n",
      "# Iteration  2206 -> Loss: 0.6311054209365262 \t| Accuracy: 110.208\n",
      "# Iteration  2207 -> Loss: 0.630942960777863 \t| Accuracy: 110.208\n",
      "# Iteration  2208 -> Loss: 0.6307805829244275 \t| Accuracy: 110.208\n",
      "# Iteration  2209 -> Loss: 0.6306182872660777 \t| Accuracy: 110.208\n",
      "# Iteration  2210 -> Loss: 0.6304560736934911 \t| Accuracy: 110.208\n",
      "# Iteration  2211 -> Loss: 0.6302939420981667 \t| Accuracy: 110.208\n",
      "# Iteration  2212 -> Loss: 0.6301318923724276 \t| Accuracy: 110.208\n",
      "# Iteration  2213 -> Loss: 0.6299699244094241 \t| Accuracy: 110.208\n",
      "# Iteration  2214 -> Loss: 0.6298080381031348 \t| Accuracy: 110.208\n",
      "# Iteration  2215 -> Loss: 0.6296462333483696 \t| Accuracy: 110.208\n",
      "# Iteration  2216 -> Loss: 0.629484510040772 \t| Accuracy: 110.208\n",
      "# Iteration  2217 -> Loss: 0.6293228680768219 \t| Accuracy: 110.208\n",
      "# Iteration  2218 -> Loss: 0.6291613073538368 \t| Accuracy: 110.208\n",
      "# Iteration  2219 -> Loss: 0.6289998277699743 \t| Accuracy: 110.208\n",
      "# Iteration  2220 -> Loss: 0.6288384292242355 \t| Accuracy: 110.208\n",
      "# Iteration  2221 -> Loss: 0.6286771116164656 \t| Accuracy: 110.208\n",
      "# Iteration  2222 -> Loss: 0.6285158748473568 \t| Accuracy: 110.208\n",
      "# Iteration  2223 -> Loss: 0.6283547188184512 \t| Accuracy: 110.208\n",
      "# Iteration  2224 -> Loss: 0.6281936434321412 \t| Accuracy: 110.208\n",
      "# Iteration  2225 -> Loss: 0.6280326485916736 \t| Accuracy: 110.208\n",
      "# Iteration  2226 -> Loss: 0.62787173420115 \t| Accuracy: 110.208\n",
      "# Iteration  2227 -> Loss: 0.6277109001655298 \t| Accuracy: 110.208\n",
      "# Iteration  2228 -> Loss: 0.6275501463906323 \t| Accuracy: 110.208\n",
      "# Iteration  2229 -> Loss: 0.6273894727831383 \t| Accuracy: 110.208\n",
      "# Iteration  2230 -> Loss: 0.627228879250592 \t| Accuracy: 110.208\n",
      "# Iteration  2231 -> Loss: 0.6270683657014035 \t| Accuracy: 110.208\n",
      "# Iteration  2232 -> Loss: 0.6269079320448506 \t| Accuracy: 110.208\n",
      "# Iteration  2233 -> Loss: 0.6267475781910798 \t| Accuracy: 110.208\n",
      "# Iteration  2234 -> Loss: 0.6265873040511103 \t| Accuracy: 110.208\n",
      "# Iteration  2235 -> Loss: 0.6264271095368331 \t| Accuracy: 110.208\n",
      "# Iteration  2236 -> Loss: 0.6262669945610154 \t| Accuracy: 110.208\n",
      "# Iteration  2237 -> Loss: 0.6261069590373001 \t| Accuracy: 110.208\n",
      "# Iteration  2238 -> Loss: 0.6259470028802099 \t| Accuracy: 110.208\n",
      "# Iteration  2239 -> Loss: 0.625787126005147 \t| Accuracy: 110.208\n",
      "# Iteration  2240 -> Loss: 0.6256273283283962 \t| Accuracy: 110.208\n",
      "# Iteration  2241 -> Loss: 0.6254676097671258 \t| Accuracy: 110.208\n",
      "# Iteration  2242 -> Loss: 0.6253079702393893 \t| Accuracy: 110.208\n",
      "# Iteration  2243 -> Loss: 0.625148409664128 \t| Accuracy: 110.208\n",
      "# Iteration  2244 -> Loss: 0.6249889279611712 \t| Accuracy: 110.208\n",
      "# Iteration  2245 -> Loss: 0.6248295250512382 \t| Accuracy: 110.208\n",
      "# Iteration  2246 -> Loss: 0.6246702008559408 \t| Accuracy: 110.208\n",
      "# Iteration  2247 -> Loss: 0.6245109552977834 \t| Accuracy: 110.208\n",
      "# Iteration  2248 -> Loss: 0.6243517883001654 \t| Accuracy: 110.208\n",
      "# Iteration  2249 -> Loss: 0.6241926997873816 \t| Accuracy: 110.208\n",
      "# Iteration  2250 -> Loss: 0.6240336896846251 \t| Accuracy: 110.208\n",
      "# Iteration  2251 -> Loss: 0.623874757917988 \t| Accuracy: 110.208\n",
      "# Iteration  2252 -> Loss: 0.6237159044144611 \t| Accuracy: 110.208\n",
      "# Iteration  2253 -> Loss: 0.6235571291019384 \t| Accuracy: 110.208\n",
      "# Iteration  2254 -> Loss: 0.6233984319092157 \t| Accuracy: 110.000\n",
      "# Iteration  2255 -> Loss: 0.6232398127659927 \t| Accuracy: 110.000\n",
      "# Iteration  2256 -> Loss: 0.6230812716028744 \t| Accuracy: 110.000\n",
      "# Iteration  2257 -> Loss: 0.6229228083513723 \t| Accuracy: 110.000\n",
      "# Iteration  2258 -> Loss: 0.6227644229439043 \t| Accuracy: 110.000\n",
      "# Iteration  2259 -> Loss: 0.6226061153137978 \t| Accuracy: 110.000\n",
      "# Iteration  2260 -> Loss: 0.6224478853952891 \t| Accuracy: 110.000\n",
      "# Iteration  2261 -> Loss: 0.6222897331235248 \t| Accuracy: 110.000\n",
      "# Iteration  2262 -> Loss: 0.622131658434563 \t| Accuracy: 110.000\n",
      "# Iteration  2263 -> Loss: 0.6219736612653742 \t| Accuracy: 110.000\n",
      "# Iteration  2264 -> Loss: 0.6218157415538421 \t| Accuracy: 110.000\n",
      "# Iteration  2265 -> Loss: 0.6216578992387639 \t| Accuracy: 110.000\n",
      "# Iteration  2266 -> Loss: 0.6215001342598518 \t| Accuracy: 110.000\n",
      "# Iteration  2267 -> Loss: 0.6213424465577339 \t| Accuracy: 110.000\n",
      "# Iteration  2268 -> Loss: 0.621184836073954 \t| Accuracy: 110.000\n",
      "# Iteration  2269 -> Loss: 0.6210273027509728 \t| Accuracy: 110.000\n",
      "# Iteration  2270 -> Loss: 0.6208698465321689 \t| Accuracy: 110.000\n",
      "# Iteration  2271 -> Loss: 0.6207124673618383 \t| Accuracy: 110.000\n",
      "# Iteration  2272 -> Loss: 0.6205551651851965 \t| Accuracy: 110.000\n",
      "# Iteration  2273 -> Loss: 0.6203979399483773 \t| Accuracy: 110.000\n",
      "# Iteration  2274 -> Loss: 0.6202407915984343 \t| Accuracy: 110.000\n",
      "# Iteration  2275 -> Loss: 0.620083720083341 \t| Accuracy: 110.000\n",
      "# Iteration  2276 -> Loss: 0.6199267253519913 \t| Accuracy: 110.000\n",
      "# Iteration  2277 -> Loss: 0.6197698073541995 \t| Accuracy: 110.000\n",
      "# Iteration  2278 -> Loss: 0.6196129660407013 \t| Accuracy: 110.000\n",
      "# Iteration  2279 -> Loss: 0.6194562013631526 \t| Accuracy: 110.000\n",
      "# Iteration  2280 -> Loss: 0.6192995132741314 \t| Accuracy: 110.000\n",
      "# Iteration  2281 -> Loss: 0.6191429017271365 \t| Accuracy: 110.000\n",
      "# Iteration  2282 -> Loss: 0.6189863666765886 \t| Accuracy: 110.000\n",
      "# Iteration  2283 -> Loss: 0.6188299080778296 \t| Accuracy: 110.000\n",
      "# Iteration  2284 -> Loss: 0.6186735258871225 \t| Accuracy: 110.000\n",
      "# Iteration  2285 -> Loss: 0.618517220061653 \t| Accuracy: 110.000\n",
      "# Iteration  2286 -> Loss: 0.6183609905595268 \t| Accuracy: 110.000\n",
      "# Iteration  2287 -> Loss: 0.6182048373397712 \t| Accuracy: 110.000\n",
      "# Iteration  2288 -> Loss: 0.6180487603623344 \t| Accuracy: 110.000\n",
      "# Iteration  2289 -> Loss: 0.6178927595880854 \t| Accuracy: 110.000\n",
      "# Iteration  2290 -> Loss: 0.6177368349788134 \t| Accuracy: 110.000\n",
      "# Iteration  2291 -> Loss: 0.6175809864972275 \t| Accuracy: 110.000\n",
      "# Iteration  2292 -> Loss: 0.6174252141069566 \t| Accuracy: 110.000\n",
      "# Iteration  2293 -> Loss: 0.6172695177725487 \t| Accuracy: 110.000\n",
      "# Iteration  2294 -> Loss: 0.61711389745947 \t| Accuracy: 110.000\n",
      "# Iteration  2295 -> Loss: 0.6169583531341052 \t| Accuracy: 110.000\n",
      "# Iteration  2296 -> Loss: 0.6168028847637561 \t| Accuracy: 110.000\n",
      "# Iteration  2297 -> Loss: 0.6166474923166415 \t| Accuracy: 110.000\n",
      "# Iteration  2298 -> Loss: 0.6164921757618961 \t| Accuracy: 110.000\n",
      "# Iteration  2299 -> Loss: 0.6163369350695695 \t| Accuracy: 110.000\n",
      "# Iteration  2300 -> Loss: 0.6161817702106265 \t| Accuracy: 110.000\n",
      "# Iteration  2301 -> Loss: 0.6160266811569445 \t| Accuracy: 110.000\n",
      "# Iteration  2302 -> Loss: 0.6158716678813139 \t| Accuracy: 110.000\n",
      "# Iteration  2303 -> Loss: 0.6157167303574371 \t| Accuracy: 110.000\n",
      "# Iteration  2304 -> Loss: 0.6155618685599262 \t| Accuracy: 110.000\n",
      "# Iteration  2305 -> Loss: 0.6154070824643035 \t| Accuracy: 110.000\n",
      "# Iteration  2306 -> Loss: 0.6152523720469995 \t| Accuracy: 110.000\n",
      "# Iteration  2307 -> Loss: 0.6150977372853511 \t| Accuracy: 110.000\n",
      "# Iteration  2308 -> Loss: 0.6149431781576018 \t| Accuracy: 110.000\n",
      "# Iteration  2309 -> Loss: 0.6147886946428992 \t| Accuracy: 110.000\n",
      "# Iteration  2310 -> Loss: 0.6146342867212943 \t| Accuracy: 110.000\n",
      "# Iteration  2311 -> Loss: 0.6144799543737396 \t| Accuracy: 110.000\n",
      "# Iteration  2312 -> Loss: 0.6143256975820875 \t| Accuracy: 110.000\n",
      "# Iteration  2313 -> Loss: 0.61417151632909 \t| Accuracy: 110.000\n",
      "# Iteration  2314 -> Loss: 0.614017410598395 \t| Accuracy: 110.000\n",
      "# Iteration  2315 -> Loss: 0.6138633803745466 \t| Accuracy: 110.000\n",
      "# Iteration  2316 -> Loss: 0.6137094256429824 \t| Accuracy: 110.000\n",
      "# Iteration  2317 -> Loss: 0.6135555463900316 \t| Accuracy: 110.000\n",
      "# Iteration  2318 -> Loss: 0.6134017426029141 \t| Accuracy: 110.000\n",
      "# Iteration  2319 -> Loss: 0.6132480142697375 \t| Accuracy: 110.000\n",
      "# Iteration  2320 -> Loss: 0.6130943613794957 \t| Accuracy: 110.000\n",
      "# Iteration  2321 -> Loss: 0.6129407839220671 \t| Accuracy: 110.000\n",
      "# Iteration  2322 -> Loss: 0.612787281888212 \t| Accuracy: 110.000\n",
      "# Iteration  2323 -> Loss: 0.6126338552695711 \t| Accuracy: 110.000\n",
      "# Iteration  2324 -> Loss: 0.6124805040586626 \t| Accuracy: 110.000\n",
      "# Iteration  2325 -> Loss: 0.6123272282488808 \t| Accuracy: 110.000\n",
      "# Iteration  2326 -> Loss: 0.6121740278344929 \t| Accuracy: 110.000\n",
      "# Iteration  2327 -> Loss: 0.6120209028106376 \t| Accuracy: 110.000\n",
      "# Iteration  2328 -> Loss: 0.6118678531733219 \t| Accuracy: 110.000\n",
      "# Iteration  2329 -> Loss: 0.6117148789194189 \t| Accuracy: 110.000\n",
      "# Iteration  2330 -> Loss: 0.6115619800466657 \t| Accuracy: 110.000\n",
      "# Iteration  2331 -> Loss: 0.6114091565536598 \t| Accuracy: 110.000\n",
      "# Iteration  2332 -> Loss: 0.6112564084398577 \t| Accuracy: 110.000\n",
      "# Iteration  2333 -> Loss: 0.6111037357055713 \t| Accuracy: 110.000\n",
      "# Iteration  2334 -> Loss: 0.6109511383519652 \t| Accuracy: 110.000\n",
      "# Iteration  2335 -> Loss: 0.6107986163810546 \t| Accuracy: 109.375\n",
      "# Iteration  2336 -> Loss: 0.6106461697957014 \t| Accuracy: 109.375\n",
      "# Iteration  2337 -> Loss: 0.6104937985996121 \t| Accuracy: 109.375\n",
      "# Iteration  2338 -> Loss: 0.6103415027973342 \t| Accuracy: 109.375\n",
      "# Iteration  2339 -> Loss: 0.6101892823942536 \t| Accuracy: 109.375\n",
      "# Iteration  2340 -> Loss: 0.6100371373965914 \t| Accuracy: 109.375\n",
      "# Iteration  2341 -> Loss: 0.6098850678114002 \t| Accuracy: 109.375\n",
      "# Iteration  2342 -> Loss: 0.6097330736465617 \t| Accuracy: 109.375\n",
      "# Iteration  2343 -> Loss: 0.6095811549107828 \t| Accuracy: 109.375\n",
      "# Iteration  2344 -> Loss: 0.6094293116135923 \t| Accuracy: 109.375\n",
      "# Iteration  2345 -> Loss: 0.6092775437653379 \t| Accuracy: 109.375\n",
      "# Iteration  2346 -> Loss: 0.6091258513771823 \t| Accuracy: 109.375\n",
      "# Iteration  2347 -> Loss: 0.6089742344610998 \t| Accuracy: 109.375\n",
      "# Iteration  2348 -> Loss: 0.6088226930298726 \t| Accuracy: 109.375\n",
      "# Iteration  2349 -> Loss: 0.6086712270970875 \t| Accuracy: 109.375\n",
      "# Iteration  2350 -> Loss: 0.608519836677132 \t| Accuracy: 109.375\n",
      "# Iteration  2351 -> Loss: 0.6083685217851901 \t| Accuracy: 109.375\n",
      "# Iteration  2352 -> Loss: 0.6082172824372393 \t| Accuracy: 109.375\n",
      "# Iteration  2353 -> Loss: 0.6080661186500461 \t| Accuracy: 109.375\n",
      "# Iteration  2354 -> Loss: 0.6079150304411626 \t| Accuracy: 109.375\n",
      "# Iteration  2355 -> Loss: 0.6077640178289214 \t| Accuracy: 109.375\n",
      "# Iteration  2356 -> Loss: 0.6076130808324332 \t| Accuracy: 109.375\n",
      "# Iteration  2357 -> Loss: 0.6074622194715812 \t| Accuracy: 109.375\n",
      "# Iteration  2358 -> Loss: 0.6073114337670179 \t| Accuracy: 109.375\n",
      "# Iteration  2359 -> Loss: 0.6071607237401601 \t| Accuracy: 109.375\n",
      "# Iteration  2360 -> Loss: 0.6070100894131852 \t| Accuracy: 109.375\n",
      "# Iteration  2361 -> Loss: 0.6068595308090272 \t| Accuracy: 109.375\n",
      "# Iteration  2362 -> Loss: 0.606709047951371 \t| Accuracy: 109.375\n",
      "# Iteration  2363 -> Loss: 0.6065586408646491 \t| Accuracy: 109.375\n",
      "# Iteration  2364 -> Loss: 0.6064083095740367 \t| Accuracy: 109.375\n",
      "# Iteration  2365 -> Loss: 0.6062580541054473 \t| Accuracy: 109.375\n",
      "# Iteration  2366 -> Loss: 0.6061078744855276 \t| Accuracy: 109.375\n",
      "# Iteration  2367 -> Loss: 0.6059577707416537 \t| Accuracy: 109.375\n",
      "# Iteration  2368 -> Loss: 0.6058077429019253 \t| Accuracy: 109.375\n",
      "# Iteration  2369 -> Loss: 0.6056577909951617 \t| Accuracy: 109.375\n",
      "# Iteration  2370 -> Loss: 0.6055079150508968 \t| Accuracy: 109.375\n",
      "# Iteration  2371 -> Loss: 0.6053581150993743 \t| Accuracy: 109.375\n",
      "# Iteration  2372 -> Loss: 0.6052083911715421 \t| Accuracy: 109.375\n",
      "# Iteration  2373 -> Loss: 0.6050587432990481 \t| Accuracy: 109.375\n",
      "# Iteration  2374 -> Loss: 0.6049091715142348 \t| Accuracy: 109.375\n",
      "# Iteration  2375 -> Loss: 0.6047596758501345 \t| Accuracy: 109.375\n",
      "# Iteration  2376 -> Loss: 0.6046102563404634 \t| Accuracy: 109.375\n",
      "# Iteration  2377 -> Loss: 0.6044609130196172 \t| Accuracy: 109.375\n",
      "# Iteration  2378 -> Loss: 0.6043116459226655 \t| Accuracy: 109.375\n",
      "# Iteration  2379 -> Loss: 0.6041624550853465 \t| Accuracy: 109.375\n",
      "# Iteration  2380 -> Loss: 0.6040133405440615 \t| Accuracy: 109.375\n",
      "# Iteration  2381 -> Loss: 0.6038643023358702 \t| Accuracy: 109.375\n",
      "# Iteration  2382 -> Loss: 0.6037153404984837 \t| Accuracy: 109.375\n",
      "# Iteration  2383 -> Loss: 0.6035664550702611 \t| Accuracy: 109.375\n",
      "# Iteration  2384 -> Loss: 0.6034176460902018 \t| Accuracy: 109.375\n",
      "# Iteration  2385 -> Loss: 0.6032689135979418 \t| Accuracy: 109.375\n",
      "# Iteration  2386 -> Loss: 0.6031202576337465 \t| Accuracy: 109.375\n",
      "# Iteration  2387 -> Loss: 0.6029716782385064 \t| Accuracy: 109.375\n",
      "# Iteration  2388 -> Loss: 0.60282317545373 \t| Accuracy: 109.375\n",
      "# Iteration  2389 -> Loss: 0.6026747493215384 \t| Accuracy: 109.375\n",
      "# Iteration  2390 -> Loss: 0.6025263998846608 \t| Accuracy: 109.375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Iteration  2391 -> Loss: 0.6023781271864264 \t| Accuracy: 109.375\n",
      "# Iteration  2392 -> Loss: 0.6022299312707601 \t| Accuracy: 109.375\n",
      "# Iteration  2393 -> Loss: 0.602081812182176 \t| Accuracy: 109.375\n",
      "# Iteration  2394 -> Loss: 0.6019337699657713 \t| Accuracy: 109.375\n",
      "# Iteration  2395 -> Loss: 0.6017858046672202 \t| Accuracy: 109.375\n",
      "# Iteration  2396 -> Loss: 0.6016379163327683 \t| Accuracy: 109.375\n",
      "# Iteration  2397 -> Loss: 0.6014901050092258 \t| Accuracy: 109.375\n",
      "# Iteration  2398 -> Loss: 0.6013423707439616 \t| Accuracy: 109.375\n",
      "# Iteration  2399 -> Loss: 0.6011947135848973 \t| Accuracy: 109.375\n",
      "# Iteration  2400 -> Loss: 0.6010471335805005 \t| Accuracy: 109.375\n",
      "# Iteration  2401 -> Loss: 0.6008996307797788 \t| Accuracy: 109.375\n",
      "# Iteration  2402 -> Loss: 0.6007522052322732 \t| Accuracy: 109.375\n",
      "# Iteration  2403 -> Loss: 0.6006048569880523 \t| Accuracy: 109.375\n",
      "# Iteration  2404 -> Loss: 0.6004575860977049 \t| Accuracy: 109.375\n",
      "# Iteration  2405 -> Loss: 0.6003103926123348 \t| Accuracy: 109.375\n",
      "# Iteration  2406 -> Loss: 0.6001632765835528 \t| Accuracy: 109.375\n",
      "# Iteration  2407 -> Loss: 0.6000162380634715 \t| Accuracy: 109.375\n",
      "# Iteration  2408 -> Loss: 0.5998692771046982 \t| Accuracy: 109.375\n",
      "# Iteration  2409 -> Loss: 0.599722393760328 \t| Accuracy: 109.375\n",
      "# Iteration  2410 -> Loss: 0.599575588083938 \t| Accuracy: 109.375\n",
      "# Iteration  2411 -> Loss: 0.5994288601295795 \t| Accuracy: 109.375\n",
      "# Iteration  2412 -> Loss: 0.5992822099517725 \t| Accuracy: 109.375\n",
      "# Iteration  2413 -> Loss: 0.5991356376054983 \t| Accuracy: 109.375\n",
      "# Iteration  2414 -> Loss: 0.5989891431461922 \t| Accuracy: 109.375\n",
      "# Iteration  2415 -> Loss: 0.5988427266297384 \t| Accuracy: 109.375\n",
      "# Iteration  2416 -> Loss: 0.5986963881124612 \t| Accuracy: 109.375\n",
      "# Iteration  2417 -> Loss: 0.5985501276511194 \t| Accuracy: 109.375\n",
      "# Iteration  2418 -> Loss: 0.5984039453028993 \t| Accuracy: 109.375\n",
      "# Iteration  2419 -> Loss: 0.5982578411254077 \t| Accuracy: 109.375\n",
      "# Iteration  2420 -> Loss: 0.5981118151766642 \t| Accuracy: 109.375\n",
      "# Iteration  2421 -> Loss: 0.5979658675150958 \t| Accuracy: 109.375\n",
      "# Iteration  2422 -> Loss: 0.5978199981995284 \t| Accuracy: 109.375\n",
      "# Iteration  2423 -> Loss: 0.5976742072891805 \t| Accuracy: 109.375\n",
      "# Iteration  2424 -> Loss: 0.5975284948436563 \t| Accuracy: 109.375\n",
      "# Iteration  2425 -> Loss: 0.5973828609229387 \t| Accuracy: 109.375\n",
      "# Iteration  2426 -> Loss: 0.5972373055873814 \t| Accuracy: 109.375\n",
      "# Iteration  2427 -> Loss: 0.597091828897703 \t| Accuracy: 109.375\n",
      "# Iteration  2428 -> Loss: 0.5969464309149789 \t| Accuracy: 109.375\n",
      "# Iteration  2429 -> Loss: 0.5968011117006344 \t| Accuracy: 109.375\n",
      "# Iteration  2430 -> Loss: 0.5966558713164384 \t| Accuracy: 109.375\n",
      "# Iteration  2431 -> Loss: 0.596510709824495 \t| Accuracy: 109.375\n",
      "# Iteration  2432 -> Loss: 0.5963656272872371 \t| Accuracy: 109.375\n",
      "# Iteration  2433 -> Loss: 0.5962206237674184 \t| Accuracy: 109.375\n",
      "# Iteration  2434 -> Loss: 0.5960756993281074 \t| Accuracy: 109.375\n",
      "# Iteration  2435 -> Loss: 0.5959308540326795 \t| Accuracy: 109.375\n",
      "# Iteration  2436 -> Loss: 0.5957860879448096 \t| Accuracy: 109.375\n",
      "# Iteration  2437 -> Loss: 0.5956414011284648 \t| Accuracy: 109.375\n",
      "# Iteration  2438 -> Loss: 0.5954967936478976 \t| Accuracy: 109.375\n",
      "# Iteration  2439 -> Loss: 0.5953522655676382 \t| Accuracy: 109.375\n",
      "# Iteration  2440 -> Loss: 0.5952078169524879 \t| Accuracy: 109.375\n",
      "# Iteration  2441 -> Loss: 0.5950634478675104 \t| Accuracy: 109.375\n",
      "# Iteration  2442 -> Loss: 0.5949191583780262 \t| Accuracy: 109.375\n",
      "# Iteration  2443 -> Loss: 0.594774948549604 \t| Accuracy: 109.375\n",
      "# Iteration  2444 -> Loss: 0.5946308184480538 \t| Accuracy: 109.375\n",
      "# Iteration  2445 -> Loss: 0.59448676813942 \t| Accuracy: 109.375\n",
      "# Iteration  2446 -> Loss: 0.5943427976899733 \t| Accuracy: 109.375\n",
      "# Iteration  2447 -> Loss: 0.594198907166204 \t| Accuracy: 109.375\n",
      "# Iteration  2448 -> Loss: 0.5940550966348144 \t| Accuracy: 109.375\n",
      "# Iteration  2449 -> Loss: 0.5939113661627112 \t| Accuracy: 109.375\n",
      "# Iteration  2450 -> Loss: 0.5937677158169986 \t| Accuracy: 109.375\n",
      "# Iteration  2451 -> Loss: 0.5936241456649708 \t| Accuracy: 109.375\n",
      "# Iteration  2452 -> Loss: 0.5934806557741049 \t| Accuracy: 109.375\n",
      "# Iteration  2453 -> Loss: 0.5933372462120524 \t| Accuracy: 109.375\n",
      "# Iteration  2454 -> Loss: 0.5931939170466339 \t| Accuracy: 109.375\n",
      "# Iteration  2455 -> Loss: 0.5930506683458296 \t| Accuracy: 109.375\n",
      "# Iteration  2456 -> Loss: 0.5929075001777737 \t| Accuracy: 109.375\n",
      "# Iteration  2457 -> Loss: 0.592764412610746 \t| Accuracy: 109.375\n",
      "# Iteration  2458 -> Loss: 0.5926214057131647 \t| Accuracy: 109.375\n",
      "# Iteration  2459 -> Loss: 0.59247847955358 \t| Accuracy: 109.375\n",
      "# Iteration  2460 -> Loss: 0.5923356342006654 \t| Accuracy: 109.375\n",
      "# Iteration  2461 -> Loss: 0.5921928697232113 \t| Accuracy: 109.375\n",
      "# Iteration  2462 -> Loss: 0.5920501861901175 \t| Accuracy: 109.375\n",
      "# Iteration  2463 -> Loss: 0.5919075836703859 \t| Accuracy: 109.375\n",
      "# Iteration  2464 -> Loss: 0.5917650622331134 \t| Accuracy: 109.375\n",
      "# Iteration  2465 -> Loss: 0.5916226219474842 \t| Accuracy: 109.375\n",
      "# Iteration  2466 -> Loss: 0.5914802628827625 \t| Accuracy: 109.375\n",
      "# Iteration  2467 -> Loss: 0.5913379851082865 \t| Accuracy: 109.375\n",
      "# Iteration  2468 -> Loss: 0.5911957886934592 \t| Accuracy: 109.375\n",
      "# Iteration  2469 -> Loss: 0.5910536737077429 \t| Accuracy: 109.375\n",
      "# Iteration  2470 -> Loss: 0.5909116402206509 \t| Accuracy: 109.375\n",
      "# Iteration  2471 -> Loss: 0.5907696883017415 \t| Accuracy: 109.375\n",
      "# Iteration  2472 -> Loss: 0.5906278180206087 \t| Accuracy: 109.375\n",
      "# Iteration  2473 -> Loss: 0.590486029446878 \t| Accuracy: 109.375\n",
      "# Iteration  2474 -> Loss: 0.5903443226501967 \t| Accuracy: 109.375\n",
      "# Iteration  2475 -> Loss: 0.5902026977002279 \t| Accuracy: 109.375\n",
      "# Iteration  2476 -> Loss: 0.5900611546666438 \t| Accuracy: 109.375\n",
      "# Iteration  2477 -> Loss: 0.5899196936191178 \t| Accuracy: 109.375\n",
      "# Iteration  2478 -> Loss: 0.5897783146273177 \t| Accuracy: 109.375\n",
      "# Iteration  2479 -> Loss: 0.589637017760899 \t| Accuracy: 109.375\n",
      "# Iteration  2480 -> Loss: 0.589495803089498 \t| Accuracy: 109.375\n",
      "# Iteration  2481 -> Loss: 0.5893546706827237 \t| Accuracy: 109.375\n",
      "# Iteration  2482 -> Loss: 0.5892136206101524 \t| Accuracy: 109.375\n",
      "# Iteration  2483 -> Loss: 0.5890726529413195 \t| Accuracy: 109.375\n",
      "# Iteration  2484 -> Loss: 0.5889317677457138 \t| Accuracy: 109.375\n",
      "# Iteration  2485 -> Loss: 0.5887909650927695 \t| Accuracy: 109.375\n",
      "# Iteration  2486 -> Loss: 0.5886502450518599 \t| Accuracy: 109.375\n",
      "# Iteration  2487 -> Loss: 0.5885096076922907 \t| Accuracy: 109.375\n",
      "# Iteration  2488 -> Loss: 0.5883690530832929 \t| Accuracy: 109.375\n",
      "# Iteration  2489 -> Loss: 0.5882285812940163 \t| Accuracy: 109.375\n",
      "# Iteration  2490 -> Loss: 0.5880881923935227 \t| Accuracy: 109.375\n",
      "# Iteration  2491 -> Loss: 0.5879478864507792 \t| Accuracy: 109.375\n",
      "# Iteration  2492 -> Loss: 0.5878076635346514 \t| Accuracy: 109.375\n",
      "# Iteration  2493 -> Loss: 0.5876675237138971 \t| Accuracy: 109.375\n",
      "# Iteration  2494 -> Loss: 0.5875274670571592 \t| Accuracy: 109.375\n",
      "# Iteration  2495 -> Loss: 0.5873874936329602 \t| Accuracy: 109.375\n",
      "# Iteration  2496 -> Loss: 0.5872476035096942 \t| Accuracy: 109.375\n",
      "# Iteration  2497 -> Loss: 0.5871077967556213 \t| Accuracy: 109.375\n",
      "# Iteration  2498 -> Loss: 0.5869680734388618 \t| Accuracy: 109.375\n",
      "# Iteration  2499 -> Loss: 0.5868284336273878 \t| Accuracy: 109.375\n",
      "# Iteration  2500 -> Loss: 0.5866888773890191 \t| Accuracy: 109.375\n",
      "# Iteration  2501 -> Loss: 0.5865494047914155 \t| Accuracy: 109.375\n",
      "# Iteration  2502 -> Loss: 0.5864100159020702 \t| Accuracy: 109.375\n",
      "# Iteration  2503 -> Loss: 0.5862707107883055 \t| Accuracy: 108.750\n",
      "# Iteration  2504 -> Loss: 0.5861314895172639 \t| Accuracy: 108.750\n",
      "# Iteration  2505 -> Loss: 0.5859923521559041 \t| Accuracy: 108.750\n",
      "# Iteration  2506 -> Loss: 0.585853298770994 \t| Accuracy: 108.750\n",
      "# Iteration  2507 -> Loss: 0.5857143294291045 \t| Accuracy: 108.750\n",
      "# Iteration  2508 -> Loss: 0.5855754441966037 \t| Accuracy: 107.500\n",
      "# Iteration  2509 -> Loss: 0.5854366431396508 \t| Accuracy: 107.500\n",
      "# Iteration  2510 -> Loss: 0.5852979263241903 \t| Accuracy: 107.500\n",
      "# Iteration  2511 -> Loss: 0.5851592938159458 \t| Accuracy: 107.500\n",
      "# Iteration  2512 -> Loss: 0.5850207456804146 \t| Accuracy: 107.500\n",
      "# Iteration  2513 -> Loss: 0.5848822819828612 \t| Accuracy: 107.500\n",
      "# Iteration  2514 -> Loss: 0.5847439027883119 \t| Accuracy: 107.500\n",
      "# Iteration  2515 -> Loss: 0.5846056081615498 \t| Accuracy: 107.500\n",
      "# Iteration  2516 -> Loss: 0.5844673981671076 \t| Accuracy: 107.500\n",
      "# Iteration  2517 -> Loss: 0.584329272869263 \t| Accuracy: 107.500\n",
      "# Iteration  2518 -> Loss: 0.5841912323320329 \t| Accuracy: 107.500\n",
      "# Iteration  2519 -> Loss: 0.5840532766191678 \t| Accuracy: 107.500\n",
      "# Iteration  2520 -> Loss: 0.5839154057941466 \t| Accuracy: 107.500\n",
      "# Iteration  2521 -> Loss: 0.5837776199201706 \t| Accuracy: 107.500\n",
      "# Iteration  2522 -> Loss: 0.5836399190601588 \t| Accuracy: 107.500\n",
      "# Iteration  2523 -> Loss: 0.5835023032767417 \t| Accuracy: 107.500\n",
      "# Iteration  2524 -> Loss: 0.5833647726322566 \t| Accuracy: 107.500\n",
      "# Iteration  2525 -> Loss: 0.5832273271887428 \t| Accuracy: 107.500\n",
      "# Iteration  2526 -> Loss: 0.5830899670079351 \t| Accuracy: 107.500\n",
      "# Iteration  2527 -> Loss: 0.5829526921512602 \t| Accuracy: 107.500\n",
      "# Iteration  2528 -> Loss: 0.5828155026798304 \t| Accuracy: 107.500\n",
      "# Iteration  2529 -> Loss: 0.5826783986544389 \t| Accuracy: 107.500\n",
      "# Iteration  2530 -> Loss: 0.5825413801355555 \t| Accuracy: 107.500\n",
      "# Iteration  2531 -> Loss: 0.5824044471833206 \t| Accuracy: 107.500\n",
      "# Iteration  2532 -> Loss: 0.5822675998575415 \t| Accuracy: 107.500\n",
      "# Iteration  2533 -> Loss: 0.5821308382176862 \t| Accuracy: 107.500\n",
      "# Iteration  2534 -> Loss: 0.5819941623228798 \t| Accuracy: 107.500\n",
      "# Iteration  2535 -> Loss: 0.5818575722318994 \t| Accuracy: 107.500\n",
      "# Iteration  2536 -> Loss: 0.5817210680031695 \t| Accuracy: 107.500\n",
      "# Iteration  2537 -> Loss: 0.5815846496947574 \t| Accuracy: 107.500\n",
      "# Iteration  2538 -> Loss: 0.5814483173643681 \t| Accuracy: 107.500\n",
      "# Iteration  2539 -> Loss: 0.5813120710693411 \t| Accuracy: 107.500\n",
      "# Iteration  2540 -> Loss: 0.5811759108666444 \t| Accuracy: 107.500\n",
      "# Iteration  2541 -> Loss: 0.5810398368128716 \t| Accuracy: 107.500\n",
      "# Iteration  2542 -> Loss: 0.5809038489642366 \t| Accuracy: 107.500\n",
      "# Iteration  2543 -> Loss: 0.5807679473765694 \t| Accuracy: 107.500\n",
      "# Iteration  2544 -> Loss: 0.5806321321053128 \t| Accuracy: 107.500\n",
      "# Iteration  2545 -> Loss: 0.580496403205517 \t| Accuracy: 107.500\n",
      "# Iteration  2546 -> Loss: 0.5803607607318362 \t| Accuracy: 107.500\n",
      "# Iteration  2547 -> Loss: 0.5802252047385249 \t| Accuracy: 107.500\n",
      "# Iteration  2548 -> Loss: 0.580089735279433 \t| Accuracy: 107.500\n",
      "# Iteration  2549 -> Loss: 0.5799543524080026 \t| Accuracy: 107.500\n",
      "# Iteration  2550 -> Loss: 0.5798190561772643 \t| Accuracy: 107.500\n",
      "# Iteration  2551 -> Loss: 0.5796838466398321 \t| Accuracy: 107.500\n",
      "# Iteration  2552 -> Loss: 0.5795487238479015 \t| Accuracy: 107.500\n",
      "# Iteration  2553 -> Loss: 0.5794136878532443 \t| Accuracy: 107.500\n",
      "# Iteration  2554 -> Loss: 0.5792787387072061 \t| Accuracy: 107.500\n",
      "# Iteration  2555 -> Loss: 0.5791438764607018 \t| Accuracy: 107.500\n",
      "# Iteration  2556 -> Loss: 0.5790091011642123 \t| Accuracy: 107.500\n",
      "# Iteration  2557 -> Loss: 0.5788744128677822 \t| Accuracy: 107.500\n",
      "# Iteration  2558 -> Loss: 0.5787398116210142 \t| Accuracy: 107.500\n",
      "# Iteration  2559 -> Loss: 0.5786052974730679 \t| Accuracy: 107.500\n",
      "# Iteration  2560 -> Loss: 0.578470870472655 \t| Accuracy: 107.500\n",
      "# Iteration  2561 -> Loss: 0.5783365306680374 \t| Accuracy: 107.500\n",
      "# Iteration  2562 -> Loss: 0.5782022781070227 \t| Accuracy: 107.500\n",
      "# Iteration  2563 -> Loss: 0.5780681128369622 \t| Accuracy: 107.500\n",
      "# Iteration  2564 -> Loss: 0.577934034904747 \t| Accuracy: 107.500\n",
      "# Iteration  2565 -> Loss: 0.5778000443568059 \t| Accuracy: 107.500\n",
      "# Iteration  2566 -> Loss: 0.5776661412391015 \t| Accuracy: 107.500\n",
      "# Iteration  2567 -> Loss: 0.577532325597128 \t| Accuracy: 107.500\n",
      "# Iteration  2568 -> Loss: 0.5773985974759083 \t| Accuracy: 107.500\n",
      "# Iteration  2569 -> Loss: 0.5772649569199911 \t| Accuracy: 107.500\n",
      "# Iteration  2570 -> Loss: 0.5771314039734481 \t| Accuracy: 107.500\n",
      "# Iteration  2571 -> Loss: 0.5769979386798715 \t| Accuracy: 107.500\n",
      "# Iteration  2572 -> Loss: 0.5768645610823716 \t| Accuracy: 107.500\n",
      "# Iteration  2573 -> Loss: 0.5767312712235741 \t| Accuracy: 107.500\n",
      "# Iteration  2574 -> Loss: 0.5765980691456176 \t| Accuracy: 107.500\n",
      "# Iteration  2575 -> Loss: 0.5764649548901509 \t| Accuracy: 107.500\n",
      "# Iteration  2576 -> Loss: 0.5763319284983316 \t| Accuracy: 107.500\n",
      "# Iteration  2577 -> Loss: 0.5761989900108226 \t| Accuracy: 107.500\n",
      "# Iteration  2578 -> Loss: 0.5760661394677907 \t| Accuracy: 107.500\n",
      "# Iteration  2579 -> Loss: 0.5759333769089043 \t| Accuracy: 106.875\n",
      "# Iteration  2580 -> Loss: 0.5758007023733307 \t| Accuracy: 106.875\n",
      "# Iteration  2581 -> Loss: 0.5756681158997349 \t| Accuracy: 106.875\n",
      "# Iteration  2582 -> Loss: 0.5755356175262769 \t| Accuracy: 106.875\n",
      "# Iteration  2583 -> Loss: 0.5754032072906101 \t| Accuracy: 106.875\n",
      "# Iteration  2584 -> Loss: 0.5752708852298796 \t| Accuracy: 106.875\n",
      "# Iteration  2585 -> Loss: 0.5751386513807191 \t| Accuracy: 106.875\n",
      "# Iteration  2586 -> Loss: 0.5750065057792509 \t| Accuracy: 106.875\n",
      "# Iteration  2587 -> Loss: 0.5748744484610834 \t| Accuracy: 106.875\n",
      "# Iteration  2588 -> Loss: 0.5747424794613089 \t| Accuracy: 106.875\n",
      "# Iteration  2589 -> Loss: 0.5746105988145023 \t| Accuracy: 106.875\n",
      "# Iteration  2590 -> Loss: 0.5744788065547202 \t| Accuracy: 106.875\n",
      "# Iteration  2591 -> Loss: 0.5743471027154984 \t| Accuracy: 106.875\n",
      "# Iteration  2592 -> Loss: 0.5742154873298512 \t| Accuracy: 106.875\n",
      "# Iteration  2593 -> Loss: 0.5740839604302692 \t| Accuracy: 106.875\n",
      "# Iteration  2594 -> Loss: 0.573952522048719 \t| Accuracy: 106.875\n",
      "# Iteration  2595 -> Loss: 0.5738211722166405 \t| Accuracy: 106.875\n",
      "# Iteration  2596 -> Loss: 0.5736899109649475 \t| Accuracy: 106.875\n",
      "# Iteration  2597 -> Loss: 0.5735587383240244 \t| Accuracy: 106.875\n",
      "# Iteration  2598 -> Loss: 0.5734276543237267 \t| Accuracy: 106.875\n",
      "# Iteration  2599 -> Loss: 0.573296658993379 \t| Accuracy: 106.875\n",
      "# Iteration  2600 -> Loss: 0.5731657523617749 \t| Accuracy: 106.875\n",
      "# Iteration  2601 -> Loss: 0.5730349344571745 \t| Accuracy: 106.875\n",
      "# Iteration  2602 -> Loss: 0.5729042053073047 \t| Accuracy: 106.875\n",
      "# Iteration  2603 -> Loss: 0.5727735649393579 \t| Accuracy: 106.875\n",
      "# Iteration  2604 -> Loss: 0.5726430133799916 \t| Accuracy: 106.875\n",
      "# Iteration  2605 -> Loss: 0.5725125506553262 \t| Accuracy: 106.875\n",
      "# Iteration  2606 -> Loss: 0.5723821767909465 \t| Accuracy: 106.875\n",
      "# Iteration  2607 -> Loss: 0.5722518918118987 \t| Accuracy: 106.875\n",
      "# Iteration  2608 -> Loss: 0.5721216957426918 \t| Accuracy: 106.875\n",
      "# Iteration  2609 -> Loss: 0.5719915886072947 \t| Accuracy: 106.875\n",
      "# Iteration  2610 -> Loss: 0.5718615704291385 \t| Accuracy: 106.875\n",
      "# Iteration  2611 -> Loss: 0.5717316412311131 \t| Accuracy: 106.875\n",
      "# Iteration  2612 -> Loss: 0.5716018010355692 \t| Accuracy: 106.875\n",
      "# Iteration  2613 -> Loss: 0.5714720498643159 \t| Accuracy: 106.875\n",
      "# Iteration  2614 -> Loss: 0.5713423877386218 \t| Accuracy: 106.875\n",
      "# Iteration  2615 -> Loss: 0.5712128146792141 \t| Accuracy: 106.875\n",
      "# Iteration  2616 -> Loss: 0.571083330706278 \t| Accuracy: 106.875\n",
      "# Iteration  2617 -> Loss: 0.5709539358394572 \t| Accuracy: 106.875\n",
      "# Iteration  2618 -> Loss: 0.570824630097853 \t| Accuracy: 106.875\n",
      "# Iteration  2619 -> Loss: 0.5706954135000247 \t| Accuracy: 106.875\n",
      "# Iteration  2620 -> Loss: 0.5705662860639896 \t| Accuracy: 106.875\n",
      "# Iteration  2621 -> Loss: 0.570437247807222 \t| Accuracy: 106.875\n",
      "# Iteration  2622 -> Loss: 0.5703082987466545 \t| Accuracy: 106.875\n",
      "# Iteration  2623 -> Loss: 0.5701794388986765 \t| Accuracy: 106.875\n",
      "# Iteration  2624 -> Loss: 0.5700506682791364 \t| Accuracy: 106.875\n",
      "# Iteration  2625 -> Loss: 0.5699219869033392 \t| Accuracy: 106.875\n",
      "# Iteration  2626 -> Loss: 0.569793394786049 \t| Accuracy: 106.875\n",
      "# Iteration  2627 -> Loss: 0.5696648919414874 \t| Accuracy: 106.875\n",
      "# Iteration  2628 -> Loss: 0.5695364783833352 \t| Accuracy: 106.875\n",
      "# Iteration  2629 -> Loss: 0.5694081541247307 \t| Accuracy: 106.875\n",
      "# Iteration  2630 -> Loss: 0.569279919178273 \t| Accuracy: 106.875\n",
      "# Iteration  2631 -> Loss: 0.5691517735560192 \t| Accuracy: 106.875\n",
      "# Iteration  2632 -> Loss: 0.5690237172694869 \t| Accuracy: 106.875\n",
      "# Iteration  2633 -> Loss: 0.5688957503296538 \t| Accuracy: 106.875\n",
      "# Iteration  2634 -> Loss: 0.5687678727469583 \t| Accuracy: 106.875\n",
      "# Iteration  2635 -> Loss: 0.5686400845313001 \t| Accuracy: 106.875\n",
      "# Iteration  2636 -> Loss: 0.5685123856920409 \t| Accuracy: 106.875\n",
      "# Iteration  2637 -> Loss: 0.5683847762380044 \t| Accuracy: 106.875\n",
      "# Iteration  2638 -> Loss: 0.5682572561774775 \t| Accuracy: 106.875\n",
      "# Iteration  2639 -> Loss: 0.5681298255182108 \t| Accuracy: 106.875\n",
      "# Iteration  2640 -> Loss: 0.5680024842674191 \t| Accuracy: 106.875\n",
      "# Iteration  2641 -> Loss: 0.5678752324317821 \t| Accuracy: 106.875\n",
      "# Iteration  2642 -> Loss: 0.5677480700174459 \t| Accuracy: 106.875\n",
      "# Iteration  2643 -> Loss: 0.5676209970300227 \t| Accuracy: 106.875\n",
      "# Iteration  2644 -> Loss: 0.567494013474592 \t| Accuracy: 106.875\n",
      "# Iteration  2645 -> Loss: 0.5673671193557024 \t| Accuracy: 106.875\n",
      "# Iteration  2646 -> Loss: 0.5672403146773705 \t| Accuracy: 106.875\n",
      "# Iteration  2647 -> Loss: 0.5671135994430839 \t| Accuracy: 106.875\n",
      "# Iteration  2648 -> Loss: 0.5669869736558009 \t| Accuracy: 106.875\n",
      "# Iteration  2649 -> Loss: 0.5668604373179517 \t| Accuracy: 106.875\n",
      "# Iteration  2650 -> Loss: 0.5667339904314398 \t| Accuracy: 106.875\n",
      "# Iteration  2651 -> Loss: 0.5666076329976428 \t| Accuracy: 106.875\n",
      "# Iteration  2652 -> Loss: 0.566481365017413 \t| Accuracy: 106.875\n",
      "# Iteration  2653 -> Loss: 0.5663551864910792 \t| Accuracy: 106.875\n",
      "# Iteration  2654 -> Loss: 0.566229097418448 \t| Accuracy: 106.875\n",
      "# Iteration  2655 -> Loss: 0.5661030977988036 \t| Accuracy: 106.875\n",
      "# Iteration  2656 -> Loss: 0.5659771876309108 \t| Accuracy: 106.875\n",
      "# Iteration  2657 -> Loss: 0.5658513669130146 \t| Accuracy: 106.875\n",
      "# Iteration  2658 -> Loss: 0.5657256356428426 \t| Accuracy: 106.875\n",
      "# Iteration  2659 -> Loss: 0.5655999938176057 \t| Accuracy: 106.875\n",
      "# Iteration  2660 -> Loss: 0.5654744414339994 \t| Accuracy: 106.875\n",
      "# Iteration  2661 -> Loss: 0.5653489784882053 \t| Accuracy: 106.875\n",
      "# Iteration  2662 -> Loss: 0.5652236049758924 \t| Accuracy: 106.875\n",
      "# Iteration  2663 -> Loss: 0.5650983208922182 \t| Accuracy: 106.875\n",
      "# Iteration  2664 -> Loss: 0.5649731262318305 \t| Accuracy: 106.875\n",
      "# Iteration  2665 -> Loss: 0.5648480209888684 \t| Accuracy: 106.875\n",
      "# Iteration  2666 -> Loss: 0.564723005156964 \t| Accuracy: 106.875\n",
      "# Iteration  2667 -> Loss: 0.5645980787292442 \t| Accuracy: 106.875\n",
      "# Iteration  2668 -> Loss: 0.5644732416983309 \t| Accuracy: 106.875\n",
      "# Iteration  2669 -> Loss: 0.5643484940563445 \t| Accuracy: 106.875\n",
      "# Iteration  2670 -> Loss: 0.5642238357949031 \t| Accuracy: 106.875\n",
      "# Iteration  2671 -> Loss: 0.5640992669051258 \t| Accuracy: 106.875\n",
      "# Iteration  2672 -> Loss: 0.5639747873776338 \t| Accuracy: 106.875\n",
      "# Iteration  2673 -> Loss: 0.5638503972025519 \t| Accuracy: 106.875\n",
      "# Iteration  2674 -> Loss: 0.5637260963695095 \t| Accuracy: 106.875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Iteration  2675 -> Loss: 0.5636018848676437 \t| Accuracy: 106.875\n",
      "# Iteration  2676 -> Loss: 0.563477762685599 \t| Accuracy: 106.875\n",
      "# Iteration  2677 -> Loss: 0.5633537298115305 \t| Accuracy: 106.875\n",
      "# Iteration  2678 -> Loss: 0.5632297862331053 \t| Accuracy: 106.875\n",
      "# Iteration  2679 -> Loss: 0.5631059319375034 \t| Accuracy: 106.875\n",
      "# Iteration  2680 -> Loss: 0.5629821669114201 \t| Accuracy: 106.875\n",
      "# Iteration  2681 -> Loss: 0.5628584911410679 \t| Accuracy: 106.875\n",
      "# Iteration  2682 -> Loss: 0.5627349046121777 \t| Accuracy: 106.875\n",
      "# Iteration  2683 -> Loss: 0.5626114073100007 \t| Accuracy: 106.875\n",
      "# Iteration  2684 -> Loss: 0.5624879992193103 \t| Accuracy: 106.875\n",
      "# Iteration  2685 -> Loss: 0.5623646803244038 \t| Accuracy: 106.875\n",
      "# Iteration  2686 -> Loss: 0.5622414506091042 \t| Accuracy: 106.875\n",
      "# Iteration  2687 -> Loss: 0.5621183100567623 \t| Accuracy: 106.875\n",
      "# Iteration  2688 -> Loss: 0.5619952586502581 \t| Accuracy: 106.875\n",
      "# Iteration  2689 -> Loss: 0.5618722963720026 \t| Accuracy: 106.875\n",
      "# Iteration  2690 -> Loss: 0.5617494232039405 \t| Accuracy: 106.875\n",
      "# Iteration  2691 -> Loss: 0.561626639127551 \t| Accuracy: 106.875\n",
      "# Iteration  2692 -> Loss: 0.5615039441238497 \t| Accuracy: 106.875\n",
      "# Iteration  2693 -> Loss: 0.5613813381733921 \t| Accuracy: 106.875\n",
      "# Iteration  2694 -> Loss: 0.5612588212562732 \t| Accuracy: 106.875\n",
      "# Iteration  2695 -> Loss: 0.5611363933521314 \t| Accuracy: 106.875\n",
      "# Iteration  2696 -> Loss: 0.561014054440149 \t| Accuracy: 106.875\n",
      "# Iteration  2697 -> Loss: 0.560891804499055 \t| Accuracy: 106.875\n",
      "# Iteration  2698 -> Loss: 0.5607696435071267 \t| Accuracy: 106.875\n",
      "# Iteration  2699 -> Loss: 0.5606475714421916 \t| Accuracy: 106.875\n",
      "# Iteration  2700 -> Loss: 0.5605255882816299 \t| Accuracy: 106.875\n",
      "# Iteration  2701 -> Loss: 0.5604036940023758 \t| Accuracy: 106.875\n",
      "# Iteration  2702 -> Loss: 0.5602818885809197 \t| Accuracy: 106.875\n",
      "# Iteration  2703 -> Loss: 0.5601601719933103 \t| Accuracy: 106.875\n",
      "# Iteration  2704 -> Loss: 0.5600385442151566 \t| Accuracy: 106.875\n",
      "# Iteration  2705 -> Loss: 0.5599170052216301 \t| Accuracy: 106.875\n",
      "# Iteration  2706 -> Loss: 0.5597955549874665 \t| Accuracy: 106.875\n",
      "# Iteration  2707 -> Loss: 0.5596741934869675 \t| Accuracy: 106.875\n",
      "# Iteration  2708 -> Loss: 0.5595529206940033 \t| Accuracy: 106.875\n",
      "# Iteration  2709 -> Loss: 0.5594317365820148 \t| Accuracy: 106.875\n",
      "# Iteration  2710 -> Loss: 0.5593106411240155 \t| Accuracy: 106.875\n",
      "# Iteration  2711 -> Loss: 0.5591896342925925 \t| Accuracy: 106.875\n",
      "# Iteration  2712 -> Loss: 0.5590687160599105 \t| Accuracy: 106.875\n",
      "# Iteration  2713 -> Loss: 0.5589478863977121 \t| Accuracy: 106.875\n",
      "# Iteration  2714 -> Loss: 0.5588271452773211 \t| Accuracy: 106.875\n",
      "# Iteration  2715 -> Loss: 0.5587064926696437 \t| Accuracy: 106.875\n",
      "# Iteration  2716 -> Loss: 0.5585859285451713 \t| Accuracy: 106.875\n",
      "# Iteration  2717 -> Loss: 0.5584654528739822 \t| Accuracy: 106.875\n",
      "# Iteration  2718 -> Loss: 0.5583450656257436 \t| Accuracy: 106.875\n",
      "# Iteration  2719 -> Loss: 0.5582247667697137 \t| Accuracy: 106.875\n",
      "# Iteration  2720 -> Loss: 0.5581045562747443 \t| Accuracy: 106.875\n",
      "# Iteration  2721 -> Loss: 0.5579844341092824 \t| Accuracy: 106.875\n",
      "# Iteration  2722 -> Loss: 0.5578644002413724 \t| Accuracy: 106.875\n",
      "# Iteration  2723 -> Loss: 0.5577444546386582 \t| Accuracy: 106.875\n",
      "# Iteration  2724 -> Loss: 0.5576245972683854 \t| Accuracy: 106.875\n",
      "# Iteration  2725 -> Loss: 0.5575048280974036 \t| Accuracy: 106.875\n",
      "# Iteration  2726 -> Loss: 0.5573851470921681 \t| Accuracy: 106.875\n",
      "# Iteration  2727 -> Loss: 0.5572655542187421 \t| Accuracy: 106.875\n",
      "# Iteration  2728 -> Loss: 0.5571460494427993 \t| Accuracy: 106.875\n",
      "# Iteration  2729 -> Loss: 0.5570266327296247 \t| Accuracy: 106.875\n",
      "# Iteration  2730 -> Loss: 0.5569073040441194 \t| Accuracy: 106.875\n",
      "# Iteration  2731 -> Loss: 0.5567880633507993 \t| Accuracy: 106.875\n",
      "# Iteration  2732 -> Loss: 0.5566689106137999 \t| Accuracy: 106.875\n",
      "# Iteration  2733 -> Loss: 0.556549845796877 \t| Accuracy: 106.875\n",
      "# Iteration  2734 -> Loss: 0.5564308688634098 \t| Accuracy: 106.875\n",
      "# Iteration  2735 -> Loss: 0.5563119797764017 \t| Accuracy: 106.875\n",
      "# Iteration  2736 -> Loss: 0.5561931784984842 \t| Accuracy: 106.875\n",
      "# Iteration  2737 -> Loss: 0.5560744649919172 \t| Accuracy: 106.875\n",
      "# Iteration  2738 -> Loss: 0.5559558392185925 \t| Accuracy: 106.875\n",
      "# Iteration  2739 -> Loss: 0.5558373011400354 \t| Accuracy: 106.875\n",
      "# Iteration  2740 -> Loss: 0.5557188507174066 \t| Accuracy: 106.875\n",
      "# Iteration  2741 -> Loss: 0.5556004879115048 \t| Accuracy: 106.875\n",
      "# Iteration  2742 -> Loss: 0.5554822126827685 \t| Accuracy: 106.875\n",
      "# Iteration  2743 -> Loss: 0.5553640249912783 \t| Accuracy: 106.875\n",
      "# Iteration  2744 -> Loss: 0.5552459247967588 \t| Accuracy: 106.875\n",
      "# Iteration  2745 -> Loss: 0.5551279120585813 \t| Accuracy: 106.875\n",
      "# Iteration  2746 -> Loss: 0.5550099867357648 \t| Accuracy: 106.875\n",
      "# Iteration  2747 -> Loss: 0.5548921487869793 \t| Accuracy: 106.875\n",
      "# Iteration  2748 -> Loss: 0.5547743981705474 \t| Accuracy: 106.875\n",
      "# Iteration  2749 -> Loss: 0.5546567348444464 \t| Accuracy: 106.875\n",
      "# Iteration  2750 -> Loss: 0.5545391587663103 \t| Accuracy: 106.875\n",
      "# Iteration  2751 -> Loss: 0.5544216698934324 \t| Accuracy: 106.875\n",
      "# Iteration  2752 -> Loss: 0.5543042681827668 \t| Accuracy: 106.875\n",
      "# Iteration  2753 -> Loss: 0.554186953590931 \t| Accuracy: 106.875\n",
      "# Iteration  2754 -> Loss: 0.5540697260742075 \t| Accuracy: 106.875\n",
      "# Iteration  2755 -> Loss: 0.5539525855885465 \t| Accuracy: 106.875\n",
      "# Iteration  2756 -> Loss: 0.5538355320895678 \t| Accuracy: 106.875\n",
      "# Iteration  2757 -> Loss: 0.553718565532562 \t| Accuracy: 106.875\n",
      "# Iteration  2758 -> Loss: 0.5536016858724948 \t| Accuracy: 106.875\n",
      "# Iteration  2759 -> Loss: 0.5534848930640062 \t| Accuracy: 106.875\n",
      "# Iteration  2760 -> Loss: 0.5533681870614153 \t| Accuracy: 106.875\n",
      "# Iteration  2761 -> Loss: 0.55325156781872 \t| Accuracy: 106.875\n",
      "# Iteration  2762 -> Loss: 0.5531350352896013 \t| Accuracy: 106.875\n",
      "# Iteration  2763 -> Loss: 0.5530185894274235 \t| Accuracy: 106.875\n",
      "# Iteration  2764 -> Loss: 0.5529022301852378 \t| Accuracy: 106.875\n",
      "# Iteration  2765 -> Loss: 0.5527859575157829 \t| Accuracy: 106.875\n",
      "# Iteration  2766 -> Loss: 0.5526697713714883 \t| Accuracy: 106.875\n",
      "# Iteration  2767 -> Loss: 0.552553671704476 \t| Accuracy: 106.875\n",
      "# Iteration  2768 -> Loss: 0.552437658466562 \t| Accuracy: 106.875\n",
      "# Iteration  2769 -> Loss: 0.552321731609259 \t| Accuracy: 106.875\n",
      "# Iteration  2770 -> Loss: 0.5522058910837783 \t| Accuracy: 106.875\n",
      "# Iteration  2771 -> Loss: 0.5520901368410319 \t| Accuracy: 106.875\n",
      "# Iteration  2772 -> Loss: 0.5519744688316341 \t| Accuracy: 106.875\n",
      "# Iteration  2773 -> Loss: 0.5518588870059044 \t| Accuracy: 106.875\n",
      "# Iteration  2774 -> Loss: 0.5517433913138682 \t| Accuracy: 106.875\n",
      "# Iteration  2775 -> Loss: 0.5516279817052605 \t| Accuracy: 106.875\n",
      "# Iteration  2776 -> Loss: 0.5515126581295265 \t| Accuracy: 106.875\n",
      "# Iteration  2777 -> Loss: 0.5513974205358245 \t| Accuracy: 106.875\n",
      "# Iteration  2778 -> Loss: 0.5512822688730272 \t| Accuracy: 106.875\n",
      "# Iteration  2779 -> Loss: 0.5511672030897246 \t| Accuracy: 106.875\n",
      "# Iteration  2780 -> Loss: 0.551052223134225 \t| Accuracy: 106.875\n",
      "# Iteration  2781 -> Loss: 0.550937328954558 \t| Accuracy: 106.875\n",
      "# Iteration  2782 -> Loss: 0.5508225204984759 \t| Accuracy: 106.875\n",
      "# Iteration  2783 -> Loss: 0.5507077977134556 \t| Accuracy: 106.875\n",
      "# Iteration  2784 -> Loss: 0.5505931605467007 \t| Accuracy: 106.875\n",
      "# Iteration  2785 -> Loss: 0.5504786089451438 \t| Accuracy: 106.875\n",
      "# Iteration  2786 -> Loss: 0.5503641428554484 \t| Accuracy: 106.875\n",
      "# Iteration  2787 -> Loss: 0.5502497622240106 \t| Accuracy: 106.875\n",
      "# Iteration  2788 -> Loss: 0.5501354669969607 \t| Accuracy: 106.875\n",
      "# Iteration  2789 -> Loss: 0.5500212571201663 \t| Accuracy: 106.875\n",
      "# Iteration  2790 -> Loss: 0.5499071325392334 \t| Accuracy: 106.875\n",
      "# Iteration  2791 -> Loss: 0.5497930931995085 \t| Accuracy: 106.875\n",
      "# Iteration  2792 -> Loss: 0.5496791390460807 \t| Accuracy: 106.875\n",
      "# Iteration  2793 -> Loss: 0.5495652700237832 \t| Accuracy: 106.875\n",
      "# Iteration  2794 -> Loss: 0.5494514860771962 \t| Accuracy: 106.875\n",
      "# Iteration  2795 -> Loss: 0.5493377871506476 \t| Accuracy: 106.875\n",
      "# Iteration  2796 -> Loss: 0.549224173188216 \t| Accuracy: 106.875\n",
      "# Iteration  2797 -> Loss: 0.5491106441337318 \t| Accuracy: 106.875\n",
      "# Iteration  2798 -> Loss: 0.5489971999307796 \t| Accuracy: 106.875\n",
      "# Iteration  2799 -> Loss: 0.5488838405227002 \t| Accuracy: 106.875\n",
      "# Iteration  2800 -> Loss: 0.5487705658525921 \t| Accuracy: 106.875\n",
      "# Iteration  2801 -> Loss: 0.5486573758633135 \t| Accuracy: 106.875\n",
      "# Iteration  2802 -> Loss: 0.5485442704974843 \t| Accuracy: 106.875\n",
      "# Iteration  2803 -> Loss: 0.5484312496974882 \t| Accuracy: 106.875\n",
      "# Iteration  2804 -> Loss: 0.5483183134054741 \t| Accuracy: 106.875\n",
      "# Iteration  2805 -> Loss: 0.5482054615633584 \t| Accuracy: 106.875\n",
      "# Iteration  2806 -> Loss: 0.5480926941128265 \t| Accuracy: 106.875\n",
      "# Iteration  2807 -> Loss: 0.5479800109953352 \t| Accuracy: 106.875\n",
      "# Iteration  2808 -> Loss: 0.5478674121521139 \t| Accuracy: 106.875\n",
      "# Iteration  2809 -> Loss: 0.547754897524167 \t| Accuracy: 106.875\n",
      "# Iteration  2810 -> Loss: 0.5476424670522755 \t| Accuracy: 106.875\n",
      "# Iteration  2811 -> Loss: 0.5475301206769987 \t| Accuracy: 106.875\n",
      "# Iteration  2812 -> Loss: 0.5474178583386764 \t| Accuracy: 106.875\n",
      "# Iteration  2813 -> Loss: 0.5473056799774308 \t| Accuracy: 106.875\n",
      "# Iteration  2814 -> Loss: 0.5471935855331675 \t| Accuracy: 106.875\n",
      "# Iteration  2815 -> Loss: 0.5470815749455786 \t| Accuracy: 106.875\n",
      "# Iteration  2816 -> Loss: 0.5469696481541433 \t| Accuracy: 107.083\n",
      "# Iteration  2817 -> Loss: 0.5468578050981305 \t| Accuracy: 107.083\n",
      "# Iteration  2818 -> Loss: 0.5467460457166003 \t| Accuracy: 107.083\n",
      "# Iteration  2819 -> Loss: 0.5466343699484059 \t| Accuracy: 107.083\n",
      "# Iteration  2820 -> Loss: 0.5465227777321952 \t| Accuracy: 107.083\n",
      "# Iteration  2821 -> Loss: 0.546411269006413 \t| Accuracy: 107.083\n",
      "# Iteration  2822 -> Loss: 0.5462998437093025 \t| Accuracy: 107.083\n",
      "# Iteration  2823 -> Loss: 0.5461885017789069 \t| Accuracy: 107.083\n",
      "# Iteration  2824 -> Loss: 0.5460772431530716 \t| Accuracy: 107.083\n",
      "# Iteration  2825 -> Loss: 0.5459660677694457 \t| Accuracy: 107.083\n",
      "# Iteration  2826 -> Loss: 0.5458549755654838 \t| Accuracy: 107.083\n",
      "# Iteration  2827 -> Loss: 0.5457439664784481 \t| Accuracy: 107.083\n",
      "# Iteration  2828 -> Loss: 0.5456330404454095 \t| Accuracy: 107.083\n",
      "# Iteration  2829 -> Loss: 0.5455221974032499 \t| Accuracy: 107.083\n",
      "# Iteration  2830 -> Loss: 0.5454114372886634 \t| Accuracy: 107.083\n",
      "# Iteration  2831 -> Loss: 0.5453007600381587 \t| Accuracy: 107.083\n",
      "# Iteration  2832 -> Loss: 0.5451901655880609 \t| Accuracy: 107.083\n",
      "# Iteration  2833 -> Loss: 0.545079653874512 \t| Accuracy: 107.083\n",
      "# Iteration  2834 -> Loss: 0.5449692248334742 \t| Accuracy: 107.083\n",
      "# Iteration  2835 -> Loss: 0.5448588784007304 \t| Accuracy: 107.083\n",
      "# Iteration  2836 -> Loss: 0.5447486145118869 \t| Accuracy: 107.083\n",
      "# Iteration  2837 -> Loss: 0.5446384331023744 \t| Accuracy: 107.083\n",
      "# Iteration  2838 -> Loss: 0.5445283341074496 \t| Accuracy: 107.083\n",
      "# Iteration  2839 -> Loss: 0.5444183174621978 \t| Accuracy: 107.083\n",
      "# Iteration  2840 -> Loss: 0.5443083831015336 \t| Accuracy: 107.083\n",
      "# Iteration  2841 -> Loss: 0.5441985309602034 \t| Accuracy: 107.083\n",
      "# Iteration  2842 -> Loss: 0.5440887609727861 \t| Accuracy: 107.083\n",
      "# Iteration  2843 -> Loss: 0.5439790730736964 \t| Accuracy: 107.083\n",
      "# Iteration  2844 -> Loss: 0.5438694671971842 \t| Accuracy: 107.083\n",
      "# Iteration  2845 -> Loss: 0.5437599432773383 \t| Accuracy: 107.083\n",
      "# Iteration  2846 -> Loss: 0.5436505012480873 \t| Accuracy: 107.083\n",
      "# Iteration  2847 -> Loss: 0.543541141043201 \t| Accuracy: 107.083\n",
      "# Iteration  2848 -> Loss: 0.5434318625962922 \t| Accuracy: 107.083\n",
      "# Iteration  2849 -> Loss: 0.5433226658408188 \t| Accuracy: 107.083\n",
      "# Iteration  2850 -> Loss: 0.5432135507100849 \t| Accuracy: 107.083\n",
      "# Iteration  2851 -> Loss: 0.5431045171372426 \t| Accuracy: 107.083\n",
      "# Iteration  2852 -> Loss: 0.5429955650552939 \t| Accuracy: 107.083\n",
      "# Iteration  2853 -> Loss: 0.5428866943970917 \t| Accuracy: 107.083\n",
      "# Iteration  2854 -> Loss: 0.5427779050953423 \t| Accuracy: 107.083\n",
      "# Iteration  2855 -> Loss: 0.5426691970826061 \t| Accuracy: 107.083\n",
      "# Iteration  2856 -> Loss: 0.5425605702913001 \t| Accuracy: 107.083\n",
      "# Iteration  2857 -> Loss: 0.5424520246536988 \t| Accuracy: 107.083\n",
      "# Iteration  2858 -> Loss: 0.542343560101936 \t| Accuracy: 107.083\n",
      "# Iteration  2859 -> Loss: 0.5422351765680068 \t| Accuracy: 107.083\n",
      "# Iteration  2860 -> Loss: 0.5421268739837688 \t| Accuracy: 107.083\n",
      "# Iteration  2861 -> Loss: 0.5420186522809434 \t| Accuracy: 107.083\n",
      "# Iteration  2862 -> Loss: 0.5419105113911183 \t| Accuracy: 107.083\n",
      "# Iteration  2863 -> Loss: 0.5418024512457484 \t| Accuracy: 107.083\n",
      "# Iteration  2864 -> Loss: 0.5416944717761571 \t| Accuracy: 107.083\n",
      "# Iteration  2865 -> Loss: 0.5415865729135391 \t| Accuracy: 107.083\n",
      "# Iteration  2866 -> Loss: 0.5414787545889601 \t| Accuracy: 107.083\n",
      "# Iteration  2867 -> Loss: 0.5413710167333607 \t| Accuracy: 107.083\n",
      "# Iteration  2868 -> Loss: 0.5412633592775556 \t| Accuracy: 107.083\n",
      "# Iteration  2869 -> Loss: 0.5411557821522368 \t| Accuracy: 107.083\n",
      "# Iteration  2870 -> Loss: 0.5410482852879746 \t| Accuracy: 107.083\n",
      "# Iteration  2871 -> Loss: 0.5409408686152188 \t| Accuracy: 107.083\n",
      "# Iteration  2872 -> Loss: 0.540833532064301 \t| Accuracy: 107.083\n",
      "# Iteration  2873 -> Loss: 0.5407262755654355 \t| Accuracy: 107.083\n",
      "# Iteration  2874 -> Loss: 0.5406190990487212 \t| Accuracy: 107.083\n",
      "# Iteration  2875 -> Loss: 0.5405120024441428 \t| Accuracy: 107.083\n",
      "# Iteration  2876 -> Loss: 0.5404049856815725 \t| Accuracy: 107.083\n",
      "# Iteration  2877 -> Loss: 0.5402980486907718 \t| Accuracy: 107.083\n",
      "# Iteration  2878 -> Loss: 0.5401911914013924 \t| Accuracy: 107.083\n",
      "# Iteration  2879 -> Loss: 0.5400844137429781 \t| Accuracy: 107.083\n",
      "# Iteration  2880 -> Loss: 0.5399777156449668 \t| Accuracy: 107.083\n",
      "# Iteration  2881 -> Loss: 0.5398710970366909 \t| Accuracy: 107.083\n",
      "# Iteration  2882 -> Loss: 0.5397645578473791 \t| Accuracy: 107.083\n",
      "# Iteration  2883 -> Loss: 0.539658098006159 \t| Accuracy: 107.083\n",
      "# Iteration  2884 -> Loss: 0.5395517174420571 \t| Accuracy: 107.083\n",
      "# Iteration  2885 -> Loss: 0.539445416084001 \t| Accuracy: 107.083\n",
      "# Iteration  2886 -> Loss: 0.5393391938608209 \t| Accuracy: 106.875\n",
      "# Iteration  2887 -> Loss: 0.5392330507012508 \t| Accuracy: 106.875\n",
      "# Iteration  2888 -> Loss: 0.5391269865339305 \t| Accuracy: 106.875\n",
      "# Iteration  2889 -> Loss: 0.5390210012874065 \t| Accuracy: 106.875\n",
      "# Iteration  2890 -> Loss: 0.5389150948901336 \t| Accuracy: 106.875\n",
      "# Iteration  2891 -> Loss: 0.5388092672704762 \t| Accuracy: 106.875\n",
      "# Iteration  2892 -> Loss: 0.5387035183567104 \t| Accuracy: 106.875\n",
      "# Iteration  2893 -> Loss: 0.5385978480770248 \t| Accuracy: 106.875\n",
      "# Iteration  2894 -> Loss: 0.5384922563595221 \t| Accuracy: 106.875\n",
      "# Iteration  2895 -> Loss: 0.538386743132221 \t| Accuracy: 106.875\n",
      "# Iteration  2896 -> Loss: 0.5382813083230563 \t| Accuracy: 106.875\n",
      "# Iteration  2897 -> Loss: 0.5381759518598827 \t| Accuracy: 106.875\n",
      "# Iteration  2898 -> Loss: 0.5380706736704731 \t| Accuracy: 106.875\n",
      "# Iteration  2899 -> Loss: 0.537965473682523 \t| Accuracy: 106.875\n",
      "# Iteration  2900 -> Loss: 0.5378603518236501 \t| Accuracy: 106.875\n",
      "# Iteration  2901 -> Loss: 0.5377553080213962 \t| Accuracy: 106.875\n",
      "# Iteration  2902 -> Loss: 0.5376503422032287 \t| Accuracy: 106.875\n",
      "# Iteration  2903 -> Loss: 0.537545454296542 \t| Accuracy: 106.875\n",
      "# Iteration  2904 -> Loss: 0.5374406442286587 \t| Accuracy: 106.875\n",
      "# Iteration  2905 -> Loss: 0.5373359119268313 \t| Accuracy: 106.875\n",
      "# Iteration  2906 -> Loss: 0.5372312573182432 \t| Accuracy: 106.875\n",
      "# Iteration  2907 -> Loss: 0.5371266803300104 \t| Accuracy: 106.875\n",
      "# Iteration  2908 -> Loss: 0.5370221808891829 \t| Accuracy: 106.875\n",
      "# Iteration  2909 -> Loss: 0.5369177589227455 \t| Accuracy: 106.875\n",
      "# Iteration  2910 -> Loss: 0.5368134143576202 \t| Accuracy: 106.875\n",
      "# Iteration  2911 -> Loss: 0.5367091471206664 \t| Accuracy: 106.875\n",
      "# Iteration  2912 -> Loss: 0.5366049571386835 \t| Accuracy: 106.875\n",
      "# Iteration  2913 -> Loss: 0.536500844338411 \t| Accuracy: 106.875\n",
      "# Iteration  2914 -> Loss: 0.5363968086465308 \t| Accuracy: 106.875\n",
      "# Iteration  2915 -> Loss: 0.536292849989668 \t| Accuracy: 106.875\n",
      "# Iteration  2916 -> Loss: 0.5361889682943927 \t| Accuracy: 106.875\n",
      "# Iteration  2917 -> Loss: 0.5360851634872204 \t| Accuracy: 106.875\n",
      "# Iteration  2918 -> Loss: 0.535981435494615 \t| Accuracy: 106.875\n",
      "# Iteration  2919 -> Loss: 0.5358777842429882 \t| Accuracy: 106.875\n",
      "# Iteration  2920 -> Loss: 0.5357742096587024 \t| Accuracy: 106.875\n",
      "# Iteration  2921 -> Loss: 0.535670711668071 \t| Accuracy: 106.875\n",
      "# Iteration  2922 -> Loss: 0.5355672901973603 \t| Accuracy: 106.875\n",
      "# Iteration  2923 -> Loss: 0.5354639451727905 \t| Accuracy: 106.875\n",
      "# Iteration  2924 -> Loss: 0.5353606765205371 \t| Accuracy: 106.875\n",
      "# Iteration  2925 -> Loss: 0.5352574841667324 \t| Accuracy: 106.875\n",
      "# Iteration  2926 -> Loss: 0.5351543680374659 \t| Accuracy: 106.875\n",
      "# Iteration  2927 -> Loss: 0.5350513280587871 \t| Accuracy: 106.875\n",
      "# Iteration  2928 -> Loss: 0.5349483641567055 \t| Accuracy: 106.875\n",
      "# Iteration  2929 -> Loss: 0.5348454762571927 \t| Accuracy: 106.875\n",
      "# Iteration  2930 -> Loss: 0.5347426642861829 \t| Accuracy: 106.875\n",
      "# Iteration  2931 -> Loss: 0.5346399281695747 \t| Accuracy: 106.875\n",
      "# Iteration  2932 -> Loss: 0.5345372678332325 \t| Accuracy: 106.875\n",
      "# Iteration  2933 -> Loss: 0.534434683202987 \t| Accuracy: 106.875\n",
      "# Iteration  2934 -> Loss: 0.5343321742046379 \t| Accuracy: 106.875\n",
      "# Iteration  2935 -> Loss: 0.534229740763953 \t| Accuracy: 106.875\n",
      "# Iteration  2936 -> Loss: 0.5341273828066716 \t| Accuracy: 106.875\n",
      "# Iteration  2937 -> Loss: 0.5340251002585044 \t| Accuracy: 106.875\n",
      "# Iteration  2938 -> Loss: 0.5339228930451353 \t| Accuracy: 106.875\n",
      "# Iteration  2939 -> Loss: 0.5338207610922223 \t| Accuracy: 106.875\n",
      "# Iteration  2940 -> Loss: 0.5337187043253988 \t| Accuracy: 106.875\n",
      "# Iteration  2941 -> Loss: 0.5336167226702754 \t| Accuracy: 106.875\n",
      "# Iteration  2942 -> Loss: 0.53351481605244 \t| Accuracy: 106.875\n",
      "# Iteration  2943 -> Loss: 0.53341298439746 \t| Accuracy: 106.875\n",
      "# Iteration  2944 -> Loss: 0.533311227630883 \t| Accuracy: 106.875\n",
      "# Iteration  2945 -> Loss: 0.5332095456782384 \t| Accuracy: 106.875\n",
      "# Iteration  2946 -> Loss: 0.533107938465038 \t| Accuracy: 106.875\n",
      "# Iteration  2947 -> Loss: 0.5330064059167776 \t| Accuracy: 106.875\n",
      "# Iteration  2948 -> Loss: 0.5329049479589381 \t| Accuracy: 106.875\n",
      "# Iteration  2949 -> Loss: 0.5328035645169867 \t| Accuracy: 106.875\n",
      "# Iteration  2950 -> Loss: 0.5327022555163782 \t| Accuracy: 106.875\n",
      "# Iteration  2951 -> Loss: 0.5326010208825557 \t| Accuracy: 106.875\n",
      "# Iteration  2952 -> Loss: 0.5324998605409523 \t| Accuracy: 106.875\n",
      "# Iteration  2953 -> Loss: 0.5323987744169921 \t| Accuracy: 106.875\n",
      "# Iteration  2954 -> Loss: 0.532297762436091 \t| Accuracy: 106.875\n",
      "# Iteration  2955 -> Loss: 0.5321968245236584 \t| Accuracy: 106.875\n",
      "# Iteration  2956 -> Loss: 0.5320959606050978 \t| Accuracy: 106.875\n",
      "# Iteration  2957 -> Loss: 0.5319951706058086 \t| Accuracy: 106.875\n",
      "# Iteration  2958 -> Loss: 0.5318944544511863 \t| Accuracy: 106.875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Iteration  2959 -> Loss: 0.5317938120666249 \t| Accuracy: 106.875\n",
      "# Iteration  2960 -> Loss: 0.5316932433775164 \t| Accuracy: 106.875\n",
      "# Iteration  2961 -> Loss: 0.5315927483092534 \t| Accuracy: 106.875\n",
      "# Iteration  2962 -> Loss: 0.5314923267872295 \t| Accuracy: 106.875\n",
      "# Iteration  2963 -> Loss: 0.5313919787368402 \t| Accuracy: 106.875\n",
      "# Iteration  2964 -> Loss: 0.5312917040834845 \t| Accuracy: 106.875\n",
      "# Iteration  2965 -> Loss: 0.531191502752566 \t| Accuracy: 106.875\n",
      "# Iteration  2966 -> Loss: 0.5310913746694934 \t| Accuracy: 106.875\n",
      "# Iteration  2967 -> Loss: 0.530991319759682 \t| Accuracy: 106.875\n",
      "# Iteration  2968 -> Loss: 0.5308913379485548 \t| Accuracy: 106.875\n",
      "# Iteration  2969 -> Loss: 0.5307914291615438 \t| Accuracy: 106.875\n",
      "# Iteration  2970 -> Loss: 0.5306915933240899 \t| Accuracy: 106.875\n",
      "# Iteration  2971 -> Loss: 0.5305918303616457 \t| Accuracy: 106.875\n",
      "# Iteration  2972 -> Loss: 0.530492140199675 \t| Accuracy: 106.875\n",
      "# Iteration  2973 -> Loss: 0.5303925227636551 \t| Accuracy: 106.875\n",
      "# Iteration  2974 -> Loss: 0.5302929779790766 \t| Accuracy: 106.875\n",
      "# Iteration  2975 -> Loss: 0.5301935057714454 \t| Accuracy: 106.875\n",
      "# Iteration  2976 -> Loss: 0.5300941060662835 \t| Accuracy: 106.875\n",
      "# Iteration  2977 -> Loss: 0.5299947787891295 \t| Accuracy: 106.875\n",
      "# Iteration  2978 -> Loss: 0.5298955238655404 \t| Accuracy: 106.875\n",
      "# Iteration  2979 -> Loss: 0.529796341221092 \t| Accuracy: 106.875\n",
      "# Iteration  2980 -> Loss: 0.5296972307813803 \t| Accuracy: 106.875\n",
      "# Iteration  2981 -> Loss: 0.5295981924720223 \t| Accuracy: 106.875\n",
      "# Iteration  2982 -> Loss: 0.5294992262186569 \t| Accuracy: 106.875\n",
      "# Iteration  2983 -> Loss: 0.529400331946946 \t| Accuracy: 106.875\n",
      "# Iteration  2984 -> Loss: 0.5293015095825756 \t| Accuracy: 106.875\n",
      "# Iteration  2985 -> Loss: 0.5292027590512567 \t| Accuracy: 106.875\n",
      "# Iteration  2986 -> Loss: 0.5291040802787259 \t| Accuracy: 106.875\n",
      "# Iteration  2987 -> Loss: 0.5290054731907471 \t| Accuracy: 106.875\n",
      "# Iteration  2988 -> Loss: 0.5289069377131118 \t| Accuracy: 106.875\n",
      "# Iteration  2989 -> Loss: 0.5288084737716403 \t| Accuracy: 106.875\n",
      "# Iteration  2990 -> Loss: 0.5287100812921826 \t| Accuracy: 106.875\n",
      "# Iteration  2991 -> Loss: 0.5286117602006192 \t| Accuracy: 106.875\n",
      "# Iteration  2992 -> Loss: 0.5285135104228625 \t| Accuracy: 106.875\n",
      "# Iteration  2993 -> Loss: 0.5284153318848576 \t| Accuracy: 106.875\n",
      "# Iteration  2994 -> Loss: 0.528317224512582 \t| Accuracy: 106.875\n",
      "# Iteration  2995 -> Loss: 0.5282191882320486 \t| Accuracy: 106.875\n",
      "# Iteration  2996 -> Loss: 0.5281212229693053 \t| Accuracy: 106.875\n",
      "# Iteration  2997 -> Loss: 0.5280233286504357 \t| Accuracy: 106.875\n",
      "# Iteration  2998 -> Loss: 0.5279255052015607 \t| Accuracy: 106.875\n",
      "# Iteration  2999 -> Loss: 0.5278277525488393 \t| Accuracy: 106.875\n",
      "# Iteration  3000 -> Loss: 0.5277300706184688 \t| Accuracy: 106.875\n",
      "# Iteration  3001 -> Loss: 0.5276324593366868 \t| Accuracy: 106.875\n",
      "# Iteration  3002 -> Loss: 0.5275349186297706 \t| Accuracy: 106.875\n",
      "# Iteration  3003 -> Loss: 0.52743744842404 \t| Accuracy: 106.875\n",
      "# Iteration  3004 -> Loss: 0.5273400486458557 \t| Accuracy: 106.875\n",
      "# Iteration  3005 -> Loss: 0.5272427192216225 \t| Accuracy: 106.875\n",
      "# Iteration  3006 -> Loss: 0.5271454600777888 \t| Accuracy: 106.875\n",
      "# Iteration  3007 -> Loss: 0.5270482711408476 \t| Accuracy: 106.875\n",
      "# Iteration  3008 -> Loss: 0.5269511523373377 \t| Accuracy: 106.875\n",
      "# Iteration  3009 -> Loss: 0.5268541035938441 \t| Accuracy: 106.875\n",
      "# Iteration  3010 -> Loss: 0.5267571248369992 \t| Accuracy: 106.875\n",
      "# Iteration  3011 -> Loss: 0.5266602159934832 \t| Accuracy: 106.875\n",
      "# Iteration  3012 -> Loss: 0.5265633769900255 \t| Accuracy: 106.875\n",
      "# Iteration  3013 -> Loss: 0.5264666077534047 \t| Accuracy: 106.875\n",
      "# Iteration  3014 -> Loss: 0.5263699082104499 \t| Accuracy: 106.875\n",
      "# Iteration  3015 -> Loss: 0.5262732782880416 \t| Accuracy: 106.875\n",
      "# Iteration  3016 -> Loss: 0.5261767179131122 \t| Accuracy: 106.875\n",
      "# Iteration  3017 -> Loss: 0.5260802270126463 \t| Accuracy: 106.875\n",
      "# Iteration  3018 -> Loss: 0.5259838055136828 \t| Accuracy: 106.875\n",
      "# Iteration  3019 -> Loss: 0.5258874533433145 \t| Accuracy: 106.875\n",
      "# Iteration  3020 -> Loss: 0.525791170428689 \t| Accuracy: 106.875\n",
      "# Iteration  3021 -> Loss: 0.5256949566970097 \t| Accuracy: 106.875\n",
      "# Iteration  3022 -> Loss: 0.5255988120755367 \t| Accuracy: 106.875\n",
      "# Iteration  3023 -> Loss: 0.5255027364915872 \t| Accuracy: 106.875\n",
      "# Iteration  3024 -> Loss: 0.5254067298725362 \t| Accuracy: 106.875\n",
      "# Iteration  3025 -> Loss: 0.5253107921458173 \t| Accuracy: 106.875\n",
      "# Iteration  3026 -> Loss: 0.5252149232389236 \t| Accuracy: 106.875\n",
      "# Iteration  3027 -> Loss: 0.5251191230794083 \t| Accuracy: 106.875\n",
      "# Iteration  3028 -> Loss: 0.5250233915948851 \t| Accuracy: 106.875\n",
      "# Iteration  3029 -> Loss: 0.5249277287130292 \t| Accuracy: 106.875\n",
      "# Iteration  3030 -> Loss: 0.524832134361578 \t| Accuracy: 106.875\n",
      "# Iteration  3031 -> Loss: 0.5247366084683317 \t| Accuracy: 106.875\n",
      "# Iteration  3032 -> Loss: 0.5246411509611539 \t| Accuracy: 106.875\n",
      "# Iteration  3033 -> Loss: 0.5245457617679723 \t| Accuracy: 106.875\n",
      "# Iteration  3034 -> Loss: 0.5244504408167792 \t| Accuracy: 106.875\n",
      "# Iteration  3035 -> Loss: 0.5243551880356327 \t| Accuracy: 106.875\n",
      "# Iteration  3036 -> Loss: 0.5242600033526568 \t| Accuracy: 106.875\n",
      "# Iteration  3037 -> Loss: 0.524164886696042 \t| Accuracy: 106.875\n",
      "# Iteration  3038 -> Loss: 0.5240698379940464 \t| Accuracy: 106.875\n",
      "# Iteration  3039 -> Loss: 0.5239748571749958 \t| Accuracy: 106.875\n",
      "# Iteration  3040 -> Loss: 0.5238799441672851 \t| Accuracy: 106.875\n",
      "# Iteration  3041 -> Loss: 0.5237850988993776 \t| Accuracy: 106.875\n",
      "# Iteration  3042 -> Loss: 0.5236903212998069 \t| Accuracy: 106.875\n",
      "# Iteration  3043 -> Loss: 0.5235956112971767 \t| Accuracy: 106.875\n",
      "# Iteration  3044 -> Loss: 0.523500968820162 \t| Accuracy: 106.875\n",
      "# Iteration  3045 -> Loss: 0.5234063937975091 \t| Accuracy: 106.875\n",
      "# Iteration  3046 -> Loss: 0.5233118861580363 \t| Accuracy: 106.875\n",
      "# Iteration  3047 -> Loss: 0.523217445830635 \t| Accuracy: 106.875\n",
      "# Iteration  3048 -> Loss: 0.5231230727442696 \t| Accuracy: 106.875\n",
      "# Iteration  3049 -> Loss: 0.5230287668279783 \t| Accuracy: 106.875\n",
      "# Iteration  3050 -> Loss: 0.5229345280108736 \t| Accuracy: 106.875\n",
      "# Iteration  3051 -> Loss: 0.5228403562221431 \t| Accuracy: 106.875\n",
      "# Iteration  3052 -> Loss: 0.5227462513910496 \t| Accuracy: 106.875\n",
      "# Iteration  3053 -> Loss: 0.5226522134469324 \t| Accuracy: 106.875\n",
      "# Iteration  3054 -> Loss: 0.5225582423192063 \t| Accuracy: 106.875\n",
      "# Iteration  3055 -> Loss: 0.5224643379373645 \t| Accuracy: 106.875\n",
      "# Iteration  3056 -> Loss: 0.5223705002309762 \t| Accuracy: 106.875\n",
      "# Iteration  3057 -> Loss: 0.5222767291296899 \t| Accuracy: 106.875\n",
      "# Iteration  3058 -> Loss: 0.5221830245632321 \t| Accuracy: 106.875\n",
      "# Iteration  3059 -> Loss: 0.522089386461408 \t| Accuracy: 106.875\n",
      "# Iteration  3060 -> Loss: 0.5219958147541031 \t| Accuracy: 106.875\n",
      "# Iteration  3061 -> Loss: 0.5219023093712819 \t| Accuracy: 106.875\n",
      "# Iteration  3062 -> Loss: 0.5218088702429904 \t| Accuracy: 106.875\n",
      "# Iteration  3063 -> Loss: 0.5217154972993546 \t| Accuracy: 106.875\n",
      "# Iteration  3064 -> Loss: 0.5216221904705827 \t| Accuracy: 106.875\n",
      "# Iteration  3065 -> Loss: 0.521528949686964 \t| Accuracy: 106.875\n",
      "# Iteration  3066 -> Loss: 0.5214357748788703 \t| Accuracy: 106.875\n",
      "# Iteration  3067 -> Loss: 0.5213426659767566 \t| Accuracy: 106.875\n",
      "# Iteration  3068 -> Loss: 0.5212496229111604 \t| Accuracy: 106.875\n",
      "# Iteration  3069 -> Loss: 0.5211566456127033 \t| Accuracy: 106.875\n",
      "# Iteration  3070 -> Loss: 0.5210637340120903 \t| Accuracy: 106.875\n",
      "# Iteration  3071 -> Loss: 0.5209708880401115 \t| Accuracy: 106.875\n",
      "# Iteration  3072 -> Loss: 0.5208781076276413 \t| Accuracy: 106.875\n",
      "# Iteration  3073 -> Loss: 0.5207853927056398 \t| Accuracy: 106.875\n",
      "# Iteration  3074 -> Loss: 0.5206927432051522 \t| Accuracy: 106.875\n",
      "# Iteration  3075 -> Loss: 0.52060015905731 \t| Accuracy: 106.875\n",
      "# Iteration  3076 -> Loss: 0.5205076401933314 \t| Accuracy: 106.875\n",
      "# Iteration  3077 -> Loss: 0.5204151865445207 \t| Accuracy: 106.875\n",
      "# Iteration  3078 -> Loss: 0.52032279804227 \t| Accuracy: 106.875\n",
      "# Iteration  3079 -> Loss: 0.5202304746180583 \t| Accuracy: 106.875\n",
      "# Iteration  3080 -> Loss: 0.520138216203453 \t| Accuracy: 106.875\n",
      "# Iteration  3081 -> Loss: 0.5200460227301096 \t| Accuracy: 106.875\n",
      "# Iteration  3082 -> Loss: 0.519953894129772 \t| Accuracy: 106.875\n",
      "# Iteration  3083 -> Loss: 0.5198618303342731 \t| Accuracy: 106.875\n",
      "# Iteration  3084 -> Loss: 0.5197698312755352 \t| Accuracy: 107.083\n",
      "# Iteration  3085 -> Loss: 0.51967789688557 \t| Accuracy: 107.083\n",
      "# Iteration  3086 -> Loss: 0.519586027096479 \t| Accuracy: 107.083\n",
      "# Iteration  3087 -> Loss: 0.5194942218404546 \t| Accuracy: 107.083\n",
      "# Iteration  3088 -> Loss: 0.5194024810497786 \t| Accuracy: 107.083\n",
      "# Iteration  3089 -> Loss: 0.5193108046568246 \t| Accuracy: 107.083\n",
      "# Iteration  3090 -> Loss: 0.519219192594057 \t| Accuracy: 107.083\n",
      "# Iteration  3091 -> Loss: 0.5191276447940314 \t| Accuracy: 107.083\n",
      "# Iteration  3092 -> Loss: 0.5190361611893958 \t| Accuracy: 107.083\n",
      "# Iteration  3093 -> Loss: 0.5189447417128894 \t| Accuracy: 107.083\n",
      "# Iteration  3094 -> Loss: 0.5188533862973441 \t| Accuracy: 107.083\n",
      "# Iteration  3095 -> Loss: 0.5187620948756845 \t| Accuracy: 107.083\n",
      "# Iteration  3096 -> Loss: 0.5186708673809276 \t| Accuracy: 107.083\n",
      "# Iteration  3097 -> Loss: 0.5185797037461837 \t| Accuracy: 107.083\n",
      "# Iteration  3098 -> Loss: 0.5184886039046565 \t| Accuracy: 107.292\n",
      "# Iteration  3099 -> Loss: 0.5183975677896432 \t| Accuracy: 107.292\n",
      "# Iteration  3100 -> Loss: 0.5183065953345347 \t| Accuracy: 107.292\n",
      "# Iteration  3101 -> Loss: 0.5182156864728162 \t| Accuracy: 107.292\n",
      "# Iteration  3102 -> Loss: 0.5181248411380672 \t| Accuracy: 107.292\n",
      "# Iteration  3103 -> Loss: 0.5180340592639613 \t| Accuracy: 107.292\n",
      "# Iteration  3104 -> Loss: 0.5179433407842673 \t| Accuracy: 107.292\n",
      "# Iteration  3105 -> Loss: 0.5178526856328487 \t| Accuracy: 107.292\n",
      "# Iteration  3106 -> Loss: 0.5177620937436644 \t| Accuracy: 107.292\n",
      "# Iteration  3107 -> Loss: 0.5176715650507683 \t| Accuracy: 107.292\n",
      "# Iteration  3108 -> Loss: 0.5175810994883102 \t| Accuracy: 107.292\n",
      "# Iteration  3109 -> Loss: 0.5174906969905353 \t| Accuracy: 107.292\n",
      "# Iteration  3110 -> Loss: 0.5174003574917854 \t| Accuracy: 107.292\n",
      "# Iteration  3111 -> Loss: 0.5173100809264976 \t| Accuracy: 107.292\n",
      "# Iteration  3112 -> Loss: 0.5172198672292059 \t| Accuracy: 107.292\n",
      "# Iteration  3113 -> Loss: 0.5171297163345406 \t| Accuracy: 107.292\n",
      "# Iteration  3114 -> Loss: 0.5170396281772285 \t| Accuracy: 107.292\n",
      "# Iteration  3115 -> Loss: 0.5169496026920937 \t| Accuracy: 107.292\n",
      "# Iteration  3116 -> Loss: 0.5168596398140566 \t| Accuracy: 107.292\n",
      "# Iteration  3117 -> Loss: 0.5167697394781351 \t| Accuracy: 107.292\n",
      "# Iteration  3118 -> Loss: 0.5166799016194444 \t| Accuracy: 107.292\n",
      "# Iteration  3119 -> Loss: 0.5165901261731971 \t| Accuracy: 107.292\n",
      "# Iteration  3120 -> Loss: 0.5165004130747031 \t| Accuracy: 107.292\n",
      "# Iteration  3121 -> Loss: 0.5164107622593703 \t| Accuracy: 107.292\n",
      "# Iteration  3122 -> Loss: 0.5163211736627042 \t| Accuracy: 107.292\n",
      "# Iteration  3123 -> Loss: 0.5162316472203085 \t| Accuracy: 107.292\n",
      "# Iteration  3124 -> Loss: 0.5161421828678844 \t| Accuracy: 107.292\n",
      "# Iteration  3125 -> Loss: 0.5160527805412317 \t| Accuracy: 107.292\n",
      "# Iteration  3126 -> Loss: 0.5159634401762487 \t| Accuracy: 107.292\n",
      "# Iteration  3127 -> Loss: 0.5158741617089314 \t| Accuracy: 107.292\n",
      "# Iteration  3128 -> Loss: 0.5157849450753748 \t| Accuracy: 107.292\n",
      "# Iteration  3129 -> Loss: 0.5156957902117724 \t| Accuracy: 107.292\n",
      "# Iteration  3130 -> Loss: 0.515606697054416 \t| Accuracy: 107.292\n",
      "# Iteration  3131 -> Loss: 0.5155176655396967 \t| Accuracy: 107.292\n",
      "# Iteration  3132 -> Loss: 0.5154286956041039 \t| Accuracy: 107.292\n",
      "# Iteration  3133 -> Loss: 0.5153397871842261 \t| Accuracy: 107.292\n",
      "# Iteration  3134 -> Loss: 0.5152509402167511 \t| Accuracy: 107.292\n",
      "# Iteration  3135 -> Loss: 0.5151621546384653 \t| Accuracy: 107.292\n",
      "# Iteration  3136 -> Loss: 0.5150734303862542 \t| Accuracy: 107.292\n",
      "# Iteration  3137 -> Loss: 0.5149847673971027 \t| Accuracy: 107.292\n",
      "# Iteration  3138 -> Loss: 0.5148961656080949 \t| Accuracy: 107.292\n",
      "# Iteration  3139 -> Loss: 0.514807624956414 \t| Accuracy: 107.292\n",
      "# Iteration  3140 -> Loss: 0.5147191453793426 \t| Accuracy: 107.292\n",
      "# Iteration  3141 -> Loss: 0.5146307268142627 \t| Accuracy: 107.292\n",
      "# Iteration  3142 -> Loss: 0.5145423691986556 \t| Accuracy: 107.292\n",
      "# Iteration  3143 -> Loss: 0.5144540724701019 \t| Accuracy: 107.292\n",
      "# Iteration  3144 -> Loss: 0.5143658365662821 \t| Accuracy: 107.292\n",
      "# Iteration  3145 -> Loss: 0.5142776614249756 \t| Accuracy: 107.292\n",
      "# Iteration  3146 -> Loss: 0.5141895469840618 \t| Accuracy: 107.292\n",
      "# Iteration  3147 -> Loss: 0.5141014931815192 \t| Accuracy: 107.292\n",
      "# Iteration  3148 -> Loss: 0.5140134999554262 \t| Accuracy: 107.292\n",
      "# Iteration  3149 -> Loss: 0.5139255672439604 \t| Accuracy: 107.292\n",
      "# Iteration  3150 -> Loss: 0.5138376949853992 \t| Accuracy: 107.292\n",
      "# Iteration  3151 -> Loss: 0.5137498831181194 \t| Accuracy: 107.292\n",
      "# Iteration  3152 -> Loss: 0.5136621315805974 \t| Accuracy: 107.292\n",
      "# Iteration  3153 -> Loss: 0.5135744403114089 \t| Accuracy: 107.292\n",
      "# Iteration  3154 -> Loss: 0.5134868092492295 \t| Accuracy: 107.292\n",
      "# Iteration  3155 -> Loss: 0.5133992383328342 \t| Accuracy: 107.292\n",
      "# Iteration  3156 -> Loss: 0.5133117275010972 \t| Accuracy: 107.292\n",
      "# Iteration  3157 -> Loss: 0.5132242766929926 \t| Accuracy: 107.292\n",
      "# Iteration  3158 -> Loss: 0.5131368858475935 \t| Accuracy: 107.292\n",
      "# Iteration  3159 -> Loss: 0.513049554904073 \t| Accuracy: 107.292\n",
      "# Iteration  3160 -> Loss: 0.512962283801703 \t| Accuracy: 107.292\n",
      "# Iteration  3161 -> Loss: 0.5128750724798549 \t| Accuracy: 107.292\n",
      "# Iteration  3162 -> Loss: 0.5127879208779997 \t| Accuracy: 107.292\n",
      "# Iteration  3163 -> Loss: 0.5127008289357072 \t| Accuracy: 107.292\n",
      "# Iteration  3164 -> Loss: 0.5126137965926472 \t| Accuracy: 107.292\n",
      "# Iteration  3165 -> Loss: 0.5125268237885875 \t| Accuracy: 107.292\n",
      "# Iteration  3166 -> Loss: 0.5124399104633961 \t| Accuracy: 107.292\n",
      "# Iteration  3167 -> Loss: 0.5123530565570396 \t| Accuracy: 107.292\n",
      "# Iteration  3168 -> Loss: 0.5122662620095834 \t| Accuracy: 107.292\n",
      "# Iteration  3169 -> Loss: 0.5121795267611924 \t| Accuracy: 107.292\n",
      "# Iteration  3170 -> Loss: 0.5120928507521298 \t| Accuracy: 107.292\n",
      "# Iteration  3171 -> Loss: 0.5120062339227581 \t| Accuracy: 107.292\n",
      "# Iteration  3172 -> Loss: 0.5119196762135383 \t| Accuracy: 107.292\n",
      "# Iteration  3173 -> Loss: 0.5118331775650301 \t| Accuracy: 107.292\n",
      "# Iteration  3174 -> Loss: 0.5117467379178915 \t| Accuracy: 107.292\n",
      "# Iteration  3175 -> Loss: 0.5116603572128796 \t| Accuracy: 107.292\n",
      "# Iteration  3176 -> Loss: 0.5115740353908496 \t| Accuracy: 107.292\n",
      "# Iteration  3177 -> Loss: 0.5114877723927549 \t| Accuracy: 107.292\n",
      "# Iteration  3178 -> Loss: 0.5114015681596474 \t| Accuracy: 107.292\n",
      "# Iteration  3179 -> Loss: 0.511315422632677 \t| Accuracy: 107.292\n",
      "# Iteration  3180 -> Loss: 0.5112293357530918 \t| Accuracy: 107.292\n",
      "# Iteration  3181 -> Loss: 0.5111433074622376 \t| Accuracy: 107.292\n",
      "# Iteration  3182 -> Loss: 0.5110573377015585 \t| Accuracy: 107.292\n",
      "# Iteration  3183 -> Loss: 0.5109714264125959 \t| Accuracy: 107.292\n",
      "# Iteration  3184 -> Loss: 0.510885573536989 \t| Accuracy: 107.292\n",
      "# Iteration  3185 -> Loss: 0.5107997790164749 \t| Accuracy: 107.292\n",
      "# Iteration  3186 -> Loss: 0.5107140427928877 \t| Accuracy: 107.292\n",
      "# Iteration  3187 -> Loss: 0.5106283648081589 \t| Accuracy: 107.292\n",
      "# Iteration  3188 -> Loss: 0.5105427450043173 \t| Accuracy: 107.292\n",
      "# Iteration  3189 -> Loss: 0.5104571833234888 \t| Accuracy: 107.292\n",
      "# Iteration  3190 -> Loss: 0.5103716797078963 \t| Accuracy: 107.292\n",
      "# Iteration  3191 -> Loss: 0.5102862340998595 \t| Accuracy: 107.292\n",
      "# Iteration  3192 -> Loss: 0.5102008464417946 \t| Accuracy: 107.292\n",
      "# Iteration  3193 -> Loss: 0.5101155166762148 \t| Accuracy: 107.292\n",
      "# Iteration  3194 -> Loss: 0.5100302447457293 \t| Accuracy: 107.292\n",
      "# Iteration  3195 -> Loss: 0.5099450305930442 \t| Accuracy: 107.292\n",
      "# Iteration  3196 -> Loss: 0.5098598741609612 \t| Accuracy: 107.292\n",
      "# Iteration  3197 -> Loss: 0.5097747753923785 \t| Accuracy: 107.292\n",
      "# Iteration  3198 -> Loss: 0.5096897342302901 \t| Accuracy: 107.292\n",
      "# Iteration  3199 -> Loss: 0.5096047506177854 \t| Accuracy: 107.292\n",
      "# Iteration  3200 -> Loss: 0.5095198244980499 \t| Accuracy: 107.292\n",
      "# Iteration  3201 -> Loss: 0.5094349558143646 \t| Accuracy: 107.292\n",
      "# Iteration  3202 -> Loss: 0.5093501445101054 \t| Accuracy: 107.292\n",
      "# Iteration  3203 -> Loss: 0.5092653905287439 \t| Accuracy: 107.292\n",
      "# Iteration  3204 -> Loss: 0.5091806938138463 \t| Accuracy: 107.292\n",
      "# Iteration  3205 -> Loss: 0.509096054309074 \t| Accuracy: 107.292\n",
      "# Iteration  3206 -> Loss: 0.5090114719581829 \t| Accuracy: 107.292\n",
      "# Iteration  3207 -> Loss: 0.5089269467050237 \t| Accuracy: 107.292\n",
      "# Iteration  3208 -> Loss: 0.5088424784935413 \t| Accuracy: 107.292\n",
      "# Iteration  3209 -> Loss: 0.5087580672677747 \t| Accuracy: 107.292\n",
      "# Iteration  3210 -> Loss: 0.5086737129718576 \t| Accuracy: 107.292\n",
      "# Iteration  3211 -> Loss: 0.508589415550017 \t| Accuracy: 107.292\n",
      "# Iteration  3212 -> Loss: 0.508505174946574 \t| Accuracy: 107.292\n",
      "# Iteration  3213 -> Loss: 0.508420991105943 \t| Accuracy: 107.292\n",
      "# Iteration  3214 -> Loss: 0.508336863972632 \t| Accuracy: 107.292\n",
      "# Iteration  3215 -> Loss: 0.5082527934912424 \t| Accuracy: 107.292\n",
      "# Iteration  3216 -> Loss: 0.5081687796064682 \t| Accuracy: 107.292\n",
      "# Iteration  3217 -> Loss: 0.5080848222630969 \t| Accuracy: 107.292\n",
      "# Iteration  3218 -> Loss: 0.5080009214060083 \t| Accuracy: 107.292\n",
      "# Iteration  3219 -> Loss: 0.5079170769801745 \t| Accuracy: 107.292\n",
      "# Iteration  3220 -> Loss: 0.5078332889306607 \t| Accuracy: 107.292\n",
      "# Iteration  3221 -> Loss: 0.5077495572026238 \t| Accuracy: 107.292\n",
      "# Iteration  3222 -> Loss: 0.5076658817413126 \t| Accuracy: 107.292\n",
      "# Iteration  3223 -> Loss: 0.5075822624920681 \t| Accuracy: 107.292\n",
      "# Iteration  3224 -> Loss: 0.5074986994003224 \t| Accuracy: 107.292\n",
      "# Iteration  3225 -> Loss: 0.5074151924115995 \t| Accuracy: 107.292\n",
      "# Iteration  3226 -> Loss: 0.5073317414715147 \t| Accuracy: 107.292\n",
      "# Iteration  3227 -> Loss: 0.5072483465257739 \t| Accuracy: 107.292\n",
      "# Iteration  3228 -> Loss: 0.5071650075201742 \t| Accuracy: 107.292\n",
      "# Iteration  3229 -> Loss: 0.5070817244006033 \t| Accuracy: 107.292\n",
      "# Iteration  3230 -> Loss: 0.5069984971130395 \t| Accuracy: 107.292\n",
      "# Iteration  3231 -> Loss: 0.5069153256035509 \t| Accuracy: 107.292\n",
      "# Iteration  3232 -> Loss: 0.5068322098182966 \t| Accuracy: 107.292\n",
      "# Iteration  3233 -> Loss: 0.5067491497035248 \t| Accuracy: 107.292\n",
      "# Iteration  3234 -> Loss: 0.5066661452055736 \t| Accuracy: 107.292\n",
      "# Iteration  3235 -> Loss: 0.5065831962708708 \t| Accuracy: 107.292\n",
      "# Iteration  3236 -> Loss: 0.5065003028459335 \t| Accuracy: 107.292\n",
      "# Iteration  3237 -> Loss: 0.5064174648773676 \t| Accuracy: 107.292\n",
      "# Iteration  3238 -> Loss: 0.5063346823118681 \t| Accuracy: 107.292\n",
      "# Iteration  3239 -> Loss: 0.5062519550962186 \t| Accuracy: 107.292\n",
      "# Iteration  3240 -> Loss: 0.5061692831772915 \t| Accuracy: 107.292\n",
      "# Iteration  3241 -> Loss: 0.506086666502047 \t| Accuracy: 107.292\n",
      "# Iteration  3242 -> Loss: 0.5060041050175339 \t| Accuracy: 107.292\n",
      "# Iteration  3243 -> Loss: 0.5059215986708884 \t| Accuracy: 107.292\n",
      "# Iteration  3244 -> Loss: 0.5058391474093346 \t| Accuracy: 107.292\n",
      "# Iteration  3245 -> Loss: 0.5057567511801841 \t| Accuracy: 107.292\n",
      "# Iteration  3246 -> Loss: 0.5056744099308356 \t| Accuracy: 107.292\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Iteration  3247 -> Loss: 0.505592123608775 \t| Accuracy: 107.292\n",
      "# Iteration  3248 -> Loss: 0.5055098921615748 \t| Accuracy: 107.292\n",
      "# Iteration  3249 -> Loss: 0.5054277155368945 \t| Accuracy: 107.292\n",
      "# Iteration  3250 -> Loss: 0.5053455936824794 \t| Accuracy: 107.292\n",
      "# Iteration  3251 -> Loss: 0.5052635265461616 \t| Accuracy: 107.292\n",
      "# Iteration  3252 -> Loss: 0.505181514075859 \t| Accuracy: 107.292\n",
      "# Iteration  3253 -> Loss: 0.5050995562195749 \t| Accuracy: 107.292\n",
      "# Iteration  3254 -> Loss: 0.5050176529253986 \t| Accuracy: 106.042\n",
      "# Iteration  3255 -> Loss: 0.5049358041415045 \t| Accuracy: 106.042\n",
      "# Iteration  3256 -> Loss: 0.5048540098161521 \t| Accuracy: 106.042\n",
      "# Iteration  3257 -> Loss: 0.5047722698976861 \t| Accuracy: 106.042\n",
      "# Iteration  3258 -> Loss: 0.5046905843345354 \t| Accuracy: 106.042\n",
      "# Iteration  3259 -> Loss: 0.5046089530752136 \t| Accuracy: 106.042\n",
      "# Iteration  3260 -> Loss: 0.5045273760683187 \t| Accuracy: 106.042\n",
      "# Iteration  3261 -> Loss: 0.5044458532625323 \t| Accuracy: 106.042\n",
      "# Iteration  3262 -> Loss: 0.5043643846066204 \t| Accuracy: 106.042\n",
      "# Iteration  3263 -> Loss: 0.5042829700494319 \t| Accuracy: 106.042\n",
      "# Iteration  3264 -> Loss: 0.5042016095398996 \t| Accuracy: 106.042\n",
      "# Iteration  3265 -> Loss: 0.5041203030270393 \t| Accuracy: 106.042\n",
      "# Iteration  3266 -> Loss: 0.5040390504599495 \t| Accuracy: 106.042\n",
      "# Iteration  3267 -> Loss: 0.5039578517878115 \t| Accuracy: 106.042\n",
      "# Iteration  3268 -> Loss: 0.5038767069598894 \t| Accuracy: 106.042\n",
      "# Iteration  3269 -> Loss: 0.503795615925529 \t| Accuracy: 106.042\n",
      "# Iteration  3270 -> Loss: 0.5037145786341585 \t| Accuracy: 106.042\n",
      "# Iteration  3271 -> Loss: 0.503633595035288 \t| Accuracy: 106.042\n",
      "# Iteration  3272 -> Loss: 0.5035526650785087 \t| Accuracy: 106.042\n",
      "# Iteration  3273 -> Loss: 0.5034717887134937 \t| Accuracy: 106.042\n",
      "# Iteration  3274 -> Loss: 0.5033909658899969 \t| Accuracy: 106.042\n",
      "# Iteration  3275 -> Loss: 0.5033101965578533 \t| Accuracy: 106.042\n",
      "# Iteration  3276 -> Loss: 0.5032294806669783 \t| Accuracy: 106.042\n",
      "# Iteration  3277 -> Loss: 0.503148818167368 \t| Accuracy: 106.042\n",
      "# Iteration  3278 -> Loss: 0.5030682090090988 \t| Accuracy: 106.042\n",
      "# Iteration  3279 -> Loss: 0.502987653142327 \t| Accuracy: 106.042\n",
      "# Iteration  3280 -> Loss: 0.5029071505172884 \t| Accuracy: 106.042\n",
      "# Iteration  3281 -> Loss: 0.502826701084299 \t| Accuracy: 106.042\n",
      "# Iteration  3282 -> Loss: 0.5027463047937537 \t| Accuracy: 106.042\n",
      "# Iteration  3283 -> Loss: 0.5026659615961265 \t| Accuracy: 106.042\n",
      "# Iteration  3284 -> Loss: 0.5025856714419705 \t| Accuracy: 106.042\n",
      "# Iteration  3285 -> Loss: 0.5025054342819172 \t| Accuracy: 106.042\n",
      "# Iteration  3286 -> Loss: 0.5024252500666768 \t| Accuracy: 106.042\n",
      "# Iteration  3287 -> Loss: 0.5023451187470375 \t| Accuracy: 106.042\n",
      "# Iteration  3288 -> Loss: 0.5022650402738655 \t| Accuracy: 106.042\n",
      "# Iteration  3289 -> Loss: 0.5021850145981048 \t| Accuracy: 105.417\n",
      "# Iteration  3290 -> Loss: 0.5021050416707774 \t| Accuracy: 105.417\n",
      "# Iteration  3291 -> Loss: 0.5020251214429816 \t| Accuracy: 105.417\n",
      "# Iteration  3292 -> Loss: 0.5019452538658935 \t| Accuracy: 105.417\n",
      "# Iteration  3293 -> Loss: 0.501865438890766 \t| Accuracy: 105.417\n",
      "# Iteration  3294 -> Loss: 0.5017856764689284 \t| Accuracy: 105.417\n",
      "# Iteration  3295 -> Loss: 0.5017059665517867 \t| Accuracy: 105.417\n",
      "# Iteration  3296 -> Loss: 0.5016263090908225 \t| Accuracy: 105.417\n",
      "# Iteration  3297 -> Loss: 0.501546704037594 \t| Accuracy: 105.417\n",
      "# Iteration  3298 -> Loss: 0.5014671513437351 \t| Accuracy: 105.417\n",
      "# Iteration  3299 -> Loss: 0.5013876509609546 \t| Accuracy: 105.417\n",
      "# Iteration  3300 -> Loss: 0.5013082028410372 \t| Accuracy: 105.417\n",
      "# Iteration  3301 -> Loss: 0.5012288069358422 \t| Accuracy: 105.417\n",
      "# Iteration  3302 -> Loss: 0.5011494631973038 \t| Accuracy: 105.417\n",
      "# Iteration  3303 -> Loss: 0.5010701715774313 \t| Accuracy: 105.417\n",
      "# Iteration  3304 -> Loss: 0.5009909320283079 \t| Accuracy: 105.417\n",
      "# Iteration  3305 -> Loss: 0.500911744502091 \t| Accuracy: 105.417\n",
      "# Iteration  3306 -> Loss: 0.5008326089510119 \t| Accuracy: 105.417\n",
      "# Iteration  3307 -> Loss: 0.5007535253273758 \t| Accuracy: 105.417\n",
      "# Iteration  3308 -> Loss: 0.5006744935835614 \t| Accuracy: 105.417\n",
      "# Iteration  3309 -> Loss: 0.5005955136720203 \t| Accuracy: 105.417\n",
      "# Iteration  3310 -> Loss: 0.5005165855452774 \t| Accuracy: 105.417\n",
      "# Iteration  3311 -> Loss: 0.5004377091559306 \t| Accuracy: 105.417\n",
      "# Iteration  3312 -> Loss: 0.5003588844566499 \t| Accuracy: 105.417\n",
      "# Iteration  3313 -> Loss: 0.5002801114001781 \t| Accuracy: 105.417\n",
      "# Iteration  3314 -> Loss: 0.50020138993933 \t| Accuracy: 105.417\n",
      "# Iteration  3315 -> Loss: 0.5001227200269923 \t| Accuracy: 105.417\n",
      "# Iteration  3316 -> Loss: 0.5000441016161234 \t| Accuracy: 105.417\n",
      "# Iteration  3317 -> Loss: 0.4999655346597533 \t| Accuracy: 105.417\n",
      "# Iteration  3318 -> Loss: 0.49988701911098316 \t| Accuracy: 105.417\n",
      "# Iteration  3319 -> Loss: 0.499808554922985 \t| Accuracy: 105.417\n",
      "# Iteration  3320 -> Loss: 0.49973014204900207 \t| Accuracy: 105.417\n",
      "# Iteration  3321 -> Loss: 0.49965178044234804 \t| Accuracy: 105.417\n",
      "# Iteration  3322 -> Loss: 0.4995734700564067 \t| Accuracy: 105.417\n",
      "# Iteration  3323 -> Loss: 0.49949521084463244 \t| Accuracy: 105.417\n",
      "# Iteration  3324 -> Loss: 0.49941700276054934 \t| Accuracy: 105.417\n",
      "# Iteration  3325 -> Loss: 0.4993388457577513 \t| Accuracy: 105.417\n",
      "# Iteration  3326 -> Loss: 0.4992607397899018 \t| Accuracy: 105.417\n",
      "# Iteration  3327 -> Loss: 0.49918268481073325 \t| Accuracy: 105.417\n",
      "# Iteration  3328 -> Loss: 0.4991046807740478 \t| Accuracy: 105.417\n",
      "# Iteration  3329 -> Loss: 0.4990267276337157 \t| Accuracy: 105.417\n",
      "# Iteration  3330 -> Loss: 0.4989488253436764 \t| Accuracy: 105.417\n",
      "# Iteration  3331 -> Loss: 0.49887097385793755 \t| Accuracy: 105.417\n",
      "# Iteration  3332 -> Loss: 0.4987931731305751 \t| Accuracy: 105.417\n",
      "# Iteration  3333 -> Loss: 0.49871542311573297 \t| Accuracy: 105.417\n",
      "# Iteration  3334 -> Loss: 0.498637723767623 \t| Accuracy: 105.417\n",
      "# Iteration  3335 -> Loss: 0.49856007504052413 \t| Accuracy: 105.417\n",
      "# Iteration  3336 -> Loss: 0.4984824768887833 \t| Accuracy: 105.417\n",
      "# Iteration  3337 -> Loss: 0.4984049292668143 \t| Accuracy: 105.417\n",
      "# Iteration  3338 -> Loss: 0.4983274321290977 \t| Accuracy: 105.417\n",
      "# Iteration  3339 -> Loss: 0.49824998543018095 \t| Accuracy: 105.417\n",
      "# Iteration  3340 -> Loss: 0.49817258912467827 \t| Accuracy: 105.417\n",
      "# Iteration  3341 -> Loss: 0.4980952431672698 \t| Accuracy: 105.417\n",
      "# Iteration  3342 -> Loss: 0.49801794751270184 \t| Accuracy: 105.417\n",
      "# Iteration  3343 -> Loss: 0.4979407021157866 \t| Accuracy: 105.417\n",
      "# Iteration  3344 -> Loss: 0.4978635069314023 \t| Accuracy: 105.417\n",
      "# Iteration  3345 -> Loss: 0.49778636191449216 \t| Accuracy: 105.417\n",
      "# Iteration  3346 -> Loss: 0.4977092670200649 \t| Accuracy: 105.417\n",
      "# Iteration  3347 -> Loss: 0.49763222220319403 \t| Accuracy: 105.417\n",
      "# Iteration  3348 -> Loss: 0.49755522741901836 \t| Accuracy: 105.417\n",
      "# Iteration  3349 -> Loss: 0.497478282622741 \t| Accuracy: 105.417\n",
      "# Iteration  3350 -> Loss: 0.4974013877696297 \t| Accuracy: 105.417\n",
      "# Iteration  3351 -> Loss: 0.49732454281501615 \t| Accuracy: 105.417\n",
      "# Iteration  3352 -> Loss: 0.49724774771429636 \t| Accuracy: 105.417\n",
      "# Iteration  3353 -> Loss: 0.4971710024229301 \t| Accuracy: 105.417\n",
      "# Iteration  3354 -> Loss: 0.49709430689644046 \t| Accuracy: 105.417\n",
      "# Iteration  3355 -> Loss: 0.4970176610904145 \t| Accuracy: 105.417\n",
      "# Iteration  3356 -> Loss: 0.496941064960502 \t| Accuracy: 105.417\n",
      "# Iteration  3357 -> Loss: 0.4968645184624161 \t| Accuracy: 105.417\n",
      "# Iteration  3358 -> Loss: 0.49678802155193236 \t| Accuracy: 105.417\n",
      "# Iteration  3359 -> Loss: 0.4967115741848894 \t| Accuracy: 105.417\n",
      "# Iteration  3360 -> Loss: 0.4966351763171879 \t| Accuracy: 105.417\n",
      "# Iteration  3361 -> Loss: 0.4965588279047911 \t| Accuracy: 105.417\n",
      "# Iteration  3362 -> Loss: 0.49648252890372374 \t| Accuracy: 105.417\n",
      "# Iteration  3363 -> Loss: 0.4964062792700728 \t| Accuracy: 105.417\n",
      "# Iteration  3364 -> Loss: 0.4963300789599869 \t| Accuracy: 105.417\n",
      "# Iteration  3365 -> Loss: 0.4962539279296758 \t| Accuracy: 105.417\n",
      "# Iteration  3366 -> Loss: 0.49617782613541067 \t| Accuracy: 105.417\n",
      "# Iteration  3367 -> Loss: 0.49610177353352375 \t| Accuracy: 105.417\n",
      "# Iteration  3368 -> Loss: 0.4960257700804079 \t| Accuracy: 105.417\n",
      "# Iteration  3369 -> Loss: 0.4959498157325169 \t| Accuracy: 105.417\n",
      "# Iteration  3370 -> Loss: 0.4958739104463648 \t| Accuracy: 105.417\n",
      "# Iteration  3371 -> Loss: 0.49579805417852607 \t| Accuracy: 105.417\n",
      "# Iteration  3372 -> Loss: 0.4957222468856349 \t| Accuracy: 105.417\n",
      "# Iteration  3373 -> Loss: 0.4956464885243857 \t| Accuracy: 105.417\n",
      "# Iteration  3374 -> Loss: 0.4955707790515327 \t| Accuracy: 105.417\n",
      "# Iteration  3375 -> Loss: 0.495495118423889 \t| Accuracy: 105.417\n",
      "# Iteration  3376 -> Loss: 0.4954195065983277 \t| Accuracy: 105.417\n",
      "# Iteration  3377 -> Loss: 0.49534394353178035 \t| Accuracy: 105.417\n",
      "# Iteration  3378 -> Loss: 0.4952684291812382 \t| Accuracy: 105.417\n",
      "# Iteration  3379 -> Loss: 0.4951929635037506 \t| Accuracy: 105.417\n",
      "# Iteration  3380 -> Loss: 0.49511754645642564 \t| Accuracy: 105.417\n",
      "# Iteration  3381 -> Loss: 0.4950421779964299 \t| Accuracy: 105.417\n",
      "# Iteration  3382 -> Loss: 0.49496685808098817 \t| Accuracy: 105.417\n",
      "# Iteration  3383 -> Loss: 0.4948915866673829 \t| Accuracy: 105.417\n",
      "# Iteration  3384 -> Loss: 0.4948163637129548 \t| Accuracy: 105.417\n",
      "# Iteration  3385 -> Loss: 0.4947411891751019 \t| Accuracy: 105.417\n",
      "# Iteration  3386 -> Loss: 0.4946660630112798 \t| Accuracy: 105.417\n",
      "# Iteration  3387 -> Loss: 0.4945909851790014 \t| Accuracy: 105.417\n",
      "# Iteration  3388 -> Loss: 0.49451595563583656 \t| Accuracy: 105.417\n",
      "# Iteration  3389 -> Loss: 0.49444097433941214 \t| Accuracy: 105.417\n",
      "# Iteration  3390 -> Loss: 0.4943660412474118 \t| Accuracy: 105.417\n",
      "# Iteration  3391 -> Loss: 0.49429115631757553 \t| Accuracy: 105.417\n",
      "# Iteration  3392 -> Loss: 0.49421631950769995 \t| Accuracy: 105.417\n",
      "# Iteration  3393 -> Loss: 0.49414153077563755 \t| Accuracy: 105.417\n",
      "# Iteration  3394 -> Loss: 0.49406679007929716 \t| Accuracy: 105.417\n",
      "# Iteration  3395 -> Loss: 0.4939920973766434 \t| Accuracy: 105.417\n",
      "# Iteration  3396 -> Loss: 0.49391745262569625 \t| Accuracy: 105.417\n",
      "# Iteration  3397 -> Loss: 0.49384285578453163 \t| Accuracy: 105.417\n",
      "# Iteration  3398 -> Loss: 0.4937683068112803 \t| Accuracy: 105.417\n",
      "# Iteration  3399 -> Loss: 0.4936938056641285 \t| Accuracy: 105.417\n",
      "# Iteration  3400 -> Loss: 0.4936193523013173 \t| Accuracy: 105.417\n",
      "# Iteration  3401 -> Loss: 0.49354494668114274 \t| Accuracy: 105.417\n",
      "# Iteration  3402 -> Loss: 0.4934705887619552 \t| Accuracy: 105.417\n",
      "# Iteration  3403 -> Loss: 0.49339627850215967 \t| Accuracy: 105.417\n",
      "# Iteration  3404 -> Loss: 0.4933220158602154 \t| Accuracy: 105.417\n",
      "# Iteration  3405 -> Loss: 0.4932478007946359 \t| Accuracy: 105.417\n",
      "# Iteration  3406 -> Loss: 0.4931736332639883 \t| Accuracy: 105.417\n",
      "# Iteration  3407 -> Loss: 0.49309951322689366 \t| Accuracy: 105.417\n",
      "# Iteration  3408 -> Loss: 0.4930254406420269 \t| Accuracy: 105.417\n",
      "# Iteration  3409 -> Loss: 0.492951415468116 \t| Accuracy: 105.417\n",
      "# Iteration  3410 -> Loss: 0.49287743766394243 \t| Accuracy: 105.417\n",
      "# Iteration  3411 -> Loss: 0.4928035071883407 \t| Accuracy: 105.417\n",
      "# Iteration  3412 -> Loss: 0.49272962400019826 \t| Accuracy: 105.417\n",
      "# Iteration  3413 -> Loss: 0.4926557880584555 \t| Accuracy: 105.417\n",
      "# Iteration  3414 -> Loss: 0.49258199932210534 \t| Accuracy: 105.417\n",
      "# Iteration  3415 -> Loss: 0.49250825775019286 \t| Accuracy: 105.417\n",
      "# Iteration  3416 -> Loss: 0.4924345633018159 \t| Accuracy: 105.417\n",
      "# Iteration  3417 -> Loss: 0.4923609159361242 \t| Accuracy: 105.417\n",
      "# Iteration  3418 -> Loss: 0.49228731561231953 \t| Accuracy: 105.417\n",
      "# Iteration  3419 -> Loss: 0.49221376228965535 \t| Accuracy: 105.417\n",
      "# Iteration  3420 -> Loss: 0.49214025592743704 \t| Accuracy: 105.417\n",
      "# Iteration  3421 -> Loss: 0.49206679648502133 \t| Accuracy: 105.417\n",
      "# Iteration  3422 -> Loss: 0.491993383921816 \t| Accuracy: 105.417\n",
      "# Iteration  3423 -> Loss: 0.4919200181972807 \t| Accuracy: 105.417\n",
      "# Iteration  3424 -> Loss: 0.49184669927092545 \t| Accuracy: 105.417\n",
      "# Iteration  3425 -> Loss: 0.49177342710231153 \t| Accuracy: 105.417\n",
      "# Iteration  3426 -> Loss: 0.4917002016510507 \t| Accuracy: 105.417\n",
      "# Iteration  3427 -> Loss: 0.4916270228768056 \t| Accuracy: 105.417\n",
      "# Iteration  3428 -> Loss: 0.49155389073928896 \t| Accuracy: 105.417\n",
      "# Iteration  3429 -> Loss: 0.4914808051982638 \t| Accuracy: 105.417\n",
      "# Iteration  3430 -> Loss: 0.49140776621354326 \t| Accuracy: 105.417\n",
      "# Iteration  3431 -> Loss: 0.4913347737449909 \t| Accuracy: 105.417\n",
      "# Iteration  3432 -> Loss: 0.4912618277525194 \t| Accuracy: 105.417\n",
      "# Iteration  3433 -> Loss: 0.49118892819609145 \t| Accuracy: 105.417\n",
      "# Iteration  3434 -> Loss: 0.49111607503571914 \t| Accuracy: 105.417\n",
      "# Iteration  3435 -> Loss: 0.4910432682314641 \t| Accuracy: 105.417\n",
      "# Iteration  3436 -> Loss: 0.4909705077434369 \t| Accuracy: 105.417\n",
      "# Iteration  3437 -> Loss: 0.4908977935317974 \t| Accuracy: 105.417\n",
      "# Iteration  3438 -> Loss: 0.4908251255567544 \t| Accuracy: 105.417\n",
      "# Iteration  3439 -> Loss: 0.49075250377856505 \t| Accuracy: 105.417\n",
      "# Iteration  3440 -> Loss: 0.4906799281575357 \t| Accuracy: 105.417\n",
      "# Iteration  3441 -> Loss: 0.49060739865402075 \t| Accuracy: 105.417\n",
      "# Iteration  3442 -> Loss: 0.49053491522842324 \t| Accuracy: 105.417\n",
      "# Iteration  3443 -> Loss: 0.4904624778411941 \t| Accuracy: 105.417\n",
      "# Iteration  3444 -> Loss: 0.49039008645283283 \t| Accuracy: 105.417\n",
      "# Iteration  3445 -> Loss: 0.490317741023886 \t| Accuracy: 105.417\n",
      "# Iteration  3446 -> Loss: 0.49024544151494875 \t| Accuracy: 105.417\n",
      "# Iteration  3447 -> Loss: 0.4901731878866634 \t| Accuracy: 105.417\n",
      "# Iteration  3448 -> Loss: 0.4901009800997201 \t| Accuracy: 105.417\n",
      "# Iteration  3449 -> Loss: 0.49002881811485594 \t| Accuracy: 105.417\n",
      "# Iteration  3450 -> Loss: 0.4899567018928555 \t| Accuracy: 105.417\n",
      "# Iteration  3451 -> Loss: 0.48988463139455035 \t| Accuracy: 105.417\n",
      "# Iteration  3452 -> Loss: 0.4898126065808191 \t| Accuracy: 105.417\n",
      "# Iteration  3453 -> Loss: 0.48974062741258695 \t| Accuracy: 105.417\n",
      "# Iteration  3454 -> Loss: 0.48966869385082584 \t| Accuracy: 105.417\n",
      "# Iteration  3455 -> Loss: 0.4895968058565545 \t| Accuracy: 105.417\n",
      "# Iteration  3456 -> Loss: 0.4895249633908377 \t| Accuracy: 105.417\n",
      "# Iteration  3457 -> Loss: 0.4894531664147866 \t| Accuracy: 105.417\n",
      "# Iteration  3458 -> Loss: 0.4893814148895585 \t| Accuracy: 105.417\n",
      "# Iteration  3459 -> Loss: 0.48930970877635666 \t| Accuracy: 105.417\n",
      "# Iteration  3460 -> Loss: 0.4892380480364304 \t| Accuracy: 105.417\n",
      "# Iteration  3461 -> Loss: 0.4891664326310745 \t| Accuracy: 105.417\n",
      "# Iteration  3462 -> Loss: 0.48909486252162954 \t| Accuracy: 105.417\n",
      "# Iteration  3463 -> Loss: 0.4890233376694813 \t| Accuracy: 105.417\n",
      "# Iteration  3464 -> Loss: 0.4889518580360612 \t| Accuracy: 105.417\n",
      "# Iteration  3465 -> Loss: 0.4888804235828459 \t| Accuracy: 105.417\n",
      "# Iteration  3466 -> Loss: 0.48880903427135675 \t| Accuracy: 105.417\n",
      "# Iteration  3467 -> Loss: 0.4887376900631606 \t| Accuracy: 105.417\n",
      "# Iteration  3468 -> Loss: 0.4886663909198686 \t| Accuracy: 105.417\n",
      "# Iteration  3469 -> Loss: 0.4885951368031369 \t| Accuracy: 105.417\n",
      "# Iteration  3470 -> Loss: 0.4885239276746662 \t| Accuracy: 105.417\n",
      "# Iteration  3471 -> Loss: 0.4884527634962018 \t| Accuracy: 105.417\n",
      "# Iteration  3472 -> Loss: 0.48838164422953306 \t| Accuracy: 105.417\n",
      "# Iteration  3473 -> Loss: 0.4883105698364936 \t| Accuracy: 105.417\n",
      "# Iteration  3474 -> Loss: 0.48823954027896127 \t| Accuracy: 105.417\n",
      "# Iteration  3475 -> Loss: 0.48816855551885774 \t| Accuracy: 105.417\n",
      "# Iteration  3476 -> Loss: 0.4880976155181486 \t| Accuracy: 105.417\n",
      "# Iteration  3477 -> Loss: 0.48802672023884314 \t| Accuracy: 105.417\n",
      "# Iteration  3478 -> Loss: 0.48795586964299426 \t| Accuracy: 105.417\n",
      "# Iteration  3479 -> Loss: 0.4878850636926984 \t| Accuracy: 105.417\n",
      "# Iteration  3480 -> Loss: 0.4878143023500951 \t| Accuracy: 105.417\n",
      "# Iteration  3481 -> Loss: 0.48774358557736736 \t| Accuracy: 105.417\n",
      "# Iteration  3482 -> Loss: 0.4876729133367414 \t| Accuracy: 105.417\n",
      "# Iteration  3483 -> Loss: 0.4876022855904862 \t| Accuracy: 105.417\n",
      "# Iteration  3484 -> Loss: 0.48753170230091375 \t| Accuracy: 105.417\n",
      "# Iteration  3485 -> Loss: 0.48746116343037876 \t| Accuracy: 105.417\n",
      "# Iteration  3486 -> Loss: 0.4873906689412788 \t| Accuracy: 105.417\n",
      "# Iteration  3487 -> Loss: 0.4873202187960535 \t| Accuracy: 105.417\n",
      "# Iteration  3488 -> Loss: 0.48724981295718545 \t| Accuracy: 105.417\n",
      "# Iteration  3489 -> Loss: 0.4871794513871993 \t| Accuracy: 105.417\n",
      "# Iteration  3490 -> Loss: 0.48710913404866196 \t| Accuracy: 105.417\n",
      "# Iteration  3491 -> Loss: 0.48703886090418225 \t| Accuracy: 105.417\n",
      "# Iteration  3492 -> Loss: 0.4869686319164113 \t| Accuracy: 105.417\n",
      "# Iteration  3493 -> Loss: 0.4868984470480417 \t| Accuracy: 105.417\n",
      "# Iteration  3494 -> Loss: 0.4868283062618081 \t| Accuracy: 105.417\n",
      "# Iteration  3495 -> Loss: 0.48675820952048665 \t| Accuracy: 105.417\n",
      "# Iteration  3496 -> Loss: 0.4866881567868951 \t| Accuracy: 105.417\n",
      "# Iteration  3497 -> Loss: 0.4866181480238926 \t| Accuracy: 105.417\n",
      "# Iteration  3498 -> Loss: 0.4865481831943794 \t| Accuracy: 105.417\n",
      "# Iteration  3499 -> Loss: 0.48647826226129737 \t| Accuracy: 105.417\n",
      "# Iteration  3500 -> Loss: 0.4864083851876291 \t| Accuracy: 105.417\n",
      "# Iteration  3501 -> Loss: 0.48633855193639836 \t| Accuracy: 105.417\n",
      "# Iteration  3502 -> Loss: 0.4862687624706698 \t| Accuracy: 105.417\n",
      "# Iteration  3503 -> Loss: 0.48619901675354865 \t| Accuracy: 105.417\n",
      "# Iteration  3504 -> Loss: 0.4861293147481811 \t| Accuracy: 105.417\n",
      "# Iteration  3505 -> Loss: 0.4860596564177538 \t| Accuracy: 105.417\n",
      "# Iteration  3506 -> Loss: 0.48599004172549354 \t| Accuracy: 105.417\n",
      "# Iteration  3507 -> Loss: 0.4859204706346681 \t| Accuracy: 105.417\n",
      "# Iteration  3508 -> Loss: 0.4858509431085849 \t| Accuracy: 105.417\n",
      "# Iteration  3509 -> Loss: 0.48578145911059173 \t| Accuracy: 105.417\n",
      "# Iteration  3510 -> Loss: 0.4857120186040767 \t| Accuracy: 105.417\n",
      "# Iteration  3511 -> Loss: 0.4856426215524675 \t| Accuracy: 105.417\n",
      "# Iteration  3512 -> Loss: 0.4855732679192316 \t| Accuracy: 105.417\n",
      "# Iteration  3513 -> Loss: 0.48550395766787663 \t| Accuracy: 105.417\n",
      "# Iteration  3514 -> Loss: 0.48543469076194967 \t| Accuracy: 105.417\n",
      "# Iteration  3515 -> Loss: 0.48536546716503715 \t| Accuracy: 105.417\n",
      "# Iteration  3516 -> Loss: 0.48529628684076503 \t| Accuracy: 105.417\n",
      "# Iteration  3517 -> Loss: 0.48522714975279885 \t| Accuracy: 105.417\n",
      "# Iteration  3518 -> Loss: 0.4851580558648431 \t| Accuracy: 105.417\n",
      "# Iteration  3519 -> Loss: 0.48508900514064174 \t| Accuracy: 105.417\n",
      "# Iteration  3520 -> Loss: 0.48501999754397734 \t| Accuracy: 105.417\n",
      "# Iteration  3521 -> Loss: 0.4849510330386718 \t| Accuracy: 105.417\n",
      "# Iteration  3522 -> Loss: 0.48488211158858563 \t| Accuracy: 105.417\n",
      "# Iteration  3523 -> Loss: 0.48481323315761843 \t| Accuracy: 105.417\n",
      "# Iteration  3524 -> Loss: 0.484744397709708 \t| Accuracy: 105.417\n",
      "# Iteration  3525 -> Loss: 0.4846756052088312 \t| Accuracy: 105.417\n",
      "# Iteration  3526 -> Loss: 0.48460685561900296 \t| Accuracy: 105.417\n",
      "# Iteration  3527 -> Loss: 0.484538148904277 \t| Accuracy: 105.417\n",
      "# Iteration  3528 -> Loss: 0.4844694850287448 \t| Accuracy: 105.417\n",
      "# Iteration  3529 -> Loss: 0.48440086395653664 \t| Accuracy: 105.417\n",
      "# Iteration  3530 -> Loss: 0.4843322856518204 \t| Accuracy: 105.417\n",
      "# Iteration  3531 -> Loss: 0.4842637500788021 \t| Accuracy: 105.417\n",
      "# Iteration  3532 -> Loss: 0.48419525720172624 \t| Accuracy: 105.417\n",
      "# Iteration  3533 -> Loss: 0.48412680698487426 \t| Accuracy: 105.417\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Iteration  3534 -> Loss: 0.48405839939256595 \t| Accuracy: 105.417\n",
      "# Iteration  3535 -> Loss: 0.48399003438915844 \t| Accuracy: 105.417\n",
      "# Iteration  3536 -> Loss: 0.48392171193904665 \t| Accuracy: 105.417\n",
      "# Iteration  3537 -> Loss: 0.4838534320066628 \t| Accuracy: 105.417\n",
      "# Iteration  3538 -> Loss: 0.4837851945564768 \t| Accuracy: 105.417\n",
      "# Iteration  3539 -> Loss: 0.48371699955299535 \t| Accuracy: 105.417\n",
      "# Iteration  3540 -> Loss: 0.48364884696076277 \t| Accuracy: 105.417\n",
      "# Iteration  3541 -> Loss: 0.4835807367443605 \t| Accuracy: 105.417\n",
      "# Iteration  3542 -> Loss: 0.48351266886840666 \t| Accuracy: 105.417\n",
      "# Iteration  3543 -> Loss: 0.4834446432975567 \t| Accuracy: 105.417\n",
      "# Iteration  3544 -> Loss: 0.48337665999650287 \t| Accuracy: 105.417\n",
      "# Iteration  3545 -> Loss: 0.483308718929974 \t| Accuracy: 105.417\n",
      "# Iteration  3546 -> Loss: 0.48324082006273583 \t| Accuracy: 105.417\n",
      "# Iteration  3547 -> Loss: 0.48317296335959053 \t| Accuracy: 105.417\n",
      "# Iteration  3548 -> Loss: 0.483105148785377 \t| Accuracy: 105.417\n",
      "# Iteration  3549 -> Loss: 0.48303737630497046 \t| Accuracy: 105.417\n",
      "# Iteration  3550 -> Loss: 0.48296964588328245 \t| Accuracy: 105.417\n",
      "# Iteration  3551 -> Loss: 0.4829019574852609 \t| Accuracy: 105.417\n",
      "# Iteration  3552 -> Loss: 0.48283431107588987 \t| Accuracy: 105.417\n",
      "# Iteration  3553 -> Loss: 0.4827667066201895 \t| Accuracy: 105.417\n",
      "# Iteration  3554 -> Loss: 0.48269914408321607 \t| Accuracy: 105.417\n",
      "# Iteration  3555 -> Loss: 0.4826316234300618 \t| Accuracy: 105.417\n",
      "# Iteration  3556 -> Loss: 0.4825641446258545 \t| Accuracy: 105.417\n",
      "# Iteration  3557 -> Loss: 0.48249670763575847 \t| Accuracy: 105.417\n",
      "# Iteration  3558 -> Loss: 0.4824293124249729 \t| Accuracy: 105.417\n",
      "# Iteration  3559 -> Loss: 0.4823619589587331 \t| Accuracy: 105.417\n",
      "# Iteration  3560 -> Loss: 0.4822946472023097 \t| Accuracy: 105.417\n",
      "# Iteration  3561 -> Loss: 0.4822273771210091 \t| Accuracy: 105.417\n",
      "# Iteration  3562 -> Loss: 0.4821601486801729 \t| Accuracy: 105.417\n",
      "# Iteration  3563 -> Loss: 0.4820929618451779 \t| Accuracy: 105.417\n",
      "# Iteration  3564 -> Loss: 0.48202581658143645 \t| Accuracy: 105.417\n",
      "# Iteration  3565 -> Loss: 0.4819587128543958 \t| Accuracy: 105.417\n",
      "# Iteration  3566 -> Loss: 0.48189165062953837 \t| Accuracy: 105.417\n",
      "# Iteration  3567 -> Loss: 0.4818246298723817 \t| Accuracy: 105.417\n",
      "# Iteration  3568 -> Loss: 0.48175765054847824 \t| Accuracy: 105.417\n",
      "# Iteration  3569 -> Loss: 0.4816907126234152 \t| Accuracy: 105.417\n",
      "# Iteration  3570 -> Loss: 0.48162381606281457 \t| Accuracy: 105.417\n",
      "# Iteration  3571 -> Loss: 0.4815569608323332 \t| Accuracy: 105.417\n",
      "# Iteration  3572 -> Loss: 0.4814901468976625 \t| Accuracy: 105.417\n",
      "# Iteration  3573 -> Loss: 0.4814233742245284 \t| Accuracy: 105.417\n",
      "# Iteration  3574 -> Loss: 0.48135664277869145 \t| Accuracy: 105.417\n",
      "# Iteration  3575 -> Loss: 0.48128995252594653 \t| Accuracy: 105.417\n",
      "# Iteration  3576 -> Loss: 0.481223303432123 \t| Accuracy: 105.417\n",
      "# Iteration  3577 -> Loss: 0.48115669546308415 \t| Accuracy: 105.417\n",
      "# Iteration  3578 -> Loss: 0.48109012858472794 \t| Accuracy: 105.417\n",
      "# Iteration  3579 -> Loss: 0.48102360276298606 \t| Accuracy: 105.417\n",
      "# Iteration  3580 -> Loss: 0.4809571179638246 \t| Accuracy: 105.417\n",
      "# Iteration  3581 -> Loss: 0.48089067415324355 \t| Accuracy: 105.417\n",
      "# Iteration  3582 -> Loss: 0.48082427129727656 \t| Accuracy: 105.417\n",
      "# Iteration  3583 -> Loss: 0.48075790936199153 \t| Accuracy: 105.417\n",
      "# Iteration  3584 -> Loss: 0.48069158831348974 \t| Accuracy: 105.417\n",
      "# Iteration  3585 -> Loss: 0.4806253081179066 \t| Accuracy: 105.417\n",
      "# Iteration  3586 -> Loss: 0.48055906874141113 \t| Accuracy: 105.417\n",
      "# Iteration  3587 -> Loss: 0.4804928701502053 \t| Accuracy: 105.417\n",
      "# Iteration  3588 -> Loss: 0.4804267123105253 \t| Accuracy: 105.417\n",
      "# Iteration  3589 -> Loss: 0.4803605951886406 \t| Accuracy: 105.417\n",
      "# Iteration  3590 -> Loss: 0.4802945187508539 \t| Accuracy: 105.417\n",
      "# Iteration  3591 -> Loss: 0.48022848296350135 \t| Accuracy: 105.417\n",
      "# Iteration  3592 -> Loss: 0.48016248779295206 \t| Accuracy: 105.417\n",
      "# Iteration  3593 -> Loss: 0.4800965332056088 \t| Accuracy: 105.417\n",
      "# Iteration  3594 -> Loss: 0.4800306191679069 \t| Accuracy: 105.417\n",
      "# Iteration  3595 -> Loss: 0.4799647456463153 \t| Accuracy: 105.417\n",
      "# Iteration  3596 -> Loss: 0.4798989126073354 \t| Accuracy: 105.417\n",
      "# Iteration  3597 -> Loss: 0.47983312001750195 \t| Accuracy: 105.417\n",
      "# Iteration  3598 -> Loss: 0.4797673678433822 \t| Accuracy: 105.417\n",
      "# Iteration  3599 -> Loss: 0.47970165605157633 \t| Accuracy: 105.417\n",
      "# Iteration  3600 -> Loss: 0.4796359846087173 \t| Accuracy: 105.417\n",
      "# Iteration  3601 -> Loss: 0.4795703534814707 \t| Accuracy: 105.417\n",
      "# Iteration  3602 -> Loss: 0.4795047626365347 \t| Accuracy: 105.417\n",
      "# Iteration  3603 -> Loss: 0.47943921204063983 \t| Accuracy: 105.417\n",
      "# Iteration  3604 -> Loss: 0.4793737016605494 \t| Accuracy: 105.417\n",
      "# Iteration  3605 -> Loss: 0.47930823146305895 \t| Accuracy: 105.417\n",
      "# Iteration  3606 -> Loss: 0.4792428014149965 \t| Accuracy: 105.417\n",
      "# Iteration  3607 -> Loss: 0.47917741148322207 \t| Accuracy: 105.417\n",
      "# Iteration  3608 -> Loss: 0.4791120616346282 \t| Accuracy: 105.417\n",
      "# Iteration  3609 -> Loss: 0.47904675183613943 \t| Accuracy: 105.417\n",
      "# Iteration  3610 -> Loss: 0.4789814820547125 \t| Accuracy: 105.417\n",
      "# Iteration  3611 -> Loss: 0.478916252257336 \t| Accuracy: 105.417\n",
      "# Iteration  3612 -> Loss: 0.47885106241103087 \t| Accuracy: 105.417\n",
      "# Iteration  3613 -> Loss: 0.4787859124828496 \t| Accuracy: 105.417\n",
      "# Iteration  3614 -> Loss: 0.47872080243987664 \t| Accuracy: 105.417\n",
      "# Iteration  3615 -> Loss: 0.4786557322492284 \t| Accuracy: 105.417\n",
      "# Iteration  3616 -> Loss: 0.4785907018780529 \t| Accuracy: 105.417\n",
      "# Iteration  3617 -> Loss: 0.4785257112935298 \t| Accuracy: 105.417\n",
      "# Iteration  3618 -> Loss: 0.47846076046287056 \t| Accuracy: 105.417\n",
      "# Iteration  3619 -> Loss: 0.4783958493533181 \t| Accuracy: 105.417\n",
      "# Iteration  3620 -> Loss: 0.4783309779321467 \t| Accuracy: 105.417\n",
      "# Iteration  3621 -> Loss: 0.47826614616666235 \t| Accuracy: 105.417\n",
      "# Iteration  3622 -> Loss: 0.4782013540242023 \t| Accuracy: 105.417\n",
      "# Iteration  3623 -> Loss: 0.4781366014721353 \t| Accuracy: 105.417\n",
      "# Iteration  3624 -> Loss: 0.47807188847786114 \t| Accuracy: 105.417\n",
      "# Iteration  3625 -> Loss: 0.47800721500881105 \t| Accuracy: 105.417\n",
      "# Iteration  3626 -> Loss: 0.47794258103244736 \t| Accuracy: 105.417\n",
      "# Iteration  3627 -> Loss: 0.47787798651626345 \t| Accuracy: 105.417\n",
      "# Iteration  3628 -> Loss: 0.4778134314277837 \t| Accuracy: 105.417\n",
      "# Iteration  3629 -> Loss: 0.4777489157345639 \t| Accuracy: 105.417\n",
      "# Iteration  3630 -> Loss: 0.4776844394041902 \t| Accuracy: 105.417\n",
      "# Iteration  3631 -> Loss: 0.4776200024042801 \t| Accuracy: 105.417\n",
      "# Iteration  3632 -> Loss: 0.47755560470248165 \t| Accuracy: 105.417\n",
      "# Iteration  3633 -> Loss: 0.477491246266474 \t| Accuracy: 105.417\n",
      "# Iteration  3634 -> Loss: 0.4774269270639669 \t| Accuracy: 105.417\n",
      "# Iteration  3635 -> Loss: 0.4773626470627006 \t| Accuracy: 105.417\n",
      "# Iteration  3636 -> Loss: 0.4772984062304461 \t| Accuracy: 105.417\n",
      "# Iteration  3637 -> Loss: 0.47723420453500515 \t| Accuracy: 105.417\n",
      "# Iteration  3638 -> Loss: 0.4771700419442098 \t| Accuracy: 105.417\n",
      "# Iteration  3639 -> Loss: 0.47710591842592265 \t| Accuracy: 105.417\n",
      "# Iteration  3640 -> Loss: 0.47704183394803673 \t| Accuracy: 105.417\n",
      "# Iteration  3641 -> Loss: 0.47697778847847533 \t| Accuracy: 105.417\n",
      "# Iteration  3642 -> Loss: 0.4769137819851922 \t| Accuracy: 105.417\n",
      "# Iteration  3643 -> Loss: 0.4768498144361714 \t| Accuracy: 105.417\n",
      "# Iteration  3644 -> Loss: 0.4767858857994271 \t| Accuracy: 105.417\n",
      "# Iteration  3645 -> Loss: 0.4767219960430034 \t| Accuracy: 105.417\n",
      "# Iteration  3646 -> Loss: 0.47665814513497495 \t| Accuracy: 105.417\n",
      "# Iteration  3647 -> Loss: 0.4765943330434463 \t| Accuracy: 105.417\n",
      "# Iteration  3648 -> Loss: 0.47653055973655173 \t| Accuracy: 105.417\n",
      "# Iteration  3649 -> Loss: 0.476466825182456 \t| Accuracy: 105.417\n",
      "# Iteration  3650 -> Loss: 0.4764031293493532 \t| Accuracy: 105.417\n",
      "# Iteration  3651 -> Loss: 0.4763394722054679 \t| Accuracy: 105.417\n",
      "# Iteration  3652 -> Loss: 0.47627585371905407 \t| Accuracy: 105.417\n",
      "# Iteration  3653 -> Loss: 0.4762122738583953 \t| Accuracy: 105.417\n",
      "# Iteration  3654 -> Loss: 0.4761487325918055 \t| Accuracy: 105.417\n",
      "# Iteration  3655 -> Loss: 0.47608522988762775 \t| Accuracy: 105.417\n",
      "# Iteration  3656 -> Loss: 0.47602176571423477 \t| Accuracy: 105.417\n",
      "# Iteration  3657 -> Loss: 0.47595834004002907 \t| Accuracy: 105.417\n",
      "# Iteration  3658 -> Loss: 0.47589495283344263 \t| Accuracy: 105.417\n",
      "# Iteration  3659 -> Loss: 0.4758316040629368 \t| Accuracy: 105.417\n",
      "# Iteration  3660 -> Loss: 0.4757682936970024 \t| Accuracy: 105.417\n",
      "# Iteration  3661 -> Loss: 0.4757050217041596 \t| Accuracy: 105.417\n",
      "# Iteration  3662 -> Loss: 0.47564178805295804 \t| Accuracy: 105.417\n",
      "# Iteration  3663 -> Loss: 0.4755785927119766 \t| Accuracy: 105.417\n",
      "# Iteration  3664 -> Loss: 0.47551543564982324 \t| Accuracy: 105.417\n",
      "# Iteration  3665 -> Loss: 0.4754523168351354 \t| Accuracy: 105.417\n",
      "# Iteration  3666 -> Loss: 0.47538923623657936 \t| Accuracy: 105.417\n",
      "# Iteration  3667 -> Loss: 0.4753261938228508 \t| Accuracy: 105.417\n",
      "# Iteration  3668 -> Loss: 0.4752631895626741 \t| Accuracy: 105.417\n",
      "# Iteration  3669 -> Loss: 0.4752002234248031 \t| Accuracy: 105.417\n",
      "# Iteration  3670 -> Loss: 0.47513729537802035 \t| Accuracy: 105.417\n",
      "# Iteration  3671 -> Loss: 0.4750744053911372 \t| Accuracy: 105.417\n",
      "# Iteration  3672 -> Loss: 0.4750115534329943 \t| Accuracy: 105.417\n",
      "# Iteration  3673 -> Loss: 0.47494873947246063 \t| Accuracy: 105.417\n",
      "# Iteration  3674 -> Loss: 0.4748859634784343 \t| Accuracy: 105.417\n",
      "# Iteration  3675 -> Loss: 0.4748232254198421 \t| Accuracy: 105.417\n",
      "# Iteration  3676 -> Loss: 0.47476052526563955 \t| Accuracy: 105.417\n",
      "# Iteration  3677 -> Loss: 0.4746978629848106 \t| Accuracy: 105.417\n",
      "# Iteration  3678 -> Loss: 0.474635238546368 \t| Accuracy: 105.417\n",
      "# Iteration  3679 -> Loss: 0.4745726519193534 \t| Accuracy: 105.417\n",
      "# Iteration  3680 -> Loss: 0.47451010307283625 \t| Accuracy: 105.417\n",
      "# Iteration  3681 -> Loss: 0.47444759197591513 \t| Accuracy: 105.417\n",
      "# Iteration  3682 -> Loss: 0.4743851185977167 \t| Accuracy: 105.417\n",
      "# Iteration  3683 -> Loss: 0.47432268290739626 \t| Accuracy: 105.417\n",
      "# Iteration  3684 -> Loss: 0.47426028487413735 \t| Accuracy: 105.417\n",
      "# Iteration  3685 -> Loss: 0.47419792446715175 \t| Accuracy: 105.417\n",
      "# Iteration  3686 -> Loss: 0.4741356016556797 \t| Accuracy: 105.417\n",
      "# Iteration  3687 -> Loss: 0.4740733164089894 \t| Accuracy: 105.417\n",
      "# Iteration  3688 -> Loss: 0.4740110686963778 \t| Accuracy: 105.417\n",
      "# Iteration  3689 -> Loss: 0.4739488584871694 \t| Accuracy: 105.417\n",
      "# Iteration  3690 -> Loss: 0.47388668575071696 \t| Accuracy: 105.417\n",
      "# Iteration  3691 -> Loss: 0.47382455045640154 \t| Accuracy: 105.417\n",
      "# Iteration  3692 -> Loss: 0.4737624525736321 \t| Accuracy: 105.417\n",
      "# Iteration  3693 -> Loss: 0.4737003920718456 \t| Accuracy: 105.417\n",
      "# Iteration  3694 -> Loss: 0.4736383689205068 \t| Accuracy: 105.417\n",
      "# Iteration  3695 -> Loss: 0.4735763830891087 \t| Accuracy: 105.417\n",
      "# Iteration  3696 -> Loss: 0.47351443454717185 \t| Accuracy: 105.417\n",
      "# Iteration  3697 -> Loss: 0.4734525232642448 \t| Accuracy: 105.417\n",
      "# Iteration  3698 -> Loss: 0.4733906492099039 \t| Accuracy: 105.417\n",
      "# Iteration  3699 -> Loss: 0.4733288123537531 \t| Accuracy: 105.417\n",
      "# Iteration  3700 -> Loss: 0.47326701266542437 \t| Accuracy: 105.417\n",
      "# Iteration  3701 -> Loss: 0.4732052501145769 \t| Accuracy: 105.417\n",
      "# Iteration  3702 -> Loss: 0.473143524670898 \t| Accuracy: 105.417\n",
      "# Iteration  3703 -> Loss: 0.47308183630410217 \t| Accuracy: 105.417\n",
      "# Iteration  3704 -> Loss: 0.4730201849839319 \t| Accuracy: 105.417\n",
      "# Iteration  3705 -> Loss: 0.47295857068015673 \t| Accuracy: 105.417\n",
      "# Iteration  3706 -> Loss: 0.47289699336257396 \t| Accuracy: 105.417\n",
      "# Iteration  3707 -> Loss: 0.47283545300100827 \t| Accuracy: 105.417\n",
      "# Iteration  3708 -> Loss: 0.47277394956531194 \t| Accuracy: 105.417\n",
      "# Iteration  3709 -> Loss: 0.47271248302536417 \t| Accuracy: 105.417\n",
      "# Iteration  3710 -> Loss: 0.47265105335107194 \t| Accuracy: 105.417\n",
      "# Iteration  3711 -> Loss: 0.47258966051236934 \t| Accuracy: 105.417\n",
      "# Iteration  3712 -> Loss: 0.4725283044792177 \t| Accuracy: 105.417\n",
      "# Iteration  3713 -> Loss: 0.4724669852216056 \t| Accuracy: 105.417\n",
      "# Iteration  3714 -> Loss: 0.47240570270954874 \t| Accuracy: 105.417\n",
      "# Iteration  3715 -> Loss: 0.47234445691309024 \t| Accuracy: 105.417\n",
      "# Iteration  3716 -> Loss: 0.47228324780229985 \t| Accuracy: 105.417\n",
      "# Iteration  3717 -> Loss: 0.47222207534727495 \t| Accuracy: 105.417\n",
      "# Iteration  3718 -> Loss: 0.47216093951813953 \t| Accuracy: 105.417\n",
      "# Iteration  3719 -> Loss: 0.4720998402850447 \t| Accuracy: 105.417\n",
      "# Iteration  3720 -> Loss: 0.4720387776181687 \t| Accuracy: 105.417\n",
      "# Iteration  3721 -> Loss: 0.4719777514877164 \t| Accuracy: 105.417\n",
      "# Iteration  3722 -> Loss: 0.47191676186391995 \t| Accuracy: 105.417\n",
      "# Iteration  3723 -> Loss: 0.47185580871703814 \t| Accuracy: 105.417\n",
      "# Iteration  3724 -> Loss: 0.4717948920173564 \t| Accuracy: 105.417\n",
      "# Iteration  3725 -> Loss: 0.4717340117351874 \t| Accuracy: 105.417\n",
      "# Iteration  3726 -> Loss: 0.47167316784087016 \t| Accuracy: 105.417\n",
      "# Iteration  3727 -> Loss: 0.47161236030477055 \t| Accuracy: 105.417\n",
      "# Iteration  3728 -> Loss: 0.47155158909728134 \t| Accuracy: 105.417\n",
      "# Iteration  3729 -> Loss: 0.4714908541888217 \t| Accuracy: 105.417\n",
      "# Iteration  3730 -> Loss: 0.4714301555498376 \t| Accuracy: 105.417\n",
      "# Iteration  3731 -> Loss: 0.47136949315080123 \t| Accuracy: 105.417\n",
      "# Iteration  3732 -> Loss: 0.47130886696221175 \t| Accuracy: 105.417\n",
      "# Iteration  3733 -> Loss: 0.47124827695459465 \t| Accuracy: 105.417\n",
      "# Iteration  3734 -> Loss: 0.47118772309850193 \t| Accuracy: 105.417\n",
      "# Iteration  3735 -> Loss: 0.47112720536451214 \t| Accuracy: 105.417\n",
      "# Iteration  3736 -> Loss: 0.47106672372323005 \t| Accuracy: 105.417\n",
      "# Iteration  3737 -> Loss: 0.471006278145287 \t| Accuracy: 105.417\n",
      "# Iteration  3738 -> Loss: 0.4709458686013404 \t| Accuracy: 105.417\n",
      "# Iteration  3739 -> Loss: 0.47088549506207433 \t| Accuracy: 105.417\n",
      "# Iteration  3740 -> Loss: 0.4708251574981991 \t| Accuracy: 105.417\n",
      "# Iteration  3741 -> Loss: 0.47076485588045086 \t| Accuracy: 105.417\n",
      "# Iteration  3742 -> Loss: 0.4707045901795926 \t| Accuracy: 105.417\n",
      "# Iteration  3743 -> Loss: 0.470644360366413 \t| Accuracy: 105.417\n",
      "# Iteration  3744 -> Loss: 0.47058416641172707 \t| Accuracy: 105.417\n",
      "# Iteration  3745 -> Loss: 0.47052400828637614 \t| Accuracy: 105.417\n",
      "# Iteration  3746 -> Loss: 0.47046388596122735 \t| Accuracy: 105.417\n",
      "# Iteration  3747 -> Loss: 0.47040379940717375 \t| Accuracy: 105.417\n",
      "# Iteration  3748 -> Loss: 0.470343748595135 \t| Accuracy: 105.417\n",
      "# Iteration  3749 -> Loss: 0.47028373349605623 \t| Accuracy: 105.417\n",
      "# Iteration  3750 -> Loss: 0.4702237540809088 \t| Accuracy: 105.417\n",
      "# Iteration  3751 -> Loss: 0.4701638103206899 \t| Accuracy: 105.417\n",
      "# Iteration  3752 -> Loss: 0.4701039021864226 \t| Accuracy: 105.417\n",
      "# Iteration  3753 -> Loss: 0.470044029649156 \t| Accuracy: 105.417\n",
      "# Iteration  3754 -> Loss: 0.46998419267996483 \t| Accuracy: 105.417\n",
      "# Iteration  3755 -> Loss: 0.46992439124994967 \t| Accuracy: 105.417\n",
      "# Iteration  3756 -> Loss: 0.469864625330237 \t| Accuracy: 105.417\n",
      "# Iteration  3757 -> Loss: 0.46980489489197896 \t| Accuracy: 105.417\n",
      "# Iteration  3758 -> Loss: 0.4697451999063534 \t| Accuracy: 105.417\n",
      "# Iteration  3759 -> Loss: 0.4696855403445639 \t| Accuracy: 105.417\n",
      "# Iteration  3760 -> Loss: 0.4696259161778398 \t| Accuracy: 105.417\n",
      "# Iteration  3761 -> Loss: 0.46956632737743564 \t| Accuracy: 105.417\n",
      "# Iteration  3762 -> Loss: 0.4695067739146321 \t| Accuracy: 105.417\n",
      "# Iteration  3763 -> Loss: 0.46944725576073515 \t| Accuracy: 105.417\n",
      "# Iteration  3764 -> Loss: 0.4693877728870763 \t| Accuracy: 105.417\n",
      "# Iteration  3765 -> Loss: 0.46932832526501256 \t| Accuracy: 105.417\n",
      "# Iteration  3766 -> Loss: 0.4692689128659264 \t| Accuracy: 105.417\n",
      "# Iteration  3767 -> Loss: 0.469209535661226 \t| Accuracy: 105.417\n",
      "# Iteration  3768 -> Loss: 0.4691501936223446 \t| Accuracy: 105.417\n",
      "# Iteration  3769 -> Loss: 0.46909088672074095 \t| Accuracy: 105.417\n",
      "# Iteration  3770 -> Loss: 0.46903161492789924 \t| Accuracy: 105.417\n",
      "# Iteration  3771 -> Loss: 0.468972378215329 \t| Accuracy: 105.417\n",
      "# Iteration  3772 -> Loss: 0.4689131765545648 \t| Accuracy: 105.417\n",
      "# Iteration  3773 -> Loss: 0.4688540099171669 \t| Accuracy: 105.417\n",
      "# Iteration  3774 -> Loss: 0.4687948782747206 \t| Accuracy: 105.417\n",
      "# Iteration  3775 -> Loss: 0.4687357815988362 \t| Accuracy: 105.417\n",
      "# Iteration  3776 -> Loss: 0.46867671986114956 \t| Accuracy: 105.417\n",
      "# Iteration  3777 -> Loss: 0.4686176930333215 \t| Accuracy: 105.417\n",
      "# Iteration  3778 -> Loss: 0.46855870108703795 \t| Accuracy: 105.417\n",
      "# Iteration  3779 -> Loss: 0.4684997439940101 \t| Accuracy: 105.417\n",
      "# Iteration  3780 -> Loss: 0.468440821725974 \t| Accuracy: 105.417\n",
      "# Iteration  3781 -> Loss: 0.46838193425469105 \t| Accuracy: 105.417\n",
      "# Iteration  3782 -> Loss: 0.46832308155194724 \t| Accuracy: 105.417\n",
      "# Iteration  3783 -> Loss: 0.4682642635895539 \t| Accuracy: 105.417\n",
      "# Iteration  3784 -> Loss: 0.4682054803393473 \t| Accuracy: 105.417\n",
      "# Iteration  3785 -> Loss: 0.46814673177318844 \t| Accuracy: 105.417\n",
      "# Iteration  3786 -> Loss: 0.46808801786296356 \t| Accuracy: 105.417\n",
      "# Iteration  3787 -> Loss: 0.4680293385805833 \t| Accuracy: 105.417\n",
      "# Iteration  3788 -> Loss: 0.4679706938979836 \t| Accuracy: 105.417\n",
      "# Iteration  3789 -> Loss: 0.4679120837871251 \t| Accuracy: 105.417\n",
      "# Iteration  3790 -> Loss: 0.4678535082199931 \t| Accuracy: 105.417\n",
      "# Iteration  3791 -> Loss: 0.46779496716859786 \t| Accuracy: 105.417\n",
      "# Iteration  3792 -> Loss: 0.46773646060497426 \t| Accuracy: 105.417\n",
      "# Iteration  3793 -> Loss: 0.4676779885011819 \t| Accuracy: 105.417\n",
      "# Iteration  3794 -> Loss: 0.4676195508293053 \t| Accuracy: 105.417\n",
      "# Iteration  3795 -> Loss: 0.4675611475614534 \t| Accuracy: 105.417\n",
      "# Iteration  3796 -> Loss: 0.46750277866975964 \t| Accuracy: 105.417\n",
      "# Iteration  3797 -> Loss: 0.4674444441263826 \t| Accuracy: 105.417\n",
      "# Iteration  3798 -> Loss: 0.467386143903505 \t| Accuracy: 105.417\n",
      "# Iteration  3799 -> Loss: 0.4673278779733343 \t| Accuracy: 105.417\n",
      "# Iteration  3800 -> Loss: 0.4672696463081024 \t| Accuracy: 105.417\n",
      "# Iteration  3801 -> Loss: 0.4672114488800658 \t| Accuracy: 105.417\n",
      "# Iteration  3802 -> Loss: 0.46715328566150555 \t| Accuracy: 105.417\n",
      "# Iteration  3803 -> Loss: 0.46709515662472695 \t| Accuracy: 105.417\n",
      "# Iteration  3804 -> Loss: 0.46703706174205983 \t| Accuracy: 105.417\n",
      "# Iteration  3805 -> Loss: 0.4669790009858586 \t| Accuracy: 105.417\n",
      "# Iteration  3806 -> Loss: 0.4669209743285017 \t| Accuracy: 105.417\n",
      "# Iteration  3807 -> Loss: 0.4668629817423923 \t| Accuracy: 105.417\n",
      "# Iteration  3808 -> Loss: 0.4668050231999577 \t| Accuracy: 105.417\n",
      "# Iteration  3809 -> Loss: 0.4667470986736496 \t| Accuracy: 105.417\n",
      "# Iteration  3810 -> Loss: 0.4666892081359438 \t| Accuracy: 105.417\n",
      "# Iteration  3811 -> Loss: 0.4666313515593405 \t| Accuracy: 105.417\n",
      "# Iteration  3812 -> Loss: 0.4665735289163642 \t| Accuracy: 105.417\n",
      "# Iteration  3813 -> Loss: 0.46651574017956354 \t| Accuracy: 105.417\n",
      "# Iteration  3814 -> Loss: 0.46645798532151117 \t| Accuracy: 105.417\n",
      "# Iteration  3815 -> Loss: 0.4664002643148042 \t| Accuracy: 105.417\n",
      "# Iteration  3816 -> Loss: 0.46634257713206373 \t| Accuracy: 105.417\n",
      "# Iteration  3817 -> Loss: 0.4662849237459348 \t| Accuracy: 105.417\n",
      "# Iteration  3818 -> Loss: 0.466227304129087 \t| Accuracy: 105.417\n",
      "# Iteration  3819 -> Loss: 0.4661697182542133 \t| Accuracy: 105.417\n",
      "# Iteration  3820 -> Loss: 0.4661121660940315 \t| Accuracy: 105.417\n",
      "# Iteration  3821 -> Loss: 0.46605464762128257 \t| Accuracy: 105.417\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Iteration  3822 -> Loss: 0.4659971628087323 \t| Accuracy: 105.417\n",
      "# Iteration  3823 -> Loss: 0.46593971162916986 \t| Accuracy: 105.417\n",
      "# Iteration  3824 -> Loss: 0.46588229405540843 \t| Accuracy: 105.417\n",
      "# Iteration  3825 -> Loss: 0.46582491006028554 \t| Accuracy: 105.417\n",
      "# Iteration  3826 -> Loss: 0.46576755961666205 \t| Accuracy: 105.417\n",
      "# Iteration  3827 -> Loss: 0.4657102426974229 \t| Accuracy: 105.417\n",
      "# Iteration  3828 -> Loss: 0.46565295927547706 \t| Accuracy: 105.417\n",
      "# Iteration  3829 -> Loss: 0.4655957093237571 \t| Accuracy: 105.417\n",
      "# Iteration  3830 -> Loss: 0.4655384928152194 \t| Accuracy: 105.417\n",
      "# Iteration  3831 -> Loss: 0.4654813097228444 \t| Accuracy: 105.417\n",
      "# Iteration  3832 -> Loss: 0.4654241600196357 \t| Accuracy: 105.417\n",
      "# Iteration  3833 -> Loss: 0.4653670436786213 \t| Accuracy: 105.417\n",
      "# Iteration  3834 -> Loss: 0.4653099606728525 \t| Accuracy: 105.417\n",
      "# Iteration  3835 -> Loss: 0.4652529109754043 \t| Accuracy: 105.417\n",
      "# Iteration  3836 -> Loss: 0.4651958945593755 \t| Accuracy: 105.417\n",
      "# Iteration  3837 -> Loss: 0.4651389113978885 \t| Accuracy: 105.417\n",
      "# Iteration  3838 -> Loss: 0.46508196146408926 \t| Accuracy: 105.417\n",
      "# Iteration  3839 -> Loss: 0.46502504473114736 \t| Accuracy: 105.417\n",
      "# Iteration  3840 -> Loss: 0.46496816117225587 \t| Accuracy: 105.417\n",
      "# Iteration  3841 -> Loss: 0.46491131076063164 \t| Accuracy: 105.417\n",
      "# Iteration  3842 -> Loss: 0.4648544934695148 \t| Accuracy: 105.417\n",
      "# Iteration  3843 -> Loss: 0.464797709272169 \t| Accuracy: 105.417\n",
      "# Iteration  3844 -> Loss: 0.46474095814188143 \t| Accuracy: 105.417\n",
      "# Iteration  3845 -> Loss: 0.46468424005196285 \t| Accuracy: 105.417\n",
      "# Iteration  3846 -> Loss: 0.46462755497574737 \t| Accuracy: 105.417\n",
      "# Iteration  3847 -> Loss: 0.4645709028865923 \t| Accuracy: 105.417\n",
      "# Iteration  3848 -> Loss: 0.46451428375787857 \t| Accuracy: 105.417\n",
      "# Iteration  3849 -> Loss: 0.46445769756301053 \t| Accuracy: 105.417\n",
      "# Iteration  3850 -> Loss: 0.4644011442754155 \t| Accuracy: 105.417\n",
      "# Iteration  3851 -> Loss: 0.4643446238685448 \t| Accuracy: 105.417\n",
      "# Iteration  3852 -> Loss: 0.4642881363158722 \t| Accuracy: 105.417\n",
      "# Iteration  3853 -> Loss: 0.46423168159089556 \t| Accuracy: 105.417\n",
      "# Iteration  3854 -> Loss: 0.46417525966713535 \t| Accuracy: 105.417\n",
      "# Iteration  3855 -> Loss: 0.4641188705181358 \t| Accuracy: 105.417\n",
      "# Iteration  3856 -> Loss: 0.4640625141174638 \t| Accuracy: 105.417\n",
      "# Iteration  3857 -> Loss: 0.46400619043871005 \t| Accuracy: 105.417\n",
      "# Iteration  3858 -> Loss: 0.4639498994554878 \t| Accuracy: 105.417\n",
      "# Iteration  3859 -> Loss: 0.463893641141434 \t| Accuracy: 105.417\n",
      "# Iteration  3860 -> Loss: 0.46383741547020846 \t| Accuracy: 105.417\n",
      "# Iteration  3861 -> Loss: 0.46378122241549385 \t| Accuracy: 105.417\n",
      "# Iteration  3862 -> Loss: 0.4637250619509965 \t| Accuracy: 105.417\n",
      "# Iteration  3863 -> Loss: 0.46366893405044535 \t| Accuracy: 105.417\n",
      "# Iteration  3864 -> Loss: 0.4636128386875925 \t| Accuracy: 105.417\n",
      "# Iteration  3865 -> Loss: 0.4635567758362132 \t| Accuracy: 105.417\n",
      "# Iteration  3866 -> Loss: 0.4635007454701055 \t| Accuracy: 105.417\n",
      "# Iteration  3867 -> Loss: 0.4634447475630906 \t| Accuracy: 105.417\n",
      "# Iteration  3868 -> Loss: 0.4633887820890126 \t| Accuracy: 105.417\n",
      "# Iteration  3869 -> Loss: 0.46333284902173844 \t| Accuracy: 105.417\n",
      "# Iteration  3870 -> Loss: 0.46327694833515803 \t| Accuracy: 105.417\n",
      "# Iteration  3871 -> Loss: 0.46322108000318435 \t| Accuracy: 105.417\n",
      "# Iteration  3872 -> Loss: 0.46316524399975306 \t| Accuracy: 105.417\n",
      "# Iteration  3873 -> Loss: 0.4631094402988226 \t| Accuracy: 105.417\n",
      "# Iteration  3874 -> Loss: 0.46305366887437455 \t| Accuracy: 105.417\n",
      "# Iteration  3875 -> Loss: 0.46299792970041304 \t| Accuracy: 105.417\n",
      "# Iteration  3876 -> Loss: 0.4629422227509651 \t| Accuracy: 105.417\n",
      "# Iteration  3877 -> Loss: 0.4628865480000804 \t| Accuracy: 105.417\n",
      "# Iteration  3878 -> Loss: 0.46283090542183153 \t| Accuracy: 105.417\n",
      "# Iteration  3879 -> Loss: 0.4627752949903139 \t| Accuracy: 105.417\n",
      "# Iteration  3880 -> Loss: 0.46271971667964534 \t| Accuracy: 105.417\n",
      "# Iteration  3881 -> Loss: 0.4626641704639666 \t| Accuracy: 105.417\n",
      "# Iteration  3882 -> Loss: 0.46260865631744086 \t| Accuracy: 105.417\n",
      "# Iteration  3883 -> Loss: 0.4625531742142543 \t| Accuracy: 105.417\n",
      "# Iteration  3884 -> Loss: 0.4624977241286153 \t| Accuracy: 105.417\n",
      "# Iteration  3885 -> Loss: 0.4624423060347554 \t| Accuracy: 105.417\n",
      "# Iteration  3886 -> Loss: 0.46238691990692815 \t| Accuracy: 105.417\n",
      "# Iteration  3887 -> Loss: 0.46233156571941014 \t| Accuracy: 105.417\n",
      "# Iteration  3888 -> Loss: 0.46227624344650026 \t| Accuracy: 105.417\n",
      "# Iteration  3889 -> Loss: 0.46222095306252 \t| Accuracy: 105.417\n",
      "# Iteration  3890 -> Loss: 0.46216569454181333 \t| Accuracy: 105.417\n",
      "# Iteration  3891 -> Loss: 0.4621104678587467 \t| Accuracy: 105.417\n",
      "# Iteration  3892 -> Loss: 0.4620552729877091 \t| Accuracy: 105.417\n",
      "# Iteration  3893 -> Loss: 0.462000109903112 \t| Accuracy: 105.417\n",
      "# Iteration  3894 -> Loss: 0.46194497857938915 \t| Accuracy: 105.417\n",
      "# Iteration  3895 -> Loss: 0.46188987899099687 \t| Accuracy: 105.417\n",
      "# Iteration  3896 -> Loss: 0.46183481111241376 \t| Accuracy: 105.417\n",
      "# Iteration  3897 -> Loss: 0.461779774918141 \t| Accuracy: 105.417\n",
      "# Iteration  3898 -> Loss: 0.46172477038270177 \t| Accuracy: 105.417\n",
      "# Iteration  3899 -> Loss: 0.4616697974806419 \t| Accuracy: 105.417\n",
      "# Iteration  3900 -> Loss: 0.4616148561865295 \t| Accuracy: 106.042\n",
      "# Iteration  3901 -> Loss: 0.4615599464749548 \t| Accuracy: 106.042\n",
      "# Iteration  3902 -> Loss: 0.46150506832053045 \t| Accuracy: 106.042\n",
      "# Iteration  3903 -> Loss: 0.46145022169789135 \t| Accuracy: 106.042\n",
      "# Iteration  3904 -> Loss: 0.4613954065816947 \t| Accuracy: 106.042\n",
      "# Iteration  3905 -> Loss: 0.4613406229466197 \t| Accuracy: 106.042\n",
      "# Iteration  3906 -> Loss: 0.46128587076736793 \t| Accuracy: 106.042\n",
      "# Iteration  3907 -> Loss: 0.46123115001866327 \t| Accuracy: 106.042\n",
      "# Iteration  3908 -> Loss: 0.46117646067525153 \t| Accuracy: 106.042\n",
      "# Iteration  3909 -> Loss: 0.4611218027119006 \t| Accuracy: 106.042\n",
      "# Iteration  3910 -> Loss: 0.46106717610340103 \t| Accuracy: 106.042\n",
      "# Iteration  3911 -> Loss: 0.46101258082456475 \t| Accuracy: 106.042\n",
      "# Iteration  3912 -> Loss: 0.46095801685022625 \t| Accuracy: 106.042\n",
      "# Iteration  3913 -> Loss: 0.4609034841552422 \t| Accuracy: 106.042\n",
      "# Iteration  3914 -> Loss: 0.4608489827144907 \t| Accuracy: 106.042\n",
      "# Iteration  3915 -> Loss: 0.4607945125028727 \t| Accuracy: 106.042\n",
      "# Iteration  3916 -> Loss: 0.46074007349531043 \t| Accuracy: 106.042\n",
      "# Iteration  3917 -> Loss: 0.46068566566674846 \t| Accuracy: 106.042\n",
      "# Iteration  3918 -> Loss: 0.46063128899215355 \t| Accuracy: 106.042\n",
      "# Iteration  3919 -> Loss: 0.4605769434465142 \t| Accuracy: 106.042\n",
      "# Iteration  3920 -> Loss: 0.46052262900484053 \t| Accuracy: 106.042\n",
      "# Iteration  3921 -> Loss: 0.46046834564216527 \t| Accuracy: 106.042\n",
      "# Iteration  3922 -> Loss: 0.4604140933335425 \t| Accuracy: 106.042\n",
      "# Iteration  3923 -> Loss: 0.46035987205404827 \t| Accuracy: 106.042\n",
      "# Iteration  3924 -> Loss: 0.46030568177878084 \t| Accuracy: 106.042\n",
      "# Iteration  3925 -> Loss: 0.46025152248286 \t| Accuracy: 106.042\n",
      "# Iteration  3926 -> Loss: 0.4601973941414274 \t| Accuracy: 106.042\n",
      "# Iteration  3927 -> Loss: 0.46014329672964677 \t| Accuracy: 106.042\n",
      "# Iteration  3928 -> Loss: 0.4600892302227032 \t| Accuracy: 106.042\n",
      "# Iteration  3929 -> Loss: 0.460035194595804 \t| Accuracy: 106.042\n",
      "# Iteration  3930 -> Loss: 0.45998118982417785 \t| Accuracy: 106.042\n",
      "# Iteration  3931 -> Loss: 0.45992721588307545 \t| Accuracy: 106.042\n",
      "# Iteration  3932 -> Loss: 0.4598732727477692 \t| Accuracy: 106.042\n",
      "# Iteration  3933 -> Loss: 0.4598193603935531 \t| Accuracy: 106.042\n",
      "# Iteration  3934 -> Loss: 0.45976547879574287 \t| Accuracy: 106.042\n",
      "# Iteration  3935 -> Loss: 0.4597116279296759 \t| Accuracy: 106.042\n",
      "# Iteration  3936 -> Loss: 0.45965780777071136 \t| Accuracy: 106.042\n",
      "# Iteration  3937 -> Loss: 0.45960401829422987 \t| Accuracy: 106.042\n",
      "# Iteration  3938 -> Loss: 0.4595502594756338 \t| Accuracy: 106.042\n",
      "# Iteration  3939 -> Loss: 0.459496531290347 \t| Accuracy: 106.042\n",
      "# Iteration  3940 -> Loss: 0.45944283371381506 \t| Accuracy: 106.042\n",
      "# Iteration  3941 -> Loss: 0.45938916672150504 \t| Accuracy: 106.042\n",
      "# Iteration  3942 -> Loss: 0.4593355302889056 \t| Accuracy: 106.042\n",
      "# Iteration  3943 -> Loss: 0.459281924391527 \t| Accuracy: 106.042\n",
      "# Iteration  3944 -> Loss: 0.45922834900490056 \t| Accuracy: 106.042\n",
      "# Iteration  3945 -> Loss: 0.4591748041045798 \t| Accuracy: 106.042\n",
      "# Iteration  3946 -> Loss: 0.45912128966613924 \t| Accuracy: 106.042\n",
      "# Iteration  3947 -> Loss: 0.459067805665175 \t| Accuracy: 106.042\n",
      "# Iteration  3948 -> Loss: 0.45901435207730473 \t| Accuracy: 106.042\n",
      "# Iteration  3949 -> Loss: 0.45896092887816725 \t| Accuracy: 106.042\n",
      "# Iteration  3950 -> Loss: 0.45890753604342305 \t| Accuracy: 106.042\n",
      "# Iteration  3951 -> Loss: 0.45885417354875385 \t| Accuracy: 106.042\n",
      "# Iteration  3952 -> Loss: 0.458800841369863 \t| Accuracy: 106.042\n",
      "# Iteration  3953 -> Loss: 0.4587475394824749 \t| Accuracy: 106.042\n",
      "# Iteration  3954 -> Loss: 0.4586942678623354 \t| Accuracy: 106.042\n",
      "# Iteration  3955 -> Loss: 0.4586410264852117 \t| Accuracy: 106.042\n",
      "# Iteration  3956 -> Loss: 0.4585878153268925 \t| Accuracy: 106.042\n",
      "# Iteration  3957 -> Loss: 0.45853463436318737 \t| Accuracy: 106.042\n",
      "# Iteration  3958 -> Loss: 0.4584814835699274 \t| Accuracy: 106.042\n",
      "# Iteration  3959 -> Loss: 0.458428362922965 \t| Accuracy: 106.042\n",
      "# Iteration  3960 -> Loss: 0.45837527239817377 \t| Accuracy: 106.042\n",
      "# Iteration  3961 -> Loss: 0.4583222119714484 \t| Accuracy: 106.042\n",
      "# Iteration  3962 -> Loss: 0.45826918161870495 \t| Accuracy: 106.042\n",
      "# Iteration  3963 -> Loss: 0.45821618131588054 \t| Accuracy: 106.042\n",
      "# Iteration  3964 -> Loss: 0.45816321103893354 \t| Accuracy: 106.042\n",
      "# Iteration  3965 -> Loss: 0.4581102707638436 \t| Accuracy: 106.042\n",
      "# Iteration  3966 -> Loss: 0.4580573604666113 \t| Accuracy: 106.042\n",
      "# Iteration  3967 -> Loss: 0.4580044801232583 \t| Accuracy: 106.042\n",
      "# Iteration  3968 -> Loss: 0.45795162970982767 \t| Accuracy: 106.042\n",
      "# Iteration  3969 -> Loss: 0.45789880920238324 \t| Accuracy: 106.042\n",
      "# Iteration  3970 -> Loss: 0.4578460185770101 \t| Accuracy: 106.042\n",
      "# Iteration  3971 -> Loss: 0.45779325780981445 \t| Accuracy: 106.042\n",
      "# Iteration  3972 -> Loss: 0.4577405268769234 \t| Accuracy: 106.042\n",
      "# Iteration  3973 -> Loss: 0.4576878257544851 \t| Accuracy: 106.042\n",
      "# Iteration  3974 -> Loss: 0.4576351544186686 \t| Accuracy: 106.042\n",
      "# Iteration  3975 -> Loss: 0.45758251284566415 \t| Accuracy: 106.042\n",
      "# Iteration  3976 -> Loss: 0.45752990101168306 \t| Accuracy: 106.042\n",
      "# Iteration  3977 -> Loss: 0.4574773188929572 \t| Accuracy: 106.042\n",
      "# Iteration  3978 -> Loss: 0.45742476646573965 \t| Accuracy: 106.042\n",
      "# Iteration  3979 -> Loss: 0.45737224370630436 \t| Accuracy: 106.042\n",
      "# Iteration  3980 -> Loss: 0.4573197505909464 \t| Accuracy: 106.042\n",
      "# Iteration  3981 -> Loss: 0.45726728709598136 \t| Accuracy: 106.042\n",
      "# Iteration  3982 -> Loss: 0.4572148531977458 \t| Accuracy: 106.042\n",
      "# Iteration  3983 -> Loss: 0.45716244887259744 \t| Accuracy: 106.042\n",
      "# Iteration  3984 -> Loss: 0.45711007409691445 \t| Accuracy: 106.042\n",
      "# Iteration  3985 -> Loss: 0.4570577288470961 \t| Accuracy: 106.042\n",
      "# Iteration  3986 -> Loss: 0.4570054130995622 \t| Accuracy: 106.042\n",
      "# Iteration  3987 -> Loss: 0.4569531268307538 \t| Accuracy: 106.042\n",
      "# Iteration  3988 -> Loss: 0.45690087001713225 \t| Accuracy: 106.042\n",
      "# Iteration  3989 -> Loss: 0.4568486426351799 \t| Accuracy: 106.042\n",
      "# Iteration  3990 -> Loss: 0.4567964446613999 \t| Accuracy: 106.042\n",
      "# Iteration  3991 -> Loss: 0.45674427607231594 \t| Accuracy: 106.042\n",
      "# Iteration  3992 -> Loss: 0.4566921368444725 \t| Accuracy: 106.250\n",
      "# Iteration  3993 -> Loss: 0.4566400269544349 \t| Accuracy: 106.250\n",
      "# Iteration  3994 -> Loss: 0.456587946378789 \t| Accuracy: 106.250\n",
      "# Iteration  3995 -> Loss: 0.45653589509414144 \t| Accuracy: 106.250\n",
      "# Iteration  3996 -> Loss: 0.4564838730771193 \t| Accuracy: 106.250\n",
      "# Iteration  3997 -> Loss: 0.4564318803043703 \t| Accuracy: 106.250\n",
      "# Iteration  3998 -> Loss: 0.4563799167525631 \t| Accuracy: 106.250\n",
      "# Iteration  3999 -> Loss: 0.45632798239838673 \t| Accuracy: 106.250\n",
      "# Iteration  4000 -> Loss: 0.45627607721855085 \t| Accuracy: 106.250\n",
      "# Iteration  4001 -> Loss: 0.4562242011897857 \t| Accuracy: 106.250\n",
      "# Iteration  4002 -> Loss: 0.4561723542888418 \t| Accuracy: 106.250\n",
      "# Iteration  4003 -> Loss: 0.4561205364924909 \t| Accuracy: 106.250\n",
      "# Iteration  4004 -> Loss: 0.45606874777752454 \t| Accuracy: 106.250\n",
      "# Iteration  4005 -> Loss: 0.4560169881207551 \t| Accuracy: 106.250\n",
      "# Iteration  4006 -> Loss: 0.45596525749901556 \t| Accuracy: 106.250\n",
      "# Iteration  4007 -> Loss: 0.455913555889159 \t| Accuracy: 106.250\n",
      "# Iteration  4008 -> Loss: 0.4558618832680595 \t| Accuracy: 106.250\n",
      "# Iteration  4009 -> Loss: 0.4558102396126111 \t| Accuracy: 106.250\n",
      "# Iteration  4010 -> Loss: 0.45575862489972835 \t| Accuracy: 106.250\n",
      "# Iteration  4011 -> Loss: 0.45570703910634663 \t| Accuracy: 106.250\n",
      "# Iteration  4012 -> Loss: 0.4556554822094212 \t| Accuracy: 106.250\n",
      "# Iteration  4013 -> Loss: 0.4556039541859279 \t| Accuracy: 106.250\n",
      "# Iteration  4014 -> Loss: 0.45555245501286296 \t| Accuracy: 106.250\n",
      "# Iteration  4015 -> Loss: 0.45550098466724304 \t| Accuracy: 106.250\n",
      "# Iteration  4016 -> Loss: 0.4554495431261051 \t| Accuracy: 106.250\n",
      "# Iteration  4017 -> Loss: 0.45539813036650606 \t| Accuracy: 106.250\n",
      "# Iteration  4018 -> Loss: 0.4553467463655238 \t| Accuracy: 106.250\n",
      "# Iteration  4019 -> Loss: 0.45529539110025596 \t| Accuracy: 106.250\n",
      "# Iteration  4020 -> Loss: 0.45524406454782074 \t| Accuracy: 106.250\n",
      "# Iteration  4021 -> Loss: 0.4551927666853564 \t| Accuracy: 106.250\n",
      "# Iteration  4022 -> Loss: 0.45514149749002164 \t| Accuracy: 106.250\n",
      "# Iteration  4023 -> Loss: 0.4550902569389952 \t| Accuracy: 106.250\n",
      "# Iteration  4024 -> Loss: 0.4550390450094762 \t| Accuracy: 106.250\n",
      "# Iteration  4025 -> Loss: 0.4549878616786839 \t| Accuracy: 106.250\n",
      "# Iteration  4026 -> Loss: 0.45493670692385757 \t| Accuracy: 106.250\n",
      "# Iteration  4027 -> Loss: 0.45488558072225704 \t| Accuracy: 106.250\n",
      "# Iteration  4028 -> Loss: 0.4548344830511619 \t| Accuracy: 106.250\n",
      "# Iteration  4029 -> Loss: 0.45478341388787197 \t| Accuracy: 106.250\n",
      "# Iteration  4030 -> Loss: 0.45473237320970755 \t| Accuracy: 106.250\n",
      "# Iteration  4031 -> Loss: 0.4546813609940085 \t| Accuracy: 106.250\n",
      "# Iteration  4032 -> Loss: 0.4546303772181351 \t| Accuracy: 106.250\n",
      "# Iteration  4033 -> Loss: 0.4545794218594676 \t| Accuracy: 106.250\n",
      "# Iteration  4034 -> Loss: 0.45452849489540637 \t| Accuracy: 106.250\n",
      "# Iteration  4035 -> Loss: 0.4544775963033718 \t| Accuracy: 106.250\n",
      "# Iteration  4036 -> Loss: 0.45442672606080436 \t| Accuracy: 106.250\n",
      "# Iteration  4037 -> Loss: 0.45437588414516433 \t| Accuracy: 106.250\n",
      "# Iteration  4038 -> Loss: 0.45432507053393234 \t| Accuracy: 106.250\n",
      "# Iteration  4039 -> Loss: 0.45427428520460866 \t| Accuracy: 106.250\n",
      "# Iteration  4040 -> Loss: 0.4542235281347138 \t| Accuracy: 106.250\n",
      "# Iteration  4041 -> Loss: 0.45417279930178794 \t| Accuracy: 106.250\n",
      "# Iteration  4042 -> Loss: 0.45412209868339165 \t| Accuracy: 106.250\n",
      "# Iteration  4043 -> Loss: 0.45407142625710484 \t| Accuracy: 106.250\n",
      "# Iteration  4044 -> Loss: 0.4540207820005278 \t| Accuracy: 106.250\n",
      "# Iteration  4045 -> Loss: 0.45397016589128053 \t| Accuracy: 106.250\n",
      "# Iteration  4046 -> Loss: 0.4539195779070029 \t| Accuracy: 106.250\n",
      "# Iteration  4047 -> Loss: 0.45386901802535473 \t| Accuracy: 106.250\n",
      "# Iteration  4048 -> Loss: 0.45381848622401555 \t| Accuracy: 106.250\n",
      "# Iteration  4049 -> Loss: 0.45376798248068484 \t| Accuracy: 106.250\n",
      "# Iteration  4050 -> Loss: 0.4537175067730819 \t| Accuracy: 106.250\n",
      "# Iteration  4051 -> Loss: 0.4536670590789459 \t| Accuracy: 106.250\n",
      "# Iteration  4052 -> Loss: 0.45361663937603564 \t| Accuracy: 106.250\n",
      "# Iteration  4053 -> Loss: 0.4535662476421297 \t| Accuracy: 106.250\n",
      "# Iteration  4054 -> Loss: 0.45351588385502656 \t| Accuracy: 106.250\n",
      "# Iteration  4055 -> Loss: 0.45346554799254424 \t| Accuracy: 106.250\n",
      "# Iteration  4056 -> Loss: 0.4534152400325208 \t| Accuracy: 106.250\n",
      "# Iteration  4057 -> Loss: 0.4533649599528138 \t| Accuracy: 106.250\n",
      "# Iteration  4058 -> Loss: 0.4533147077313005 \t| Accuracy: 106.250\n",
      "# Iteration  4059 -> Loss: 0.453264483345878 \t| Accuracy: 106.250\n",
      "# Iteration  4060 -> Loss: 0.4532142867744629 \t| Accuracy: 106.250\n",
      "# Iteration  4061 -> Loss: 0.4531641179949915 \t| Accuracy: 106.250\n",
      "# Iteration  4062 -> Loss: 0.4531139769854197 \t| Accuracy: 106.250\n",
      "# Iteration  4063 -> Loss: 0.45306386372372326 \t| Accuracy: 106.250\n",
      "# Iteration  4064 -> Loss: 0.4530137781878974 \t| Accuracy: 106.250\n",
      "# Iteration  4065 -> Loss: 0.4529637203559567 \t| Accuracy: 106.250\n",
      "# Iteration  4066 -> Loss: 0.4529136902059358 \t| Accuracy: 106.250\n",
      "# Iteration  4067 -> Loss: 0.4528636877158886 \t| Accuracy: 106.250\n",
      "# Iteration  4068 -> Loss: 0.45281371286388855 \t| Accuracy: 106.250\n",
      "# Iteration  4069 -> Loss: 0.4527637656280287 \t| Accuracy: 106.250\n",
      "# Iteration  4070 -> Loss: 0.4527138459864218 \t| Accuracy: 106.250\n",
      "# Iteration  4071 -> Loss: 0.4526639539171998 \t| Accuracy: 106.250\n",
      "# Iteration  4072 -> Loss: 0.4526140893985144 \t| Accuracy: 106.250\n",
      "# Iteration  4073 -> Loss: 0.45256425240853665 \t| Accuracy: 106.250\n",
      "# Iteration  4074 -> Loss: 0.45251444292545695 \t| Accuracy: 106.250\n",
      "# Iteration  4075 -> Loss: 0.45246466092748544 \t| Accuracy: 106.250\n",
      "# Iteration  4076 -> Loss: 0.45241490639285165 \t| Accuracy: 106.250\n",
      "# Iteration  4077 -> Loss: 0.45236517929980424 \t| Accuracy: 106.250\n",
      "# Iteration  4078 -> Loss: 0.4523154796266116 \t| Accuracy: 106.250\n",
      "# Iteration  4079 -> Loss: 0.45226580735156136 \t| Accuracy: 106.250\n",
      "# Iteration  4080 -> Loss: 0.45221616245296065 \t| Accuracy: 106.250\n",
      "# Iteration  4081 -> Loss: 0.45216654490913577 \t| Accuracy: 106.250\n",
      "# Iteration  4082 -> Loss: 0.4521169546984326 \t| Accuracy: 106.250\n",
      "# Iteration  4083 -> Loss: 0.4520673917992161 \t| Accuracy: 106.250\n",
      "# Iteration  4084 -> Loss: 0.4520178561898709 \t| Accuracy: 106.250\n",
      "# Iteration  4085 -> Loss: 0.4519683478488005 \t| Accuracy: 106.250\n",
      "# Iteration  4086 -> Loss: 0.4519188667544281 \t| Accuracy: 106.250\n",
      "# Iteration  4087 -> Loss: 0.45186941288519594 \t| Accuracy: 106.250\n",
      "# Iteration  4088 -> Loss: 0.4518199862195657 \t| Accuracy: 106.250\n",
      "# Iteration  4089 -> Loss: 0.45177058673601805 \t| Accuracy: 106.250\n",
      "# Iteration  4090 -> Loss: 0.4517212144130532 \t| Accuracy: 106.250\n",
      "# Iteration  4091 -> Loss: 0.4516718692291903 \t| Accuracy: 106.250\n",
      "# Iteration  4092 -> Loss: 0.4516225511629679 \t| Accuracy: 106.250\n",
      "# Iteration  4093 -> Loss: 0.4515732601929437 \t| Accuracy: 106.250\n",
      "# Iteration  4094 -> Loss: 0.4515239962976945 \t| Accuracy: 106.250\n",
      "# Iteration  4095 -> Loss: 0.4514747594558164 \t| Accuracy: 106.250\n",
      "# Iteration  4096 -> Loss: 0.45142554964592474 \t| Accuracy: 106.250\n",
      "# Iteration  4097 -> Loss: 0.45137636684665344 \t| Accuracy: 106.250\n",
      "# Iteration  4098 -> Loss: 0.45132721103665624 \t| Accuracy: 106.250\n",
      "# Iteration  4099 -> Loss: 0.45127808219460563 \t| Accuracy: 106.250\n",
      "# Iteration  4100 -> Loss: 0.4512289802991932 \t| Accuracy: 106.250\n",
      "# Iteration  4101 -> Loss: 0.45117990532912977 \t| Accuracy: 106.250\n",
      "# Iteration  4102 -> Loss: 0.45113085726314495 \t| Accuracy: 106.250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Iteration  4103 -> Loss: 0.45108183607998775 \t| Accuracy: 106.250\n",
      "# Iteration  4104 -> Loss: 0.45103284175842595 \t| Accuracy: 106.250\n",
      "# Iteration  4105 -> Loss: 0.45098387427724634 \t| Accuracy: 106.250\n",
      "# Iteration  4106 -> Loss: 0.4509349336152551 \t| Accuracy: 106.250\n",
      "# Iteration  4107 -> Loss: 0.45088601975127696 \t| Accuracy: 106.250\n",
      "# Iteration  4108 -> Loss: 0.45083713266415576 \t| Accuracy: 106.250\n",
      "# Iteration  4109 -> Loss: 0.45078827233275426 \t| Accuracy: 106.250\n",
      "# Iteration  4110 -> Loss: 0.4507394387359545 \t| Accuracy: 106.250\n",
      "# Iteration  4111 -> Loss: 0.4506906318526571 \t| Accuracy: 106.250\n",
      "# Iteration  4112 -> Loss: 0.45064185166178156 \t| Accuracy: 106.250\n",
      "# Iteration  4113 -> Loss: 0.4505930981422666 \t| Accuracy: 106.250\n",
      "# Iteration  4114 -> Loss: 0.4505443712730698 \t| Accuracy: 106.250\n",
      "# Iteration  4115 -> Loss: 0.4504956710331672 \t| Accuracy: 106.250\n",
      "# Iteration  4116 -> Loss: 0.45044699740155414 \t| Accuracy: 106.250\n",
      "# Iteration  4117 -> Loss: 0.4503983503572447 \t| Accuracy: 106.250\n",
      "# Iteration  4118 -> Loss: 0.45034972987927174 \t| Accuracy: 106.250\n",
      "# Iteration  4119 -> Loss: 0.450301135946687 \t| Accuracy: 106.250\n",
      "# Iteration  4120 -> Loss: 0.4502525685385609 \t| Accuracy: 106.250\n",
      "# Iteration  4121 -> Loss: 0.45020402763398276 \t| Accuracy: 106.250\n",
      "# Iteration  4122 -> Loss: 0.450155513212061 \t| Accuracy: 106.250\n",
      "# Iteration  4123 -> Loss: 0.4501070252519221 \t| Accuracy: 106.250\n",
      "# Iteration  4124 -> Loss: 0.45005856373271186 \t| Accuracy: 106.250\n",
      "# Iteration  4125 -> Loss: 0.45001012863359463 \t| Accuracy: 106.250\n",
      "# Iteration  4126 -> Loss: 0.4499617199337535 \t| Accuracy: 106.250\n",
      "# Iteration  4127 -> Loss: 0.4499133376123903 \t| Accuracy: 106.250\n",
      "# Iteration  4128 -> Loss: 0.4498649816487254 \t| Accuracy: 106.250\n",
      "# Iteration  4129 -> Loss: 0.44981665202199816 \t| Accuracy: 106.250\n",
      "# Iteration  4130 -> Loss: 0.44976834871146626 \t| Accuracy: 106.250\n",
      "# Iteration  4131 -> Loss: 0.4497200716964064 \t| Accuracy: 106.250\n",
      "# Iteration  4132 -> Loss: 0.4496718209561134 \t| Accuracy: 106.250\n",
      "# Iteration  4133 -> Loss: 0.4496235964699015 \t| Accuracy: 106.250\n",
      "# Iteration  4134 -> Loss: 0.44957539821710263 \t| Accuracy: 106.250\n",
      "# Iteration  4135 -> Loss: 0.4495272261770681 \t| Accuracy: 106.250\n",
      "# Iteration  4136 -> Loss: 0.4494790803291674 \t| Accuracy: 106.250\n",
      "# Iteration  4137 -> Loss: 0.44943096065278865 \t| Accuracy: 106.250\n",
      "# Iteration  4138 -> Loss: 0.44938286712733855 \t| Accuracy: 106.250\n",
      "# Iteration  4139 -> Loss: 0.4493347997322423 \t| Accuracy: 106.250\n",
      "# Iteration  4140 -> Loss: 0.44928675844694366 \t| Accuracy: 106.250\n",
      "# Iteration  4141 -> Loss: 0.4492387432509051 \t| Accuracy: 106.250\n",
      "# Iteration  4142 -> Loss: 0.4491907541236072 \t| Accuracy: 106.250\n",
      "# Iteration  4143 -> Loss: 0.44914279104454946 \t| Accuracy: 106.250\n",
      "# Iteration  4144 -> Loss: 0.44909485399324944 \t| Accuracy: 106.250\n",
      "# Iteration  4145 -> Loss: 0.44904694294924347 \t| Accuracy: 106.250\n",
      "# Iteration  4146 -> Loss: 0.4489990578920862 \t| Accuracy: 106.250\n",
      "# Iteration  4147 -> Loss: 0.44895119880135065 \t| Accuracy: 106.250\n",
      "# Iteration  4148 -> Loss: 0.44890336565662836 \t| Accuracy: 106.250\n",
      "# Iteration  4149 -> Loss: 0.44885555843752933 \t| Accuracy: 106.250\n",
      "# Iteration  4150 -> Loss: 0.4488077771236818 \t| Accuracy: 106.250\n",
      "# Iteration  4151 -> Loss: 0.4487600216947325 \t| Accuracy: 106.250\n",
      "# Iteration  4152 -> Loss: 0.4487122921303464 \t| Accuracy: 106.250\n",
      "# Iteration  4153 -> Loss: 0.44866458841020695 \t| Accuracy: 106.250\n",
      "# Iteration  4154 -> Loss: 0.44861691051401587 \t| Accuracy: 106.250\n",
      "# Iteration  4155 -> Loss: 0.4485692584214932 \t| Accuracy: 106.250\n",
      "# Iteration  4156 -> Loss: 0.44852163211237744 \t| Accuracy: 106.250\n",
      "# Iteration  4157 -> Loss: 0.44847403156642507 \t| Accuracy: 106.250\n",
      "# Iteration  4158 -> Loss: 0.448426456763411 \t| Accuracy: 106.250\n",
      "# Iteration  4159 -> Loss: 0.4483789076831285 \t| Accuracy: 106.250\n",
      "# Iteration  4160 -> Loss: 0.4483313843053891 \t| Accuracy: 106.250\n",
      "# Iteration  4161 -> Loss: 0.4482838866100225 \t| Accuracy: 106.250\n",
      "# Iteration  4162 -> Loss: 0.44823641457687646 \t| Accuracy: 106.250\n",
      "# Iteration  4163 -> Loss: 0.44818896818581716 \t| Accuracy: 106.250\n",
      "# Iteration  4164 -> Loss: 0.44814154741672896 \t| Accuracy: 106.250\n",
      "# Iteration  4165 -> Loss: 0.4480941522495145 \t| Accuracy: 106.250\n",
      "# Iteration  4166 -> Loss: 0.4480467826640942 \t| Accuracy: 106.250\n",
      "# Iteration  4167 -> Loss: 0.447999438640407 \t| Accuracy: 106.250\n",
      "# Iteration  4168 -> Loss: 0.4479521201584098 \t| Accuracy: 106.250\n",
      "# Iteration  4169 -> Loss: 0.44790482719807795 \t| Accuracy: 106.250\n",
      "# Iteration  4170 -> Loss: 0.44785755973940444 \t| Accuracy: 106.250\n",
      "# Iteration  4171 -> Loss: 0.4478103177624007 \t| Accuracy: 106.250\n",
      "# Iteration  4172 -> Loss: 0.44776310124709606 \t| Accuracy: 106.250\n",
      "# Iteration  4173 -> Loss: 0.447715910173538 \t| Accuracy: 106.250\n",
      "# Iteration  4174 -> Loss: 0.44766874452179206 \t| Accuracy: 106.250\n",
      "# Iteration  4175 -> Loss: 0.4476216042719418 \t| Accuracy: 106.250\n",
      "# Iteration  4176 -> Loss: 0.447574489404089 \t| Accuracy: 106.250\n",
      "# Iteration  4177 -> Loss: 0.44752739989835305 \t| Accuracy: 106.250\n",
      "# Iteration  4178 -> Loss: 0.44748033573487184 \t| Accuracy: 106.250\n",
      "# Iteration  4179 -> Loss: 0.4474332968938006 \t| Accuracy: 106.250\n",
      "# Iteration  4180 -> Loss: 0.44738628335531344 \t| Accuracy: 106.250\n",
      "# Iteration  4181 -> Loss: 0.4473392950996016 \t| Accuracy: 106.250\n",
      "# Iteration  4182 -> Loss: 0.4472923321068747 \t| Accuracy: 106.250\n",
      "# Iteration  4183 -> Loss: 0.44724539435736005 \t| Accuracy: 106.250\n",
      "# Iteration  4184 -> Loss: 0.4471984818313033 \t| Accuracy: 106.250\n",
      "# Iteration  4185 -> Loss: 0.44715159450896746 \t| Accuracy: 106.250\n",
      "# Iteration  4186 -> Loss: 0.44710473237063386 \t| Accuracy: 106.250\n",
      "# Iteration  4187 -> Loss: 0.4470578953966015 \t| Accuracy: 106.250\n",
      "# Iteration  4188 -> Loss: 0.4470110835671873 \t| Accuracy: 106.250\n",
      "# Iteration  4189 -> Loss: 0.4469642968627261 \t| Accuracy: 106.250\n",
      "# Iteration  4190 -> Loss: 0.44691753526357053 \t| Accuracy: 106.250\n",
      "# Iteration  4191 -> Loss: 0.4468707987500909 \t| Accuracy: 106.250\n",
      "# Iteration  4192 -> Loss: 0.4468240873026756 \t| Accuracy: 106.250\n",
      "# Iteration  4193 -> Loss: 0.44677740090173057 \t| Accuracy: 106.250\n",
      "# Iteration  4194 -> Loss: 0.4467307395276798 \t| Accuracy: 106.250\n",
      "# Iteration  4195 -> Loss: 0.4466841031609648 \t| Accuracy: 106.250\n",
      "# Iteration  4196 -> Loss: 0.44663749178204487 \t| Accuracy: 106.250\n",
      "# Iteration  4197 -> Loss: 0.44659090537139734 \t| Accuracy: 106.250\n",
      "# Iteration  4198 -> Loss: 0.4465443439095169 \t| Accuracy: 106.250\n",
      "# Iteration  4199 -> Loss: 0.44649780737691613 \t| Accuracy: 106.250\n",
      "# Iteration  4200 -> Loss: 0.44645129575412523 \t| Accuracy: 106.250\n",
      "# Iteration  4201 -> Loss: 0.4464048090216924 \t| Accuracy: 106.250\n",
      "# Iteration  4202 -> Loss: 0.4463583471601831 \t| Accuracy: 106.250\n",
      "# Iteration  4203 -> Loss: 0.44631191015018046 \t| Accuracy: 106.250\n",
      "# Iteration  4204 -> Loss: 0.44626549797228576 \t| Accuracy: 106.250\n",
      "# Iteration  4205 -> Loss: 0.4462191106071174 \t| Accuracy: 106.250\n",
      "# Iteration  4206 -> Loss: 0.4461727480353115 \t| Accuracy: 106.250\n",
      "# Iteration  4207 -> Loss: 0.44612641023752203 \t| Accuracy: 106.250\n",
      "# Iteration  4208 -> Loss: 0.4460800971944203 \t| Accuracy: 106.250\n",
      "# Iteration  4209 -> Loss: 0.44603380888669536 \t| Accuracy: 106.250\n",
      "# Iteration  4210 -> Loss: 0.44598754529505363 \t| Accuracy: 106.250\n",
      "# Iteration  4211 -> Loss: 0.4459413064002195 \t| Accuracy: 106.250\n",
      "# Iteration  4212 -> Loss: 0.44589509218293427 \t| Accuracy: 106.250\n",
      "# Iteration  4213 -> Loss: 0.4458489026239574 \t| Accuracy: 106.250\n",
      "# Iteration  4214 -> Loss: 0.4458027377040652 \t| Accuracy: 106.250\n",
      "# Iteration  4215 -> Loss: 0.4457565974040522 \t| Accuracy: 106.250\n",
      "# Iteration  4216 -> Loss: 0.44571048170472993 \t| Accuracy: 106.250\n",
      "# Iteration  4217 -> Loss: 0.44566439058692753 \t| Accuracy: 106.250\n",
      "# Iteration  4218 -> Loss: 0.4456183240314917 \t| Accuracy: 106.250\n",
      "# Iteration  4219 -> Loss: 0.44557228201928634 \t| Accuracy: 106.250\n",
      "# Iteration  4220 -> Loss: 0.44552626453119293 \t| Accuracy: 106.250\n",
      "# Iteration  4221 -> Loss: 0.4454802715481103 \t| Accuracy: 106.250\n",
      "# Iteration  4222 -> Loss: 0.44543430305095505 \t| Accuracy: 106.250\n",
      "# Iteration  4223 -> Loss: 0.44538835902066065 \t| Accuracy: 106.250\n",
      "# Iteration  4224 -> Loss: 0.44534243943817803 \t| Accuracy: 106.250\n",
      "# Iteration  4225 -> Loss: 0.44529654428447596 \t| Accuracy: 106.250\n",
      "# Iteration  4226 -> Loss: 0.44525067354053993 \t| Accuracy: 106.250\n",
      "# Iteration  4227 -> Loss: 0.44520482718737314 \t| Accuracy: 106.250\n",
      "# Iteration  4228 -> Loss: 0.44515900520599594 \t| Accuracy: 106.250\n",
      "# Iteration  4229 -> Loss: 0.4451132075774462 \t| Accuracy: 106.250\n",
      "# Iteration  4230 -> Loss: 0.44506743428277895 \t| Accuracy: 106.250\n",
      "# Iteration  4231 -> Loss: 0.4450216853030665 \t| Accuracy: 106.250\n",
      "# Iteration  4232 -> Loss: 0.44497596061939837 \t| Accuracy: 106.250\n",
      "# Iteration  4233 -> Loss: 0.44493026021288135 \t| Accuracy: 106.250\n",
      "# Iteration  4234 -> Loss: 0.44488458406463977 \t| Accuracy: 106.250\n",
      "# Iteration  4235 -> Loss: 0.44483893215581477 \t| Accuracy: 106.250\n",
      "# Iteration  4236 -> Loss: 0.4447933044675649 \t| Accuracy: 106.250\n",
      "# Iteration  4237 -> Loss: 0.44474770098106586 \t| Accuracy: 106.250\n",
      "# Iteration  4238 -> Loss: 0.4447021216775106 \t| Accuracy: 106.250\n",
      "# Iteration  4239 -> Loss: 0.4446565665381092 \t| Accuracy: 106.250\n",
      "# Iteration  4240 -> Loss: 0.44461103554408893 \t| Accuracy: 106.250\n",
      "# Iteration  4241 -> Loss: 0.44456552867669413 \t| Accuracy: 106.250\n",
      "# Iteration  4242 -> Loss: 0.44452004591718636 \t| Accuracy: 106.250\n",
      "# Iteration  4243 -> Loss: 0.44447458724684435 \t| Accuracy: 106.250\n",
      "# Iteration  4244 -> Loss: 0.4444291526469637 \t| Accuracy: 106.250\n",
      "# Iteration  4245 -> Loss: 0.4443837420988574 \t| Accuracy: 106.250\n",
      "# Iteration  4246 -> Loss: 0.44433835558385515 \t| Accuracy: 106.250\n",
      "# Iteration  4247 -> Loss: 0.44429299308330433 \t| Accuracy: 106.250\n",
      "# Iteration  4248 -> Loss: 0.4442476545785687 \t| Accuracy: 106.250\n",
      "# Iteration  4249 -> Loss: 0.4442023400510292 \t| Accuracy: 106.250\n",
      "# Iteration  4250 -> Loss: 0.4441570494820843 \t| Accuracy: 106.250\n",
      "# Iteration  4251 -> Loss: 0.4441117828531489 \t| Accuracy: 106.250\n",
      "# Iteration  4252 -> Loss: 0.4440665401456551 \t| Accuracy: 106.250\n",
      "# Iteration  4253 -> Loss: 0.4440213213410522 \t| Accuracy: 106.250\n",
      "# Iteration  4254 -> Loss: 0.4439761264208062 \t| Accuracy: 106.250\n",
      "# Iteration  4255 -> Loss: 0.44393095536639987 \t| Accuracy: 106.250\n",
      "# Iteration  4256 -> Loss: 0.44388580815933343 \t| Accuracy: 106.250\n",
      "# Iteration  4257 -> Loss: 0.4438406847811238 \t| Accuracy: 106.250\n",
      "# Iteration  4258 -> Loss: 0.4437955852133047 \t| Accuracy: 106.250\n",
      "# Iteration  4259 -> Loss: 0.4437505094374269 \t| Accuracy: 106.250\n",
      "# Iteration  4260 -> Loss: 0.443705457435058 \t| Accuracy: 106.250\n",
      "# Iteration  4261 -> Loss: 0.4436604291877824 \t| Accuracy: 106.250\n",
      "# Iteration  4262 -> Loss: 0.4436154246772016 \t| Accuracy: 106.250\n",
      "# Iteration  4263 -> Loss: 0.44357044388493383 \t| Accuracy: 106.250\n",
      "# Iteration  4264 -> Loss: 0.443525486792614 \t| Accuracy: 106.250\n",
      "# Iteration  4265 -> Loss: 0.443480553381894 \t| Accuracy: 106.250\n",
      "# Iteration  4266 -> Loss: 0.44343564363444254 \t| Accuracy: 106.250\n",
      "# Iteration  4267 -> Loss: 0.44339075753194507 \t| Accuracy: 106.250\n",
      "# Iteration  4268 -> Loss: 0.44334589505610383 \t| Accuracy: 106.250\n",
      "# Iteration  4269 -> Loss: 0.4433010561886379 \t| Accuracy: 106.250\n",
      "# Iteration  4270 -> Loss: 0.4432562409112829 \t| Accuracy: 106.250\n",
      "# Iteration  4271 -> Loss: 0.44321144920579153 \t| Accuracy: 106.250\n",
      "# Iteration  4272 -> Loss: 0.44316668105393286 \t| Accuracy: 106.250\n",
      "# Iteration  4273 -> Loss: 0.443121936437493 \t| Accuracy: 106.250\n",
      "# Iteration  4274 -> Loss: 0.4430772153382745 \t| Accuracy: 106.250\n",
      "# Iteration  4275 -> Loss: 0.4430325177380967 \t| Accuracy: 106.250\n",
      "# Iteration  4276 -> Loss: 0.4429878436187958 \t| Accuracy: 106.250\n",
      "# Iteration  4277 -> Loss: 0.44294319296222434 \t| Accuracy: 106.250\n",
      "# Iteration  4278 -> Loss: 0.4428985657502516 \t| Accuracy: 106.250\n",
      "# Iteration  4279 -> Loss: 0.44285396196476357 \t| Accuracy: 106.250\n",
      "# Iteration  4280 -> Loss: 0.44280938158766303 \t| Accuracy: 106.250\n",
      "# Iteration  4281 -> Loss: 0.4427648246008689 \t| Accuracy: 106.250\n",
      "# Iteration  4282 -> Loss: 0.44272029098631716 \t| Accuracy: 106.250\n",
      "# Iteration  4283 -> Loss: 0.44267578072596 \t| Accuracy: 106.250\n",
      "# Iteration  4284 -> Loss: 0.4426312938017665 \t| Accuracy: 106.250\n",
      "# Iteration  4285 -> Loss: 0.4425868301957221 \t| Accuracy: 106.250\n",
      "# Iteration  4286 -> Loss: 0.44254238988982875 \t| Accuracy: 106.250\n",
      "# Iteration  4287 -> Loss: 0.44249797286610526 \t| Accuracy: 106.250\n",
      "# Iteration  4288 -> Loss: 0.4424535791065863 \t| Accuracy: 106.250\n",
      "# Iteration  4289 -> Loss: 0.4424092085933238 \t| Accuracy: 106.250\n",
      "# Iteration  4290 -> Loss: 0.44236486130838554 \t| Accuracy: 106.250\n",
      "# Iteration  4291 -> Loss: 0.44232053723385634 \t| Accuracy: 106.250\n",
      "# Iteration  4292 -> Loss: 0.4422762363518368 \t| Accuracy: 106.250\n",
      "# Iteration  4293 -> Loss: 0.44223195864444476 \t| Accuracy: 106.250\n",
      "# Iteration  4294 -> Loss: 0.44218770409381386 \t| Accuracy: 106.250\n",
      "# Iteration  4295 -> Loss: 0.4421434726820943 \t| Accuracy: 106.250\n",
      "# Iteration  4296 -> Loss: 0.4420992643914529 \t| Accuracy: 106.250\n",
      "# Iteration  4297 -> Loss: 0.4420550792040728 \t| Accuracy: 106.250\n",
      "# Iteration  4298 -> Loss: 0.4420109171021533 \t| Accuracy: 106.250\n",
      "# Iteration  4299 -> Loss: 0.44196677806791035 \t| Accuracy: 106.250\n",
      "# Iteration  4300 -> Loss: 0.44192266208357606 \t| Accuracy: 106.250\n",
      "# Iteration  4301 -> Loss: 0.44187856913139884 \t| Accuracy: 106.250\n",
      "# Iteration  4302 -> Loss: 0.44183449919364376 \t| Accuracy: 106.250\n",
      "# Iteration  4303 -> Loss: 0.44179045225259184 \t| Accuracy: 106.250\n",
      "# Iteration  4304 -> Loss: 0.4417464282905405 \t| Accuracy: 106.250\n",
      "# Iteration  4305 -> Loss: 0.44170242728980363 \t| Accuracy: 106.250\n",
      "# Iteration  4306 -> Loss: 0.44165844923271097 \t| Accuracy: 106.250\n",
      "# Iteration  4307 -> Loss: 0.441614494101609 \t| Accuracy: 106.250\n",
      "# Iteration  4308 -> Loss: 0.44157056187886023 \t| Accuracy: 106.250\n",
      "# Iteration  4309 -> Loss: 0.4415266525468433 \t| Accuracy: 106.250\n",
      "# Iteration  4310 -> Loss: 0.4414827660879531 \t| Accuracy: 106.250\n",
      "# Iteration  4311 -> Loss: 0.44143890248460105 \t| Accuracy: 106.250\n",
      "# Iteration  4312 -> Loss: 0.4413950617192144 \t| Accuracy: 106.250\n",
      "# Iteration  4313 -> Loss: 0.44135124377423646 \t| Accuracy: 106.250\n",
      "# Iteration  4314 -> Loss: 0.4413074486321271 \t| Accuracy: 106.250\n",
      "# Iteration  4315 -> Loss: 0.44126367627536217 \t| Accuracy: 106.250\n",
      "# Iteration  4316 -> Loss: 0.44121992668643367 \t| Accuracy: 106.250\n",
      "# Iteration  4317 -> Loss: 0.44117619984784967 \t| Accuracy: 106.250\n",
      "# Iteration  4318 -> Loss: 0.44113249574213426 \t| Accuracy: 106.250\n",
      "# Iteration  4319 -> Loss: 0.44108881435182806 \t| Accuracy: 106.250\n",
      "# Iteration  4320 -> Loss: 0.4410451556594872 \t| Accuracy: 106.250\n",
      "# Iteration  4321 -> Loss: 0.44100151964768425 \t| Accuracy: 106.250\n",
      "# Iteration  4322 -> Loss: 0.4409579062990077 \t| Accuracy: 106.250\n",
      "# Iteration  4323 -> Loss: 0.44091431559606226 \t| Accuracy: 106.250\n",
      "# Iteration  4324 -> Loss: 0.4408707475214683 \t| Accuracy: 106.250\n",
      "# Iteration  4325 -> Loss: 0.4408272020578625 \t| Accuracy: 106.250\n",
      "# Iteration  4326 -> Loss: 0.4407836791878977 \t| Accuracy: 106.250\n",
      "# Iteration  4327 -> Loss: 0.4407401788942422 \t| Accuracy: 106.250\n",
      "# Iteration  4328 -> Loss: 0.44069670115958065 \t| Accuracy: 106.250\n",
      "# Iteration  4329 -> Loss: 0.4406532459666138 \t| Accuracy: 106.250\n",
      "# Iteration  4330 -> Loss: 0.440609813298058 \t| Accuracy: 106.250\n",
      "# Iteration  4331 -> Loss: 0.44056640313664563 \t| Accuracy: 106.250\n",
      "# Iteration  4332 -> Loss: 0.4405230154651252 \t| Accuracy: 106.250\n",
      "# Iteration  4333 -> Loss: 0.44047965026626085 \t| Accuracy: 106.250\n",
      "# Iteration  4334 -> Loss: 0.44043630752283275 \t| Accuracy: 106.250\n",
      "# Iteration  4335 -> Loss: 0.44039298721763703 \t| Accuracy: 106.250\n",
      "# Iteration  4336 -> Loss: 0.4403496893334856 \t| Accuracy: 106.250\n",
      "# Iteration  4337 -> Loss: 0.4403064138532062 \t| Accuracy: 106.250\n",
      "# Iteration  4338 -> Loss: 0.4402631607596424 \t| Accuracy: 106.250\n",
      "# Iteration  4339 -> Loss: 0.4402199300356537 \t| Accuracy: 106.250\n",
      "# Iteration  4340 -> Loss: 0.4401767216641153 \t| Accuracy: 106.250\n",
      "# Iteration  4341 -> Loss: 0.44013353562791835 \t| Accuracy: 106.250\n",
      "# Iteration  4342 -> Loss: 0.44009037190996964 \t| Accuracy: 106.250\n",
      "# Iteration  4343 -> Loss: 0.4400472304931919 \t| Accuracy: 106.250\n",
      "# Iteration  4344 -> Loss: 0.4400041113605234 \t| Accuracy: 106.250\n",
      "# Iteration  4345 -> Loss: 0.43996101449491837 \t| Accuracy: 106.250\n",
      "# Iteration  4346 -> Loss: 0.4399179398793466 \t| Accuracy: 106.250\n",
      "# Iteration  4347 -> Loss: 0.4398748874967938 \t| Accuracy: 106.250\n",
      "# Iteration  4348 -> Loss: 0.4398318573302611 \t| Accuracy: 106.250\n",
      "# Iteration  4349 -> Loss: 0.4397888493627657 \t| Accuracy: 106.250\n",
      "# Iteration  4350 -> Loss: 0.4397458635773402 \t| Accuracy: 106.250\n",
      "# Iteration  4351 -> Loss: 0.439702899957033 \t| Accuracy: 106.250\n",
      "# Iteration  4352 -> Loss: 0.439659958484908 \t| Accuracy: 106.250\n",
      "# Iteration  4353 -> Loss: 0.43961703914404493 \t| Accuracy: 106.250\n",
      "# Iteration  4354 -> Loss: 0.4395741419175392 \t| Accuracy: 106.250\n",
      "# Iteration  4355 -> Loss: 0.4395312667885015 \t| Accuracy: 106.250\n",
      "# Iteration  4356 -> Loss: 0.4394884137400584 \t| Accuracy: 106.250\n",
      "# Iteration  4357 -> Loss: 0.43944558275535206 \t| Accuracy: 106.250\n",
      "# Iteration  4358 -> Loss: 0.43940277381754017 \t| Accuracy: 106.250\n",
      "# Iteration  4359 -> Loss: 0.4393599869097958 \t| Accuracy: 106.250\n",
      "# Iteration  4360 -> Loss: 0.4393172220153078 \t| Accuracy: 106.250\n",
      "# Iteration  4361 -> Loss: 0.43927447911728057 \t| Accuracy: 106.250\n",
      "# Iteration  4362 -> Loss: 0.43923175819893384 \t| Accuracy: 106.250\n",
      "# Iteration  4363 -> Loss: 0.439189059243503 \t| Accuracy: 106.250\n",
      "# Iteration  4364 -> Loss: 0.43914638223423896 \t| Accuracy: 106.250\n",
      "# Iteration  4365 -> Loss: 0.4391037271544079 \t| Accuracy: 106.250\n",
      "# Iteration  4366 -> Loss: 0.43906109398729165 \t| Accuracy: 106.250\n",
      "# Iteration  4367 -> Loss: 0.43901848271618743 \t| Accuracy: 106.250\n",
      "# Iteration  4368 -> Loss: 0.4389758933244082 \t| Accuracy: 106.250\n",
      "# Iteration  4369 -> Loss: 0.43893332579528177 \t| Accuracy: 106.250\n",
      "# Iteration  4370 -> Loss: 0.4388907801121517 \t| Accuracy: 106.250\n",
      "# Iteration  4371 -> Loss: 0.43884825625837715 \t| Accuracy: 106.250\n",
      "# Iteration  4372 -> Loss: 0.4388057542173321 \t| Accuracy: 106.250\n",
      "# Iteration  4373 -> Loss: 0.4387632739724066 \t| Accuracy: 106.250\n",
      "# Iteration  4374 -> Loss: 0.43872081550700553 \t| Accuracy: 106.250\n",
      "# Iteration  4375 -> Loss: 0.43867837880454935 \t| Accuracy: 106.250\n",
      "# Iteration  4376 -> Loss: 0.43863596384847386 \t| Accuracy: 106.250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Iteration  4377 -> Loss: 0.43859357062223 \t| Accuracy: 106.250\n",
      "# Iteration  4378 -> Loss: 0.4385511991092843 \t| Accuracy: 106.250\n",
      "# Iteration  4379 -> Loss: 0.4385088492931183 \t| Accuracy: 106.250\n",
      "# Iteration  4380 -> Loss: 0.4384665211572291 \t| Accuracy: 106.250\n",
      "# Iteration  4381 -> Loss: 0.4384242146851289 \t| Accuracy: 106.250\n",
      "# Iteration  4382 -> Loss: 0.438381929860345 \t| Accuracy: 106.250\n",
      "# Iteration  4383 -> Loss: 0.43833966666642027 \t| Accuracy: 106.250\n",
      "# Iteration  4384 -> Loss: 0.4382974250869128 \t| Accuracy: 106.250\n",
      "# Iteration  4385 -> Loss: 0.4382552051053955 \t| Accuracy: 106.250\n",
      "# Iteration  4386 -> Loss: 0.43821300670545693 \t| Accuracy: 106.250\n",
      "# Iteration  4387 -> Loss: 0.43817082987070066 \t| Accuracy: 106.250\n",
      "# Iteration  4388 -> Loss: 0.43812867458474525 \t| Accuracy: 106.250\n",
      "# Iteration  4389 -> Loss: 0.4380865408312248 \t| Accuracy: 106.250\n",
      "# Iteration  4390 -> Loss: 0.4380444285937883 \t| Accuracy: 106.250\n",
      "# Iteration  4391 -> Loss: 0.43800233785609993 \t| Accuracy: 106.250\n",
      "# Iteration  4392 -> Loss: 0.43796026860183895 \t| Accuracy: 106.250\n",
      "# Iteration  4393 -> Loss: 0.43791822081469994 \t| Accuracy: 106.250\n",
      "# Iteration  4394 -> Loss: 0.4378761944783922 \t| Accuracy: 106.250\n",
      "# Iteration  4395 -> Loss: 0.4378341895766405 \t| Accuracy: 106.250\n",
      "# Iteration  4396 -> Loss: 0.4377922060931846 \t| Accuracy: 106.250\n",
      "# Iteration  4397 -> Loss: 0.4377502440117789 \t| Accuracy: 106.250\n",
      "# Iteration  4398 -> Loss: 0.4377083033161934 \t| Accuracy: 106.250\n",
      "# Iteration  4399 -> Loss: 0.43766638399021296 \t| Accuracy: 106.250\n",
      "# Iteration  4400 -> Loss: 0.4376244860176372 \t| Accuracy: 106.250\n",
      "# Iteration  4401 -> Loss: 0.43758260938228116 \t| Accuracy: 106.250\n",
      "# Iteration  4402 -> Loss: 0.4375407540679745 \t| Accuracy: 106.250\n",
      "# Iteration  4403 -> Loss: 0.43749892005856195 \t| Accuracy: 106.250\n",
      "# Iteration  4404 -> Loss: 0.4374571073379035 \t| Accuracy: 106.250\n",
      "# Iteration  4405 -> Loss: 0.43741531588987365 \t| Accuracy: 106.250\n",
      "# Iteration  4406 -> Loss: 0.43737354569836207 \t| Accuracy: 106.250\n",
      "# Iteration  4407 -> Loss: 0.4373317967472734 \t| Accuracy: 106.250\n",
      "# Iteration  4408 -> Loss: 0.43729006902052714 \t| Accuracy: 106.250\n",
      "# Iteration  4409 -> Loss: 0.43724836250205745 \t| Accuracy: 106.250\n",
      "# Iteration  4410 -> Loss: 0.43720667717581385 \t| Accuracy: 106.250\n",
      "# Iteration  4411 -> Loss: 0.4371650130257604 \t| Accuracy: 106.250\n",
      "# Iteration  4412 -> Loss: 0.43712337003587587 \t| Accuracy: 106.250\n",
      "# Iteration  4413 -> Loss: 0.43708174819015444 \t| Accuracy: 106.250\n",
      "# Iteration  4414 -> Loss: 0.43704014747260456 \t| Accuracy: 106.250\n",
      "# Iteration  4415 -> Loss: 0.4369985678672496 \t| Accuracy: 106.250\n",
      "# Iteration  4416 -> Loss: 0.4369570093581281 \t| Accuracy: 106.250\n",
      "# Iteration  4417 -> Loss: 0.436915471929293 \t| Accuracy: 106.250\n",
      "# Iteration  4418 -> Loss: 0.4368739555648122 \t| Accuracy: 106.250\n",
      "# Iteration  4419 -> Loss: 0.4368324602487682 \t| Accuracy: 106.250\n",
      "# Iteration  4420 -> Loss: 0.4367909859652585 \t| Accuracy: 106.250\n",
      "# Iteration  4421 -> Loss: 0.4367495326983953 \t| Accuracy: 106.250\n",
      "# Iteration  4422 -> Loss: 0.43670810043230496 \t| Accuracy: 106.250\n",
      "# Iteration  4423 -> Loss: 0.43666668915112966 \t| Accuracy: 106.250\n",
      "# Iteration  4424 -> Loss: 0.4366252988390252 \t| Accuracy: 106.250\n",
      "# Iteration  4425 -> Loss: 0.4365839294801627 \t| Accuracy: 106.250\n",
      "# Iteration  4426 -> Loss: 0.4365425810587278 \t| Accuracy: 106.250\n",
      "# Iteration  4427 -> Loss: 0.43650125355892055 \t| Accuracy: 106.250\n",
      "# Iteration  4428 -> Loss: 0.4364599469649561 \t| Accuracy: 106.250\n",
      "# Iteration  4429 -> Loss: 0.43641866126106377 \t| Accuracy: 106.250\n",
      "# Iteration  4430 -> Loss: 0.43637739643148793 \t| Accuracy: 106.250\n",
      "# Iteration  4431 -> Loss: 0.4363361524604872 \t| Accuracy: 106.250\n",
      "# Iteration  4432 -> Loss: 0.436294929332335 \t| Accuracy: 106.250\n",
      "# Iteration  4433 -> Loss: 0.43625372703131915 \t| Accuracy: 106.250\n",
      "# Iteration  4434 -> Loss: 0.43621254554174244 \t| Accuracy: 106.250\n",
      "# Iteration  4435 -> Loss: 0.4361713848479215 \t| Accuracy: 106.250\n",
      "# Iteration  4436 -> Loss: 0.43613024493418834 \t| Accuracy: 106.250\n",
      "# Iteration  4437 -> Loss: 0.4360891257848889 \t| Accuracy: 106.250\n",
      "# Iteration  4438 -> Loss: 0.4360480273843837 \t| Accuracy: 106.250\n",
      "# Iteration  4439 -> Loss: 0.4360069497170482 \t| Accuracy: 106.250\n",
      "# Iteration  4440 -> Loss: 0.43596589276727177 \t| Accuracy: 106.250\n",
      "# Iteration  4441 -> Loss: 0.4359248565194586 \t| Accuracy: 106.250\n",
      "# Iteration  4442 -> Loss: 0.43588384095802724 \t| Accuracy: 106.250\n",
      "# Iteration  4443 -> Loss: 0.4358428460674107 \t| Accuracy: 106.250\n",
      "# Iteration  4444 -> Loss: 0.4358018718320564 \t| Accuracy: 106.250\n",
      "# Iteration  4445 -> Loss: 0.4357609182364263 \t| Accuracy: 106.250\n",
      "# Iteration  4446 -> Loss: 0.4357199852649966 \t| Accuracy: 106.250\n",
      "# Iteration  4447 -> Loss: 0.43567907290225805 \t| Accuracy: 106.250\n",
      "# Iteration  4448 -> Loss: 0.43563818113271546 \t| Accuracy: 106.250\n",
      "# Iteration  4449 -> Loss: 0.43559730994088863 \t| Accuracy: 106.250\n",
      "# Iteration  4450 -> Loss: 0.4355564593113111 \t| Accuracy: 106.250\n",
      "# Iteration  4451 -> Loss: 0.43551562922853104 \t| Accuracy: 106.250\n",
      "# Iteration  4452 -> Loss: 0.43547481967711094 \t| Accuracy: 106.250\n",
      "# Iteration  4453 -> Loss: 0.4354340306416274 \t| Accuracy: 106.250\n",
      "# Iteration  4454 -> Loss: 0.43539326210667184 \t| Accuracy: 106.250\n",
      "# Iteration  4455 -> Loss: 0.4353525140568492 \t| Accuracy: 106.250\n",
      "# Iteration  4456 -> Loss: 0.43531178647677937 \t| Accuracy: 106.250\n",
      "# Iteration  4457 -> Loss: 0.43527107935109616 \t| Accuracy: 106.250\n",
      "# Iteration  4458 -> Loss: 0.435230392664448 \t| Accuracy: 106.250\n",
      "# Iteration  4459 -> Loss: 0.4351897264014969 \t| Accuracy: 106.250\n",
      "# Iteration  4460 -> Loss: 0.43514908054691964 \t| Accuracy: 106.250\n",
      "# Iteration  4461 -> Loss: 0.4351084550854071 \t| Accuracy: 106.250\n",
      "# Iteration  4462 -> Loss: 0.43506785000166415 \t| Accuracy: 106.250\n",
      "# Iteration  4463 -> Loss: 0.4350272652804101 \t| Accuracy: 106.250\n",
      "# Iteration  4464 -> Loss: 0.43498670090637837 \t| Accuracy: 106.250\n",
      "# Iteration  4465 -> Loss: 0.4349461568643164 \t| Accuracy: 106.250\n",
      "# Iteration  4466 -> Loss: 0.434905633138986 \t| Accuracy: 106.250\n",
      "# Iteration  4467 -> Loss: 0.43486512971516283 \t| Accuracy: 106.250\n",
      "# Iteration  4468 -> Loss: 0.434824646577637 \t| Accuracy: 106.250\n",
      "# Iteration  4469 -> Loss: 0.43478418371121247 \t| Accuracy: 106.250\n",
      "# Iteration  4470 -> Loss: 0.4347437411007074 \t| Accuracy: 106.250\n",
      "# Iteration  4471 -> Loss: 0.4347033187309539 \t| Accuracy: 106.250\n",
      "# Iteration  4472 -> Loss: 0.43466291658679856 \t| Accuracy: 106.250\n",
      "# Iteration  4473 -> Loss: 0.43462253465310136 \t| Accuracy: 106.250\n",
      "# Iteration  4474 -> Loss: 0.43458217291473683 \t| Accuracy: 106.250\n",
      "# Iteration  4475 -> Loss: 0.4345418313565935 \t| Accuracy: 106.250\n",
      "# Iteration  4476 -> Loss: 0.4345015099635736 \t| Accuracy: 106.250\n",
      "# Iteration  4477 -> Loss: 0.4344612087205936 \t| Accuracy: 106.250\n",
      "# Iteration  4478 -> Loss: 0.43442092761258405 \t| Accuracy: 106.250\n",
      "# Iteration  4479 -> Loss: 0.43438066662448915 \t| Accuracy: 106.250\n",
      "# Iteration  4480 -> Loss: 0.43434042574126747 \t| Accuracy: 106.250\n",
      "# Iteration  4481 -> Loss: 0.43430020494789107 \t| Accuracy: 106.250\n",
      "# Iteration  4482 -> Loss: 0.43426000422934635 \t| Accuracy: 106.250\n",
      "# Iteration  4483 -> Loss: 0.4342198235706335 \t| Accuracy: 106.250\n",
      "# Iteration  4484 -> Loss: 0.43417966295676647 \t| Accuracy: 106.250\n",
      "# Iteration  4485 -> Loss: 0.43413952237277337 \t| Accuracy: 106.250\n",
      "# Iteration  4486 -> Loss: 0.4340994018036959 \t| Accuracy: 106.250\n",
      "# Iteration  4487 -> Loss: 0.43405930123458986 \t| Accuracy: 106.250\n",
      "# Iteration  4488 -> Loss: 0.4340192206505249 \t| Accuracy: 106.250\n",
      "# Iteration  4489 -> Loss: 0.4339791600365844 \t| Accuracy: 106.250\n",
      "# Iteration  4490 -> Loss: 0.4339391193778655 \t| Accuracy: 106.250\n",
      "# Iteration  4491 -> Loss: 0.4338990986594795 \t| Accuracy: 106.250\n",
      "# Iteration  4492 -> Loss: 0.4338590978665512 \t| Accuracy: 106.250\n",
      "# Iteration  4493 -> Loss: 0.4338191169842192 \t| Accuracy: 106.250\n",
      "# Iteration  4494 -> Loss: 0.43377915599763606 \t| Accuracy: 106.250\n",
      "# Iteration  4495 -> Loss: 0.43373921489196793 \t| Accuracy: 106.250\n",
      "# Iteration  4496 -> Loss: 0.43369929365239485 \t| Accuracy: 106.250\n",
      "# Iteration  4497 -> Loss: 0.4336593922641105 \t| Accuracy: 106.250\n",
      "# Iteration  4498 -> Loss: 0.43361951071232246 \t| Accuracy: 106.250\n",
      "# Iteration  4499 -> Loss: 0.4335796489822518 \t| Accuracy: 106.250\n",
      "# Iteration  4500 -> Loss: 0.43353980705913325 \t| Accuracy: 106.250\n",
      "# Iteration  4501 -> Loss: 0.4334999849282157 \t| Accuracy: 106.250\n",
      "# Iteration  4502 -> Loss: 0.43346018257476104 \t| Accuracy: 106.250\n",
      "# Iteration  4503 -> Loss: 0.43342039998404536 \t| Accuracy: 106.250\n",
      "# Iteration  4504 -> Loss: 0.4333806371413582 \t| Accuracy: 106.250\n",
      "# Iteration  4505 -> Loss: 0.43334089403200265 \t| Accuracy: 106.250\n",
      "# Iteration  4506 -> Loss: 0.4333011706412957 \t| Accuracy: 106.250\n",
      "# Iteration  4507 -> Loss: 0.43326146695456746 \t| Accuracy: 106.250\n",
      "# Iteration  4508 -> Loss: 0.4332217829571623 \t| Accuracy: 106.250\n",
      "# Iteration  4509 -> Loss: 0.4331821186344377 \t| Accuracy: 106.250\n",
      "# Iteration  4510 -> Loss: 0.4331424739717646 \t| Accuracy: 106.250\n",
      "# Iteration  4511 -> Loss: 0.43310284895452816 \t| Accuracy: 106.250\n",
      "# Iteration  4512 -> Loss: 0.43306324356812637 \t| Accuracy: 106.250\n",
      "# Iteration  4513 -> Loss: 0.43302365779797103 \t| Accuracy: 106.250\n",
      "# Iteration  4514 -> Loss: 0.43298409162948764 \t| Accuracy: 106.250\n",
      "# Iteration  4515 -> Loss: 0.432944545048115 \t| Accuracy: 106.250\n",
      "# Iteration  4516 -> Loss: 0.43290501803930537 \t| Accuracy: 106.250\n",
      "# Iteration  4517 -> Loss: 0.4328655105885248 \t| Accuracy: 106.250\n",
      "# Iteration  4518 -> Loss: 0.4328260226812523 \t| Accuracy: 106.250\n",
      "# Iteration  4519 -> Loss: 0.43278655430298063 \t| Accuracy: 106.250\n",
      "# Iteration  4520 -> Loss: 0.43274710543921624 \t| Accuracy: 106.250\n",
      "# Iteration  4521 -> Loss: 0.4327076760754785 \t| Accuracy: 106.250\n",
      "# Iteration  4522 -> Loss: 0.43266826619730053 \t| Accuracy: 106.250\n",
      "# Iteration  4523 -> Loss: 0.4326288757902289 \t| Accuracy: 106.250\n",
      "# Iteration  4524 -> Loss: 0.4325895048398233 \t| Accuracy: 106.250\n",
      "# Iteration  4525 -> Loss: 0.4325501533316568 \t| Accuracy: 106.250\n",
      "# Iteration  4526 -> Loss: 0.43251082125131635 \t| Accuracy: 106.250\n",
      "# Iteration  4527 -> Loss: 0.4324715085844016 \t| Accuracy: 106.250\n",
      "# Iteration  4528 -> Loss: 0.432432215316526 \t| Accuracy: 106.250\n",
      "# Iteration  4529 -> Loss: 0.4323929414333159 \t| Accuracy: 106.250\n",
      "# Iteration  4530 -> Loss: 0.4323536869204115 \t| Accuracy: 106.250\n",
      "# Iteration  4531 -> Loss: 0.4323144517634659 \t| Accuracy: 106.250\n",
      "# Iteration  4532 -> Loss: 0.43227523594814554 \t| Accuracy: 106.250\n",
      "# Iteration  4533 -> Loss: 0.4322360394601304 \t| Accuracy: 106.250\n",
      "# Iteration  4534 -> Loss: 0.4321968622851132 \t| Accuracy: 106.250\n",
      "# Iteration  4535 -> Loss: 0.4321577044088006 \t| Accuracy: 106.250\n",
      "# Iteration  4536 -> Loss: 0.4321185658169118 \t| Accuracy: 106.250\n",
      "# Iteration  4537 -> Loss: 0.4320794464951799 \t| Accuracy: 106.250\n",
      "# Iteration  4538 -> Loss: 0.4320403464293505 \t| Accuracy: 106.250\n",
      "# Iteration  4539 -> Loss: 0.432001265605183 \t| Accuracy: 106.250\n",
      "# Iteration  4540 -> Loss: 0.43196220400844976 \t| Accuracy: 106.250\n",
      "# Iteration  4541 -> Loss: 0.43192316162493616 \t| Accuracy: 106.250\n",
      "# Iteration  4542 -> Loss: 0.43188413844044105 \t| Accuracy: 106.250\n",
      "# Iteration  4543 -> Loss: 0.4318451344407762 \t| Accuracy: 106.250\n",
      "# Iteration  4544 -> Loss: 0.43180614961176655 \t| Accuracy: 106.250\n",
      "# Iteration  4545 -> Loss: 0.4317671839392503 \t| Accuracy: 106.250\n",
      "# Iteration  4546 -> Loss: 0.43172823740907845 \t| Accuracy: 106.250\n",
      "# Iteration  4547 -> Loss: 0.4316893100071155 \t| Accuracy: 106.250\n",
      "# Iteration  4548 -> Loss: 0.43165040171923874 \t| Accuracy: 106.250\n",
      "# Iteration  4549 -> Loss: 0.4316115125313386 \t| Accuracy: 106.250\n",
      "# Iteration  4550 -> Loss: 0.43157264242931864 \t| Accuracy: 106.250\n",
      "# Iteration  4551 -> Loss: 0.4315337913990954 \t| Accuracy: 106.250\n",
      "# Iteration  4552 -> Loss: 0.43149495942659855 \t| Accuracy: 106.250\n",
      "# Iteration  4553 -> Loss: 0.4314561464977706 \t| Accuracy: 106.250\n",
      "# Iteration  4554 -> Loss: 0.43141735259856717 \t| Accuracy: 106.250\n",
      "# Iteration  4555 -> Loss: 0.43137857771495686 \t| Accuracy: 106.250\n",
      "# Iteration  4556 -> Loss: 0.4313398218329214 \t| Accuracy: 106.250\n",
      "# Iteration  4557 -> Loss: 0.4313010849384552 \t| Accuracy: 106.250\n",
      "# Iteration  4558 -> Loss: 0.4312623670175659 \t| Accuracy: 106.250\n",
      "# Iteration  4559 -> Loss: 0.4312236680562738 \t| Accuracy: 106.250\n",
      "# Iteration  4560 -> Loss: 0.4311849880406124 \t| Accuracy: 106.250\n",
      "# Iteration  4561 -> Loss: 0.4311463269566281 \t| Accuracy: 106.250\n",
      "# Iteration  4562 -> Loss: 0.43110768479038003 \t| Accuracy: 106.250\n",
      "# Iteration  4563 -> Loss: 0.4310690615279402 \t| Accuracy: 106.250\n",
      "# Iteration  4564 -> Loss: 0.43103045715539384 \t| Accuracy: 106.250\n",
      "# Iteration  4565 -> Loss: 0.43099187165883857 \t| Accuracy: 106.250\n",
      "# Iteration  4566 -> Loss: 0.4309533050243852 \t| Accuracy: 106.250\n",
      "# Iteration  4567 -> Loss: 0.43091475723815736 \t| Accuracy: 106.250\n",
      "# Iteration  4568 -> Loss: 0.4308762282862914 \t| Accuracy: 106.250\n",
      "# Iteration  4569 -> Loss: 0.4308377181549363 \t| Accuracy: 106.250\n",
      "# Iteration  4570 -> Loss: 0.4307992268302545 \t| Accuracy: 106.250\n",
      "# Iteration  4571 -> Loss: 0.4307607542984204 \t| Accuracy: 106.250\n",
      "# Iteration  4572 -> Loss: 0.4307223005456219 \t| Accuracy: 106.250\n",
      "# Iteration  4573 -> Loss: 0.43068386555805904 \t| Accuracy: 106.250\n",
      "# Iteration  4574 -> Loss: 0.43064544932194515 \t| Accuracy: 106.250\n",
      "# Iteration  4575 -> Loss: 0.43060705182350584 \t| Accuracy: 106.250\n",
      "# Iteration  4576 -> Loss: 0.43056867304897994 \t| Accuracy: 106.250\n",
      "# Iteration  4577 -> Loss: 0.43053031298461863 \t| Accuracy: 106.250\n",
      "# Iteration  4578 -> Loss: 0.4304919716166858 \t| Accuracy: 106.250\n",
      "# Iteration  4579 -> Loss: 0.4304536489314582 \t| Accuracy: 106.250\n",
      "# Iteration  4580 -> Loss: 0.4304153449152253 \t| Accuracy: 106.250\n",
      "# Iteration  4581 -> Loss: 0.43037705955428884 \t| Accuracy: 106.250\n",
      "# Iteration  4582 -> Loss: 0.43033879283496385 \t| Accuracy: 106.250\n",
      "# Iteration  4583 -> Loss: 0.4303005447435774 \t| Accuracy: 106.250\n",
      "# Iteration  4584 -> Loss: 0.4302623152664695 \t| Accuracy: 106.250\n",
      "# Iteration  4585 -> Loss: 0.4302241043899927 \t| Accuracy: 106.250\n",
      "# Iteration  4586 -> Loss: 0.43018591210051216 \t| Accuracy: 106.250\n",
      "# Iteration  4587 -> Loss: 0.4301477383844055 \t| Accuracy: 106.250\n",
      "# Iteration  4588 -> Loss: 0.43010958322806336 \t| Accuracy: 106.250\n",
      "# Iteration  4589 -> Loss: 0.4300714466178884 \t| Accuracy: 106.250\n",
      "# Iteration  4590 -> Loss: 0.4300333285402961 \t| Accuracy: 106.250\n",
      "# Iteration  4591 -> Loss: 0.42999522898171444 \t| Accuracy: 106.250\n",
      "# Iteration  4592 -> Loss: 0.429957147928584 \t| Accuracy: 106.250\n",
      "# Iteration  4593 -> Loss: 0.4299190853673577 \t| Accuracy: 106.250\n",
      "# Iteration  4594 -> Loss: 0.4298810412845012 \t| Accuracy: 106.250\n",
      "# Iteration  4595 -> Loss: 0.4298430156664924 \t| Accuracy: 106.250\n",
      "# Iteration  4596 -> Loss: 0.42980500849982173 \t| Accuracy: 106.250\n",
      "# Iteration  4597 -> Loss: 0.42976701977099246 \t| Accuracy: 106.250\n",
      "# Iteration  4598 -> Loss: 0.4297290494665198 \t| Accuracy: 106.250\n",
      "# Iteration  4599 -> Loss: 0.42969109757293156 \t| Accuracy: 106.250\n",
      "# Iteration  4600 -> Loss: 0.42965316407676807 \t| Accuracy: 106.250\n",
      "# Iteration  4601 -> Loss: 0.429615248964582 \t| Accuracy: 106.250\n",
      "# Iteration  4602 -> Loss: 0.42957735222293847 \t| Accuracy: 106.250\n",
      "# Iteration  4603 -> Loss: 0.42953947383841506 \t| Accuracy: 106.250\n",
      "# Iteration  4604 -> Loss: 0.4295016137976015 \t| Accuracy: 106.250\n",
      "# Iteration  4605 -> Loss: 0.4294637720871001 \t| Accuracy: 106.250\n",
      "# Iteration  4606 -> Loss: 0.42942594869352546 \t| Accuracy: 106.250\n",
      "# Iteration  4607 -> Loss: 0.42938814360350436 \t| Accuracy: 106.250\n",
      "# Iteration  4608 -> Loss: 0.4293503568036763 \t| Accuracy: 106.250\n",
      "# Iteration  4609 -> Loss: 0.42931258828069274 \t| Accuracy: 106.250\n",
      "# Iteration  4610 -> Loss: 0.4292748380212173 \t| Accuracy: 106.250\n",
      "# Iteration  4611 -> Loss: 0.4292371060119265 \t| Accuracy: 106.250\n",
      "# Iteration  4612 -> Loss: 0.4291993922395086 \t| Accuracy: 106.250\n",
      "# Iteration  4613 -> Loss: 0.42916169669066445 \t| Accuracy: 106.250\n",
      "# Iteration  4614 -> Loss: 0.4291240193521068 \t| Accuracy: 106.250\n",
      "# Iteration  4615 -> Loss: 0.42908636021056107 \t| Accuracy: 106.250\n",
      "# Iteration  4616 -> Loss: 0.4290487192527646 \t| Accuracy: 106.250\n",
      "# Iteration  4617 -> Loss: 0.4290110964654669 \t| Accuracy: 106.250\n",
      "# Iteration  4618 -> Loss: 0.42897349183543 \t| Accuracy: 106.250\n",
      "# Iteration  4619 -> Loss: 0.4289359053494279 \t| Accuracy: 106.250\n",
      "# Iteration  4620 -> Loss: 0.4288983369942468 \t| Accuracy: 106.250\n",
      "# Iteration  4621 -> Loss: 0.42886078675668515 \t| Accuracy: 106.250\n",
      "# Iteration  4622 -> Loss: 0.4288232546235533 \t| Accuracy: 106.250\n",
      "# Iteration  4623 -> Loss: 0.4287857405816742 \t| Accuracy: 106.250\n",
      "# Iteration  4624 -> Loss: 0.42874824461788247 \t| Accuracy: 106.250\n",
      "# Iteration  4625 -> Loss: 0.4287107667190252 \t| Accuracy: 106.250\n",
      "# Iteration  4626 -> Loss: 0.4286733068719613 \t| Accuracy: 106.250\n",
      "# Iteration  4627 -> Loss: 0.428635865063562 \t| Accuracy: 106.250\n",
      "# Iteration  4628 -> Loss: 0.42859844128071045 \t| Accuracy: 106.250\n",
      "# Iteration  4629 -> Loss: 0.4285610355103019 \t| Accuracy: 106.250\n",
      "# Iteration  4630 -> Loss: 0.4285236477392438 \t| Accuracy: 106.250\n",
      "# Iteration  4631 -> Loss: 0.42848627795445543 \t| Accuracy: 106.250\n",
      "# Iteration  4632 -> Loss: 0.4284489261428683 \t| Accuracy: 106.250\n",
      "# Iteration  4633 -> Loss: 0.4284115922914257 \t| Accuracy: 106.250\n",
      "# Iteration  4634 -> Loss: 0.42837427638708325 \t| Accuracy: 106.250\n",
      "# Iteration  4635 -> Loss: 0.42833697841680823 \t| Accuracy: 106.250\n",
      "# Iteration  4636 -> Loss: 0.42829969836758036 \t| Accuracy: 106.250\n",
      "# Iteration  4637 -> Loss: 0.42826243622639054 \t| Accuracy: 106.250\n",
      "# Iteration  4638 -> Loss: 0.42822519198024256 \t| Accuracy: 106.250\n",
      "# Iteration  4639 -> Loss: 0.4281879656161514 \t| Accuracy: 106.250\n",
      "# Iteration  4640 -> Loss: 0.42815075712114453 \t| Accuracy: 106.250\n",
      "# Iteration  4641 -> Loss: 0.4281135664822611 \t| Accuracy: 106.250\n",
      "# Iteration  4642 -> Loss: 0.428076393686552 \t| Accuracy: 106.250\n",
      "# Iteration  4643 -> Loss: 0.42803923872108024 \t| Accuracy: 106.250\n",
      "# Iteration  4644 -> Loss: 0.4280021015729208 \t| Accuracy: 106.250\n",
      "# Iteration  4645 -> Loss: 0.4279649822291603 \t| Accuracy: 106.250\n",
      "# Iteration  4646 -> Loss: 0.42792788067689724 \t| Accuracy: 106.250\n",
      "# Iteration  4647 -> Loss: 0.4278907969032421 \t| Accuracy: 106.250\n",
      "# Iteration  4648 -> Loss: 0.4278537308953173 \t| Accuracy: 106.250\n",
      "# Iteration  4649 -> Loss: 0.4278166826402567 \t| Accuracy: 106.250\n",
      "# Iteration  4650 -> Loss: 0.4277796521252064 \t| Accuracy: 106.250\n",
      "# Iteration  4651 -> Loss: 0.427742639337324 \t| Accuracy: 106.250\n",
      "# Iteration  4652 -> Loss: 0.42770564426377894 \t| Accuracy: 106.250\n",
      "# Iteration  4653 -> Loss: 0.4276686668917527 \t| Accuracy: 106.250\n",
      "# Iteration  4654 -> Loss: 0.42763170720843807 \t| Accuracy: 106.250\n",
      "# Iteration  4655 -> Loss: 0.42759476520104006 \t| Accuracy: 106.250\n",
      "# Iteration  4656 -> Loss: 0.4275578408567749 \t| Accuracy: 106.250\n",
      "# Iteration  4657 -> Loss: 0.42752093416287107 \t| Accuracy: 106.250\n",
      "# Iteration  4658 -> Loss: 0.4274840451065685 \t| Accuracy: 106.250\n",
      "# Iteration  4659 -> Loss: 0.427447173675119 \t| Accuracy: 106.250\n",
      "# Iteration  4660 -> Loss: 0.42741031985578565 \t| Accuracy: 106.250\n",
      "# Iteration  4661 -> Loss: 0.4273734836358436 \t| Accuracy: 106.250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Iteration  4662 -> Loss: 0.4273366650025797 \t| Accuracy: 106.250\n",
      "# Iteration  4663 -> Loss: 0.4272998639432921 \t| Accuracy: 106.250\n",
      "# Iteration  4664 -> Loss: 0.4272630804452911 \t| Accuracy: 106.250\n",
      "# Iteration  4665 -> Loss: 0.42722631449589804 \t| Accuracy: 106.250\n",
      "# Iteration  4666 -> Loss: 0.42718956608244635 \t| Accuracy: 106.250\n",
      "# Iteration  4667 -> Loss: 0.42715283519228087 \t| Accuracy: 106.250\n",
      "# Iteration  4668 -> Loss: 0.4271161218127581 \t| Accuracy: 106.250\n",
      "# Iteration  4669 -> Loss: 0.42707942593124604 \t| Accuracy: 106.250\n",
      "# Iteration  4670 -> Loss: 0.42704274753512433 \t| Accuracy: 106.250\n",
      "# Iteration  4671 -> Loss: 0.4270060866117842 \t| Accuracy: 106.250\n",
      "# Iteration  4672 -> Loss: 0.4269694431486282 \t| Accuracy: 106.250\n",
      "# Iteration  4673 -> Loss: 0.4269328171330709 \t| Accuracy: 106.250\n",
      "# Iteration  4674 -> Loss: 0.42689620855253796 \t| Accuracy: 106.250\n",
      "# Iteration  4675 -> Loss: 0.42685961739446654 \t| Accuracy: 106.250\n",
      "# Iteration  4676 -> Loss: 0.4268230436463056 \t| Accuracy: 106.250\n",
      "# Iteration  4677 -> Loss: 0.4267864872955153 \t| Accuracy: 106.250\n",
      "# Iteration  4678 -> Loss: 0.4267499483295676 \t| Accuracy: 106.250\n",
      "# Iteration  4679 -> Loss: 0.4267134267359455 \t| Accuracy: 106.250\n",
      "# Iteration  4680 -> Loss: 0.42667692250214395 \t| Accuracy: 106.250\n",
      "# Iteration  4681 -> Loss: 0.4266404356156689 \t| Accuracy: 106.250\n",
      "# Iteration  4682 -> Loss: 0.42660396606403794 \t| Accuracy: 106.250\n",
      "# Iteration  4683 -> Loss: 0.42656751383478003 \t| Accuracy: 106.250\n",
      "# Iteration  4684 -> Loss: 0.4265310789154356 \t| Accuracy: 106.250\n",
      "# Iteration  4685 -> Loss: 0.42649466129355645 \t| Accuracy: 106.250\n",
      "# Iteration  4686 -> Loss: 0.4264582609567056 \t| Accuracy: 106.250\n",
      "# Iteration  4687 -> Loss: 0.42642187789245767 \t| Accuracy: 106.250\n",
      "# Iteration  4688 -> Loss: 0.4263855120883984 \t| Accuracy: 106.250\n",
      "# Iteration  4689 -> Loss: 0.4263491635321253 \t| Accuracy: 106.250\n",
      "# Iteration  4690 -> Loss: 0.42631283221124683 \t| Accuracy: 106.250\n",
      "# Iteration  4691 -> Loss: 0.4262765181133826 \t| Accuracy: 106.250\n",
      "# Iteration  4692 -> Loss: 0.4262402212261642 \t| Accuracy: 106.250\n",
      "# Iteration  4693 -> Loss: 0.4262039415372338 \t| Accuracy: 106.250\n",
      "# Iteration  4694 -> Loss: 0.42616767903424535 \t| Accuracy: 106.250\n",
      "# Iteration  4695 -> Loss: 0.4261314337048639 \t| Accuracy: 106.250\n",
      "# Iteration  4696 -> Loss: 0.42609520553676566 \t| Accuracy: 106.250\n",
      "# Iteration  4697 -> Loss: 0.42605899451763835 \t| Accuracy: 106.250\n",
      "# Iteration  4698 -> Loss: 0.4260228006351807 \t| Accuracy: 106.250\n",
      "# Iteration  4699 -> Loss: 0.42598662387710284 \t| Accuracy: 106.250\n",
      "# Iteration  4700 -> Loss: 0.42595046423112587 \t| Accuracy: 106.250\n",
      "# Iteration  4701 -> Loss: 0.4259143216849824 \t| Accuracy: 106.250\n",
      "# Iteration  4702 -> Loss: 0.4258781962264159 \t| Accuracy: 106.250\n",
      "# Iteration  4703 -> Loss: 0.4258420878431814 \t| Accuracy: 106.250\n",
      "# Iteration  4704 -> Loss: 0.4258059965230448 \t| Accuracy: 106.250\n",
      "# Iteration  4705 -> Loss: 0.4257699222537833 \t| Accuracy: 106.250\n",
      "# Iteration  4706 -> Loss: 0.4257338650231852 \t| Accuracy: 106.250\n",
      "# Iteration  4707 -> Loss: 0.42569782481904994 \t| Accuracy: 106.250\n",
      "# Iteration  4708 -> Loss: 0.42566180162918815 \t| Accuracy: 106.250\n",
      "# Iteration  4709 -> Loss: 0.42562579544142143 \t| Accuracy: 106.250\n",
      "# Iteration  4710 -> Loss: 0.42558980624358267 \t| Accuracy: 106.250\n",
      "# Iteration  4711 -> Loss: 0.42555383402351565 \t| Accuracy: 106.250\n",
      "# Iteration  4712 -> Loss: 0.4255178787690752 \t| Accuracy: 106.250\n",
      "# Iteration  4713 -> Loss: 0.42548194046812765 \t| Accuracy: 106.250\n",
      "# Iteration  4714 -> Loss: 0.42544601910854973 \t| Accuracy: 106.250\n",
      "# Iteration  4715 -> Loss: 0.4254101146782298 \t| Accuracy: 106.250\n",
      "# Iteration  4716 -> Loss: 0.4253742271650668 \t| Accuracy: 106.250\n",
      "# Iteration  4717 -> Loss: 0.42533835655697105 \t| Accuracy: 106.250\n",
      "# Iteration  4718 -> Loss: 0.42530250284186366 \t| Accuracy: 106.250\n",
      "# Iteration  4719 -> Loss: 0.4252666660076766 \t| Accuracy: 106.250\n",
      "# Iteration  4720 -> Loss: 0.4252308460423535 \t| Accuracy: 106.250\n",
      "# Iteration  4721 -> Loss: 0.425195042933848 \t| Accuracy: 106.250\n",
      "# Iteration  4722 -> Loss: 0.4251592566701254 \t| Accuracy: 106.250\n",
      "# Iteration  4723 -> Loss: 0.42512348723916166 \t| Accuracy: 106.250\n",
      "# Iteration  4724 -> Loss: 0.4250877346289438 \t| Accuracy: 106.250\n",
      "# Iteration  4725 -> Loss: 0.4250519988274695 \t| Accuracy: 106.250\n",
      "# Iteration  4726 -> Loss: 0.4250162798227479 \t| Accuracy: 106.250\n",
      "# Iteration  4727 -> Loss: 0.4249805776027986 \t| Accuracy: 106.250\n",
      "# Iteration  4728 -> Loss: 0.42494489215565207 \t| Accuracy: 106.250\n",
      "# Iteration  4729 -> Loss: 0.4249092234693499 \t| Accuracy: 106.250\n",
      "# Iteration  4730 -> Loss: 0.42487357153194444 \t| Accuracy: 106.250\n",
      "# Iteration  4731 -> Loss: 0.42483793633149897 \t| Accuracy: 106.250\n",
      "# Iteration  4732 -> Loss: 0.4248023178560874 \t| Accuracy: 106.250\n",
      "# Iteration  4733 -> Loss: 0.4247667160937947 \t| Accuracy: 106.250\n",
      "# Iteration  4734 -> Loss: 0.4247311310327166 \t| Accuracy: 106.250\n",
      "# Iteration  4735 -> Loss: 0.42469556266095954 \t| Accuracy: 106.250\n",
      "# Iteration  4736 -> Loss: 0.424660010966641 \t| Accuracy: 106.250\n",
      "# Iteration  4737 -> Loss: 0.42462447593788905 \t| Accuracy: 106.250\n",
      "# Iteration  4738 -> Loss: 0.42458895756284254 \t| Accuracy: 106.250\n",
      "# Iteration  4739 -> Loss: 0.4245534558296512 \t| Accuracy: 106.250\n",
      "# Iteration  4740 -> Loss: 0.4245179707264753 \t| Accuracy: 106.250\n",
      "# Iteration  4741 -> Loss: 0.42448250224148615 \t| Accuracy: 106.250\n",
      "# Iteration  4742 -> Loss: 0.4244470503628656 \t| Accuracy: 106.250\n",
      "# Iteration  4743 -> Loss: 0.4244116150788064 \t| Accuracy: 106.250\n",
      "# Iteration  4744 -> Loss: 0.4243761963775116 \t| Accuracy: 106.250\n",
      "# Iteration  4745 -> Loss: 0.4243407942471955 \t| Accuracy: 106.250\n",
      "# Iteration  4746 -> Loss: 0.42430540867608263 \t| Accuracy: 106.250\n",
      "# Iteration  4747 -> Loss: 0.42427003965240845 \t| Accuracy: 106.250\n",
      "# Iteration  4748 -> Loss: 0.424234687164419 \t| Accuracy: 106.250\n",
      "# Iteration  4749 -> Loss: 0.424199351200371 \t| Accuracy: 106.250\n",
      "# Iteration  4750 -> Loss: 0.42416403174853184 \t| Accuracy: 106.250\n",
      "# Iteration  4751 -> Loss: 0.4241287287971795 \t| Accuracy: 106.250\n",
      "# Iteration  4752 -> Loss: 0.4240934423346025 \t| Accuracy: 106.250\n",
      "# Iteration  4753 -> Loss: 0.4240581723491 \t| Accuracy: 106.250\n",
      "# Iteration  4754 -> Loss: 0.424022918828982 \t| Accuracy: 106.250\n",
      "# Iteration  4755 -> Loss: 0.4239876817625688 \t| Accuracy: 106.250\n",
      "# Iteration  4756 -> Loss: 0.42395246113819135 \t| Accuracy: 106.250\n",
      "# Iteration  4757 -> Loss: 0.4239172569441912 \t| Accuracy: 106.250\n",
      "# Iteration  4758 -> Loss: 0.42388206916892046 \t| Accuracy: 106.250\n",
      "# Iteration  4759 -> Loss: 0.4238468978007416 \t| Accuracy: 106.250\n",
      "# Iteration  4760 -> Loss: 0.4238117428280281 \t| Accuracy: 106.250\n",
      "# Iteration  4761 -> Loss: 0.42377660423916325 \t| Accuracy: 106.250\n",
      "# Iteration  4762 -> Loss: 0.4237414820225415 \t| Accuracy: 106.250\n",
      "# Iteration  4763 -> Loss: 0.4237063761665675 \t| Accuracy: 106.250\n",
      "# Iteration  4764 -> Loss: 0.42367128665965637 \t| Accuracy: 106.250\n",
      "# Iteration  4765 -> Loss: 0.4236362134902336 \t| Accuracy: 106.250\n",
      "# Iteration  4766 -> Loss: 0.42360115664673553 \t| Accuracy: 106.250\n",
      "# Iteration  4767 -> Loss: 0.4235661161176086 \t| Accuracy: 106.250\n",
      "# Iteration  4768 -> Loss: 0.42353109189130994 \t| Accuracy: 106.250\n",
      "# Iteration  4769 -> Loss: 0.4234960839563067 \t| Accuracy: 106.250\n",
      "# Iteration  4770 -> Loss: 0.42346109230107704 \t| Accuracy: 106.250\n",
      "# Iteration  4771 -> Loss: 0.423426116914109 \t| Accuracy: 106.250\n",
      "# Iteration  4772 -> Loss: 0.42339115778390124 \t| Accuracy: 106.250\n",
      "# Iteration  4773 -> Loss: 0.42335621489896286 \t| Accuracy: 106.250\n",
      "# Iteration  4774 -> Loss: 0.4233212882478133 \t| Accuracy: 106.250\n",
      "# Iteration  4775 -> Loss: 0.4232863778189824 \t| Accuracy: 106.250\n",
      "# Iteration  4776 -> Loss: 0.4232514836010099 \t| Accuracy: 106.250\n",
      "# Iteration  4777 -> Loss: 0.4232166055824467 \t| Accuracy: 106.250\n",
      "# Iteration  4778 -> Loss: 0.4231817437518535 \t| Accuracy: 106.250\n",
      "# Iteration  4779 -> Loss: 0.4231468980978013 \t| Accuracy: 106.250\n",
      "# Iteration  4780 -> Loss: 0.4231120686088715 \t| Accuracy: 106.250\n",
      "# Iteration  4781 -> Loss: 0.42307725527365614 \t| Accuracy: 106.250\n",
      "# Iteration  4782 -> Loss: 0.42304245808075697 \t| Accuracy: 106.250\n",
      "# Iteration  4783 -> Loss: 0.42300767701878617 \t| Accuracy: 106.250\n",
      "# Iteration  4784 -> Loss: 0.42297291207636645 \t| Accuracy: 106.250\n",
      "# Iteration  4785 -> Loss: 0.4229381632421307 \t| Accuracy: 106.250\n",
      "# Iteration  4786 -> Loss: 0.4229034305047218 \t| Accuracy: 106.250\n",
      "# Iteration  4787 -> Loss: 0.42286871385279307 \t| Accuracy: 106.250\n",
      "# Iteration  4788 -> Loss: 0.42283401327500814 \t| Accuracy: 106.250\n",
      "# Iteration  4789 -> Loss: 0.4227993287600406 \t| Accuracy: 106.250\n",
      "# Iteration  4790 -> Loss: 0.4227646602965744 \t| Accuracy: 106.250\n",
      "# Iteration  4791 -> Loss: 0.4227300078733036 \t| Accuracy: 106.250\n",
      "# Iteration  4792 -> Loss: 0.42269537147893255 \t| Accuracy: 106.250\n",
      "# Iteration  4793 -> Loss: 0.42266075110217566 \t| Accuracy: 106.250\n",
      "# Iteration  4794 -> Loss: 0.4226261467317575 \t| Accuracy: 106.250\n",
      "# Iteration  4795 -> Loss: 0.4225915583564129 \t| Accuracy: 106.250\n",
      "# Iteration  4796 -> Loss: 0.4225569859648867 \t| Accuracy: 106.250\n",
      "# Iteration  4797 -> Loss: 0.4225224295459338 \t| Accuracy: 106.250\n",
      "# Iteration  4798 -> Loss: 0.4224878890883196 \t| Accuracy: 106.250\n",
      "# Iteration  4799 -> Loss: 0.42245336458081895 \t| Accuracy: 106.250\n",
      "# Iteration  4800 -> Loss: 0.42241885601221735 \t| Accuracy: 106.250\n",
      "# Iteration  4801 -> Loss: 0.4223843633713103 \t| Accuracy: 106.250\n",
      "# Iteration  4802 -> Loss: 0.42234988664690315 \t| Accuracy: 106.250\n",
      "# Iteration  4803 -> Loss: 0.4223154258278113 \t| Accuracy: 106.250\n",
      "# Iteration  4804 -> Loss: 0.4222809809028605 \t| Accuracy: 106.250\n",
      "# Iteration  4805 -> Loss: 0.42224655186088633 \t| Accuracy: 106.250\n",
      "# Iteration  4806 -> Loss: 0.4222121386907344 \t| Accuracy: 106.250\n",
      "# Iteration  4807 -> Loss: 0.4221777413812602 \t| Accuracy: 106.250\n",
      "# Iteration  4808 -> Loss: 0.4221433599213297 \t| Accuracy: 106.250\n",
      "# Iteration  4809 -> Loss: 0.4221089942998183 \t| Accuracy: 106.250\n",
      "# Iteration  4810 -> Loss: 0.4220746445056117 \t| Accuracy: 106.250\n",
      "# Iteration  4811 -> Loss: 0.42204031052760554 \t| Accuracy: 106.250\n",
      "# Iteration  4812 -> Loss: 0.42200599235470543 \t| Accuracy: 106.250\n",
      "# Iteration  4813 -> Loss: 0.42197168997582674 \t| Accuracy: 106.250\n",
      "# Iteration  4814 -> Loss: 0.421937403379895 \t| Accuracy: 106.250\n",
      "# Iteration  4815 -> Loss: 0.42190313255584566 \t| Accuracy: 106.250\n",
      "# Iteration  4816 -> Loss: 0.42186887749262403 \t| Accuracy: 106.250\n",
      "# Iteration  4817 -> Loss: 0.42183463817918515 \t| Accuracy: 106.250\n",
      "# Iteration  4818 -> Loss: 0.42180041460449447 \t| Accuracy: 106.250\n",
      "# Iteration  4819 -> Loss: 0.42176620675752674 \t| Accuracy: 106.250\n",
      "# Iteration  4820 -> Loss: 0.42173201462726695 \t| Accuracy: 106.250\n",
      "# Iteration  4821 -> Loss: 0.4216978382027098 \t| Accuracy: 106.250\n",
      "# Iteration  4822 -> Loss: 0.4216636774728599 \t| Accuracy: 106.250\n",
      "# Iteration  4823 -> Loss: 0.4216295324267317 \t| Accuracy: 106.250\n",
      "# Iteration  4824 -> Loss: 0.4215954030533496 \t| Accuracy: 106.250\n",
      "# Iteration  4825 -> Loss: 0.42156128934174764 \t| Accuracy: 106.250\n",
      "# Iteration  4826 -> Loss: 0.4215271912809697 \t| Accuracy: 106.250\n",
      "# Iteration  4827 -> Loss: 0.4214931088600696 \t| Accuracy: 106.250\n",
      "# Iteration  4828 -> Loss: 0.42145904206811075 \t| Accuracy: 106.250\n",
      "# Iteration  4829 -> Loss: 0.4214249908941667 \t| Accuracy: 106.250\n",
      "# Iteration  4830 -> Loss: 0.4213909553273203 \t| Accuracy: 106.250\n",
      "# Iteration  4831 -> Loss: 0.4213569353566646 \t| Accuracy: 106.250\n",
      "# Iteration  4832 -> Loss: 0.4213229309713019 \t| Accuracy: 106.250\n",
      "# Iteration  4833 -> Loss: 0.42128894216034474 \t| Accuracy: 106.250\n",
      "# Iteration  4834 -> Loss: 0.42125496891291525 \t| Accuracy: 106.250\n",
      "# Iteration  4835 -> Loss: 0.42122101121814504 \t| Accuracy: 106.250\n",
      "# Iteration  4836 -> Loss: 0.4211870690651757 \t| Accuracy: 106.250\n",
      "# Iteration  4837 -> Loss: 0.42115314244315866 \t| Accuracy: 106.250\n",
      "# Iteration  4838 -> Loss: 0.4211192313412544 \t| Accuracy: 106.250\n",
      "# Iteration  4839 -> Loss: 0.42108533574863355 \t| Accuracy: 106.250\n",
      "# Iteration  4840 -> Loss: 0.4210514556544766 \t| Accuracy: 106.250\n",
      "# Iteration  4841 -> Loss: 0.42101759104797337 \t| Accuracy: 106.250\n",
      "# Iteration  4842 -> Loss: 0.4209837419183232 \t| Accuracy: 106.250\n",
      "# Iteration  4843 -> Loss: 0.4209499082547355 \t| Accuracy: 106.250\n",
      "# Iteration  4844 -> Loss: 0.42091609004642894 \t| Accuracy: 106.250\n",
      "# Iteration  4845 -> Loss: 0.420882287282632 \t| Accuracy: 106.250\n",
      "# Iteration  4846 -> Loss: 0.4208484999525826 \t| Accuracy: 106.250\n",
      "# Iteration  4847 -> Loss: 0.4208147280455285 \t| Accuracy: 106.250\n",
      "# Iteration  4848 -> Loss: 0.42078097155072686 \t| Accuracy: 106.250\n",
      "# Iteration  4849 -> Loss: 0.42074723045744444 \t| Accuracy: 106.250\n",
      "# Iteration  4850 -> Loss: 0.4207135047549575 \t| Accuracy: 106.250\n",
      "# Iteration  4851 -> Loss: 0.42067979443255227 \t| Accuracy: 106.250\n",
      "# Iteration  4852 -> Loss: 0.42064609947952386 \t| Accuracy: 106.250\n",
      "# Iteration  4853 -> Loss: 0.4206124198851774 \t| Accuracy: 106.250\n",
      "# Iteration  4854 -> Loss: 0.4205787556388273 \t| Accuracy: 106.250\n",
      "# Iteration  4855 -> Loss: 0.42054510672979767 \t| Accuracy: 106.250\n",
      "# Iteration  4856 -> Loss: 0.42051147314742204 \t| Accuracy: 106.250\n",
      "# Iteration  4857 -> Loss: 0.42047785488104344 \t| Accuracy: 106.250\n",
      "# Iteration  4858 -> Loss: 0.4204442519200143 \t| Accuracy: 106.250\n",
      "# Iteration  4859 -> Loss: 0.4204106642536966 \t| Accuracy: 106.250\n",
      "# Iteration  4860 -> Loss: 0.42037709187146177 \t| Accuracy: 106.250\n",
      "# Iteration  4861 -> Loss: 0.4203435347626907 \t| Accuracy: 106.250\n",
      "# Iteration  4862 -> Loss: 0.4203099929167738 \t| Accuracy: 106.250\n",
      "# Iteration  4863 -> Loss: 0.4202764663231107 \t| Accuracy: 106.250\n",
      "# Iteration  4864 -> Loss: 0.4202429549711109 \t| Accuracy: 106.250\n",
      "# Iteration  4865 -> Loss: 0.4202094588501925 \t| Accuracy: 106.250\n",
      "# Iteration  4866 -> Loss: 0.4201759779497839 \t| Accuracy: 106.250\n",
      "# Iteration  4867 -> Loss: 0.42014251225932236 \t| Accuracy: 106.250\n",
      "# Iteration  4868 -> Loss: 0.4201090617682546 \t| Accuracy: 106.250\n",
      "# Iteration  4869 -> Loss: 0.4200756264660368 \t| Accuracy: 106.250\n",
      "# Iteration  4870 -> Loss: 0.42004220634213457 \t| Accuracy: 106.250\n",
      "# Iteration  4871 -> Loss: 0.42000880138602265 \t| Accuracy: 106.250\n",
      "# Iteration  4872 -> Loss: 0.41997541158718527 \t| Accuracy: 106.250\n",
      "# Iteration  4873 -> Loss: 0.41994203693511606 \t| Accuracy: 106.250\n",
      "# Iteration  4874 -> Loss: 0.41990867741931764 \t| Accuracy: 106.250\n",
      "# Iteration  4875 -> Loss: 0.4198753330293024 \t| Accuracy: 106.250\n",
      "# Iteration  4876 -> Loss: 0.41984200375459174 \t| Accuracy: 106.250\n",
      "# Iteration  4877 -> Loss: 0.4198086895847164 \t| Accuracy: 106.250\n",
      "# Iteration  4878 -> Loss: 0.4197753905092165 \t| Accuracy: 106.250\n",
      "# Iteration  4879 -> Loss: 0.4197421065176412 \t| Accuracy: 106.250\n",
      "# Iteration  4880 -> Loss: 0.41970883759954924 \t| Accuracy: 106.250\n",
      "# Iteration  4881 -> Loss: 0.4196755837445083 \t| Accuracy: 106.250\n",
      "# Iteration  4882 -> Loss: 0.41964234494209546 \t| Accuracy: 106.250\n",
      "# Iteration  4883 -> Loss: 0.41960912118189714 \t| Accuracy: 106.250\n",
      "# Iteration  4884 -> Loss: 0.4195759124535088 \t| Accuracy: 106.250\n",
      "# Iteration  4885 -> Loss: 0.4195427187465352 \t| Accuracy: 106.250\n",
      "# Iteration  4886 -> Loss: 0.4195095400505902 \t| Accuracy: 106.250\n",
      "# Iteration  4887 -> Loss: 0.419476376355297 \t| Accuracy: 106.250\n",
      "# Iteration  4888 -> Loss: 0.41944322765028796 \t| Accuracy: 106.250\n",
      "# Iteration  4889 -> Loss: 0.4194100939252044 \t| Accuracy: 106.250\n",
      "# Iteration  4890 -> Loss: 0.4193769751696971 \t| Accuracy: 106.250\n",
      "# Iteration  4891 -> Loss: 0.4193438713734257 \t| Accuracy: 106.250\n",
      "# Iteration  4892 -> Loss: 0.4193107825260594 \t| Accuracy: 106.250\n",
      "# Iteration  4893 -> Loss: 0.4192777086172761 \t| Accuracy: 106.250\n",
      "# Iteration  4894 -> Loss: 0.419244649636763 \t| Accuracy: 106.250\n",
      "# Iteration  4895 -> Loss: 0.4192116055742164 \t| Accuracy: 106.250\n",
      "# Iteration  4896 -> Loss: 0.4191785764193418 \t| Accuracy: 106.250\n",
      "# Iteration  4897 -> Loss: 0.41914556216185367 \t| Accuracy: 106.250\n",
      "# Iteration  4898 -> Loss: 0.41911256279147563 \t| Accuracy: 106.250\n",
      "# Iteration  4899 -> Loss: 0.41907957829794024 \t| Accuracy: 106.250\n",
      "# Iteration  4900 -> Loss: 0.4190466086709893 \t| Accuracy: 106.250\n",
      "# Iteration  4901 -> Loss: 0.41901365390037365 \t| Accuracy: 106.250\n",
      "# Iteration  4902 -> Loss: 0.4189807139758531 \t| Accuracy: 106.250\n",
      "# Iteration  4903 -> Loss: 0.4189477888871964 \t| Accuracy: 106.250\n",
      "# Iteration  4904 -> Loss: 0.4189148786241817 \t| Accuracy: 106.250\n",
      "# Iteration  4905 -> Loss: 0.4188819831765957 \t| Accuracy: 106.250\n",
      "# Iteration  4906 -> Loss: 0.4188491025342343 \t| Accuracy: 106.250\n",
      "# Iteration  4907 -> Loss: 0.4188162366869026 \t| Accuracy: 106.250\n",
      "# Iteration  4908 -> Loss: 0.41878338562441425 \t| Accuracy: 106.250\n",
      "# Iteration  4909 -> Loss: 0.41875054933659234 \t| Accuracy: 106.250\n",
      "# Iteration  4910 -> Loss: 0.41871772781326855 \t| Accuracy: 106.250\n",
      "# Iteration  4911 -> Loss: 0.4186849210442838 \t| Accuracy: 106.250\n",
      "# Iteration  4912 -> Loss: 0.41865212901948784 \t| Accuracy: 106.250\n",
      "# Iteration  4913 -> Loss: 0.4186193517287393 \t| Accuracy: 106.250\n",
      "# Iteration  4914 -> Loss: 0.41858658916190583 \t| Accuracy: 106.250\n",
      "# Iteration  4915 -> Loss: 0.41855384130886397 \t| Accuracy: 106.250\n",
      "# Iteration  4916 -> Loss: 0.4185211081594992 \t| Accuracy: 106.250\n",
      "# Iteration  4917 -> Loss: 0.41848838970370583 \t| Accuracy: 106.250\n",
      "# Iteration  4918 -> Loss: 0.418455685931387 \t| Accuracy: 106.250\n",
      "# Iteration  4919 -> Loss: 0.41842299683245515 \t| Accuracy: 106.250\n",
      "# Iteration  4920 -> Loss: 0.4183903223968309 \t| Accuracy: 106.250\n",
      "# Iteration  4921 -> Loss: 0.41835766261444435 \t| Accuracy: 106.250\n",
      "# Iteration  4922 -> Loss: 0.4183250174752341 \t| Accuracy: 106.250\n",
      "# Iteration  4923 -> Loss: 0.4182923869691477 \t| Accuracy: 106.250\n",
      "# Iteration  4924 -> Loss: 0.41825977108614165 \t| Accuracy: 106.250\n",
      "# Iteration  4925 -> Loss: 0.418227169816181 \t| Accuracy: 106.250\n",
      "# Iteration  4926 -> Loss: 0.4181945831492399 \t| Accuracy: 106.250\n",
      "# Iteration  4927 -> Loss: 0.4181620110753012 \t| Accuracy: 106.250\n",
      "# Iteration  4928 -> Loss: 0.4181294535843564 \t| Accuracy: 106.250\n",
      "# Iteration  4929 -> Loss: 0.4180969106664059 \t| Accuracy: 106.250\n",
      "# Iteration  4930 -> Loss: 0.41806438231145904 \t| Accuracy: 106.250\n",
      "# Iteration  4931 -> Loss: 0.41803186850953367 \t| Accuracy: 106.250\n",
      "# Iteration  4932 -> Loss: 0.41799936925065656 \t| Accuracy: 106.250\n",
      "# Iteration  4933 -> Loss: 0.41796688452486325 \t| Accuracy: 106.250\n",
      "# Iteration  4934 -> Loss: 0.4179344143221977 \t| Accuracy: 106.250\n",
      "# Iteration  4935 -> Loss: 0.417901958632713 \t| Accuracy: 106.250\n",
      "# Iteration  4936 -> Loss: 0.41786951744647083 \t| Accuracy: 106.250\n",
      "# Iteration  4937 -> Loss: 0.41783709075354153 \t| Accuracy: 106.250\n",
      "# Iteration  4938 -> Loss: 0.41780467854400416 \t| Accuracy: 106.250\n",
      "# Iteration  4939 -> Loss: 0.41777228080794643 \t| Accuracy: 106.250\n",
      "# Iteration  4940 -> Loss: 0.4177398975354649 \t| Accuracy: 106.250\n",
      "# Iteration  4941 -> Loss: 0.41770752871666456 \t| Accuracy: 106.250\n",
      "# Iteration  4942 -> Loss: 0.4176751743416593 \t| Accuracy: 106.250\n",
      "# Iteration  4943 -> Loss: 0.4176428344005715 \t| Accuracy: 106.250\n",
      "# Iteration  4944 -> Loss: 0.4176105088835322 \t| Accuracy: 106.250\n",
      "# Iteration  4945 -> Loss: 0.41757819778068117 \t| Accuracy: 106.250\n",
      "# Iteration  4946 -> Loss: 0.41754590108216677 \t| Accuracy: 106.250\n",
      "# Iteration  4947 -> Loss: 0.4175136187781459 \t| Accuracy: 106.250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Iteration  4948 -> Loss: 0.41748135085878424 \t| Accuracy: 106.250\n",
      "# Iteration  4949 -> Loss: 0.41744909731425595 \t| Accuracy: 106.250\n",
      "# Iteration  4950 -> Loss: 0.41741685813474366 \t| Accuracy: 106.250\n",
      "# Iteration  4951 -> Loss: 0.41738463331043885 \t| Accuracy: 106.250\n",
      "# Iteration  4952 -> Loss: 0.4173524228315414 \t| Accuracy: 106.250\n",
      "# Iteration  4953 -> Loss: 0.41732022668825974 \t| Accuracy: 106.250\n",
      "# Iteration  4954 -> Loss: 0.417288044870811 \t| Accuracy: 106.250\n",
      "# Iteration  4955 -> Loss: 0.41725587736942066 \t| Accuracy: 106.250\n",
      "# Iteration  4956 -> Loss: 0.4172237241743229 \t| Accuracy: 106.250\n",
      "# Iteration  4957 -> Loss: 0.4171915852757603 \t| Accuracy: 106.250\n",
      "# Iteration  4958 -> Loss: 0.41715946066398407 \t| Accuracy: 106.250\n",
      "# Iteration  4959 -> Loss: 0.4171273503292539 \t| Accuracy: 106.250\n",
      "# Iteration  4960 -> Loss: 0.4170952542618378 \t| Accuracy: 106.250\n",
      "# Iteration  4961 -> Loss: 0.4170631724520127 \t| Accuracy: 106.250\n",
      "# Iteration  4962 -> Loss: 0.4170311048900634 \t| Accuracy: 106.250\n",
      "# Iteration  4963 -> Loss: 0.41699905156628375 \t| Accuracy: 106.250\n",
      "# Iteration  4964 -> Loss: 0.41696701247097573 \t| Accuracy: 106.250\n",
      "# Iteration  4965 -> Loss: 0.4169349875944499 \t| Accuracy: 106.250\n",
      "# Iteration  4966 -> Loss: 0.41690297692702505 \t| Accuracy: 106.250\n",
      "# Iteration  4967 -> Loss: 0.41687098045902893 \t| Accuracy: 106.250\n",
      "# Iteration  4968 -> Loss: 0.416838998180797 \t| Accuracy: 106.250\n",
      "# Iteration  4969 -> Loss: 0.41680703008267367 \t| Accuracy: 106.250\n",
      "# Iteration  4970 -> Loss: 0.41677507615501164 \t| Accuracy: 106.250\n",
      "# Iteration  4971 -> Loss: 0.4167431363881719 \t| Accuracy: 106.250\n",
      "# Iteration  4972 -> Loss: 0.41671121077252377 \t| Accuracy: 106.250\n",
      "# Iteration  4973 -> Loss: 0.4166792992984454 \t| Accuracy: 106.250\n",
      "# Iteration  4974 -> Loss: 0.4166474019563226 \t| Accuracy: 106.250\n",
      "# Iteration  4975 -> Loss: 0.4166155187365501 \t| Accuracy: 106.250\n",
      "# Iteration  4976 -> Loss: 0.41658364962953087 \t| Accuracy: 106.250\n",
      "# Iteration  4977 -> Loss: 0.41655179462567604 \t| Accuracy: 106.250\n",
      "# Iteration  4978 -> Loss: 0.41651995371540546 \t| Accuracy: 106.250\n",
      "# Iteration  4979 -> Loss: 0.41648812688914666 \t| Accuracy: 106.250\n",
      "# Iteration  4980 -> Loss: 0.41645631413733614 \t| Accuracy: 106.250\n",
      "# Iteration  4981 -> Loss: 0.4164245154504183 \t| Accuracy: 106.250\n",
      "# Iteration  4982 -> Loss: 0.4163927308188462 \t| Accuracy: 106.250\n",
      "# Iteration  4983 -> Loss: 0.4163609602330807 \t| Accuracy: 106.250\n",
      "# Iteration  4984 -> Loss: 0.4163292036835915 \t| Accuracy: 106.250\n",
      "# Iteration  4985 -> Loss: 0.4162974611608561 \t| Accuracy: 106.250\n",
      "# Iteration  4986 -> Loss: 0.41626573265536054 \t| Accuracy: 106.250\n",
      "# Iteration  4987 -> Loss: 0.41623401815759903 \t| Accuracy: 106.250\n",
      "# Iteration  4988 -> Loss: 0.4162023176580741 \t| Accuracy: 106.250\n",
      "# Iteration  4989 -> Loss: 0.4161706311472963 \t| Accuracy: 106.250\n",
      "# Iteration  4990 -> Loss: 0.4161389586157845 \t| Accuracy: 106.250\n",
      "# Iteration  4991 -> Loss: 0.4161073000540662 \t| Accuracy: 106.250\n",
      "# Iteration  4992 -> Loss: 0.41607565545267644 \t| Accuracy: 106.250\n",
      "# Iteration  4993 -> Loss: 0.416044024802159 \t| Accuracy: 106.250\n",
      "# Iteration  4994 -> Loss: 0.4160124080930655 \t| Accuracy: 106.250\n",
      "# Iteration  4995 -> Loss: 0.4159808053159561 \t| Accuracy: 106.250\n",
      "# Iteration  4996 -> Loss: 0.41594921646139865 \t| Accuracy: 106.250\n",
      "# Iteration  4997 -> Loss: 0.4159176415199695 \t| Accuracy: 106.250\n",
      "# Iteration  4998 -> Loss: 0.41588608048225334 \t| Accuracy: 106.250\n",
      "# Iteration  4999 -> Loss: 0.41585453333884254 \t| Accuracy: 106.250\n",
      "# Iteration  5000 -> Loss: 0.415823000080338 \t| Accuracy: 106.250\n",
      "# Iteration  5001 -> Loss: 0.41579148069734845 \t| Accuracy: 106.250\n",
      "# Iteration  5002 -> Loss: 0.415759975180491 \t| Accuracy: 106.250\n",
      "# Iteration  5003 -> Loss: 0.41572848352039077 \t| Accuracy: 106.250\n",
      "# Iteration  5004 -> Loss: 0.41569700570768103 \t| Accuracy: 106.250\n",
      "# Iteration  5005 -> Loss: 0.415665541733003 \t| Accuracy: 106.250\n",
      "# Iteration  5006 -> Loss: 0.4156340915870061 \t| Accuracy: 106.250\n",
      "# Iteration  5007 -> Loss: 0.41560265526034795 \t| Accuracy: 106.250\n",
      "# Iteration  5008 -> Loss: 0.415571232743694 \t| Accuracy: 106.250\n",
      "# Iteration  5009 -> Loss: 0.41553982402771805 \t| Accuracy: 106.250\n",
      "# Iteration  5010 -> Loss: 0.41550842910310165 \t| Accuracy: 106.250\n",
      "# Iteration  5011 -> Loss: 0.41547704796053464 \t| Accuracy: 106.250\n",
      "# Iteration  5012 -> Loss: 0.4154456805907146 \t| Accuracy: 106.250\n",
      "# Iteration  5013 -> Loss: 0.41541432698434766 \t| Accuracy: 106.250\n",
      "# Iteration  5014 -> Loss: 0.41538298713214744 \t| Accuracy: 106.250\n",
      "# Iteration  5015 -> Loss: 0.4153516610248358 \t| Accuracy: 106.250\n",
      "# Iteration  5016 -> Loss: 0.41532034865314255 \t| Accuracy: 106.250\n",
      "# Iteration  5017 -> Loss: 0.4152890500078057 \t| Accuracy: 106.250\n",
      "# Iteration  5018 -> Loss: 0.41525776507957096 \t| Accuracy: 106.250\n",
      "# Iteration  5019 -> Loss: 0.4152264938591922 \t| Accuracy: 106.250\n",
      "# Iteration  5020 -> Loss: 0.41519523633743105 \t| Accuracy: 106.250\n",
      "# Iteration  5021 -> Loss: 0.4151639925050574 \t| Accuracy: 106.250\n",
      "# Iteration  5022 -> Loss: 0.415132762352849 \t| Accuracy: 106.250\n",
      "# Iteration  5023 -> Loss: 0.4151015458715913 \t| Accuracy: 106.250\n",
      "# Iteration  5024 -> Loss: 0.4150703430520781 \t| Accuracy: 106.250\n",
      "# Iteration  5025 -> Loss: 0.4150391538851106 \t| Accuracy: 106.250\n",
      "# Iteration  5026 -> Loss: 0.41500797836149855 \t| Accuracy: 106.250\n",
      "# Iteration  5027 -> Loss: 0.4149768164720591 \t| Accuracy: 106.250\n",
      "# Iteration  5028 -> Loss: 0.4149456682076175 \t| Accuracy: 106.250\n",
      "# Iteration  5029 -> Loss: 0.4149145335590069 \t| Accuracy: 106.250\n",
      "# Iteration  5030 -> Loss: 0.4148834125170684 \t| Accuracy: 106.250\n",
      "# Iteration  5031 -> Loss: 0.41485230507265086 \t| Accuracy: 106.250\n",
      "# Iteration  5032 -> Loss: 0.414821211216611 \t| Accuracy: 106.250\n",
      "# Iteration  5033 -> Loss: 0.41479013093981326 \t| Accuracy: 106.250\n",
      "# Iteration  5034 -> Loss: 0.4147590642331304 \t| Accuracy: 106.250\n",
      "# Iteration  5035 -> Loss: 0.4147280110874427 \t| Accuracy: 106.250\n",
      "# Iteration  5036 -> Loss: 0.4146969714936382 \t| Accuracy: 106.250\n",
      "# Iteration  5037 -> Loss: 0.4146659454426129 \t| Accuracy: 106.250\n",
      "# Iteration  5038 -> Loss: 0.4146349329252708 \t| Accuracy: 106.250\n",
      "# Iteration  5039 -> Loss: 0.4146039339325233 \t| Accuracy: 106.250\n",
      "# Iteration  5040 -> Loss: 0.41457294845528975 \t| Accuracy: 106.250\n",
      "# Iteration  5041 -> Loss: 0.4145419764844976 \t| Accuracy: 106.250\n",
      "# Iteration  5042 -> Loss: 0.41451101801108164 \t| Accuracy: 106.250\n",
      "# Iteration  5043 -> Loss: 0.41448007302598466 \t| Accuracy: 106.250\n",
      "# Iteration  5044 -> Loss: 0.4144491415201574 \t| Accuracy: 106.250\n",
      "# Iteration  5045 -> Loss: 0.41441822348455787 \t| Accuracy: 106.250\n",
      "# Iteration  5046 -> Loss: 0.4143873189101524 \t| Accuracy: 106.250\n",
      "# Iteration  5047 -> Loss: 0.41435642778791454 \t| Accuracy: 106.250\n",
      "# Iteration  5048 -> Loss: 0.414325550108826 \t| Accuracy: 106.250\n",
      "# Iteration  5049 -> Loss: 0.41429468586387597 \t| Accuracy: 106.250\n",
      "# Iteration  5050 -> Loss: 0.41426383504406145 \t| Accuracy: 106.250\n",
      "# Iteration  5051 -> Loss: 0.41423299764038707 \t| Accuracy: 106.250\n",
      "# Iteration  5052 -> Loss: 0.41420217364386525 \t| Accuracy: 106.250\n",
      "# Iteration  5053 -> Loss: 0.4141713630455161 \t| Accuracy: 106.250\n",
      "# Iteration  5054 -> Loss: 0.4141405658363675 \t| Accuracy: 106.250\n",
      "# Iteration  5055 -> Loss: 0.41410978200745463 \t| Accuracy: 106.250\n",
      "# Iteration  5056 -> Loss: 0.41407901154982096 \t| Accuracy: 106.250\n",
      "# Iteration  5057 -> Loss: 0.41404825445451704 \t| Accuracy: 106.250\n",
      "# Iteration  5058 -> Loss: 0.41401751071260134 \t| Accuracy: 106.250\n",
      "# Iteration  5059 -> Loss: 0.4139867803151401 \t| Accuracy: 106.250\n",
      "# Iteration  5060 -> Loss: 0.41395606325320683 \t| Accuracy: 106.250\n",
      "# Iteration  5061 -> Loss: 0.413925359517883 \t| Accuracy: 106.250\n",
      "# Iteration  5062 -> Loss: 0.4138946691002577 \t| Accuracy: 106.250\n",
      "# Iteration  5063 -> Loss: 0.4138639919914272 \t| Accuracy: 106.250\n",
      "# Iteration  5064 -> Loss: 0.41383332818249613 \t| Accuracy: 106.250\n",
      "# Iteration  5065 -> Loss: 0.4138026776645758 \t| Accuracy: 106.250\n",
      "# Iteration  5066 -> Loss: 0.4137720404287861 \t| Accuracy: 106.250\n",
      "# Iteration  5067 -> Loss: 0.4137414164662536 \t| Accuracy: 106.250\n",
      "# Iteration  5068 -> Loss: 0.4137108057681131 \t| Accuracy: 105.625\n",
      "# Iteration  5069 -> Loss: 0.41368020832550656 \t| Accuracy: 105.625\n",
      "# Iteration  5070 -> Loss: 0.4136496241295838 \t| Accuracy: 105.625\n",
      "# Iteration  5071 -> Loss: 0.4136190531715017 \t| Accuracy: 105.625\n",
      "# Iteration  5072 -> Loss: 0.4135884954424254 \t| Accuracy: 105.625\n",
      "# Iteration  5073 -> Loss: 0.4135579509335269 \t| Accuracy: 105.625\n",
      "# Iteration  5074 -> Loss: 0.4135274196359862 \t| Accuracy: 105.625\n",
      "# Iteration  5075 -> Loss: 0.41349690154099056 \t| Accuracy: 105.625\n",
      "# Iteration  5076 -> Loss: 0.413466396639735 \t| Accuracy: 105.625\n",
      "# Iteration  5077 -> Loss: 0.4134359049234214 \t| Accuracy: 105.625\n",
      "# Iteration  5078 -> Loss: 0.41340542638326006 \t| Accuracy: 105.625\n",
      "# Iteration  5079 -> Loss: 0.41337496101046817 \t| Accuracy: 105.625\n",
      "# Iteration  5080 -> Loss: 0.41334450879627044 \t| Accuracy: 105.625\n",
      "# Iteration  5081 -> Loss: 0.4133140697318992 \t| Accuracy: 105.625\n",
      "# Iteration  5082 -> Loss: 0.41328364380859417 \t| Accuracy: 105.625\n",
      "# Iteration  5083 -> Loss: 0.4132532310176026 \t| Accuracy: 105.625\n",
      "# Iteration  5084 -> Loss: 0.41322283135017907 \t| Accuracy: 105.625\n",
      "# Iteration  5085 -> Loss: 0.41319244479758566 \t| Accuracy: 105.625\n",
      "# Iteration  5086 -> Loss: 0.4131620713510918 \t| Accuracy: 105.625\n",
      "# Iteration  5087 -> Loss: 0.4131317110019746 \t| Accuracy: 105.625\n",
      "# Iteration  5088 -> Loss: 0.4131013637415182 \t| Accuracy: 105.625\n",
      "# Iteration  5089 -> Loss: 0.41307102956101444 \t| Accuracy: 105.625\n",
      "# Iteration  5090 -> Loss: 0.41304070845176255 \t| Accuracy: 105.625\n",
      "# Iteration  5091 -> Loss: 0.4130104004050689 \t| Accuracy: 105.625\n",
      "# Iteration  5092 -> Loss: 0.41298010541224767 \t| Accuracy: 105.625\n",
      "# Iteration  5093 -> Loss: 0.4129498234646198 \t| Accuracy: 105.625\n",
      "# Iteration  5094 -> Loss: 0.4129195545535142 \t| Accuracy: 105.625\n",
      "# Iteration  5095 -> Loss: 0.4128892986702669 \t| Accuracy: 105.625\n",
      "# Iteration  5096 -> Loss: 0.4128590558062212 \t| Accuracy: 105.625\n",
      "# Iteration  5097 -> Loss: 0.4128288259527279 \t| Accuracy: 105.625\n",
      "# Iteration  5098 -> Loss: 0.412798609101145 \t| Accuracy: 105.625\n",
      "# Iteration  5099 -> Loss: 0.4127684052428378 \t| Accuracy: 105.625\n",
      "# Iteration  5100 -> Loss: 0.41273821436917923 \t| Accuracy: 105.625\n",
      "# Iteration  5101 -> Loss: 0.4127080364715492 \t| Accuracy: 105.625\n",
      "# Iteration  5102 -> Loss: 0.4126778715413351 \t| Accuracy: 105.625\n",
      "# Iteration  5103 -> Loss: 0.41264771956993146 \t| Accuracy: 105.625\n",
      "# Iteration  5104 -> Loss: 0.41261758054874026 \t| Accuracy: 105.625\n",
      "# Iteration  5105 -> Loss: 0.4125874544691708 \t| Accuracy: 105.625\n",
      "# Iteration  5106 -> Loss: 0.4125573413226394 \t| Accuracy: 105.625\n",
      "# Iteration  5107 -> Loss: 0.4125272411005699 \t| Accuracy: 105.625\n",
      "# Iteration  5108 -> Loss: 0.4124971537943933 \t| Accuracy: 105.625\n",
      "# Iteration  5109 -> Loss: 0.41246707939554783 \t| Accuracy: 105.625\n",
      "# Iteration  5110 -> Loss: 0.4124370178954791 \t| Accuracy: 105.625\n",
      "# Iteration  5111 -> Loss: 0.41240696928563975 \t| Accuracy: 105.625\n",
      "# Iteration  5112 -> Loss: 0.4123769335574898 \t| Accuracy: 105.625\n",
      "# Iteration  5113 -> Loss: 0.4123469107024964 \t| Accuracy: 105.625\n",
      "# Iteration  5114 -> Loss: 0.41231690071213406 \t| Accuracy: 105.625\n",
      "# Iteration  5115 -> Loss: 0.41228690357788433 \t| Accuracy: 105.625\n",
      "# Iteration  5116 -> Loss: 0.41225691929123615 \t| Accuracy: 105.625\n",
      "# Iteration  5117 -> Loss: 0.4122269478436853 \t| Accuracy: 105.625\n",
      "# Iteration  5118 -> Loss: 0.41219698922673514 \t| Accuracy: 105.625\n",
      "# Iteration  5119 -> Loss: 0.412167043431896 \t| Accuracy: 105.625\n",
      "# Iteration  5120 -> Loss: 0.41213711045068546 \t| Accuracy: 105.625\n",
      "# Iteration  5121 -> Loss: 0.4121071902746281 \t| Accuracy: 105.625\n",
      "# Iteration  5122 -> Loss: 0.41207728289525597 \t| Accuracy: 105.625\n",
      "# Iteration  5123 -> Loss: 0.41204738830410786 \t| Accuracy: 105.625\n",
      "# Iteration  5124 -> Loss: 0.41201750649273017 \t| Accuracy: 105.625\n",
      "# Iteration  5125 -> Loss: 0.4119876374526761 \t| Accuracy: 105.625\n",
      "# Iteration  5126 -> Loss: 0.41195778117550574 \t| Accuracy: 105.625\n",
      "# Iteration  5127 -> Loss: 0.41192793765278707 \t| Accuracy: 105.625\n",
      "# Iteration  5128 -> Loss: 0.4118981068760944 \t| Accuracy: 105.625\n",
      "# Iteration  5129 -> Loss: 0.4118682888370096 \t| Accuracy: 105.625\n",
      "# Iteration  5130 -> Loss: 0.4118384835271214 \t| Accuracy: 105.625\n",
      "# Iteration  5131 -> Loss: 0.41180869093802586 \t| Accuracy: 105.625\n",
      "# Iteration  5132 -> Loss: 0.4117789110613258 \t| Accuracy: 105.625\n",
      "# Iteration  5133 -> Loss: 0.41174914388863126 \t| Accuracy: 105.625\n",
      "# Iteration  5134 -> Loss: 0.41171938941155967 \t| Accuracy: 105.625\n",
      "# Iteration  5135 -> Loss: 0.41168964762173477 \t| Accuracy: 105.625\n",
      "# Iteration  5136 -> Loss: 0.41165991851078826 \t| Accuracy: 105.625\n",
      "# Iteration  5137 -> Loss: 0.41163020207035794 \t| Accuracy: 105.625\n",
      "# Iteration  5138 -> Loss: 0.4116004982920895 \t| Accuracy: 105.625\n",
      "# Iteration  5139 -> Loss: 0.4115708071676352 \t| Accuracy: 105.625\n",
      "# Iteration  5140 -> Loss: 0.41154112868865433 \t| Accuracy: 105.625\n",
      "# Iteration  5141 -> Loss: 0.4115114628468132 \t| Accuracy: 105.625\n",
      "# Iteration  5142 -> Loss: 0.4114818096337853 \t| Accuracy: 105.625\n",
      "# Iteration  5143 -> Loss: 0.41145216904125104 \t| Accuracy: 105.625\n",
      "# Iteration  5144 -> Loss: 0.4114225410608976 \t| Accuracy: 105.625\n",
      "# Iteration  5145 -> Loss: 0.41139292568441954 \t| Accuracy: 105.625\n",
      "# Iteration  5146 -> Loss: 0.41136332290351807 \t| Accuracy: 105.625\n",
      "# Iteration  5147 -> Loss: 0.41133373270990164 \t| Accuracy: 105.625\n",
      "# Iteration  5148 -> Loss: 0.4113041550952853 \t| Accuracy: 105.625\n",
      "# Iteration  5149 -> Loss: 0.41127459005139133 \t| Accuracy: 105.625\n",
      "# Iteration  5150 -> Loss: 0.41124503756994896 \t| Accuracy: 105.625\n",
      "# Iteration  5151 -> Loss: 0.4112154976426943 \t| Accuracy: 105.625\n",
      "# Iteration  5152 -> Loss: 0.41118597026137027 \t| Accuracy: 105.625\n",
      "# Iteration  5153 -> Loss: 0.41115645541772705 \t| Accuracy: 105.625\n",
      "# Iteration  5154 -> Loss: 0.4111269531035212 \t| Accuracy: 105.625\n",
      "# Iteration  5155 -> Loss: 0.4110974633105169 \t| Accuracy: 105.625\n",
      "# Iteration  5156 -> Loss: 0.4110679860304845 \t| Accuracy: 105.625\n",
      "# Iteration  5157 -> Loss: 0.41103852125520174 \t| Accuracy: 105.625\n",
      "# Iteration  5158 -> Loss: 0.4110090689764532 \t| Accuracy: 105.625\n",
      "# Iteration  5159 -> Loss: 0.4109796291860301 \t| Accuracy: 105.625\n",
      "# Iteration  5160 -> Loss: 0.4109502018757308 \t| Accuracy: 105.625\n",
      "# Iteration  5161 -> Loss: 0.4109207870373602 \t| Accuracy: 105.625\n",
      "# Iteration  5162 -> Loss: 0.41089138466273056 \t| Accuracy: 105.625\n",
      "# Iteration  5163 -> Loss: 0.4108619947436605 \t| Accuracy: 105.625\n",
      "# Iteration  5164 -> Loss: 0.4108326172719758 \t| Accuracy: 105.625\n",
      "# Iteration  5165 -> Loss: 0.41080325223950875 \t| Accuracy: 105.625\n",
      "# Iteration  5166 -> Loss: 0.410773899638099 \t| Accuracy: 105.625\n",
      "# Iteration  5167 -> Loss: 0.41074455945959265 \t| Accuracy: 105.625\n",
      "# Iteration  5168 -> Loss: 0.4107152316958425 \t| Accuracy: 105.625\n",
      "# Iteration  5169 -> Loss: 0.4106859163387086 \t| Accuracy: 105.625\n",
      "# Iteration  5170 -> Loss: 0.4106566133800573 \t| Accuracy: 105.625\n",
      "# Iteration  5171 -> Loss: 0.4106273228117623 \t| Accuracy: 105.625\n",
      "# Iteration  5172 -> Loss: 0.4105980446257035 \t| Accuracy: 105.625\n",
      "# Iteration  5173 -> Loss: 0.4105687788137681 \t| Accuracy: 105.625\n",
      "# Iteration  5174 -> Loss: 0.4105395253678497 \t| Accuracy: 105.625\n",
      "# Iteration  5175 -> Loss: 0.4105102842798489 \t| Accuracy: 105.625\n",
      "# Iteration  5176 -> Loss: 0.41048105554167297 \t| Accuracy: 105.625\n",
      "# Iteration  5177 -> Loss: 0.4104518391452359 \t| Accuracy: 105.625\n",
      "# Iteration  5178 -> Loss: 0.4104226350824585 \t| Accuracy: 105.625\n",
      "# Iteration  5179 -> Loss: 0.41039344334526834 \t| Accuracy: 105.625\n",
      "# Iteration  5180 -> Loss: 0.4103642639255996 \t| Accuracy: 105.625\n",
      "# Iteration  5181 -> Loss: 0.41033509681539343 \t| Accuracy: 105.625\n",
      "# Iteration  5182 -> Loss: 0.41030594200659737 \t| Accuracy: 105.625\n",
      "# Iteration  5183 -> Loss: 0.41027679949116586 \t| Accuracy: 105.625\n",
      "# Iteration  5184 -> Loss: 0.41024766926106 \t| Accuracy: 105.625\n",
      "# Iteration  5185 -> Loss: 0.4102185513082477 \t| Accuracy: 105.625\n",
      "# Iteration  5186 -> Loss: 0.41018944562470344 \t| Accuracy: 105.625\n",
      "# Iteration  5187 -> Loss: 0.4101603522024085 \t| Accuracy: 105.625\n",
      "# Iteration  5188 -> Loss: 0.4101312710333507 \t| Accuracy: 105.625\n",
      "# Iteration  5189 -> Loss: 0.4101022021095246 \t| Accuracy: 105.625\n",
      "# Iteration  5190 -> Loss: 0.41007314542293133 \t| Accuracy: 105.625\n",
      "# Iteration  5191 -> Loss: 0.4100441009655789 \t| Accuracy: 105.625\n",
      "# Iteration  5192 -> Loss: 0.41001506872948185 \t| Accuracy: 105.625\n",
      "# Iteration  5193 -> Loss: 0.4099860487066611 \t| Accuracy: 105.625\n",
      "# Iteration  5194 -> Loss: 0.4099570408891447 \t| Accuracy: 105.625\n",
      "# Iteration  5195 -> Loss: 0.4099280452689671 \t| Accuracy: 105.625\n",
      "# Iteration  5196 -> Loss: 0.4098990618381691 \t| Accuracy: 105.625\n",
      "# Iteration  5197 -> Loss: 0.40987009058879864 \t| Accuracy: 105.625\n",
      "# Iteration  5198 -> Loss: 0.40984113151290996 \t| Accuracy: 105.625\n",
      "# Iteration  5199 -> Loss: 0.4098121846025638 \t| Accuracy: 105.625\n",
      "# Iteration  5200 -> Loss: 0.4097832498498279 \t| Accuracy: 105.625\n",
      "# Iteration  5201 -> Loss: 0.40975432724677613 \t| Accuracy: 105.625\n",
      "# Iteration  5202 -> Loss: 0.4097254167854891 \t| Accuracy: 105.625\n",
      "# Iteration  5203 -> Loss: 0.40969651845805427 \t| Accuracy: 105.625\n",
      "# Iteration  5204 -> Loss: 0.4096676322565652 \t| Accuracy: 105.625\n",
      "# Iteration  5205 -> Loss: 0.4096387581731225 \t| Accuracy: 105.625\n",
      "# Iteration  5206 -> Loss: 0.40960989619983285 \t| Accuracy: 105.625\n",
      "# Iteration  5207 -> Loss: 0.4095810463288099 \t| Accuracy: 105.625\n",
      "# Iteration  5208 -> Loss: 0.4095522085521735 \t| Accuracy: 105.625\n",
      "# Iteration  5209 -> Loss: 0.4095233828620503 \t| Accuracy: 105.625\n",
      "# Iteration  5210 -> Loss: 0.4094945692505733 \t| Accuracy: 105.625\n",
      "# Iteration  5211 -> Loss: 0.4094657677098822 \t| Accuracy: 105.625\n",
      "# Iteration  5212 -> Loss: 0.40943697823212294 \t| Accuracy: 105.625\n",
      "# Iteration  5213 -> Loss: 0.4094082008094483 \t| Accuracy: 105.625\n",
      "# Iteration  5214 -> Loss: 0.4093794354340172 \t| Accuracy: 105.625\n",
      "# Iteration  5215 -> Loss: 0.4093506820979956 \t| Accuracy: 105.625\n",
      "# Iteration  5216 -> Loss: 0.4093219407935552 \t| Accuracy: 105.625\n",
      "# Iteration  5217 -> Loss: 0.40929321151287484 \t| Accuracy: 105.625\n",
      "# Iteration  5218 -> Loss: 0.40926449424813943 \t| Accuracy: 105.625\n",
      "# Iteration  5219 -> Loss: 0.4092357889915406 \t| Accuracy: 105.625\n",
      "# Iteration  5220 -> Loss: 0.40920709573527625 \t| Accuracy: 105.625\n",
      "# Iteration  5221 -> Loss: 0.4091784144715509 \t| Accuracy: 105.625\n",
      "# Iteration  5222 -> Loss: 0.4091497451925754 \t| Accuracy: 105.625\n",
      "# Iteration  5223 -> Loss: 0.409121087890567 \t| Accuracy: 105.625\n",
      "# Iteration  5224 -> Loss: 0.40909244255774957 \t| Accuracy: 105.625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Iteration  5225 -> Loss: 0.40906380918635327 \t| Accuracy: 105.625\n",
      "# Iteration  5226 -> Loss: 0.4090351877686147 \t| Accuracy: 105.625\n",
      "# Iteration  5227 -> Loss: 0.4090065782967769 \t| Accuracy: 105.625\n",
      "# Iteration  5228 -> Loss: 0.4089779807630893 \t| Accuracy: 105.625\n",
      "# Iteration  5229 -> Loss: 0.40894939515980777 \t| Accuracy: 105.625\n",
      "# Iteration  5230 -> Loss: 0.4089208214791945 \t| Accuracy: 105.625\n",
      "# Iteration  5231 -> Loss: 0.40889225971351806 \t| Accuracy: 105.625\n",
      "# Iteration  5232 -> Loss: 0.40886370985505355 \t| Accuracy: 105.625\n",
      "# Iteration  5233 -> Loss: 0.40883517189608226 \t| Accuracy: 105.625\n",
      "# Iteration  5234 -> Loss: 0.4088066458288921 \t| Accuracy: 105.625\n",
      "# Iteration  5235 -> Loss: 0.4087781316457771 \t| Accuracy: 105.625\n",
      "# Iteration  5236 -> Loss: 0.4087496293390376 \t| Accuracy: 105.625\n",
      "# Iteration  5237 -> Loss: 0.4087211389009805 \t| Accuracy: 105.625\n",
      "# Iteration  5238 -> Loss: 0.4086926603239191 \t| Accuracy: 105.625\n",
      "# Iteration  5239 -> Loss: 0.4086641936001727 \t| Accuracy: 105.625\n",
      "# Iteration  5240 -> Loss: 0.4086357387220672 \t| Accuracy: 105.625\n",
      "# Iteration  5241 -> Loss: 0.4086072956819347 \t| Accuracy: 105.625\n",
      "# Iteration  5242 -> Loss: 0.4085788644721138 \t| Accuracy: 105.625\n",
      "# Iteration  5243 -> Loss: 0.4085504450849492 \t| Accuracy: 105.625\n",
      "# Iteration  5244 -> Loss: 0.4085220375127918 \t| Accuracy: 105.625\n",
      "# Iteration  5245 -> Loss: 0.40849364174799935 \t| Accuracy: 105.625\n",
      "# Iteration  5246 -> Loss: 0.40846525778293524 \t| Accuracy: 105.625\n",
      "# Iteration  5247 -> Loss: 0.40843688560996955 \t| Accuracy: 105.625\n",
      "# Iteration  5248 -> Loss: 0.40840852522147847 \t| Accuracy: 105.625\n",
      "# Iteration  5249 -> Loss: 0.40838017660984444 \t| Accuracy: 105.625\n",
      "# Iteration  5250 -> Loss: 0.4083518397674563 \t| Accuracy: 105.625\n",
      "# Iteration  5251 -> Loss: 0.4083235146867092 \t| Accuracy: 105.625\n",
      "# Iteration  5252 -> Loss: 0.4082952013600042 \t| Accuracy: 105.625\n",
      "# Iteration  5253 -> Loss: 0.40826689977974884 \t| Accuracy: 105.625\n",
      "# Iteration  5254 -> Loss: 0.40823860993835703 \t| Accuracy: 105.625\n",
      "# Iteration  5255 -> Loss: 0.40821033182824884 \t| Accuracy: 105.625\n",
      "# Iteration  5256 -> Loss: 0.4081820654418503 \t| Accuracy: 105.625\n",
      "# Iteration  5257 -> Loss: 0.408153810771594 \t| Accuracy: 105.625\n",
      "# Iteration  5258 -> Loss: 0.4081255678099185 \t| Accuracy: 105.625\n",
      "# Iteration  5259 -> Loss: 0.40809733654926866 \t| Accuracy: 105.625\n",
      "# Iteration  5260 -> Loss: 0.40806911698209564 \t| Accuracy: 105.625\n",
      "# Iteration  5261 -> Loss: 0.4080409091008568 \t| Accuracy: 105.625\n",
      "# Iteration  5262 -> Loss: 0.4080127128980154 \t| Accuracy: 105.625\n",
      "# Iteration  5263 -> Loss: 0.4079845283660411 \t| Accuracy: 105.625\n",
      "# Iteration  5264 -> Loss: 0.40795635549740994 \t| Accuracy: 105.625\n",
      "# Iteration  5265 -> Loss: 0.40792819428460364 \t| Accuracy: 105.625\n",
      "# Iteration  5266 -> Loss: 0.4079000447201105 \t| Accuracy: 105.625\n",
      "# Iteration  5267 -> Loss: 0.4078719067964248 \t| Accuracy: 105.625\n",
      "# Iteration  5268 -> Loss: 0.4078437805060472 \t| Accuracy: 105.625\n",
      "# Iteration  5269 -> Loss: 0.40781566584148393 \t| Accuracy: 105.625\n",
      "# Iteration  5270 -> Loss: 0.40778756279524797 \t| Accuracy: 105.625\n",
      "# Iteration  5271 -> Loss: 0.4077594713598582 \t| Accuracy: 105.625\n",
      "# Iteration  5272 -> Loss: 0.40773139152783955 \t| Accuracy: 105.625\n",
      "# Iteration  5273 -> Loss: 0.40770332329172326 \t| Accuracy: 105.625\n",
      "# Iteration  5274 -> Loss: 0.4076752666440464 \t| Accuracy: 105.625\n",
      "# Iteration  5275 -> Loss: 0.4076472215773524 \t| Accuracy: 105.625\n",
      "# Iteration  5276 -> Loss: 0.4076191880841909 \t| Accuracy: 105.625\n",
      "# Iteration  5277 -> Loss: 0.40759116615711727 \t| Accuracy: 105.625\n",
      "# Iteration  5278 -> Loss: 0.407563155788693 \t| Accuracy: 105.625\n",
      "# Iteration  5279 -> Loss: 0.4075351569714861 \t| Accuracy: 105.625\n",
      "# Iteration  5280 -> Loss: 0.40750716969807027 \t| Accuracy: 105.625\n",
      "# Iteration  5281 -> Loss: 0.4074791939610252 \t| Accuracy: 105.625\n",
      "# Iteration  5282 -> Loss: 0.4074512297529371 \t| Accuracy: 105.625\n",
      "# Iteration  5283 -> Loss: 0.40742327706639786 \t| Accuracy: 105.625\n",
      "# Iteration  5284 -> Loss: 0.4073953358940054 \t| Accuracy: 105.625\n",
      "# Iteration  5285 -> Loss: 0.4073674062283639 \t| Accuracy: 105.625\n",
      "# Iteration  5286 -> Loss: 0.40733948806208364 \t| Accuracy: 105.625\n",
      "# Iteration  5287 -> Loss: 0.4073115813877805 \t| Accuracy: 105.625\n",
      "# Iteration  5288 -> Loss: 0.4072836861980768 \t| Accuracy: 105.625\n",
      "# Iteration  5289 -> Loss: 0.40725580248560084 \t| Accuracy: 105.625\n",
      "# Iteration  5290 -> Loss: 0.4072279302429868 \t| Accuracy: 105.625\n",
      "# Iteration  5291 -> Loss: 0.40720006946287485 \t| Accuracy: 105.625\n",
      "# Iteration  5292 -> Loss: 0.4071722201379114 \t| Accuracy: 105.625\n",
      "# Iteration  5293 -> Loss: 0.4071443822607484 \t| Accuracy: 105.625\n",
      "# Iteration  5294 -> Loss: 0.4071165558240443 \t| Accuracy: 105.625\n",
      "# Iteration  5295 -> Loss: 0.4070887408204633 \t| Accuracy: 105.625\n",
      "# Iteration  5296 -> Loss: 0.4070609372426755 \t| Accuracy: 105.625\n",
      "# Iteration  5297 -> Loss: 0.4070331450833573 \t| Accuracy: 105.625\n",
      "# Iteration  5298 -> Loss: 0.4070053643351905 \t| Accuracy: 105.625\n",
      "# Iteration  5299 -> Loss: 0.40697759499086344 \t| Accuracy: 105.625\n",
      "# Iteration  5300 -> Loss: 0.4069498370430702 \t| Accuracy: 105.625\n",
      "# Iteration  5301 -> Loss: 0.40692209048451056 \t| Accuracy: 105.625\n",
      "# Iteration  5302 -> Loss: 0.40689435530789053 \t| Accuracy: 105.625\n",
      "# Iteration  5303 -> Loss: 0.4068666315059222 \t| Accuracy: 105.625\n",
      "# Iteration  5304 -> Loss: 0.40683891907132314 \t| Accuracy: 105.625\n",
      "# Iteration  5305 -> Loss: 0.40681121799681724 \t| Accuracy: 105.625\n",
      "# Iteration  5306 -> Loss: 0.40678352827513414 \t| Accuracy: 105.625\n",
      "# Iteration  5307 -> Loss: 0.4067558498990093 \t| Accuracy: 105.625\n",
      "# Iteration  5308 -> Loss: 0.4067281828611844 \t| Accuracy: 105.625\n",
      "# Iteration  5309 -> Loss: 0.4067005271544065 \t| Accuracy: 105.625\n",
      "# Iteration  5310 -> Loss: 0.40667288277142927 \t| Accuracy: 105.625\n",
      "# Iteration  5311 -> Loss: 0.4066452497050116 \t| Accuracy: 105.625\n",
      "# Iteration  5312 -> Loss: 0.40661762794791856 \t| Accuracy: 105.625\n",
      "# Iteration  5313 -> Loss: 0.4065900174929212 \t| Accuracy: 105.625\n",
      "# Iteration  5314 -> Loss: 0.4065624183327962 \t| Accuracy: 105.625\n",
      "# Iteration  5315 -> Loss: 0.4065348304603263 \t| Accuracy: 105.625\n",
      "# Iteration  5316 -> Loss: 0.40650725386829994 \t| Accuracy: 105.625\n",
      "# Iteration  5317 -> Loss: 0.4064796885495116 \t| Accuracy: 105.625\n",
      "# Iteration  5318 -> Loss: 0.4064521344967615 \t| Accuracy: 105.625\n",
      "# Iteration  5319 -> Loss: 0.40642459170285555 \t| Accuracy: 105.625\n",
      "# Iteration  5320 -> Loss: 0.40639706016060584 \t| Accuracy: 105.625\n",
      "# Iteration  5321 -> Loss: 0.4063695398628301 \t| Accuracy: 105.625\n",
      "# Iteration  5322 -> Loss: 0.40634203080235165 \t| Accuracy: 105.625\n",
      "# Iteration  5323 -> Loss: 0.4063145329720001 \t| Accuracy: 105.625\n",
      "# Iteration  5324 -> Loss: 0.40628704636461066 \t| Accuracy: 105.625\n",
      "# Iteration  5325 -> Loss: 0.4062595709730241 \t| Accuracy: 105.625\n",
      "# Iteration  5326 -> Loss: 0.4062321067900875 \t| Accuracy: 105.625\n",
      "# Iteration  5327 -> Loss: 0.4062046538086533 \t| Accuracy: 105.625\n",
      "# Iteration  5328 -> Loss: 0.4061772120215798 \t| Accuracy: 105.625\n",
      "# Iteration  5329 -> Loss: 0.40614978142173125 \t| Accuracy: 105.625\n",
      "# Iteration  5330 -> Loss: 0.40612236200197777 \t| Accuracy: 105.625\n",
      "# Iteration  5331 -> Loss: 0.4060949537551947 \t| Accuracy: 105.625\n",
      "# Iteration  5332 -> Loss: 0.40606755667426403 \t| Accuracy: 105.625\n",
      "# Iteration  5333 -> Loss: 0.4060401707520725 \t| Accuracy: 105.625\n",
      "# Iteration  5334 -> Loss: 0.40601279598151335 \t| Accuracy: 105.625\n",
      "# Iteration  5335 -> Loss: 0.4059854323554853 \t| Accuracy: 105.625\n",
      "# Iteration  5336 -> Loss: 0.40595807986689286 \t| Accuracy: 105.625\n",
      "# Iteration  5337 -> Loss: 0.4059307385086463 \t| Accuracy: 105.625\n",
      "# Iteration  5338 -> Loss: 0.4059034082736615 \t| Accuracy: 105.625\n",
      "# Iteration  5339 -> Loss: 0.40587608915486006 \t| Accuracy: 105.625\n",
      "# Iteration  5340 -> Loss: 0.4058487811451697 \t| Accuracy: 105.625\n",
      "# Iteration  5341 -> Loss: 0.40582148423752323 \t| Accuracy: 105.625\n",
      "# Iteration  5342 -> Loss: 0.4057941984248596 \t| Accuracy: 105.625\n",
      "# Iteration  5343 -> Loss: 0.4057669237001233 \t| Accuracy: 105.625\n",
      "# Iteration  5344 -> Loss: 0.40573966005626455 \t| Accuracy: 105.625\n",
      "# Iteration  5345 -> Loss: 0.4057124074862393 \t| Accuracy: 105.625\n",
      "# Iteration  5346 -> Loss: 0.40568516598300935 \t| Accuracy: 105.625\n",
      "# Iteration  5347 -> Loss: 0.4056579355395417 \t| Accuracy: 105.625\n",
      "# Iteration  5348 -> Loss: 0.40563071614880947 \t| Accuracy: 105.625\n",
      "# Iteration  5349 -> Loss: 0.4056035078037912 \t| Accuracy: 105.625\n",
      "# Iteration  5350 -> Loss: 0.4055763104974712 \t| Accuracy: 105.625\n",
      "# Iteration  5351 -> Loss: 0.4055491242228397 \t| Accuracy: 105.625\n",
      "# Iteration  5352 -> Loss: 0.4055219489728919 \t| Accuracy: 105.625\n",
      "# Iteration  5353 -> Loss: 0.4054947847406292 \t| Accuracy: 105.625\n",
      "# Iteration  5354 -> Loss: 0.4054676315190586 \t| Accuracy: 105.625\n",
      "# Iteration  5355 -> Loss: 0.4054404893011925 \t| Accuracy: 105.625\n",
      "# Iteration  5356 -> Loss: 0.40541335808004914 \t| Accuracy: 105.625\n",
      "# Iteration  5357 -> Loss: 0.4053862378486523 \t| Accuracy: 105.625\n",
      "# Iteration  5358 -> Loss: 0.4053591286000313 \t| Accuracy: 105.625\n",
      "# Iteration  5359 -> Loss: 0.4053320303272212 \t| Accuracy: 105.625\n",
      "# Iteration  5360 -> Loss: 0.4053049430232627 \t| Accuracy: 105.625\n",
      "# Iteration  5361 -> Loss: 0.40527786668120197 \t| Accuracy: 105.625\n",
      "# Iteration  5362 -> Loss: 0.4052508012940907 \t| Accuracy: 105.625\n",
      "# Iteration  5363 -> Loss: 0.40522374685498647 \t| Accuracy: 105.625\n",
      "# Iteration  5364 -> Loss: 0.4051967033569523 \t| Accuracy: 105.625\n",
      "# Iteration  5365 -> Loss: 0.40516967079305655 \t| Accuracy: 105.625\n",
      "# Iteration  5366 -> Loss: 0.4051426491563736 \t| Accuracy: 105.625\n",
      "# Iteration  5367 -> Loss: 0.40511563843998305 \t| Accuracy: 105.625\n",
      "# Iteration  5368 -> Loss: 0.4050886386369701 \t| Accuracy: 105.625\n",
      "# Iteration  5369 -> Loss: 0.40506164974042574 \t| Accuracy: 105.625\n",
      "# Iteration  5370 -> Loss: 0.4050346717434464 \t| Accuracy: 105.625\n",
      "# Iteration  5371 -> Loss: 0.4050077046391338 \t| Accuracy: 105.000\n",
      "# Iteration  5372 -> Loss: 0.40498074842059556 \t| Accuracy: 105.000\n",
      "# Iteration  5373 -> Loss: 0.40495380308094464 \t| Accuracy: 105.000\n",
      "# Iteration  5374 -> Loss: 0.4049268686132996 \t| Accuracy: 105.000\n",
      "# Iteration  5375 -> Loss: 0.4048999450107845 \t| Accuracy: 105.000\n",
      "# Iteration  5376 -> Loss: 0.4048730322665289 \t| Accuracy: 105.000\n",
      "# Iteration  5377 -> Loss: 0.40484613037366796 \t| Accuracy: 105.000\n",
      "# Iteration  5378 -> Loss: 0.4048192393253423 \t| Accuracy: 105.000\n",
      "# Iteration  5379 -> Loss: 0.4047923591146979 \t| Accuracy: 105.000\n",
      "# Iteration  5380 -> Loss: 0.4047654897348864 \t| Accuracy: 105.000\n",
      "# Iteration  5381 -> Loss: 0.40473863117906494 \t| Accuracy: 105.000\n",
      "# Iteration  5382 -> Loss: 0.40471178344039616 \t| Accuracy: 105.000\n",
      "# Iteration  5383 -> Loss: 0.40468494651204795 \t| Accuracy: 105.000\n",
      "# Iteration  5384 -> Loss: 0.404658120387194 \t| Accuracy: 105.000\n",
      "# Iteration  5385 -> Loss: 0.40463130505901324 \t| Accuracy: 105.000\n",
      "# Iteration  5386 -> Loss: 0.40460450052069014 \t| Accuracy: 105.000\n",
      "# Iteration  5387 -> Loss: 0.4045777067654147 \t| Accuracy: 105.000\n",
      "# Iteration  5388 -> Loss: 0.4045509237863822 \t| Accuracy: 105.000\n",
      "# Iteration  5389 -> Loss: 0.4045241515767935 \t| Accuracy: 105.000\n",
      "# Iteration  5390 -> Loss: 0.40449739012985486 \t| Accuracy: 105.000\n",
      "# Iteration  5391 -> Loss: 0.4044706394387781 \t| Accuracy: 105.000\n",
      "# Iteration  5392 -> Loss: 0.40444389949678017 \t| Accuracy: 105.000\n",
      "# Iteration  5393 -> Loss: 0.4044171702970839 \t| Accuracy: 105.000\n",
      "# Iteration  5394 -> Loss: 0.4043904518329171 \t| Accuracy: 105.000\n",
      "# Iteration  5395 -> Loss: 0.404363744097513 \t| Accuracy: 105.000\n",
      "# Iteration  5396 -> Loss: 0.4043370470841108 \t| Accuracy: 105.000\n",
      "# Iteration  5397 -> Loss: 0.40431036078595456 \t| Accuracy: 105.000\n",
      "# Iteration  5398 -> Loss: 0.40428368519629376 \t| Accuracy: 105.000\n",
      "# Iteration  5399 -> Loss: 0.4042570203083835 \t| Accuracy: 105.000\n",
      "# Iteration  5400 -> Loss: 0.4042303661154844 \t| Accuracy: 105.000\n",
      "# Iteration  5401 -> Loss: 0.40420372261086196 \t| Accuracy: 105.000\n",
      "# Iteration  5402 -> Loss: 0.40417708978778744 \t| Accuracy: 105.000\n",
      "# Iteration  5403 -> Loss: 0.40415046763953744 \t| Accuracy: 105.000\n",
      "# Iteration  5404 -> Loss: 0.40412385615939367 \t| Accuracy: 105.000\n",
      "# Iteration  5405 -> Loss: 0.4040972553406437 \t| Accuracy: 105.000\n",
      "# Iteration  5406 -> Loss: 0.4040706651765799 \t| Accuracy: 105.000\n",
      "# Iteration  5407 -> Loss: 0.4040440856605005 \t| Accuracy: 105.000\n",
      "# Iteration  5408 -> Loss: 0.4040175167857086 \t| Accuracy: 105.000\n",
      "# Iteration  5409 -> Loss: 0.40399095854551287 \t| Accuracy: 105.000\n",
      "# Iteration  5410 -> Loss: 0.4039644109332275 \t| Accuracy: 105.000\n",
      "# Iteration  5411 -> Loss: 0.40393787394217173 \t| Accuracy: 105.000\n",
      "# Iteration  5412 -> Loss: 0.4039113475656702 \t| Accuracy: 105.000\n",
      "# Iteration  5413 -> Loss: 0.40388483179705287 \t| Accuracy: 105.000\n",
      "# Iteration  5414 -> Loss: 0.403858326629655 \t| Accuracy: 105.000\n",
      "# Iteration  5415 -> Loss: 0.40383183205681733 \t| Accuracy: 105.000\n",
      "# Iteration  5416 -> Loss: 0.4038053480718856 \t| Accuracy: 105.000\n",
      "# Iteration  5417 -> Loss: 0.40377887466821133 \t| Accuracy: 105.000\n",
      "# Iteration  5418 -> Loss: 0.4037524118391506 \t| Accuracy: 105.000\n",
      "# Iteration  5419 -> Loss: 0.4037259595780656 \t| Accuracy: 105.000\n",
      "# Iteration  5420 -> Loss: 0.40369951787832314 \t| Accuracy: 105.000\n",
      "# Iteration  5421 -> Loss: 0.4036730867332957 \t| Accuracy: 105.000\n",
      "# Iteration  5422 -> Loss: 0.4036466661363609 \t| Accuracy: 105.000\n",
      "# Iteration  5423 -> Loss: 0.40362025608090163 \t| Accuracy: 105.000\n",
      "# Iteration  5424 -> Loss: 0.40359385656030605 \t| Accuracy: 105.000\n",
      "# Iteration  5425 -> Loss: 0.4035674675679675 \t| Accuracy: 105.000\n",
      "# Iteration  5426 -> Loss: 0.4035410890972848 \t| Accuracy: 105.000\n",
      "# Iteration  5427 -> Loss: 0.40351472114166187 \t| Accuracy: 105.000\n",
      "# Iteration  5428 -> Loss: 0.4034883636945077 \t| Accuracy: 105.000\n",
      "# Iteration  5429 -> Loss: 0.4034620167492368 \t| Accuracy: 105.000\n",
      "# Iteration  5430 -> Loss: 0.40343568029926885 \t| Accuracy: 105.000\n",
      "# Iteration  5431 -> Loss: 0.4034093543380286 \t| Accuracy: 105.000\n",
      "# Iteration  5432 -> Loss: 0.4033830388589463 \t| Accuracy: 105.000\n",
      "# Iteration  5433 -> Loss: 0.4033567338554571 \t| Accuracy: 105.000\n",
      "# Iteration  5434 -> Loss: 0.40333043932100154 \t| Accuracy: 105.000\n",
      "# Iteration  5435 -> Loss: 0.40330415524902546 \t| Accuracy: 105.000\n",
      "# Iteration  5436 -> Loss: 0.40327788163297956 \t| Accuracy: 105.000\n",
      "# Iteration  5437 -> Loss: 0.40325161846632024 \t| Accuracy: 105.000\n",
      "# Iteration  5438 -> Loss: 0.4032253657425085 \t| Accuracy: 105.000\n",
      "# Iteration  5439 -> Loss: 0.40319912345501113 \t| Accuracy: 105.000\n",
      "# Iteration  5440 -> Loss: 0.4031728915972996 \t| Accuracy: 105.000\n",
      "# Iteration  5441 -> Loss: 0.4031466701628508 \t| Accuracy: 105.000\n",
      "# Iteration  5442 -> Loss: 0.40312045914514677 \t| Accuracy: 105.000\n",
      "# Iteration  5443 -> Loss: 0.40309425853767483 \t| Accuracy: 105.000\n",
      "# Iteration  5444 -> Loss: 0.4030680683339271 \t| Accuracy: 105.000\n",
      "# Iteration  5445 -> Loss: 0.4030418885274013 \t| Accuracy: 105.000\n",
      "# Iteration  5446 -> Loss: 0.40301571911159995 \t| Accuracy: 105.000\n",
      "# Iteration  5447 -> Loss: 0.402989560080031 \t| Accuracy: 105.000\n",
      "# Iteration  5448 -> Loss: 0.4029634114262073 \t| Accuracy: 105.000\n",
      "# Iteration  5449 -> Loss: 0.40293727314364686 \t| Accuracy: 105.000\n",
      "# Iteration  5450 -> Loss: 0.40291114522587307 \t| Accuracy: 105.000\n",
      "# Iteration  5451 -> Loss: 0.402885027666414 \t| Accuracy: 105.000\n",
      "# Iteration  5452 -> Loss: 0.4028589204588035 \t| Accuracy: 105.000\n",
      "# Iteration  5453 -> Loss: 0.4028328235965798 \t| Accuracy: 105.000\n",
      "# Iteration  5454 -> Loss: 0.4028067370732868 \t| Accuracy: 105.000\n",
      "# Iteration  5455 -> Loss: 0.4027806608824733 \t| Accuracy: 105.000\n",
      "# Iteration  5456 -> Loss: 0.4027545950176929 \t| Accuracy: 105.000\n",
      "# Iteration  5457 -> Loss: 0.4027285394725049 \t| Accuracy: 105.000\n",
      "# Iteration  5458 -> Loss: 0.4027024942404734 \t| Accuracy: 105.000\n",
      "# Iteration  5459 -> Loss: 0.40267645931516743 \t| Accuracy: 105.000\n",
      "# Iteration  5460 -> Loss: 0.4026504346901612 \t| Accuracy: 105.000\n",
      "# Iteration  5461 -> Loss: 0.4026244203590341 \t| Accuracy: 105.000\n",
      "# Iteration  5462 -> Loss: 0.40259841631537063 \t| Accuracy: 105.000\n",
      "# Iteration  5463 -> Loss: 0.40257242255276016 \t| Accuracy: 105.000\n",
      "# Iteration  5464 -> Loss: 0.4025464390647971 \t| Accuracy: 105.000\n",
      "# Iteration  5465 -> Loss: 0.40252046584508133 \t| Accuracy: 105.000\n",
      "# Iteration  5466 -> Loss: 0.4024945028872172 \t| Accuracy: 105.000\n",
      "# Iteration  5467 -> Loss: 0.4024685501848145 \t| Accuracy: 105.000\n",
      "# Iteration  5468 -> Loss: 0.40244260773148804 \t| Accuracy: 105.000\n",
      "# Iteration  5469 -> Loss: 0.4024166755208574 \t| Accuracy: 105.000\n",
      "# Iteration  5470 -> Loss: 0.4023907535465475 \t| Accuracy: 105.000\n",
      "# Iteration  5471 -> Loss: 0.4023648418021882 \t| Accuracy: 105.000\n",
      "# Iteration  5472 -> Loss: 0.4023389402814142 \t| Accuracy: 105.000\n",
      "# Iteration  5473 -> Loss: 0.4023130489778655 \t| Accuracy: 105.000\n",
      "# Iteration  5474 -> Loss: 0.40228716788518687 \t| Accuracy: 105.000\n",
      "# Iteration  5475 -> Loss: 0.40226129699702823 \t| Accuracy: 105.000\n",
      "# Iteration  5476 -> Loss: 0.40223543630704456 \t| Accuracy: 105.000\n",
      "# Iteration  5477 -> Loss: 0.4022095858088955 \t| Accuracy: 105.000\n",
      "# Iteration  5478 -> Loss: 0.4021837454962461 \t| Accuracy: 105.000\n",
      "# Iteration  5479 -> Loss: 0.4021579153627662 \t| Accuracy: 105.000\n",
      "# Iteration  5480 -> Loss: 0.4021320954021307 \t| Accuracy: 105.000\n",
      "# Iteration  5481 -> Loss: 0.40210628560801925 \t| Accuracy: 105.000\n",
      "# Iteration  5482 -> Loss: 0.4020804859741169 \t| Accuracy: 105.000\n",
      "# Iteration  5483 -> Loss: 0.4020546964941132 \t| Accuracy: 105.000\n",
      "# Iteration  5484 -> Loss: 0.40202891716170297 \t| Accuracy: 105.000\n",
      "# Iteration  5485 -> Loss: 0.4020031479705859 \t| Accuracy: 105.000\n",
      "# Iteration  5486 -> Loss: 0.4019773889144666 \t| Accuracy: 105.000\n",
      "# Iteration  5487 -> Loss: 0.4019516399870548 \t| Accuracy: 105.000\n",
      "# Iteration  5488 -> Loss: 0.40192590118206484 \t| Accuracy: 105.000\n",
      "# Iteration  5489 -> Loss: 0.40190017249321636 \t| Accuracy: 105.000\n",
      "# Iteration  5490 -> Loss: 0.4018744539142337 \t| Accuracy: 105.000\n",
      "# Iteration  5491 -> Loss: 0.4018487454388462 \t| Accuracy: 105.000\n",
      "# Iteration  5492 -> Loss: 0.40182304706078825 \t| Accuracy: 105.000\n",
      "# Iteration  5493 -> Loss: 0.4017973587737989 \t| Accuracy: 105.000\n",
      "# Iteration  5494 -> Loss: 0.4017716805716223 \t| Accuracy: 105.000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Iteration  5495 -> Loss: 0.40174601244800756 \t| Accuracy: 105.000\n",
      "# Iteration  5496 -> Loss: 0.4017203543967085 \t| Accuracy: 105.000\n",
      "# Iteration  5497 -> Loss: 0.4016947064114839 \t| Accuracy: 105.000\n",
      "# Iteration  5498 -> Loss: 0.4016690684860978 \t| Accuracy: 105.000\n",
      "# Iteration  5499 -> Loss: 0.40164344061431856 \t| Accuracy: 105.000\n",
      "# Iteration  5500 -> Loss: 0.40161782278991964 \t| Accuracy: 105.000\n",
      "# Iteration  5501 -> Loss: 0.4015922150066796 \t| Accuracy: 105.000\n",
      "# Iteration  5502 -> Loss: 0.4015666172583816 \t| Accuracy: 105.000\n",
      "# Iteration  5503 -> Loss: 0.401541029538814 \t| Accuracy: 105.000\n",
      "# Iteration  5504 -> Loss: 0.4015154518417695 \t| Accuracy: 105.000\n",
      "# Iteration  5505 -> Loss: 0.40148988416104625 \t| Accuracy: 105.000\n",
      "# Iteration  5506 -> Loss: 0.4014643264904468 \t| Accuracy: 105.000\n",
      "# Iteration  5507 -> Loss: 0.40143877882377876 \t| Accuracy: 105.000\n",
      "# Iteration  5508 -> Loss: 0.4014132411548548 \t| Accuracy: 105.000\n",
      "# Iteration  5509 -> Loss: 0.4013877134774919 \t| Accuracy: 105.000\n",
      "# Iteration  5510 -> Loss: 0.4013621957855124 \t| Accuracy: 105.000\n",
      "# Iteration  5511 -> Loss: 0.4013366880727433 \t| Accuracy: 105.000\n",
      "# Iteration  5512 -> Loss: 0.40131119033301627 \t| Accuracy: 105.000\n",
      "# Iteration  5513 -> Loss: 0.40128570256016804 \t| Accuracy: 105.000\n",
      "# Iteration  5514 -> Loss: 0.4012602247480399 \t| Accuracy: 105.000\n",
      "# Iteration  5515 -> Loss: 0.4012347568904784 \t| Accuracy: 105.000\n",
      "# Iteration  5516 -> Loss: 0.40120929898133434 \t| Accuracy: 105.000\n",
      "# Iteration  5517 -> Loss: 0.40118385101446374 \t| Accuracy: 105.000\n",
      "# Iteration  5518 -> Loss: 0.4011584129837273 \t| Accuracy: 105.000\n",
      "# Iteration  5519 -> Loss: 0.40113298488299054 \t| Accuracy: 105.000\n",
      "# Iteration  5520 -> Loss: 0.40110756670612363 \t| Accuracy: 105.000\n",
      "# Iteration  5521 -> Loss: 0.4010821584470018 \t| Accuracy: 105.000\n",
      "# Iteration  5522 -> Loss: 0.4010567600995049 \t| Accuracy: 105.000\n",
      "# Iteration  5523 -> Loss: 0.4010313716575175 \t| Accuracy: 105.000\n",
      "# Iteration  5524 -> Loss: 0.4010059931149289 \t| Accuracy: 105.000\n",
      "# Iteration  5525 -> Loss: 0.40098062446563365 \t| Accuracy: 105.000\n",
      "# Iteration  5526 -> Loss: 0.4009552657035304 \t| Accuracy: 105.000\n",
      "# Iteration  5527 -> Loss: 0.4009299168225231 \t| Accuracy: 105.000\n",
      "# Iteration  5528 -> Loss: 0.40090457781651995 \t| Accuracy: 105.000\n",
      "# Iteration  5529 -> Loss: 0.4008792486794346 \t| Accuracy: 105.000\n",
      "# Iteration  5530 -> Loss: 0.40085392940518466 \t| Accuracy: 105.000\n",
      "# Iteration  5531 -> Loss: 0.40082861998769304 \t| Accuracy: 105.000\n",
      "# Iteration  5532 -> Loss: 0.4008033204208872 \t| Accuracy: 105.000\n",
      "# Iteration  5533 -> Loss: 0.40077803069869933 \t| Accuracy: 105.000\n",
      "# Iteration  5534 -> Loss: 0.4007527508150663 \t| Accuracy: 105.000\n",
      "# Iteration  5535 -> Loss: 0.40072748076392994 \t| Accuracy: 105.000\n",
      "# Iteration  5536 -> Loss: 0.4007022205392365 \t| Accuracy: 105.000\n",
      "# Iteration  5537 -> Loss: 0.40067697013493697 \t| Accuracy: 105.000\n",
      "# Iteration  5538 -> Loss: 0.40065172954498746 \t| Accuracy: 105.000\n",
      "# Iteration  5539 -> Loss: 0.4006264987633484 \t| Accuracy: 105.000\n",
      "# Iteration  5540 -> Loss: 0.4006012777839849 \t| Accuracy: 105.000\n",
      "# Iteration  5541 -> Loss: 0.40057606660086703 \t| Accuracy: 105.000\n",
      "# Iteration  5542 -> Loss: 0.4005508652079693 \t| Accuracy: 105.000\n",
      "# Iteration  5543 -> Loss: 0.40052567359927105 \t| Accuracy: 105.000\n",
      "# Iteration  5544 -> Loss: 0.40050049176875635 \t| Accuracy: 105.000\n",
      "# Iteration  5545 -> Loss: 0.4004753197104138 \t| Accuracy: 105.000\n",
      "# Iteration  5546 -> Loss: 0.40045015741823675 \t| Accuracy: 105.000\n",
      "# Iteration  5547 -> Loss: 0.40042500488622335 \t| Accuracy: 105.000\n",
      "# Iteration  5548 -> Loss: 0.4003998621083762 \t| Accuracy: 105.000\n",
      "# Iteration  5549 -> Loss: 0.40037472907870264 \t| Accuracy: 105.000\n",
      "# Iteration  5550 -> Loss: 0.4003496057912148 \t| Accuracy: 105.000\n",
      "# Iteration  5551 -> Loss: 0.4003244922399293 \t| Accuracy: 105.000\n",
      "# Iteration  5552 -> Loss: 0.4002993884188674 \t| Accuracy: 105.000\n",
      "# Iteration  5553 -> Loss: 0.40027429432205514 \t| Accuracy: 105.000\n",
      "# Iteration  5554 -> Loss: 0.40024920994352325 \t| Accuracy: 105.000\n",
      "# Iteration  5555 -> Loss: 0.4002241352773067 \t| Accuracy: 105.000\n",
      "# Iteration  5556 -> Loss: 0.4001990703174456 \t| Accuracy: 105.000\n",
      "# Iteration  5557 -> Loss: 0.4001740150579844 \t| Accuracy: 105.000\n",
      "# Iteration  5558 -> Loss: 0.40014896949297213 \t| Accuracy: 105.000\n",
      "# Iteration  5559 -> Loss: 0.4001239336164628 \t| Accuracy: 105.000\n",
      "# Iteration  5560 -> Loss: 0.4000989074225146 \t| Accuracy: 105.000\n",
      "# Iteration  5561 -> Loss: 0.40007389090519047 \t| Accuracy: 105.000\n",
      "# Iteration  5562 -> Loss: 0.4000488840585581 \t| Accuracy: 105.000\n",
      "# Iteration  5563 -> Loss: 0.4000238868766896 \t| Accuracy: 105.000\n",
      "# Iteration  5564 -> Loss: 0.39999889935366184 \t| Accuracy: 105.000\n",
      "# Iteration  5565 -> Loss: 0.3999739214835561 \t| Accuracy: 105.000\n",
      "# Iteration  5566 -> Loss: 0.3999489532604585 \t| Accuracy: 105.000\n",
      "# Iteration  5567 -> Loss: 0.3999239946784594 \t| Accuracy: 105.000\n",
      "# Iteration  5568 -> Loss: 0.39989904573165413 \t| Accuracy: 105.000\n",
      "# Iteration  5569 -> Loss: 0.3998741064141422 \t| Accuracy: 105.000\n",
      "# Iteration  5570 -> Loss: 0.399849176720028 \t| Accuracy: 105.000\n",
      "# Iteration  5571 -> Loss: 0.3998242566434205 \t| Accuracy: 105.000\n",
      "# Iteration  5572 -> Loss: 0.3997993461784328 \t| Accuracy: 105.000\n",
      "# Iteration  5573 -> Loss: 0.39977444531918316 \t| Accuracy: 105.000\n",
      "# Iteration  5574 -> Loss: 0.39974955405979384 \t| Accuracy: 105.000\n",
      "# Iteration  5575 -> Loss: 0.3997246723943923 \t| Accuracy: 105.000\n",
      "# Iteration  5576 -> Loss: 0.3996998003171096 \t| Accuracy: 105.000\n",
      "# Iteration  5577 -> Loss: 0.39967493782208235 \t| Accuracy: 105.000\n",
      "# Iteration  5578 -> Loss: 0.39965008490345116 \t| Accuracy: 105.000\n",
      "# Iteration  5579 -> Loss: 0.39962524155536117 \t| Accuracy: 105.000\n",
      "# Iteration  5580 -> Loss: 0.3996004077719621 \t| Accuracy: 105.000\n",
      "# Iteration  5581 -> Loss: 0.3995755835474083 \t| Accuracy: 105.000\n",
      "# Iteration  5582 -> Loss: 0.3995507688758585 \t| Accuracy: 105.000\n",
      "# Iteration  5583 -> Loss: 0.3995259637514761 \t| Accuracy: 105.000\n",
      "# Iteration  5584 -> Loss: 0.39950116816842907 \t| Accuracy: 105.000\n",
      "# Iteration  5585 -> Loss: 0.39947638212088943 \t| Accuracy: 105.000\n",
      "# Iteration  5586 -> Loss: 0.39945160560303394 \t| Accuracy: 105.000\n",
      "# Iteration  5587 -> Loss: 0.39942683860904443 \t| Accuracy: 105.000\n",
      "# Iteration  5588 -> Loss: 0.39940208113310643 \t| Accuracy: 105.000\n",
      "# Iteration  5589 -> Loss: 0.3993773331694101 \t| Accuracy: 105.000\n",
      "# Iteration  5590 -> Loss: 0.39935259471215045 \t| Accuracy: 105.000\n",
      "# Iteration  5591 -> Loss: 0.39932786575552665 \t| Accuracy: 105.000\n",
      "# Iteration  5592 -> Loss: 0.39930314629374275 \t| Accuracy: 105.000\n",
      "# Iteration  5593 -> Loss: 0.39927843632100657 \t| Accuracy: 105.000\n",
      "# Iteration  5594 -> Loss: 0.399253735831531 \t| Accuracy: 105.000\n",
      "# Iteration  5595 -> Loss: 0.39922904481953325 \t| Accuracy: 105.000\n",
      "# Iteration  5596 -> Loss: 0.3992043632792349 \t| Accuracy: 105.000\n",
      "# Iteration  5597 -> Loss: 0.3991796912048619 \t| Accuracy: 105.000\n",
      "# Iteration  5598 -> Loss: 0.399155028590645 \t| Accuracy: 105.000\n",
      "# Iteration  5599 -> Loss: 0.39913037543081914 \t| Accuracy: 105.000\n",
      "# Iteration  5600 -> Loss: 0.39910573171962355 \t| Accuracy: 105.000\n",
      "# Iteration  5601 -> Loss: 0.3990810974513023 \t| Accuracy: 105.000\n",
      "# Iteration  5602 -> Loss: 0.3990564726201037 \t| Accuracy: 105.000\n",
      "# Iteration  5603 -> Loss: 0.3990318572202804 \t| Accuracy: 105.000\n",
      "# Iteration  5604 -> Loss: 0.3990072512460893 \t| Accuracy: 105.000\n",
      "# Iteration  5605 -> Loss: 0.3989826546917925 \t| Accuracy: 105.000\n",
      "# Iteration  5606 -> Loss: 0.39895806755165564 \t| Accuracy: 105.000\n",
      "# Iteration  5607 -> Loss: 0.39893348981994925 \t| Accuracy: 105.000\n",
      "# Iteration  5608 -> Loss: 0.39890892149094803 \t| Accuracy: 105.000\n",
      "# Iteration  5609 -> Loss: 0.39888436255893134 \t| Accuracy: 105.000\n",
      "# Iteration  5610 -> Loss: 0.3988598130181827 \t| Accuracy: 105.000\n",
      "# Iteration  5611 -> Loss: 0.3988352728629902 \t| Accuracy: 105.000\n",
      "# Iteration  5612 -> Loss: 0.3988107420876463 \t| Accuracy: 105.000\n",
      "# Iteration  5613 -> Loss: 0.3987862206864476 \t| Accuracy: 105.000\n",
      "# Iteration  5614 -> Loss: 0.3987617086536956 \t| Accuracy: 105.000\n",
      "# Iteration  5615 -> Loss: 0.3987372059836956 \t| Accuracy: 105.000\n",
      "# Iteration  5616 -> Loss: 0.3987127126707578 \t| Accuracy: 105.000\n",
      "# Iteration  5617 -> Loss: 0.39868822870919635 \t| Accuracy: 105.000\n",
      "# Iteration  5618 -> Loss: 0.3986637540933298 \t| Accuracy: 105.000\n",
      "# Iteration  5619 -> Loss: 0.39863928881748156 \t| Accuracy: 105.000\n",
      "# Iteration  5620 -> Loss: 0.3986148328759789 \t| Accuracy: 105.000\n",
      "# Iteration  5621 -> Loss: 0.3985903862631534 \t| Accuracy: 105.000\n",
      "# Iteration  5622 -> Loss: 0.39856594897334136 \t| Accuracy: 105.000\n",
      "# Iteration  5623 -> Loss: 0.3985415210008833 \t| Accuracy: 105.000\n",
      "# Iteration  5624 -> Loss: 0.39851710234012405 \t| Accuracy: 105.000\n",
      "# Iteration  5625 -> Loss: 0.3984926929854125 \t| Accuracy: 105.000\n",
      "# Iteration  5626 -> Loss: 0.3984682929311025 \t| Accuracy: 105.000\n",
      "# Iteration  5627 -> Loss: 0.3984439021715515 \t| Accuracy: 105.000\n",
      "# Iteration  5628 -> Loss: 0.3984195207011221 \t| Accuracy: 105.000\n",
      "# Iteration  5629 -> Loss: 0.3983951485141804 \t| Accuracy: 105.000\n",
      "# Iteration  5630 -> Loss: 0.3983707856050973 \t| Accuracy: 105.000\n",
      "# Iteration  5631 -> Loss: 0.39834643196824804 \t| Accuracy: 105.000\n",
      "# Iteration  5632 -> Loss: 0.39832208759801185 \t| Accuracy: 105.000\n",
      "# Iteration  5633 -> Loss: 0.39829775248877247 \t| Accuracy: 105.000\n",
      "# Iteration  5634 -> Loss: 0.3982734266349182 \t| Accuracy: 105.000\n",
      "# Iteration  5635 -> Loss: 0.3982491100308411 \t| Accuracy: 105.000\n",
      "# Iteration  5636 -> Loss: 0.3982248026709379 \t| Accuracy: 105.000\n",
      "# Iteration  5637 -> Loss: 0.3982005045496095 \t| Accuracy: 105.000\n",
      "# Iteration  5638 -> Loss: 0.398176215661261 \t| Accuracy: 105.000\n",
      "# Iteration  5639 -> Loss: 0.3981519360003021 \t| Accuracy: 105.000\n",
      "# Iteration  5640 -> Loss: 0.3981276655611464 \t| Accuracy: 105.000\n",
      "# Iteration  5641 -> Loss: 0.3981034043382121 \t| Accuracy: 105.000\n",
      "# Iteration  5642 -> Loss: 0.3980791523259215 \t| Accuracy: 105.000\n",
      "# Iteration  5643 -> Loss: 0.3980549095187011 \t| Accuracy: 105.000\n",
      "# Iteration  5644 -> Loss: 0.39803067591098157 \t| Accuracy: 105.000\n",
      "# Iteration  5645 -> Loss: 0.39800645149719843 \t| Accuracy: 105.000\n",
      "# Iteration  5646 -> Loss: 0.39798223627179063 \t| Accuracy: 105.000\n",
      "# Iteration  5647 -> Loss: 0.3979580302292022 \t| Accuracy: 105.000\n",
      "# Iteration  5648 -> Loss: 0.3979338333638805 \t| Accuracy: 105.000\n",
      "# Iteration  5649 -> Loss: 0.3979096456702782 \t| Accuracy: 105.000\n",
      "# Iteration  5650 -> Loss: 0.3978854671428511 \t| Accuracy: 105.000\n",
      "# Iteration  5651 -> Loss: 0.3978612977760603 \t| Accuracy: 105.000\n",
      "# Iteration  5652 -> Loss: 0.3978371375643703 \t| Accuracy: 105.000\n",
      "# Iteration  5653 -> Loss: 0.39781298650225 \t| Accuracy: 105.000\n",
      "# Iteration  5654 -> Loss: 0.3977888445841729 \t| Accuracy: 105.000\n",
      "# Iteration  5655 -> Loss: 0.39776471180461664 \t| Accuracy: 105.000\n",
      "# Iteration  5656 -> Loss: 0.3977405881580625 \t| Accuracy: 105.000\n",
      "# Iteration  5657 -> Loss: 0.3977164736389967 \t| Accuracy: 105.000\n",
      "# Iteration  5658 -> Loss: 0.39769236824190934 \t| Accuracy: 105.000\n",
      "# Iteration  5659 -> Loss: 0.39766827196129456 \t| Accuracy: 105.000\n",
      "# Iteration  5660 -> Loss: 0.39764418479165103 \t| Accuracy: 105.000\n",
      "# Iteration  5661 -> Loss: 0.3976201067274815 \t| Accuracy: 105.000\n",
      "# Iteration  5662 -> Loss: 0.3975960377632927 \t| Accuracy: 105.000\n",
      "# Iteration  5663 -> Loss: 0.39757197789359594 \t| Accuracy: 105.000\n",
      "# Iteration  5664 -> Loss: 0.39754792711290643 \t| Accuracy: 105.000\n",
      "# Iteration  5665 -> Loss: 0.39752388541574346 \t| Accuracy: 105.000\n",
      "# Iteration  5666 -> Loss: 0.397499852796631 \t| Accuracy: 105.000\n",
      "# Iteration  5667 -> Loss: 0.3974758292500966 \t| Accuracy: 105.000\n",
      "# Iteration  5668 -> Loss: 0.3974518147706725 \t| Accuracy: 105.000\n",
      "# Iteration  5669 -> Loss: 0.3974278093528945 \t| Accuracy: 105.000\n",
      "# Iteration  5670 -> Loss: 0.39740381299130323 \t| Accuracy: 105.000\n",
      "# Iteration  5671 -> Loss: 0.39737982568044306 \t| Accuracy: 105.000\n",
      "# Iteration  5672 -> Loss: 0.3973558474148626 \t| Accuracy: 105.000\n",
      "# Iteration  5673 -> Loss: 0.3973318781891145 \t| Accuracy: 105.000\n",
      "# Iteration  5674 -> Loss: 0.39730791799775583 \t| Accuracy: 105.000\n",
      "# Iteration  5675 -> Loss: 0.3972839668353476 \t| Accuracy: 105.000\n",
      "# Iteration  5676 -> Loss: 0.39726002469645505 \t| Accuracy: 105.000\n",
      "# Iteration  5677 -> Loss: 0.39723609157564754 \t| Accuracy: 105.000\n",
      "# Iteration  5678 -> Loss: 0.3972121674674984 \t| Accuracy: 105.000\n",
      "# Iteration  5679 -> Loss: 0.39718825236658534 \t| Accuracy: 105.000\n",
      "# Iteration  5680 -> Loss: 0.39716434626749 \t| Accuracy: 105.000\n",
      "# Iteration  5681 -> Loss: 0.3971404491647983 \t| Accuracy: 105.000\n",
      "# Iteration  5682 -> Loss: 0.3971165610531 \t| Accuracy: 105.000\n",
      "# Iteration  5683 -> Loss: 0.3970926819269894 \t| Accuracy: 105.000\n",
      "# Iteration  5684 -> Loss: 0.3970688117810645 \t| Accuracy: 105.000\n",
      "# Iteration  5685 -> Loss: 0.3970449506099276 \t| Accuracy: 105.000\n",
      "# Iteration  5686 -> Loss: 0.3970210984081852 \t| Accuracy: 105.000\n",
      "# Iteration  5687 -> Loss: 0.3969972551704476 \t| Accuracy: 105.000\n",
      "# Iteration  5688 -> Loss: 0.3969734208913295 \t| Accuracy: 105.000\n",
      "# Iteration  5689 -> Loss: 0.39694959556544934 \t| Accuracy: 105.000\n",
      "# Iteration  5690 -> Loss: 0.39692577918743005 \t| Accuracy: 105.000\n",
      "# Iteration  5691 -> Loss: 0.39690197175189834 \t| Accuracy: 105.000\n",
      "# Iteration  5692 -> Loss: 0.3968781732534852 \t| Accuracy: 105.000\n",
      "# Iteration  5693 -> Loss: 0.3968543836868254 \t| Accuracy: 105.000\n",
      "# Iteration  5694 -> Loss: 0.39683060304655815 \t| Accuracy: 105.000\n",
      "# Iteration  5695 -> Loss: 0.3968068313273265 \t| Accuracy: 105.000\n",
      "# Iteration  5696 -> Loss: 0.39678306852377765 \t| Accuracy: 105.000\n",
      "# Iteration  5697 -> Loss: 0.39675931463056274 \t| Accuracy: 105.000\n",
      "# Iteration  5698 -> Loss: 0.39673556964233714 \t| Accuracy: 105.000\n",
      "# Iteration  5699 -> Loss: 0.3967118335537601 \t| Accuracy: 105.000\n",
      "# Iteration  5700 -> Loss: 0.3966881063594949 \t| Accuracy: 105.000\n",
      "# Iteration  5701 -> Loss: 0.3966643880542092 \t| Accuracy: 105.000\n",
      "# Iteration  5702 -> Loss: 0.39664067863257424 \t| Accuracy: 105.000\n",
      "# Iteration  5703 -> Loss: 0.3966169780892656 \t| Accuracy: 105.000\n",
      "# Iteration  5704 -> Loss: 0.39659328641896285 \t| Accuracy: 105.000\n",
      "# Iteration  5705 -> Loss: 0.3965696036163496 \t| Accuracy: 105.000\n",
      "# Iteration  5706 -> Loss: 0.39654592967611324 \t| Accuracy: 105.000\n",
      "# Iteration  5707 -> Loss: 0.3965222645929454 \t| Accuracy: 105.000\n",
      "# Iteration  5708 -> Loss: 0.3964986083615417 \t| Accuracy: 105.000\n",
      "# Iteration  5709 -> Loss: 0.39647496097660195 \t| Accuracy: 105.000\n",
      "# Iteration  5710 -> Loss: 0.3964513224328296 \t| Accuracy: 105.000\n",
      "# Iteration  5711 -> Loss: 0.39642769272493233 \t| Accuracy: 105.000\n",
      "# Iteration  5712 -> Loss: 0.39640407184762183 \t| Accuracy: 105.000\n",
      "# Iteration  5713 -> Loss: 0.39638045979561376 \t| Accuracy: 105.000\n",
      "# Iteration  5714 -> Loss: 0.39635685656362774 \t| Accuracy: 105.000\n",
      "# Iteration  5715 -> Loss: 0.39633326214638737 \t| Accuracy: 105.000\n",
      "# Iteration  5716 -> Loss: 0.39630967653862026 \t| Accuracy: 105.000\n",
      "# Iteration  5717 -> Loss: 0.3962860997350582 \t| Accuracy: 105.000\n",
      "# Iteration  5718 -> Loss: 0.39626253173043646 \t| Accuracy: 105.000\n",
      "# Iteration  5719 -> Loss: 0.3962389725194949 \t| Accuracy: 105.000\n",
      "# Iteration  5720 -> Loss: 0.39621542209697685 \t| Accuracy: 105.000\n",
      "# Iteration  5721 -> Loss: 0.39619188045763 \t| Accuracy: 105.000\n",
      "# Iteration  5722 -> Loss: 0.39616834759620567 \t| Accuracy: 105.000\n",
      "# Iteration  5723 -> Loss: 0.3961448235074594 \t| Accuracy: 105.000\n",
      "# Iteration  5724 -> Loss: 0.39612130818615043 \t| Accuracy: 105.000\n",
      "# Iteration  5725 -> Loss: 0.3960978016270424 \t| Accuracy: 105.000\n",
      "# Iteration  5726 -> Loss: 0.39607430382490233 \t| Accuracy: 105.000\n",
      "# Iteration  5727 -> Loss: 0.39605081477450144 \t| Accuracy: 105.000\n",
      "# Iteration  5728 -> Loss: 0.3960273344706152 \t| Accuracy: 105.000\n",
      "# Iteration  5729 -> Loss: 0.3960038629080226 \t| Accuracy: 105.000\n",
      "# Iteration  5730 -> Loss: 0.3959804000815066 \t| Accuracy: 105.000\n",
      "# Iteration  5731 -> Loss: 0.3959569459858544 \t| Accuracy: 105.000\n",
      "# Iteration  5732 -> Loss: 0.3959335006158568 \t| Accuracy: 105.000\n",
      "# Iteration  5733 -> Loss: 0.3959100639663087 \t| Accuracy: 105.000\n",
      "# Iteration  5734 -> Loss: 0.3958866360320088 \t| Accuracy: 105.000\n",
      "# Iteration  5735 -> Loss: 0.39586321680776004 \t| Accuracy: 105.000\n",
      "# Iteration  5736 -> Loss: 0.3958398062883688 \t| Accuracy: 105.000\n",
      "# Iteration  5737 -> Loss: 0.3958164044686456 \t| Accuracy: 105.000\n",
      "# Iteration  5738 -> Loss: 0.395793011343405 \t| Accuracy: 105.000\n",
      "# Iteration  5739 -> Loss: 0.39576962690746537 \t| Accuracy: 105.000\n",
      "# Iteration  5740 -> Loss: 0.3957462511556488 \t| Accuracy: 105.000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Iteration  5741 -> Loss: 0.3957228840827815 \t| Accuracy: 105.000\n",
      "# Iteration  5742 -> Loss: 0.3956995256836935 \t| Accuracy: 105.000\n",
      "# Iteration  5743 -> Loss: 0.39567617595321875 \t| Accuracy: 105.000\n",
      "# Iteration  5744 -> Loss: 0.395652834886195 \t| Accuracy: 105.000\n",
      "# Iteration  5745 -> Loss: 0.395629502477464 \t| Accuracy: 105.000\n",
      "# Iteration  5746 -> Loss: 0.39560617872187137 \t| Accuracy: 105.000\n",
      "# Iteration  5747 -> Loss: 0.39558286361426637 \t| Accuracy: 105.000\n",
      "# Iteration  5748 -> Loss: 0.3955595571495025 \t| Accuracy: 105.000\n",
      "# Iteration  5749 -> Loss: 0.3955362593224369 \t| Accuracy: 105.000\n",
      "# Iteration  5750 -> Loss: 0.3955129701279305 \t| Accuracy: 105.000\n",
      "# Iteration  5751 -> Loss: 0.3954896895608484 \t| Accuracy: 105.000\n",
      "# Iteration  5752 -> Loss: 0.39546641761605933 \t| Accuracy: 105.000\n",
      "# Iteration  5753 -> Loss: 0.39544315428843585 \t| Accuracy: 105.000\n",
      "# Iteration  5754 -> Loss: 0.3954198995728545 \t| Accuracy: 105.000\n",
      "# Iteration  5755 -> Loss: 0.3953966534641955 \t| Accuracy: 105.000\n",
      "# Iteration  5756 -> Loss: 0.39537341595734316 \t| Accuracy: 105.000\n",
      "# Iteration  5757 -> Loss: 0.3953501870471854 \t| Accuracy: 105.000\n",
      "# Iteration  5758 -> Loss: 0.3953269667286141 \t| Accuracy: 105.000\n",
      "# Iteration  5759 -> Loss: 0.395303754996525 \t| Accuracy: 105.000\n",
      "# Iteration  5760 -> Loss: 0.39528055184581756 \t| Accuracy: 105.000\n",
      "# Iteration  5761 -> Loss: 0.39525735727139505 \t| Accuracy: 105.000\n",
      "# Iteration  5762 -> Loss: 0.39523417126816485 \t| Accuracy: 105.000\n",
      "# Iteration  5763 -> Loss: 0.3952109938310377 \t| Accuracy: 105.000\n",
      "# Iteration  5764 -> Loss: 0.3951878249549285 \t| Accuracy: 105.000\n",
      "# Iteration  5765 -> Loss: 0.39516466463475586 \t| Accuracy: 105.000\n",
      "# Iteration  5766 -> Loss: 0.39514151286544225 \t| Accuracy: 105.000\n",
      "# Iteration  5767 -> Loss: 0.3951183696419138 \t| Accuracy: 105.000\n",
      "# Iteration  5768 -> Loss: 0.3950952349591006 \t| Accuracy: 105.000\n",
      "# Iteration  5769 -> Loss: 0.3950721088119365 \t| Accuracy: 105.000\n",
      "# Iteration  5770 -> Loss: 0.395048991195359 \t| Accuracy: 105.000\n",
      "# Iteration  5771 -> Loss: 0.3950258821043098 \t| Accuracy: 105.000\n",
      "# Iteration  5772 -> Loss: 0.3950027815337338 \t| Accuracy: 105.000\n",
      "# Iteration  5773 -> Loss: 0.39497968947858014 \t| Accuracy: 105.000\n",
      "# Iteration  5774 -> Loss: 0.39495660593380155 \t| Accuracy: 105.000\n",
      "# Iteration  5775 -> Loss: 0.39493353089435457 \t| Accuracy: 105.000\n",
      "# Iteration  5776 -> Loss: 0.3949104643551995 \t| Accuracy: 105.000\n",
      "# Iteration  5777 -> Loss: 0.39488740631130054 \t| Accuracy: 105.000\n",
      "# Iteration  5778 -> Loss: 0.39486435675762543 \t| Accuracy: 105.000\n",
      "# Iteration  5779 -> Loss: 0.39484131568914593 \t| Accuracy: 105.000\n",
      "# Iteration  5780 -> Loss: 0.39481828310083733 \t| Accuracy: 105.000\n",
      "# Iteration  5781 -> Loss: 0.3947952589876788 \t| Accuracy: 105.000\n",
      "# Iteration  5782 -> Loss: 0.3947722433446533 \t| Accuracy: 105.000\n",
      "# Iteration  5783 -> Loss: 0.3947492361667474 \t| Accuracy: 105.000\n",
      "# Iteration  5784 -> Loss: 0.3947262374489514 \t| Accuracy: 105.000\n",
      "# Iteration  5785 -> Loss: 0.39470324718625976 \t| Accuracy: 105.000\n",
      "# Iteration  5786 -> Loss: 0.39468026537367 \t| Accuracy: 105.000\n",
      "# Iteration  5787 -> Loss: 0.3946572920061841 \t| Accuracy: 105.000\n",
      "# Iteration  5788 -> Loss: 0.39463432707880697 \t| Accuracy: 105.000\n",
      "# Iteration  5789 -> Loss: 0.3946113705865482 \t| Accuracy: 105.000\n",
      "# Iteration  5790 -> Loss: 0.39458842252442033 \t| Accuracy: 105.000\n",
      "# Iteration  5791 -> Loss: 0.3945654828874398 \t| Accuracy: 105.000\n",
      "# Iteration  5792 -> Loss: 0.394542551670627 \t| Accuracy: 105.000\n",
      "# Iteration  5793 -> Loss: 0.39451962886900593 \t| Accuracy: 105.000\n",
      "# Iteration  5794 -> Loss: 0.39449671447760426 \t| Accuracy: 105.000\n",
      "# Iteration  5795 -> Loss: 0.3944738084914533 \t| Accuracy: 105.000\n",
      "# Iteration  5796 -> Loss: 0.39445091090558826 \t| Accuracy: 105.000\n",
      "# Iteration  5797 -> Loss: 0.39442802171504787 \t| Accuracy: 105.000\n",
      "# Iteration  5798 -> Loss: 0.3944051409148747 \t| Accuracy: 105.000\n",
      "# Iteration  5799 -> Loss: 0.3943822685001149 \t| Accuracy: 105.000\n",
      "# Iteration  5800 -> Loss: 0.3943594044658184 \t| Accuracy: 105.000\n",
      "# Iteration  5801 -> Loss: 0.3943365488070388 \t| Accuracy: 105.000\n",
      "# Iteration  5802 -> Loss: 0.39431370151883316 \t| Accuracy: 105.000\n",
      "# Iteration  5803 -> Loss: 0.39429086259626284 \t| Accuracy: 105.000\n",
      "# Iteration  5804 -> Loss: 0.39426803203439215 \t| Accuracy: 105.000\n",
      "# Iteration  5805 -> Loss: 0.3942452098282896 \t| Accuracy: 105.000\n",
      "# Iteration  5806 -> Loss: 0.39422239597302705 \t| Accuracy: 105.000\n",
      "# Iteration  5807 -> Loss: 0.3941995904636802 \t| Accuracy: 105.000\n",
      "# Iteration  5808 -> Loss: 0.39417679329532834 \t| Accuracy: 105.000\n",
      "# Iteration  5809 -> Loss: 0.3941540044630545 \t| Accuracy: 105.000\n",
      "# Iteration  5810 -> Loss: 0.3941312239619454 \t| Accuracy: 105.000\n",
      "# Iteration  5811 -> Loss: 0.3941084517870912 \t| Accuracy: 105.000\n",
      "# Iteration  5812 -> Loss: 0.3940856879335859 \t| Accuracy: 105.000\n",
      "# Iteration  5813 -> Loss: 0.3940629323965272 \t| Accuracy: 105.000\n",
      "# Iteration  5814 -> Loss: 0.3940401851710162 \t| Accuracy: 105.000\n",
      "# Iteration  5815 -> Loss: 0.39401744625215784 \t| Accuracy: 105.000\n",
      "# Iteration  5816 -> Loss: 0.3939947156350608 \t| Accuracy: 105.000\n",
      "# Iteration  5817 -> Loss: 0.3939719933148371 \t| Accuracy: 105.000\n",
      "# Iteration  5818 -> Loss: 0.3939492792866026 \t| Accuracy: 105.000\n",
      "# Iteration  5819 -> Loss: 0.3939265735454768 \t| Accuracy: 105.000\n",
      "# Iteration  5820 -> Loss: 0.3939038760865826 \t| Accuracy: 105.000\n",
      "# Iteration  5821 -> Loss: 0.3938811869050469 \t| Accuracy: 105.000\n",
      "# Iteration  5822 -> Loss: 0.39385850599599986 \t| Accuracy: 105.000\n",
      "# Iteration  5823 -> Loss: 0.3938358333545755 \t| Accuracy: 105.000\n",
      "# Iteration  5824 -> Loss: 0.39381316897591123 \t| Accuracy: 105.000\n",
      "# Iteration  5825 -> Loss: 0.3937905128551484 \t| Accuracy: 105.000\n",
      "# Iteration  5826 -> Loss: 0.3937678649874316 \t| Accuracy: 105.000\n",
      "# Iteration  5827 -> Loss: 0.3937452253679093 \t| Accuracy: 105.000\n",
      "# Iteration  5828 -> Loss: 0.39372259399173337 \t| Accuracy: 105.000\n",
      "# Iteration  5829 -> Loss: 0.39369997085405944 \t| Accuracy: 105.000\n",
      "# Iteration  5830 -> Loss: 0.39367735595004677 \t| Accuracy: 105.000\n",
      "# Iteration  5831 -> Loss: 0.393654749274858 \t| Accuracy: 105.000\n",
      "# Iteration  5832 -> Loss: 0.39363215082365943 \t| Accuracy: 105.000\n",
      "# Iteration  5833 -> Loss: 0.39360956059162094 \t| Accuracy: 105.000\n",
      "# Iteration  5834 -> Loss: 0.3935869785739162 \t| Accuracy: 105.000\n",
      "# Iteration  5835 -> Loss: 0.39356440476572213 \t| Accuracy: 105.000\n",
      "# Iteration  5836 -> Loss: 0.39354183916221963 \t| Accuracy: 105.000\n",
      "# Iteration  5837 -> Loss: 0.3935192817585927 \t| Accuracy: 105.000\n",
      "# Iteration  5838 -> Loss: 0.3934967325500292 \t| Accuracy: 105.000\n",
      "# Iteration  5839 -> Loss: 0.3934741915317206 \t| Accuracy: 105.000\n",
      "# Iteration  5840 -> Loss: 0.3934516586988616 \t| Accuracy: 105.000\n",
      "# Iteration  5841 -> Loss: 0.3934291340466509 \t| Accuracy: 105.000\n",
      "# Iteration  5842 -> Loss: 0.3934066175702905 \t| Accuracy: 105.000\n",
      "# Iteration  5843 -> Loss: 0.3933841092649859 \t| Accuracy: 105.000\n",
      "# Iteration  5844 -> Loss: 0.3933616091259464 \t| Accuracy: 105.000\n",
      "# Iteration  5845 -> Loss: 0.3933391171483846 \t| Accuracy: 105.000\n",
      "# Iteration  5846 -> Loss: 0.39331663332751665 \t| Accuracy: 105.000\n",
      "# Iteration  5847 -> Loss: 0.3932941576585625 \t| Accuracy: 105.000\n",
      "# Iteration  5848 -> Loss: 0.3932716901367454 \t| Accuracy: 105.000\n",
      "# Iteration  5849 -> Loss: 0.3932492307572921 \t| Accuracy: 105.000\n",
      "# Iteration  5850 -> Loss: 0.3932267795154331 \t| Accuracy: 105.000\n",
      "# Iteration  5851 -> Loss: 0.3932043364064023 \t| Accuracy: 105.000\n",
      "# Iteration  5852 -> Loss: 0.393181901425437 \t| Accuracy: 105.000\n",
      "# Iteration  5853 -> Loss: 0.39315947456777833 \t| Accuracy: 105.000\n",
      "# Iteration  5854 -> Loss: 0.3931370558286706 \t| Accuracy: 105.000\n",
      "# Iteration  5855 -> Loss: 0.3931146452033619 \t| Accuracy: 105.000\n",
      "# Iteration  5856 -> Loss: 0.39309224268710363 \t| Accuracy: 105.000\n",
      "# Iteration  5857 -> Loss: 0.3930698482751509 \t| Accuracy: 105.000\n",
      "# Iteration  5858 -> Loss: 0.3930474619627622 \t| Accuracy: 105.000\n",
      "# Iteration  5859 -> Loss: 0.3930250837451994 \t| Accuracy: 105.000\n",
      "# Iteration  5860 -> Loss: 0.3930027136177281 \t| Accuracy: 105.000\n",
      "# Iteration  5861 -> Loss: 0.39298035157561734 \t| Accuracy: 105.000\n",
      "# Iteration  5862 -> Loss: 0.3929579976141397 \t| Accuracy: 105.000\n",
      "# Iteration  5863 -> Loss: 0.392935651728571 \t| Accuracy: 105.000\n",
      "# Iteration  5864 -> Loss: 0.3929133139141907 \t| Accuracy: 105.000\n",
      "# Iteration  5865 -> Loss: 0.392890984166282 \t| Accuracy: 105.000\n",
      "# Iteration  5866 -> Loss: 0.3928686624801311 \t| Accuracy: 105.000\n",
      "# Iteration  5867 -> Loss: 0.39284634885102787 \t| Accuracy: 105.000\n",
      "# Iteration  5868 -> Loss: 0.39282404327426595 \t| Accuracy: 105.000\n",
      "# Iteration  5869 -> Loss: 0.392801745745142 \t| Accuracy: 105.000\n",
      "# Iteration  5870 -> Loss: 0.3927794562589565 \t| Accuracy: 105.000\n",
      "# Iteration  5871 -> Loss: 0.3927571748110131 \t| Accuracy: 105.000\n",
      "# Iteration  5872 -> Loss: 0.39273490139661904 \t| Accuracy: 105.000\n",
      "# Iteration  5873 -> Loss: 0.3927126360110852 \t| Accuracy: 105.000\n",
      "# Iteration  5874 -> Loss: 0.3926903786497257 \t| Accuracy: 105.000\n",
      "# Iteration  5875 -> Loss: 0.39266812930785805 \t| Accuracy: 105.000\n",
      "# Iteration  5876 -> Loss: 0.39264588798080335 \t| Accuracy: 105.000\n",
      "# Iteration  5877 -> Loss: 0.39262365466388627 \t| Accuracy: 105.000\n",
      "# Iteration  5878 -> Loss: 0.39260142935243453 \t| Accuracy: 105.000\n",
      "# Iteration  5879 -> Loss: 0.39257921204177965 \t| Accuracy: 105.000\n",
      "# Iteration  5880 -> Loss: 0.39255700272725663 \t| Accuracy: 105.000\n",
      "# Iteration  5881 -> Loss: 0.3925348014042034 \t| Accuracy: 105.000\n",
      "# Iteration  5882 -> Loss: 0.39251260806796195 \t| Accuracy: 105.000\n",
      "# Iteration  5883 -> Loss: 0.39249042271387724 \t| Accuracy: 105.000\n",
      "# Iteration  5884 -> Loss: 0.39246824533729796 \t| Accuracy: 105.000\n",
      "# Iteration  5885 -> Loss: 0.39244607593357606 \t| Accuracy: 105.000\n",
      "# Iteration  5886 -> Loss: 0.39242391449806674 \t| Accuracy: 105.000\n",
      "# Iteration  5887 -> Loss: 0.39240176102612906 \t| Accuracy: 105.000\n",
      "# Iteration  5888 -> Loss: 0.39237961551312506 \t| Accuracy: 105.000\n",
      "# Iteration  5889 -> Loss: 0.3923574779544206 \t| Accuracy: 105.000\n",
      "# Iteration  5890 -> Loss: 0.39233534834538447 \t| Accuracy: 105.000\n",
      "# Iteration  5891 -> Loss: 0.3923132266813892 \t| Accuracy: 105.000\n",
      "# Iteration  5892 -> Loss: 0.39229111295781083 \t| Accuracy: 105.000\n",
      "# Iteration  5893 -> Loss: 0.39226900717002827 \t| Accuracy: 105.000\n",
      "# Iteration  5894 -> Loss: 0.3922469093134243 \t| Accuracy: 105.000\n",
      "# Iteration  5895 -> Loss: 0.39222481938338505 \t| Accuracy: 105.000\n",
      "# Iteration  5896 -> Loss: 0.3922027373752998 \t| Accuracy: 105.000\n",
      "# Iteration  5897 -> Loss: 0.39218066328456136 \t| Accuracy: 105.000\n",
      "# Iteration  5898 -> Loss: 0.39215859710656603 \t| Accuracy: 105.000\n",
      "# Iteration  5899 -> Loss: 0.3921365388367132 \t| Accuracy: 105.000\n",
      "# Iteration  5900 -> Loss: 0.3921144884704059 \t| Accuracy: 105.000\n",
      "# Iteration  5901 -> Loss: 0.39209244600305054 \t| Accuracy: 105.000\n",
      "# Iteration  5902 -> Loss: 0.3920704114300566 \t| Accuracy: 105.000\n",
      "# Iteration  5903 -> Loss: 0.3920483847468373 \t| Accuracy: 105.000\n",
      "# Iteration  5904 -> Loss: 0.392026365948809 \t| Accuracy: 105.000\n",
      "# Iteration  5905 -> Loss: 0.3920043550313914 \t| Accuracy: 105.000\n",
      "# Iteration  5906 -> Loss: 0.39198235199000775 \t| Accuracy: 105.000\n",
      "# Iteration  5907 -> Loss: 0.3919603568200845 \t| Accuracy: 105.000\n",
      "# Iteration  5908 -> Loss: 0.3919383695170514 \t| Accuracy: 105.000\n",
      "# Iteration  5909 -> Loss: 0.3919163900763419 \t| Accuracy: 105.000\n",
      "# Iteration  5910 -> Loss: 0.3918944184933923 \t| Accuracy: 105.000\n",
      "# Iteration  5911 -> Loss: 0.39187245476364246 \t| Accuracy: 105.000\n",
      "# Iteration  5912 -> Loss: 0.3918504988825357 \t| Accuracy: 105.000\n",
      "# Iteration  5913 -> Loss: 0.3918285508455186 \t| Accuracy: 105.000\n",
      "# Iteration  5914 -> Loss: 0.391806610648041 \t| Accuracy: 105.000\n",
      "# Iteration  5915 -> Loss: 0.39178467828555613 \t| Accuracy: 105.000\n",
      "# Iteration  5916 -> Loss: 0.3917627537535206 \t| Accuracy: 105.000\n",
      "# Iteration  5917 -> Loss: 0.39174083704739426 \t| Accuracy: 105.000\n",
      "# Iteration  5918 -> Loss: 0.39171892816264026 \t| Accuracy: 105.000\n",
      "# Iteration  5919 -> Loss: 0.3916970270947252 \t| Accuracy: 105.000\n",
      "# Iteration  5920 -> Loss: 0.39167513383911884 \t| Accuracy: 105.000\n",
      "# Iteration  5921 -> Loss: 0.3916532483912944 \t| Accuracy: 105.000\n",
      "# Iteration  5922 -> Loss: 0.39163137074672844 \t| Accuracy: 105.000\n",
      "# Iteration  5923 -> Loss: 0.3916095009009005 \t| Accuracy: 105.000\n",
      "# Iteration  5924 -> Loss: 0.39158763884929376 \t| Accuracy: 105.000\n",
      "# Iteration  5925 -> Loss: 0.3915657845873947 \t| Accuracy: 105.000\n",
      "# Iteration  5926 -> Loss: 0.39154393811069294 \t| Accuracy: 105.000\n",
      "# Iteration  5927 -> Loss: 0.3915220994146814 \t| Accuracy: 105.000\n",
      "# Iteration  5928 -> Loss: 0.3915002684948564 \t| Accuracy: 105.000\n",
      "# Iteration  5929 -> Loss: 0.3914784453467175 \t| Accuracy: 105.000\n",
      "# Iteration  5930 -> Loss: 0.3914566299657675 \t| Accuracy: 105.000\n",
      "# Iteration  5931 -> Loss: 0.3914348223475127 \t| Accuracy: 105.000\n",
      "# Iteration  5932 -> Loss: 0.39141302248746235 \t| Accuracy: 105.000\n",
      "# Iteration  5933 -> Loss: 0.39139123038112916 \t| Accuracy: 105.000\n",
      "# Iteration  5934 -> Loss: 0.3913694460240291 \t| Accuracy: 105.000\n",
      "# Iteration  5935 -> Loss: 0.3913476694116815 \t| Accuracy: 105.000\n",
      "# Iteration  5936 -> Loss: 0.39132590053960875 \t| Accuracy: 105.000\n",
      "# Iteration  5937 -> Loss: 0.39130413940333675 \t| Accuracy: 105.000\n",
      "# Iteration  5938 -> Loss: 0.3912823859983945 \t| Accuracy: 105.000\n",
      "# Iteration  5939 -> Loss: 0.3912606403203143 \t| Accuracy: 105.000\n",
      "# Iteration  5940 -> Loss: 0.3912389023646317 \t| Accuracy: 105.000\n",
      "# Iteration  5941 -> Loss: 0.3912171721268856 \t| Accuracy: 105.000\n",
      "# Iteration  5942 -> Loss: 0.39119544960261804 \t| Accuracy: 105.000\n",
      "# Iteration  5943 -> Loss: 0.39117373478737427 \t| Accuracy: 105.000\n",
      "# Iteration  5944 -> Loss: 0.39115202767670293 \t| Accuracy: 105.000\n",
      "# Iteration  5945 -> Loss: 0.39113032826615585 \t| Accuracy: 105.000\n",
      "# Iteration  5946 -> Loss: 0.3911086365512881 \t| Accuracy: 105.000\n",
      "# Iteration  5947 -> Loss: 0.39108695252765796 \t| Accuracy: 105.000\n",
      "# Iteration  5948 -> Loss: 0.3910652761908269 \t| Accuracy: 105.000\n",
      "# Iteration  5949 -> Loss: 0.39104360753635975 \t| Accuracy: 105.000\n",
      "# Iteration  5950 -> Loss: 0.3910219465598246 \t| Accuracy: 105.000\n",
      "# Iteration  5951 -> Loss: 0.3910002932567924 \t| Accuracy: 105.000\n",
      "# Iteration  5952 -> Loss: 0.39097864762283796 \t| Accuracy: 105.000\n",
      "# Iteration  5953 -> Loss: 0.3909570096535387 \t| Accuracy: 105.000\n",
      "# Iteration  5954 -> Loss: 0.3909353793444756 \t| Accuracy: 105.000\n",
      "# Iteration  5955 -> Loss: 0.39091375669123263 \t| Accuracy: 105.000\n",
      "# Iteration  5956 -> Loss: 0.39089214168939745 \t| Accuracy: 105.000\n",
      "# Iteration  5957 -> Loss: 0.3908705343345602 \t| Accuracy: 105.000\n",
      "# Iteration  5958 -> Loss: 0.3908489346223148 \t| Accuracy: 105.000\n",
      "# Iteration  5959 -> Loss: 0.39082734254825824 \t| Accuracy: 105.000\n",
      "# Iteration  5960 -> Loss: 0.39080575810799056 \t| Accuracy: 105.000\n",
      "# Iteration  5961 -> Loss: 0.3907841812971151 \t| Accuracy: 105.000\n",
      "# Iteration  5962 -> Loss: 0.3907626121112385 \t| Accuracy: 105.000\n",
      "# Iteration  5963 -> Loss: 0.39074105054597047 \t| Accuracy: 105.000\n",
      "# Iteration  5964 -> Loss: 0.3907194965969238 \t| Accuracy: 105.000\n",
      "# Iteration  5965 -> Loss: 0.3906979502597148 \t| Accuracy: 105.000\n",
      "# Iteration  5966 -> Loss: 0.3906764115299627 \t| Accuracy: 105.000\n",
      "# Iteration  5967 -> Loss: 0.39065488040328983 \t| Accuracy: 105.000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Iteration  5968 -> Loss: 0.39063335687532197 \t| Accuracy: 105.000\n",
      "# Iteration  5969 -> Loss: 0.39061184094168805 \t| Accuracy: 105.000\n",
      "# Iteration  5970 -> Loss: 0.39059033259801995 \t| Accuracy: 105.000\n",
      "# Iteration  5971 -> Loss: 0.39056883183995283 \t| Accuracy: 105.000\n",
      "# Iteration  5972 -> Loss: 0.39054733866312513 \t| Accuracy: 105.000\n",
      "# Iteration  5973 -> Loss: 0.3905258530631784 \t| Accuracy: 105.000\n",
      "# Iteration  5974 -> Loss: 0.3905043750357571 \t| Accuracy: 105.000\n",
      "# Iteration  5975 -> Loss: 0.39048290457650925 \t| Accuracy: 105.000\n",
      "# Iteration  5976 -> Loss: 0.39046144168108576 \t| Accuracy: 105.000\n",
      "# Iteration  5977 -> Loss: 0.3904399863451408 \t| Accuracy: 105.000\n",
      "# Iteration  5978 -> Loss: 0.39041853856433184 \t| Accuracy: 105.000\n",
      "# Iteration  5979 -> Loss: 0.390397098334319 \t| Accuracy: 105.000\n",
      "# Iteration  5980 -> Loss: 0.39037566565076626 \t| Accuracy: 105.000\n",
      "# Iteration  5981 -> Loss: 0.39035424050934 \t| Accuracy: 105.000\n",
      "# Iteration  5982 -> Loss: 0.39033282290571025 \t| Accuracy: 105.000\n",
      "# Iteration  5983 -> Loss: 0.3903114128355501 \t| Accuracy: 105.000\n",
      "# Iteration  5984 -> Loss: 0.3902900102945356 \t| Accuracy: 105.000\n",
      "# Iteration  5985 -> Loss: 0.3902686152783461 \t| Accuracy: 105.000\n",
      "# Iteration  5986 -> Loss: 0.3902472277826638 \t| Accuracy: 105.000\n",
      "# Iteration  5987 -> Loss: 0.3902258478031746 \t| Accuracy: 105.000\n",
      "# Iteration  5988 -> Loss: 0.39020447533556696 \t| Accuracy: 105.000\n",
      "# Iteration  5989 -> Loss: 0.39018311037553266 \t| Accuracy: 105.000\n",
      "# Iteration  5990 -> Loss: 0.39016175291876665 \t| Accuracy: 105.000\n",
      "# Iteration  5991 -> Loss: 0.3901404029609669 \t| Accuracy: 105.000\n",
      "# Iteration  5992 -> Loss: 0.3901190604978346 \t| Accuracy: 105.000\n",
      "# Iteration  5993 -> Loss: 0.39009772552507405 \t| Accuracy: 105.000\n",
      "# Iteration  5994 -> Loss: 0.39007639803839245 \t| Accuracy: 105.000\n",
      "# Iteration  5995 -> Loss: 0.3900550780335005 \t| Accuracy: 105.000\n",
      "# Iteration  5996 -> Loss: 0.3900337655061114 \t| Accuracy: 105.000\n",
      "# Iteration  5997 -> Loss: 0.3900124604519421 \t| Accuracy: 105.000\n",
      "# Iteration  5998 -> Loss: 0.3899911628667122 \t| Accuracy: 105.000\n",
      "# Iteration  5999 -> Loss: 0.3899698727461447 \t| Accuracy: 105.000\n",
      "# Iteration  6000 -> Loss: 0.38994859008596544 \t| Accuracy: 105.000\n",
      "# Iteration  6001 -> Loss: 0.38992731488190346 \t| Accuracy: 105.000\n",
      "# Iteration  6002 -> Loss: 0.38990604712969096 \t| Accuracy: 105.000\n",
      "# Iteration  6003 -> Loss: 0.38988478682506306 \t| Accuracy: 105.000\n",
      "# Iteration  6004 -> Loss: 0.389863533963758 \t| Accuracy: 105.000\n",
      "# Iteration  6005 -> Loss: 0.38984228854151737 \t| Accuracy: 105.000\n",
      "# Iteration  6006 -> Loss: 0.3898210505540852 \t| Accuracy: 105.000\n",
      "# Iteration  6007 -> Loss: 0.3897998199972094 \t| Accuracy: 105.000\n",
      "# Iteration  6008 -> Loss: 0.38977859686664035 \t| Accuracy: 105.000\n",
      "# Iteration  6009 -> Loss: 0.38975738115813174 \t| Accuracy: 105.000\n",
      "# Iteration  6010 -> Loss: 0.3897361728674403 \t| Accuracy: 105.000\n",
      "# Iteration  6011 -> Loss: 0.3897149719903258 \t| Accuracy: 105.000\n",
      "# Iteration  6012 -> Loss: 0.38969377852255094 \t| Accuracy: 105.000\n",
      "# Iteration  6013 -> Loss: 0.3896725924598819 \t| Accuracy: 105.000\n",
      "# Iteration  6014 -> Loss: 0.38965141379808726 \t| Accuracy: 105.000\n",
      "# Iteration  6015 -> Loss: 0.38963024253293926 \t| Accuracy: 105.000\n",
      "# Iteration  6016 -> Loss: 0.389609078660213 \t| Accuracy: 105.000\n",
      "# Iteration  6017 -> Loss: 0.38958792217568633 \t| Accuracy: 105.000\n",
      "# Iteration  6018 -> Loss: 0.38956677307514054 \t| Accuracy: 105.000\n",
      "# Iteration  6019 -> Loss: 0.3895456313543598 \t| Accuracy: 105.000\n",
      "# Iteration  6020 -> Loss: 0.38952449700913133 \t| Accuracy: 105.000\n",
      "# Iteration  6021 -> Loss: 0.38950337003524527 \t| Accuracy: 105.000\n",
      "# Iteration  6022 -> Loss: 0.38948225042849505 \t| Accuracy: 105.000\n",
      "# Iteration  6023 -> Loss: 0.3894611381846768 \t| Accuracy: 105.000\n",
      "# Iteration  6024 -> Loss: 0.3894400332995901 \t| Accuracy: 105.000\n",
      "# Iteration  6025 -> Loss: 0.38941893576903713 \t| Accuracy: 105.000\n",
      "# Iteration  6026 -> Loss: 0.3893978455888233 \t| Accuracy: 105.000\n",
      "# Iteration  6027 -> Loss: 0.389376762754757 \t| Accuracy: 105.000\n",
      "# Iteration  6028 -> Loss: 0.38935568726264974 \t| Accuracy: 105.000\n",
      "# Iteration  6029 -> Loss: 0.38933461910831596 \t| Accuracy: 105.000\n",
      "# Iteration  6030 -> Loss: 0.3893135582875731 \t| Accuracy: 105.000\n",
      "# Iteration  6031 -> Loss: 0.38929250479624133 \t| Accuracy: 105.000\n",
      "# Iteration  6032 -> Loss: 0.38927145863014456 \t| Accuracy: 105.000\n",
      "# Iteration  6033 -> Loss: 0.3892504197851091 \t| Accuracy: 105.000\n",
      "# Iteration  6034 -> Loss: 0.3892293882569643 \t| Accuracy: 105.000\n",
      "# Iteration  6035 -> Loss: 0.38920836404154274 \t| Accuracy: 105.000\n",
      "# Iteration  6036 -> Loss: 0.3891873471346799 \t| Accuracy: 105.000\n",
      "# Iteration  6037 -> Loss: 0.38916633753221413 \t| Accuracy: 105.000\n",
      "# Iteration  6038 -> Loss: 0.38914533522998696 \t| Accuracy: 105.000\n",
      "# Iteration  6039 -> Loss: 0.3891243402238428 \t| Accuracy: 105.000\n",
      "# Iteration  6040 -> Loss: 0.38910335250962913 \t| Accuracy: 105.000\n",
      "# Iteration  6041 -> Loss: 0.3890823720831964 \t| Accuracy: 105.000\n",
      "# Iteration  6042 -> Loss: 0.38906139894039776 \t| Accuracy: 105.000\n",
      "# Iteration  6043 -> Loss: 0.38904043307708974 \t| Accuracy: 105.000\n",
      "# Iteration  6044 -> Loss: 0.38901947448913177 \t| Accuracy: 105.000\n",
      "# Iteration  6045 -> Loss: 0.38899852317238603 \t| Accuracy: 105.000\n",
      "# Iteration  6046 -> Loss: 0.3889775791227178 \t| Accuracy: 105.000\n",
      "# Iteration  6047 -> Loss: 0.3889566423359955 \t| Accuracy: 105.000\n",
      "# Iteration  6048 -> Loss: 0.3889357128080901 \t| Accuracy: 105.000\n",
      "# Iteration  6049 -> Loss: 0.3889147905348759 \t| Accuracy: 105.000\n",
      "# Iteration  6050 -> Loss: 0.38889387551223015 \t| Accuracy: 105.000\n",
      "# Iteration  6051 -> Loss: 0.38887296773603275 \t| Accuracy: 105.000\n",
      "# Iteration  6052 -> Loss: 0.38885206720216686 \t| Accuracy: 105.000\n",
      "# Iteration  6053 -> Loss: 0.3888311739065185 \t| Accuracy: 105.000\n",
      "# Iteration  6054 -> Loss: 0.38881028784497657 \t| Accuracy: 105.000\n",
      "# Iteration  6055 -> Loss: 0.388789409013433 \t| Accuracy: 105.000\n",
      "# Iteration  6056 -> Loss: 0.38876853740778267 \t| Accuracy: 105.000\n",
      "# Iteration  6057 -> Loss: 0.3887476730239233 \t| Accuracy: 105.000\n",
      "# Iteration  6058 -> Loss: 0.38872681585775576 \t| Accuracy: 105.000\n",
      "# Iteration  6059 -> Loss: 0.38870596590518347 \t| Accuracy: 105.000\n",
      "# Iteration  6060 -> Loss: 0.3886851231621132 \t| Accuracy: 105.000\n",
      "# Iteration  6061 -> Loss: 0.38866428762445443 \t| Accuracy: 105.000\n",
      "# Iteration  6062 -> Loss: 0.3886434592881197 \t| Accuracy: 105.000\n",
      "# Iteration  6063 -> Loss: 0.38862263814902437 \t| Accuracy: 105.000\n",
      "# Iteration  6064 -> Loss: 0.3886018242030867 \t| Accuracy: 105.000\n",
      "# Iteration  6065 -> Loss: 0.38858101744622786 \t| Accuracy: 105.000\n",
      "# Iteration  6066 -> Loss: 0.3885602178743722 \t| Accuracy: 105.000\n",
      "# Iteration  6067 -> Loss: 0.3885394254834467 \t| Accuracy: 105.000\n",
      "# Iteration  6068 -> Loss: 0.3885186402693813 \t| Accuracy: 105.000\n",
      "# Iteration  6069 -> Loss: 0.38849786222810895 \t| Accuracy: 105.000\n",
      "# Iteration  6070 -> Loss: 0.38847709135556546 \t| Accuracy: 105.000\n",
      "# Iteration  6071 -> Loss: 0.38845632764768945 \t| Accuracy: 105.000\n",
      "# Iteration  6072 -> Loss: 0.38843557110042265 \t| Accuracy: 105.000\n",
      "# Iteration  6073 -> Loss: 0.38841482170970965 \t| Accuracy: 105.000\n",
      "# Iteration  6074 -> Loss: 0.3883940794714977 \t| Accuracy: 105.000\n",
      "# Iteration  6075 -> Loss: 0.388373344381737 \t| Accuracy: 105.000\n",
      "# Iteration  6076 -> Loss: 0.38835261643638114 \t| Accuracy: 105.000\n",
      "# Iteration  6077 -> Loss: 0.3883318956313859 \t| Accuracy: 105.000\n",
      "# Iteration  6078 -> Loss: 0.3883111819627106 \t| Accuracy: 105.000\n",
      "# Iteration  6079 -> Loss: 0.3882904754263167 \t| Accuracy: 105.000\n",
      "# Iteration  6080 -> Loss: 0.3882697760181692 \t| Accuracy: 105.000\n",
      "# Iteration  6081 -> Loss: 0.3882490837342358 \t| Accuracy: 105.000\n",
      "# Iteration  6082 -> Loss: 0.3882283985704869 \t| Accuracy: 105.000\n",
      "# Iteration  6083 -> Loss: 0.388207720522896 \t| Accuracy: 105.000\n",
      "# Iteration  6084 -> Loss: 0.3881870495874392 \t| Accuracy: 105.000\n",
      "# Iteration  6085 -> Loss: 0.3881663857600959 \t| Accuracy: 105.000\n",
      "# Iteration  6086 -> Loss: 0.38814572903684785 \t| Accuracy: 105.000\n",
      "# Iteration  6087 -> Loss: 0.38812507941368013 \t| Accuracy: 105.000\n",
      "# Iteration  6088 -> Loss: 0.3881044368865804 \t| Accuracy: 105.000\n",
      "# Iteration  6089 -> Loss: 0.3880838014515393 \t| Accuracy: 105.000\n",
      "# Iteration  6090 -> Loss: 0.3880631731045503 \t| Accuracy: 105.000\n",
      "# Iteration  6091 -> Loss: 0.38804255184160985 \t| Accuracy: 105.000\n",
      "# Iteration  6092 -> Loss: 0.38802193765871684 \t| Accuracy: 105.000\n",
      "# Iteration  6093 -> Loss: 0.38800133055187347 \t| Accuracy: 105.000\n",
      "# Iteration  6094 -> Loss: 0.38798073051708476 \t| Accuracy: 105.000\n",
      "# Iteration  6095 -> Loss: 0.3879601375503582 \t| Accuracy: 105.000\n",
      "# Iteration  6096 -> Loss: 0.38793955164770444 \t| Accuracy: 105.000\n",
      "# Iteration  6097 -> Loss: 0.387918972805137 \t| Accuracy: 105.000\n",
      "# Iteration  6098 -> Loss: 0.3878984010186721 \t| Accuracy: 105.000\n",
      "# Iteration  6099 -> Loss: 0.3878778362843287 \t| Accuracy: 105.000\n",
      "# Iteration  6100 -> Loss: 0.38785727859812885 \t| Accuracy: 105.000\n",
      "# Iteration  6101 -> Loss: 0.38783672795609736 \t| Accuracy: 105.000\n",
      "# Iteration  6102 -> Loss: 0.38781618435426174 \t| Accuracy: 105.000\n",
      "# Iteration  6103 -> Loss: 0.38779564778865244 \t| Accuracy: 105.000\n",
      "# Iteration  6104 -> Loss: 0.3877751182553027 \t| Accuracy: 105.000\n",
      "# Iteration  6105 -> Loss: 0.3877545957502485 \t| Accuracy: 105.000\n",
      "# Iteration  6106 -> Loss: 0.387734080269529 \t| Accuracy: 105.000\n",
      "# Iteration  6107 -> Loss: 0.3877135718091856 \t| Accuracy: 105.000\n",
      "# Iteration  6108 -> Loss: 0.387693070365263 \t| Accuracy: 105.000\n",
      "# Iteration  6109 -> Loss: 0.3876725759338085 \t| Accuracy: 105.000\n",
      "# Iteration  6110 -> Loss: 0.3876520885108723 \t| Accuracy: 105.000\n",
      "# Iteration  6111 -> Loss: 0.38763160809250724 \t| Accuracy: 105.000\n",
      "# Iteration  6112 -> Loss: 0.38761113467476915 \t| Accuracy: 105.000\n",
      "# Iteration  6113 -> Loss: 0.3875906682537167 \t| Accuracy: 105.000\n",
      "# Iteration  6114 -> Loss: 0.3875702088254111 \t| Accuracy: 105.000\n",
      "# Iteration  6115 -> Loss: 0.38754975638591654 \t| Accuracy: 105.000\n",
      "# Iteration  6116 -> Loss: 0.3875293109313 \t| Accuracy: 105.000\n",
      "# Iteration  6117 -> Loss: 0.38750887245763127 \t| Accuracy: 105.000\n",
      "# Iteration  6118 -> Loss: 0.38748844096098295 \t| Accuracy: 105.000\n",
      "# Iteration  6119 -> Loss: 0.38746801643743034 \t| Accuracy: 105.000\n",
      "# Iteration  6120 -> Loss: 0.38744759888305147 \t| Accuracy: 105.000\n",
      "# Iteration  6121 -> Loss: 0.3874271882939274 \t| Accuracy: 105.000\n",
      "# Iteration  6122 -> Loss: 0.3874067846661417 \t| Accuracy: 105.000\n",
      "# Iteration  6123 -> Loss: 0.3873863879957809 \t| Accuracy: 105.000\n",
      "# Iteration  6124 -> Loss: 0.38736599827893425 \t| Accuracy: 105.000\n",
      "# Iteration  6125 -> Loss: 0.38734561551169383 \t| Accuracy: 105.000\n",
      "# Iteration  6126 -> Loss: 0.3873252396901543 \t| Accuracy: 105.000\n",
      "# Iteration  6127 -> Loss: 0.38730487081041337 \t| Accuracy: 105.000\n",
      "# Iteration  6128 -> Loss: 0.38728450886857135 \t| Accuracy: 105.000\n",
      "# Iteration  6129 -> Loss: 0.38726415386073126 \t| Accuracy: 105.000\n",
      "# Iteration  6130 -> Loss: 0.387243805782999 \t| Accuracy: 105.000\n",
      "# Iteration  6131 -> Loss: 0.3872234646314832 \t| Accuracy: 105.000\n",
      "# Iteration  6132 -> Loss: 0.3872031304022953 \t| Accuracy: 105.000\n",
      "# Iteration  6133 -> Loss: 0.38718280309154934 \t| Accuracy: 105.000\n",
      "# Iteration  6134 -> Loss: 0.3871624826953623 \t| Accuracy: 105.000\n",
      "# Iteration  6135 -> Loss: 0.3871421692098539 \t| Accuracy: 105.000\n",
      "# Iteration  6136 -> Loss: 0.3871218626311464 \t| Accuracy: 105.000\n",
      "# Iteration  6137 -> Loss: 0.3871015629553649 \t| Accuracy: 105.000\n",
      "# Iteration  6138 -> Loss: 0.38708127017863736 \t| Accuracy: 105.000\n",
      "# Iteration  6139 -> Loss: 0.3870609842970945 \t| Accuracy: 105.000\n",
      "# Iteration  6140 -> Loss: 0.3870407053068696 \t| Accuracy: 105.000\n",
      "# Iteration  6141 -> Loss: 0.3870204332040988 \t| Accuracy: 105.000\n",
      "# Iteration  6142 -> Loss: 0.38700016798492076 \t| Accuracy: 105.000\n",
      "# Iteration  6143 -> Loss: 0.38697990964547735 \t| Accuracy: 105.000\n",
      "# Iteration  6144 -> Loss: 0.3869596581819127 \t| Accuracy: 105.000\n",
      "# Iteration  6145 -> Loss: 0.3869394135903738 \t| Accuracy: 105.000\n",
      "# Iteration  6146 -> Loss: 0.38691917586701063 \t| Accuracy: 105.000\n",
      "# Iteration  6147 -> Loss: 0.38689894500797545 \t| Accuracy: 105.000\n",
      "# Iteration  6148 -> Loss: 0.3868787210094235 \t| Accuracy: 105.000\n",
      "# Iteration  6149 -> Loss: 0.38685850386751286 \t| Accuracy: 105.000\n",
      "# Iteration  6150 -> Loss: 0.3868382935784038 \t| Accuracy: 105.000\n",
      "# Iteration  6151 -> Loss: 0.38681809013826024 \t| Accuracy: 105.000\n",
      "# Iteration  6152 -> Loss: 0.3867978935432478 \t| Accuracy: 105.000\n",
      "# Iteration  6153 -> Loss: 0.3867777037895353 \t| Accuracy: 105.000\n",
      "# Iteration  6154 -> Loss: 0.3867575208732943 \t| Accuracy: 105.000\n",
      "# Iteration  6155 -> Loss: 0.386737344790699 \t| Accuracy: 105.000\n",
      "# Iteration  6156 -> Loss: 0.38671717553792634 \t| Accuracy: 105.000\n",
      "# Iteration  6157 -> Loss: 0.38669701311115584 \t| Accuracy: 105.000\n",
      "# Iteration  6158 -> Loss: 0.38667685750656977 \t| Accuracy: 105.000\n",
      "# Iteration  6159 -> Loss: 0.3866567087203531 \t| Accuracy: 105.000\n",
      "# Iteration  6160 -> Loss: 0.3866365667486936 \t| Accuracy: 105.000\n",
      "# Iteration  6161 -> Loss: 0.3866164315877816 \t| Accuracy: 105.000\n",
      "# Iteration  6162 -> Loss: 0.38659630323381 \t| Accuracy: 105.000\n",
      "# Iteration  6163 -> Loss: 0.38657618168297486 \t| Accuracy: 105.000\n",
      "# Iteration  6164 -> Loss: 0.38655606693147426 \t| Accuracy: 105.000\n",
      "# Iteration  6165 -> Loss: 0.3865359589755095 \t| Accuracy: 105.000\n",
      "# Iteration  6166 -> Loss: 0.3865158578112845 \t| Accuracy: 105.000\n",
      "# Iteration  6167 -> Loss: 0.3864957634350054 \t| Accuracy: 105.000\n",
      "# Iteration  6168 -> Loss: 0.38647567584288156 \t| Accuracy: 105.000\n",
      "# Iteration  6169 -> Loss: 0.3864555950311248 \t| Accuracy: 105.000\n",
      "# Iteration  6170 -> Loss: 0.3864355209959495 \t| Accuracy: 105.000\n",
      "# Iteration  6171 -> Loss: 0.386415453733573 \t| Accuracy: 105.000\n",
      "# Iteration  6172 -> Loss: 0.3863953932402148 \t| Accuracy: 105.000\n",
      "# Iteration  6173 -> Loss: 0.3863753395120977 \t| Accuracy: 105.000\n",
      "# Iteration  6174 -> Loss: 0.3863552925454467 \t| Accuracy: 105.000\n",
      "# Iteration  6175 -> Loss: 0.3863352523364896 \t| Accuracy: 105.000\n",
      "# Iteration  6176 -> Loss: 0.38631521888145687 \t| Accuracy: 105.000\n",
      "# Iteration  6177 -> Loss: 0.38629519217658176 \t| Accuracy: 105.000\n",
      "# Iteration  6178 -> Loss: 0.3862751722180998 \t| Accuracy: 105.000\n",
      "# Iteration  6179 -> Loss: 0.3862551590022496 \t| Accuracy: 105.000\n",
      "# Iteration  6180 -> Loss: 0.3862351525252721 \t| Accuracy: 105.000\n",
      "# Iteration  6181 -> Loss: 0.3862151527834112 \t| Accuracy: 105.000\n",
      "# Iteration  6182 -> Loss: 0.386195159772913 \t| Accuracy: 105.000\n",
      "# Iteration  6183 -> Loss: 0.3861751734900267 \t| Accuracy: 105.000\n",
      "# Iteration  6184 -> Loss: 0.3861551939310039 \t| Accuracy: 105.000\n",
      "# Iteration  6185 -> Loss: 0.3861352210920989 \t| Accuracy: 105.000\n",
      "# Iteration  6186 -> Loss: 0.3861152549695685 \t| Accuracy: 105.000\n",
      "# Iteration  6187 -> Loss: 0.38609529555967226 \t| Accuracy: 105.000\n",
      "# Iteration  6188 -> Loss: 0.3860753428586725 \t| Accuracy: 105.000\n",
      "# Iteration  6189 -> Loss: 0.38605539686283397 \t| Accuracy: 105.000\n",
      "# Iteration  6190 -> Loss: 0.38603545756842395 \t| Accuracy: 105.000\n",
      "# Iteration  6191 -> Loss: 0.38601552497171254 \t| Accuracy: 105.000\n",
      "# Iteration  6192 -> Loss: 0.3859955990689726 \t| Accuracy: 105.000\n",
      "# Iteration  6193 -> Loss: 0.3859756798564791 \t| Accuracy: 105.000\n",
      "# Iteration  6194 -> Loss: 0.38595576733051035 \t| Accuracy: 105.000\n",
      "# Iteration  6195 -> Loss: 0.38593586148734643 \t| Accuracy: 105.000\n",
      "# Iteration  6196 -> Loss: 0.38591596232327086 \t| Accuracy: 105.000\n",
      "# Iteration  6197 -> Loss: 0.38589606983456926 \t| Accuracy: 105.000\n",
      "# Iteration  6198 -> Loss: 0.3858761840175299 \t| Accuracy: 105.000\n",
      "# Iteration  6199 -> Loss: 0.3858563048684438 \t| Accuracy: 105.000\n",
      "# Iteration  6200 -> Loss: 0.3858364323836045 \t| Accuracy: 105.000\n",
      "# Iteration  6201 -> Loss: 0.38581656655930807 \t| Accuracy: 105.000\n",
      "# Iteration  6202 -> Loss: 0.3857967073918535 \t| Accuracy: 105.000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Iteration  6203 -> Loss: 0.38577685487754204 \t| Accuracy: 105.000\n",
      "# Iteration  6204 -> Loss: 0.3857570090126777 \t| Accuracy: 105.000\n",
      "# Iteration  6205 -> Loss: 0.38573716979356687 \t| Accuracy: 105.000\n",
      "# Iteration  6206 -> Loss: 0.3857173372165188 \t| Accuracy: 105.000\n",
      "# Iteration  6207 -> Loss: 0.38569751127784524 \t| Accuracy: 105.000\n",
      "# Iteration  6208 -> Loss: 0.3856776919738605 \t| Accuracy: 105.000\n",
      "# Iteration  6209 -> Loss: 0.38565787930088136 \t| Accuracy: 105.000\n",
      "# Iteration  6210 -> Loss: 0.3856380732552276 \t| Accuracy: 105.000\n",
      "# Iteration  6211 -> Loss: 0.3856182738332209 \t| Accuracy: 105.000\n",
      "# Iteration  6212 -> Loss: 0.3855984810311861 \t| Accuracy: 105.000\n",
      "# Iteration  6213 -> Loss: 0.38557869484545043 \t| Accuracy: 105.000\n",
      "# Iteration  6214 -> Loss: 0.3855589152723436 \t| Accuracy: 105.000\n",
      "# Iteration  6215 -> Loss: 0.385539142308198 \t| Accuracy: 105.000\n",
      "# Iteration  6216 -> Loss: 0.3855193759493486 \t| Accuracy: 105.000\n",
      "# Iteration  6217 -> Loss: 0.3854996161921328 \t| Accuracy: 105.000\n",
      "# Iteration  6218 -> Loss: 0.3854798630328908 \t| Accuracy: 105.000\n",
      "# Iteration  6219 -> Loss: 0.38546011646796496 \t| Accuracy: 105.000\n",
      "# Iteration  6220 -> Loss: 0.38544037649370066 \t| Accuracy: 105.000\n",
      "# Iteration  6221 -> Loss: 0.3854206431064457 \t| Accuracy: 105.000\n",
      "# Iteration  6222 -> Loss: 0.3854009163025503 \t| Accuracy: 105.000\n",
      "# Iteration  6223 -> Loss: 0.3853811960783672 \t| Accuracy: 105.000\n",
      "# Iteration  6224 -> Loss: 0.38536148243025187 \t| Accuracy: 105.000\n",
      "# Iteration  6225 -> Loss: 0.3853417753545623 \t| Accuracy: 105.000\n",
      "# Iteration  6226 -> Loss: 0.385322074847659 \t| Accuracy: 105.000\n",
      "# Iteration  6227 -> Loss: 0.38530238090590485 \t| Accuracy: 105.000\n",
      "# Iteration  6228 -> Loss: 0.3852826935256657 \t| Accuracy: 105.000\n",
      "# Iteration  6229 -> Loss: 0.38526301270330937 \t| Accuracy: 105.000\n",
      "# Iteration  6230 -> Loss: 0.3852433384352068 \t| Accuracy: 105.000\n",
      "# Iteration  6231 -> Loss: 0.385223670717731 \t| Accuracy: 105.000\n",
      "# Iteration  6232 -> Loss: 0.3852040095472578 \t| Accuracy: 105.000\n",
      "# Iteration  6233 -> Loss: 0.38518435492016534 \t| Accuracy: 105.000\n",
      "# Iteration  6234 -> Loss: 0.3851647068328347 \t| Accuracy: 105.000\n",
      "# Iteration  6235 -> Loss: 0.38514506528164894 \t| Accuracy: 105.000\n",
      "# Iteration  6236 -> Loss: 0.38512543026299395 \t| Accuracy: 105.000\n",
      "# Iteration  6237 -> Loss: 0.3851058017732582 \t| Accuracy: 105.000\n",
      "# Iteration  6238 -> Loss: 0.38508617980883253 \t| Accuracy: 105.000\n",
      "# Iteration  6239 -> Loss: 0.38506656436611036 \t| Accuracy: 105.000\n",
      "# Iteration  6240 -> Loss: 0.38504695544148754 \t| Accuracy: 105.000\n",
      "# Iteration  6241 -> Loss: 0.38502735303136265 \t| Accuracy: 105.000\n",
      "# Iteration  6242 -> Loss: 0.3850077571321366 \t| Accuracy: 105.000\n",
      "# Iteration  6243 -> Loss: 0.3849881677402129 \t| Accuracy: 105.000\n",
      "# Iteration  6244 -> Loss: 0.3849685848519975 \t| Accuracy: 105.000\n",
      "# Iteration  6245 -> Loss: 0.3849490084638989 \t| Accuracy: 105.000\n",
      "# Iteration  6246 -> Loss: 0.384929438572328 \t| Accuracy: 105.000\n",
      "# Iteration  6247 -> Loss: 0.3849098751736985 \t| Accuracy: 105.000\n",
      "# Iteration  6248 -> Loss: 0.38489031826442616 \t| Accuracy: 105.000\n",
      "# Iteration  6249 -> Loss: 0.38487076784092966 \t| Accuracy: 105.000\n",
      "# Iteration  6250 -> Loss: 0.38485122389962995 \t| Accuracy: 105.000\n",
      "# Iteration  6251 -> Loss: 0.3848316864369504 \t| Accuracy: 105.000\n",
      "# Iteration  6252 -> Loss: 0.3848121554493173 \t| Accuracy: 105.000\n",
      "# Iteration  6253 -> Loss: 0.38479263093315874 \t| Accuracy: 105.000\n",
      "# Iteration  6254 -> Loss: 0.38477311288490584 \t| Accuracy: 105.000\n",
      "# Iteration  6255 -> Loss: 0.38475360130099207 \t| Accuracy: 105.000\n",
      "# Iteration  6256 -> Loss: 0.3847340961778533 \t| Accuracy: 105.000\n",
      "# Iteration  6257 -> Loss: 0.384714597511928 \t| Accuracy: 105.000\n",
      "# Iteration  6258 -> Loss: 0.38469510529965706 \t| Accuracy: 105.000\n",
      "# Iteration  6259 -> Loss: 0.38467561953748386 \t| Accuracy: 105.000\n",
      "# Iteration  6260 -> Loss: 0.3846561402218541 \t| Accuracy: 105.000\n",
      "# Iteration  6261 -> Loss: 0.3846366673492163 \t| Accuracy: 105.000\n",
      "# Iteration  6262 -> Loss: 0.3846172009160211 \t| Accuracy: 105.000\n",
      "# Iteration  6263 -> Loss: 0.3845977409187218 \t| Accuracy: 105.000\n",
      "# Iteration  6264 -> Loss: 0.3845782873537742 \t| Accuracy: 105.000\n",
      "# Iteration  6265 -> Loss: 0.3845588402176363 \t| Accuracy: 105.000\n",
      "# Iteration  6266 -> Loss: 0.384539399506769 \t| Accuracy: 105.000\n",
      "# Iteration  6267 -> Loss: 0.3845199652176352 \t| Accuracy: 105.000\n",
      "# Iteration  6268 -> Loss: 0.38450053734670064 \t| Accuracy: 105.000\n",
      "# Iteration  6269 -> Loss: 0.38448111589043327 \t| Accuracy: 105.000\n",
      "# Iteration  6270 -> Loss: 0.3844617008453035 \t| Accuracy: 105.000\n",
      "# Iteration  6271 -> Loss: 0.3844422922077844 \t| Accuracy: 105.000\n",
      "# Iteration  6272 -> Loss: 0.38442288997435137 \t| Accuracy: 105.000\n",
      "# Iteration  6273 -> Loss: 0.3844034941414821 \t| Accuracy: 105.000\n",
      "# Iteration  6274 -> Loss: 0.3843841047056571 \t| Accuracy: 105.000\n",
      "# Iteration  6275 -> Loss: 0.38436472166335883 \t| Accuracy: 105.000\n",
      "# Iteration  6276 -> Loss: 0.3843453450110726 \t| Accuracy: 105.000\n",
      "# Iteration  6277 -> Loss: 0.38432597474528607 \t| Accuracy: 105.000\n",
      "# Iteration  6278 -> Loss: 0.38430661086248935 \t| Accuracy: 105.000\n",
      "# Iteration  6279 -> Loss: 0.38428725335917474 \t| Accuracy: 105.000\n",
      "# Iteration  6280 -> Loss: 0.38426790223183716 \t| Accuracy: 105.000\n",
      "# Iteration  6281 -> Loss: 0.3842485574769742 \t| Accuracy: 105.000\n",
      "# Iteration  6282 -> Loss: 0.38422921909108537 \t| Accuracy: 105.000\n",
      "# Iteration  6283 -> Loss: 0.38420988707067316 \t| Accuracy: 105.000\n",
      "# Iteration  6284 -> Loss: 0.384190561412242 \t| Accuracy: 105.000\n",
      "# Iteration  6285 -> Loss: 0.3841712421122991 \t| Accuracy: 105.000\n",
      "# Iteration  6286 -> Loss: 0.38415192916735386 \t| Accuracy: 105.000\n",
      "# Iteration  6287 -> Loss: 0.38413262257391817 \t| Accuracy: 105.000\n",
      "# Iteration  6288 -> Loss: 0.3841133223285064 \t| Accuracy: 105.000\n",
      "# Iteration  6289 -> Loss: 0.38409402842763535 \t| Accuracy: 105.000\n",
      "# Iteration  6290 -> Loss: 0.3840747408678241 \t| Accuracy: 105.000\n",
      "# Iteration  6291 -> Loss: 0.3840554596455943 \t| Accuracy: 105.000\n",
      "# Iteration  6292 -> Loss: 0.3840361847574698 \t| Accuracy: 105.000\n",
      "# Iteration  6293 -> Loss: 0.38401691619997713 \t| Accuracy: 105.000\n",
      "# Iteration  6294 -> Loss: 0.383997653969645 \t| Accuracy: 105.000\n",
      "# Iteration  6295 -> Loss: 0.3839783980630047 \t| Accuracy: 105.000\n",
      "# Iteration  6296 -> Loss: 0.38395914847658974 \t| Accuracy: 105.000\n",
      "# Iteration  6297 -> Loss: 0.38393990520693616 \t| Accuracy: 105.000\n",
      "# Iteration  6298 -> Loss: 0.3839206682505824 \t| Accuracy: 105.000\n",
      "# Iteration  6299 -> Loss: 0.3839014376040692 \t| Accuracy: 105.000\n",
      "# Iteration  6300 -> Loss: 0.3838822132639398 \t| Accuracy: 105.000\n",
      "# Iteration  6301 -> Loss: 0.3838629952267398 \t| Accuracy: 105.000\n",
      "# Iteration  6302 -> Loss: 0.38384378348901693 \t| Accuracy: 105.000\n",
      "# Iteration  6303 -> Loss: 0.38382457804732195 \t| Accuracy: 105.000\n",
      "# Iteration  6304 -> Loss: 0.38380537889820754 \t| Accuracy: 105.000\n",
      "# Iteration  6305 -> Loss: 0.3837861860382285 \t| Accuracy: 105.000\n",
      "# Iteration  6306 -> Loss: 0.38376699946394266 \t| Accuracy: 105.000\n",
      "# Iteration  6307 -> Loss: 0.38374781917190975 \t| Accuracy: 105.000\n",
      "# Iteration  6308 -> Loss: 0.38372864515869237 \t| Accuracy: 105.000\n",
      "# Iteration  6309 -> Loss: 0.3837094774208548 \t| Accuracy: 105.000\n",
      "# Iteration  6310 -> Loss: 0.3836903159549642 \t| Accuracy: 105.000\n",
      "# Iteration  6311 -> Loss: 0.38367116075758995 \t| Accuracy: 105.000\n",
      "# Iteration  6312 -> Loss: 0.3836520118253039 \t| Accuracy: 105.000\n",
      "# Iteration  6313 -> Loss: 0.38363286915468015 \t| Accuracy: 105.000\n",
      "# Iteration  6314 -> Loss: 0.38361373274229515 \t| Accuracy: 105.000\n",
      "# Iteration  6315 -> Loss: 0.3835946025847279 \t| Accuracy: 105.000\n",
      "# Iteration  6316 -> Loss: 0.38357547867855957 \t| Accuracy: 105.000\n",
      "# Iteration  6317 -> Loss: 0.3835563610203737 \t| Accuracy: 105.000\n",
      "# Iteration  6318 -> Loss: 0.3835372496067562 \t| Accuracy: 105.000\n",
      "# Iteration  6319 -> Loss: 0.3835181444342956 \t| Accuracy: 105.000\n",
      "# Iteration  6320 -> Loss: 0.3834990454995825 \t| Accuracy: 105.000\n",
      "# Iteration  6321 -> Loss: 0.3834799527992098 \t| Accuracy: 105.000\n",
      "# Iteration  6322 -> Loss: 0.383460866329773 \t| Accuracy: 105.000\n",
      "# Iteration  6323 -> Loss: 0.3834417860878697 \t| Accuracy: 105.000\n",
      "# Iteration  6324 -> Loss: 0.3834227120701001 \t| Accuracy: 105.000\n",
      "# Iteration  6325 -> Loss: 0.38340364427306645 \t| Accuracy: 105.000\n",
      "# Iteration  6326 -> Loss: 0.3833845826933736 \t| Accuracy: 105.000\n",
      "# Iteration  6327 -> Loss: 0.38336552732762863 \t| Accuracy: 105.000\n",
      "# Iteration  6328 -> Loss: 0.3833464781724411 \t| Accuracy: 105.000\n",
      "# Iteration  6329 -> Loss: 0.38332743522442264 \t| Accuracy: 105.000\n",
      "# Iteration  6330 -> Loss: 0.3833083984801874 \t| Accuracy: 105.000\n",
      "# Iteration  6331 -> Loss: 0.3832893679363518 \t| Accuracy: 105.000\n",
      "# Iteration  6332 -> Loss: 0.3832703435895347 \t| Accuracy: 105.000\n",
      "# Iteration  6333 -> Loss: 0.38325132543635715 \t| Accuracy: 105.000\n",
      "# Iteration  6334 -> Loss: 0.38323231347344255 \t| Accuracy: 105.000\n",
      "# Iteration  6335 -> Loss: 0.3832133076974166 \t| Accuracy: 105.000\n",
      "# Iteration  6336 -> Loss: 0.3831943081049077 \t| Accuracy: 105.000\n",
      "# Iteration  6337 -> Loss: 0.38317531469254584 \t| Accuracy: 105.000\n",
      "# Iteration  6338 -> Loss: 0.3831563274569641 \t| Accuracy: 105.000\n",
      "# Iteration  6339 -> Loss: 0.38313734639479735 \t| Accuracy: 105.000\n",
      "# Iteration  6340 -> Loss: 0.3831183715026829 \t| Accuracy: 105.000\n",
      "# Iteration  6341 -> Loss: 0.3830994027772605 \t| Accuracy: 105.000\n",
      "# Iteration  6342 -> Loss: 0.3830804402151722 \t| Accuracy: 105.000\n",
      "# Iteration  6343 -> Loss: 0.3830614838130622 \t| Accuracy: 105.000\n",
      "# Iteration  6344 -> Loss: 0.38304253356757717 \t| Accuracy: 105.000\n",
      "# Iteration  6345 -> Loss: 0.38302358947536597 \t| Accuracy: 105.000\n",
      "# Iteration  6346 -> Loss: 0.38300465153307983 \t| Accuracy: 105.000\n",
      "# Iteration  6347 -> Loss: 0.38298571973737233 \t| Accuracy: 105.000\n",
      "# Iteration  6348 -> Loss: 0.3829667940848992 \t| Accuracy: 105.000\n",
      "# Iteration  6349 -> Loss: 0.3829478745723187 \t| Accuracy: 105.000\n",
      "# Iteration  6350 -> Loss: 0.3829289611962911 \t| Accuracy: 105.000\n",
      "# Iteration  6351 -> Loss: 0.38291005395347927 \t| Accuracy: 105.000\n",
      "# Iteration  6352 -> Loss: 0.3828911528405481 \t| Accuracy: 105.000\n",
      "# Iteration  6353 -> Loss: 0.3828722578541649 \t| Accuracy: 105.000\n",
      "# Iteration  6354 -> Loss: 0.38285336899099925 \t| Accuracy: 105.000\n",
      "# Iteration  6355 -> Loss: 0.38283448624772315 \t| Accuracy: 105.000\n",
      "# Iteration  6356 -> Loss: 0.38281560962101063 \t| Accuracy: 105.000\n",
      "# Iteration  6357 -> Loss: 0.3827967391075382 \t| Accuracy: 105.000\n",
      "# Iteration  6358 -> Loss: 0.3827778747039846 \t| Accuracy: 105.000\n",
      "# Iteration  6359 -> Loss: 0.3827590164070308 \t| Accuracy: 105.000\n",
      "# Iteration  6360 -> Loss: 0.38274016421336016 \t| Accuracy: 105.000\n",
      "# Iteration  6361 -> Loss: 0.3827213181196581 \t| Accuracy: 105.000\n",
      "# Iteration  6362 -> Loss: 0.3827024781226126 \t| Accuracy: 105.000\n",
      "# Iteration  6363 -> Loss: 0.38268364421891377 \t| Accuracy: 105.000\n",
      "# Iteration  6364 -> Loss: 0.3826648164052539 \t| Accuracy: 105.000\n",
      "# Iteration  6365 -> Loss: 0.3826459946783278 \t| Accuracy: 105.000\n",
      "# Iteration  6366 -> Loss: 0.38262717903483223 \t| Accuracy: 105.000\n",
      "# Iteration  6367 -> Loss: 0.3826083694714664 \t| Accuracy: 105.000\n",
      "# Iteration  6368 -> Loss: 0.3825895659849319 \t| Accuracy: 105.000\n",
      "# Iteration  6369 -> Loss: 0.38257076857193234 \t| Accuracy: 105.000\n",
      "# Iteration  6370 -> Loss: 0.3825519772291736 \t| Accuracy: 105.000\n",
      "# Iteration  6371 -> Loss: 0.38253319195336405 \t| Accuracy: 105.000\n",
      "# Iteration  6372 -> Loss: 0.38251441274121417 \t| Accuracy: 105.000\n",
      "# Iteration  6373 -> Loss: 0.38249563958943655 \t| Accuracy: 105.000\n",
      "# Iteration  6374 -> Loss: 0.38247687249474643 \t| Accuracy: 105.000\n",
      "# Iteration  6375 -> Loss: 0.3824581114538609 \t| Accuracy: 105.000\n",
      "# Iteration  6376 -> Loss: 0.3824393564634994 \t| Accuracy: 105.000\n",
      "# Iteration  6377 -> Loss: 0.38242060752038387 \t| Accuracy: 105.000\n",
      "# Iteration  6378 -> Loss: 0.38240186462123804 \t| Accuracy: 105.000\n",
      "# Iteration  6379 -> Loss: 0.38238312776278827 \t| Accuracy: 105.000\n",
      "# Iteration  6380 -> Loss: 0.38236439694176305 \t| Accuracy: 105.000\n",
      "# Iteration  6381 -> Loss: 0.3823456721548932 \t| Accuracy: 105.000\n",
      "# Iteration  6382 -> Loss: 0.38232695339891126 \t| Accuracy: 105.000\n",
      "# Iteration  6383 -> Loss: 0.38230824067055275 \t| Accuracy: 105.000\n",
      "# Iteration  6384 -> Loss: 0.3822895339665551 \t| Accuracy: 105.000\n",
      "# Iteration  6385 -> Loss: 0.38227083328365774 \t| Accuracy: 105.000\n",
      "# Iteration  6386 -> Loss: 0.38225213861860285 \t| Accuracy: 105.000\n",
      "# Iteration  6387 -> Loss: 0.3822334499681342 \t| Accuracy: 105.000\n",
      "# Iteration  6388 -> Loss: 0.38221476732899834 \t| Accuracy: 105.000\n",
      "# Iteration  6389 -> Loss: 0.3821960906979438 \t| Accuracy: 105.000\n",
      "# Iteration  6390 -> Loss: 0.38217742007172123 \t| Accuracy: 105.000\n",
      "# Iteration  6391 -> Loss: 0.38215875544708394 \t| Accuracy: 105.000\n",
      "# Iteration  6392 -> Loss: 0.3821400968207868 \t| Accuracy: 105.000\n",
      "# Iteration  6393 -> Loss: 0.3821214441895873 \t| Accuracy: 105.000\n",
      "# Iteration  6394 -> Loss: 0.3821027975502454 \t| Accuracy: 105.000\n",
      "# Iteration  6395 -> Loss: 0.38208415689952263 \t| Accuracy: 105.000\n",
      "# Iteration  6396 -> Loss: 0.38206552223418316 \t| Accuracy: 105.000\n",
      "# Iteration  6397 -> Loss: 0.3820468935509934 \t| Accuracy: 105.000\n",
      "# Iteration  6398 -> Loss: 0.38202827084672175 \t| Accuracy: 105.000\n",
      "# Iteration  6399 -> Loss: 0.3820096541181389 \t| Accuracy: 105.000\n",
      "# Iteration  6400 -> Loss: 0.38199104336201783 \t| Accuracy: 105.000\n",
      "# Iteration  6401 -> Loss: 0.3819724385751336 \t| Accuracy: 105.000\n",
      "# Iteration  6402 -> Loss: 0.3819538397542635 \t| Accuracy: 105.000\n",
      "# Iteration  6403 -> Loss: 0.38193524689618713 \t| Accuracy: 105.000\n",
      "# Iteration  6404 -> Loss: 0.38191665999768615 \t| Accuracy: 105.000\n",
      "# Iteration  6405 -> Loss: 0.3818980790555444 \t| Accuracy: 105.000\n",
      "# Iteration  6406 -> Loss: 0.38187950406654814 \t| Accuracy: 105.000\n",
      "# Iteration  6407 -> Loss: 0.3818609350274856 \t| Accuracy: 105.000\n",
      "# Iteration  6408 -> Loss: 0.3818423719351473 \t| Accuracy: 105.000\n",
      "# Iteration  6409 -> Loss: 0.3818238147863257 \t| Accuracy: 105.000\n",
      "# Iteration  6410 -> Loss: 0.38180526357781597 \t| Accuracy: 105.000\n",
      "# Iteration  6411 -> Loss: 0.381786718306415 \t| Accuracy: 105.000\n",
      "# Iteration  6412 -> Loss: 0.38176817896892185 \t| Accuracy: 105.000\n",
      "# Iteration  6413 -> Loss: 0.38174964556213825 \t| Accuracy: 105.000\n",
      "# Iteration  6414 -> Loss: 0.3817311180828676 \t| Accuracy: 105.000\n",
      "# Iteration  6415 -> Loss: 0.3817125965279158 \t| Accuracy: 105.000\n",
      "# Iteration  6416 -> Loss: 0.3816940808940906 \t| Accuracy: 105.000\n",
      "# Iteration  6417 -> Loss: 0.38167557117820233 \t| Accuracy: 105.000\n",
      "# Iteration  6418 -> Loss: 0.3816570673770631 \t| Accuracy: 105.000\n",
      "# Iteration  6419 -> Loss: 0.38163856948748753 \t| Accuracy: 105.000\n",
      "# Iteration  6420 -> Loss: 0.3816200775062921 \t| Accuracy: 105.000\n",
      "# Iteration  6421 -> Loss: 0.38160159143029576 \t| Accuracy: 105.000\n",
      "# Iteration  6422 -> Loss: 0.38158311125631944 \t| Accuracy: 105.000\n",
      "# Iteration  6423 -> Loss: 0.3815646369811861 \t| Accuracy: 105.000\n",
      "# Iteration  6424 -> Loss: 0.3815461686017212 \t| Accuracy: 105.000\n",
      "# Iteration  6425 -> Loss: 0.3815277061147521 \t| Accuracy: 105.000\n",
      "# Iteration  6426 -> Loss: 0.3815092495171086 \t| Accuracy: 105.000\n",
      "# Iteration  6427 -> Loss: 0.3814907988056222 \t| Accuracy: 105.000\n",
      "# Iteration  6428 -> Loss: 0.381472353977127 \t| Accuracy: 105.000\n",
      "# Iteration  6429 -> Loss: 0.381453915028459 \t| Accuracy: 105.000\n",
      "# Iteration  6430 -> Loss: 0.3814354819564565 \t| Accuracy: 105.000\n",
      "# Iteration  6431 -> Loss: 0.3814170547579598 \t| Accuracy: 105.000\n",
      "# Iteration  6432 -> Loss: 0.3813986334298114 \t| Accuracy: 105.000\n",
      "# Iteration  6433 -> Loss: 0.381380217968856 \t| Accuracy: 105.000\n",
      "# Iteration  6434 -> Loss: 0.38136180837194045 \t| Accuracy: 105.000\n",
      "# Iteration  6435 -> Loss: 0.38134340463591376 \t| Accuracy: 105.000\n",
      "# Iteration  6436 -> Loss: 0.38132500675762687 \t| Accuracy: 105.000\n",
      "# Iteration  6437 -> Loss: 0.3813066147339331 \t| Accuracy: 105.000\n",
      "# Iteration  6438 -> Loss: 0.38128822856168787 \t| Accuracy: 105.000\n",
      "# Iteration  6439 -> Loss: 0.38126984823774873 \t| Accuracy: 105.000\n",
      "# Iteration  6440 -> Loss: 0.3812514737589751 \t| Accuracy: 105.000\n",
      "# Iteration  6441 -> Loss: 0.38123310512222897 \t| Accuracy: 105.000\n",
      "# Iteration  6442 -> Loss: 0.38121474232437413 \t| Accuracy: 105.000\n",
      "# Iteration  6443 -> Loss: 0.38119638536227674 \t| Accuracy: 105.000\n",
      "# Iteration  6444 -> Loss: 0.38117803423280494 \t| Accuracy: 105.000\n",
      "# Iteration  6445 -> Loss: 0.38115968893282876 \t| Accuracy: 105.000\n",
      "# Iteration  6446 -> Loss: 0.381141349459221 \t| Accuracy: 105.000\n",
      "# Iteration  6447 -> Loss: 0.38112301580885605 \t| Accuracy: 105.000\n",
      "# Iteration  6448 -> Loss: 0.38110468797861036 \t| Accuracy: 105.000\n",
      "# Iteration  6449 -> Loss: 0.38108636596536294 \t| Accuracy: 105.000\n",
      "# Iteration  6450 -> Loss: 0.3810680497659946 \t| Accuracy: 105.000\n",
      "# Iteration  6451 -> Loss: 0.3810497393773885 \t| Accuracy: 105.000\n",
      "# Iteration  6452 -> Loss: 0.3810314347964295 \t| Accuracy: 105.000\n",
      "# Iteration  6453 -> Loss: 0.38101313602000497 \t| Accuracy: 105.000\n",
      "# Iteration  6454 -> Loss: 0.3809948430450042 \t| Accuracy: 105.000\n",
      "# Iteration  6455 -> Loss: 0.38097655586831874 \t| Accuracy: 105.000\n",
      "# Iteration  6456 -> Loss: 0.38095827448684194 \t| Accuracy: 105.000\n",
      "# Iteration  6457 -> Loss: 0.38093999889746977 \t| Accuracy: 105.000\n",
      "# Iteration  6458 -> Loss: 0.38092172909709976 \t| Accuracy: 105.000\n",
      "# Iteration  6459 -> Loss: 0.3809034650826318 \t| Accuracy: 105.000\n",
      "# Iteration  6460 -> Loss: 0.38088520685096783 \t| Accuracy: 105.000\n",
      "# Iteration  6461 -> Loss: 0.3808669543990121 \t| Accuracy: 105.000\n",
      "# Iteration  6462 -> Loss: 0.3808487077236706 \t| Accuracy: 105.000\n",
      "# Iteration  6463 -> Loss: 0.3808304668218516 \t| Accuracy: 105.000\n",
      "# Iteration  6464 -> Loss: 0.38081223169046535 \t| Accuracy: 105.000\n",
      "# Iteration  6465 -> Loss: 0.3807940023264245 \t| Accuracy: 105.000\n",
      "# Iteration  6466 -> Loss: 0.3807757787266436 \t| Accuracy: 105.000\n",
      "# Iteration  6467 -> Loss: 0.3807575608880389 \t| Accuracy: 105.000\n",
      "# Iteration  6468 -> Loss: 0.38073934880752947 \t| Accuracy: 105.000\n",
      "# Iteration  6469 -> Loss: 0.38072114248203603 \t| Accuracy: 105.000\n",
      "# Iteration  6470 -> Loss: 0.3807029419084812 \t| Accuracy: 105.000\n",
      "# Iteration  6471 -> Loss: 0.38068474708379024 \t| Accuracy: 105.000\n",
      "# Iteration  6472 -> Loss: 0.38066655800489 \t| Accuracy: 105.000\n",
      "# Iteration  6473 -> Loss: 0.3806483746687097 \t| Accuracy: 105.000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Iteration  6474 -> Loss: 0.3806301970721804 \t| Accuracy: 105.000\n",
      "# Iteration  6475 -> Loss: 0.3806120252122354 \t| Accuracy: 105.000\n",
      "# Iteration  6476 -> Loss: 0.38059385908581006 \t| Accuracy: 105.000\n",
      "# Iteration  6477 -> Loss: 0.38057569868984176 \t| Accuracy: 105.000\n",
      "# Iteration  6478 -> Loss: 0.38055754402127007 \t| Accuracy: 105.000\n",
      "# Iteration  6479 -> Loss: 0.38053939507703627 \t| Accuracy: 105.000\n",
      "# Iteration  6480 -> Loss: 0.38052125185408414 \t| Accuracy: 105.000\n",
      "# Iteration  6481 -> Loss: 0.38050311434935935 \t| Accuracy: 105.000\n",
      "# Iteration  6482 -> Loss: 0.3804849825598096 \t| Accuracy: 105.000\n",
      "# Iteration  6483 -> Loss: 0.3804668564823847 \t| Accuracy: 105.000\n",
      "# Iteration  6484 -> Loss: 0.38044873611403646 \t| Accuracy: 105.000\n",
      "# Iteration  6485 -> Loss: 0.3804306214517188 \t| Accuracy: 105.000\n",
      "# Iteration  6486 -> Loss: 0.38041251249238767 \t| Accuracy: 105.000\n",
      "# Iteration  6487 -> Loss: 0.3803944092330011 \t| Accuracy: 105.000\n",
      "# Iteration  6488 -> Loss: 0.38037631167051916 \t| Accuracy: 105.000\n",
      "# Iteration  6489 -> Loss: 0.380358219801904 \t| Accuracy: 105.000\n",
      "# Iteration  6490 -> Loss: 0.3803401336241198 \t| Accuracy: 105.000\n",
      "# Iteration  6491 -> Loss: 0.38032205313413264 \t| Accuracy: 105.000\n",
      "# Iteration  6492 -> Loss: 0.38030397832891094 \t| Accuracy: 105.000\n",
      "# Iteration  6493 -> Loss: 0.38028590920542504 \t| Accuracy: 105.000\n",
      "# Iteration  6494 -> Loss: 0.3802678457606471 \t| Accuracy: 105.000\n",
      "# Iteration  6495 -> Loss: 0.38024978799155174 \t| Accuracy: 105.000\n",
      "# Iteration  6496 -> Loss: 0.3802317358951151 \t| Accuracy: 105.000\n",
      "# Iteration  6497 -> Loss: 0.38021368946831574 \t| Accuracy: 105.000\n",
      "# Iteration  6498 -> Loss: 0.3801956487081345 \t| Accuracy: 105.000\n",
      "# Iteration  6499 -> Loss: 0.3801776136115535 \t| Accuracy: 105.000\n",
      "# Iteration  6500 -> Loss: 0.38015958417555745 \t| Accuracy: 105.000\n",
      "# Iteration  6501 -> Loss: 0.3801415603971331 \t| Accuracy: 105.000\n",
      "# Iteration  6502 -> Loss: 0.38012354227326883 \t| Accuracy: 105.000\n",
      "# Iteration  6503 -> Loss: 0.3801055298009555 \t| Accuracy: 105.000\n",
      "# Iteration  6504 -> Loss: 0.3800875229771856 \t| Accuracy: 105.000\n",
      "# Iteration  6505 -> Loss: 0.3800695217989542 \t| Accuracy: 105.000\n",
      "# Iteration  6506 -> Loss: 0.3800515262632576 \t| Accuracy: 105.000\n",
      "# Iteration  6507 -> Loss: 0.3800335363670948 \t| Accuracy: 105.000\n",
      "# Iteration  6508 -> Loss: 0.38001555210746646 \t| Accuracy: 105.000\n",
      "# Iteration  6509 -> Loss: 0.37999757348137564 \t| Accuracy: 105.000\n",
      "# Iteration  6510 -> Loss: 0.3799796004858268 \t| Accuracy: 105.000\n",
      "# Iteration  6511 -> Loss: 0.379961633117827 \t| Accuracy: 105.000\n",
      "# Iteration  6512 -> Loss: 0.3799436713743849 \t| Accuracy: 105.000\n",
      "# Iteration  6513 -> Loss: 0.3799257152525115 \t| Accuracy: 105.000\n",
      "# Iteration  6514 -> Loss: 0.3799077647492196 \t| Accuracy: 105.000\n",
      "# Iteration  6515 -> Loss: 0.37988981986152415 \t| Accuracy: 105.000\n",
      "# Iteration  6516 -> Loss: 0.3798718805864419 \t| Accuracy: 105.000\n",
      "# Iteration  6517 -> Loss: 0.37985394692099184 \t| Accuracy: 105.000\n",
      "# Iteration  6518 -> Loss: 0.3798360188621947 \t| Accuracy: 105.000\n",
      "# Iteration  6519 -> Loss: 0.3798180964070736 \t| Accuracy: 105.000\n",
      "# Iteration  6520 -> Loss: 0.3798001795526531 \t| Accuracy: 105.000\n",
      "# Iteration  6521 -> Loss: 0.37978226829596035 \t| Accuracy: 105.000\n",
      "# Iteration  6522 -> Loss: 0.3797643626340242 \t| Accuracy: 105.000\n",
      "# Iteration  6523 -> Loss: 0.3797464625638755 \t| Accuracy: 105.000\n",
      "# Iteration  6524 -> Loss: 0.3797285680825471 \t| Accuracy: 105.000\n",
      "# Iteration  6525 -> Loss: 0.3797106791870739 \t| Accuracy: 105.000\n",
      "# Iteration  6526 -> Loss: 0.3796927958744926 \t| Accuracy: 105.000\n",
      "# Iteration  6527 -> Loss: 0.37967491814184223 \t| Accuracy: 105.000\n",
      "# Iteration  6528 -> Loss: 0.3796570459861636 \t| Accuracy: 105.000\n",
      "# Iteration  6529 -> Loss: 0.37963917940449954 \t| Accuracy: 105.000\n",
      "# Iteration  6530 -> Loss: 0.3796213183938947 \t| Accuracy: 105.000\n",
      "# Iteration  6531 -> Loss: 0.379603462951396 \t| Accuracy: 105.000\n",
      "# Iteration  6532 -> Loss: 0.37958561307405225 \t| Accuracy: 105.000\n",
      "# Iteration  6533 -> Loss: 0.37956776875891396 \t| Accuracy: 105.000\n",
      "# Iteration  6534 -> Loss: 0.3795499300030342 \t| Accuracy: 105.000\n",
      "# Iteration  6535 -> Loss: 0.37953209680346744 \t| Accuracy: 105.000\n",
      "# Iteration  6536 -> Loss: 0.3795142691572702 \t| Accuracy: 105.000\n",
      "# Iteration  6537 -> Loss: 0.3794964470615016 \t| Accuracy: 105.000\n",
      "# Iteration  6538 -> Loss: 0.3794786305132219 \t| Accuracy: 105.000\n",
      "# Iteration  6539 -> Loss: 0.3794608195094937 \t| Accuracy: 105.000\n",
      "# Iteration  6540 -> Loss: 0.3794430140473816 \t| Accuracy: 105.000\n",
      "# Iteration  6541 -> Loss: 0.37942521412395225 \t| Accuracy: 105.000\n",
      "# Iteration  6542 -> Loss: 0.37940741973627407 \t| Accuracy: 105.000\n",
      "# Iteration  6543 -> Loss: 0.3793896308814175 \t| Accuracy: 105.000\n",
      "# Iteration  6544 -> Loss: 0.3793718475564549 \t| Accuracy: 105.000\n",
      "# Iteration  6545 -> Loss: 0.3793540697584607 \t| Accuracy: 105.000\n",
      "# Iteration  6546 -> Loss: 0.3793362974845112 \t| Accuracy: 105.000\n",
      "# Iteration  6547 -> Loss: 0.3793185307316848 \t| Accuracy: 105.000\n",
      "# Iteration  6548 -> Loss: 0.3793007694970616 \t| Accuracy: 105.000\n",
      "# Iteration  6549 -> Loss: 0.3792830137777239 \t| Accuracy: 105.000\n",
      "# Iteration  6550 -> Loss: 0.3792652635707558 \t| Accuracy: 105.000\n",
      "# Iteration  6551 -> Loss: 0.37924751887324354 \t| Accuracy: 105.000\n",
      "# Iteration  6552 -> Loss: 0.379229779682275 \t| Accuracy: 105.000\n",
      "# Iteration  6553 -> Loss: 0.37921204599494035 \t| Accuracy: 105.000\n",
      "# Iteration  6554 -> Loss: 0.3791943178083314 \t| Accuracy: 105.000\n",
      "# Iteration  6555 -> Loss: 0.3791765951195421 \t| Accuracy: 105.000\n",
      "# Iteration  6556 -> Loss: 0.3791588779256683 \t| Accuracy: 105.000\n",
      "# Iteration  6557 -> Loss: 0.37914116622380784 \t| Accuracy: 105.000\n",
      "# Iteration  6558 -> Loss: 0.3791234600110603 \t| Accuracy: 105.000\n",
      "# Iteration  6559 -> Loss: 0.3791057592845275 \t| Accuracy: 105.000\n",
      "# Iteration  6560 -> Loss: 0.37908806404131284 \t| Accuracy: 105.000\n",
      "# Iteration  6561 -> Loss: 0.37907037427852214 \t| Accuracy: 105.000\n",
      "# Iteration  6562 -> Loss: 0.3790526899932625 \t| Accuracy: 105.000\n",
      "# Iteration  6563 -> Loss: 0.3790350111826437 \t| Accuracy: 105.000\n",
      "# Iteration  6564 -> Loss: 0.37901733784377684 \t| Accuracy: 105.000\n",
      "# Iteration  6565 -> Loss: 0.37899966997377527 \t| Accuracy: 105.000\n",
      "# Iteration  6566 -> Loss: 0.37898200756975414 \t| Accuracy: 105.000\n",
      "# Iteration  6567 -> Loss: 0.3789643506288306 \t| Accuracy: 105.000\n",
      "# Iteration  6568 -> Loss: 0.37894669914812357 \t| Accuracy: 105.000\n",
      "# Iteration  6569 -> Loss: 0.3789290531247541 \t| Accuracy: 105.000\n",
      "# Iteration  6570 -> Loss: 0.3789114125558452 \t| Accuracy: 105.000\n",
      "# Iteration  6571 -> Loss: 0.37889377743852154 \t| Accuracy: 105.000\n",
      "# Iteration  6572 -> Loss: 0.37887614776990985 \t| Accuracy: 105.000\n",
      "# Iteration  6573 -> Loss: 0.3788585235471388 \t| Accuracy: 105.000\n",
      "# Iteration  6574 -> Loss: 0.3788409047673389 \t| Accuracy: 105.000\n",
      "# Iteration  6575 -> Loss: 0.37882329142764276 \t| Accuracy: 105.000\n",
      "# Iteration  6576 -> Loss: 0.37880568352518457 \t| Accuracy: 105.000\n",
      "# Iteration  6577 -> Loss: 0.3787880810571007 \t| Accuracy: 105.000\n",
      "# Iteration  6578 -> Loss: 0.3787704840205295 \t| Accuracy: 105.000\n",
      "# Iteration  6579 -> Loss: 0.37875289241261084 \t| Accuracy: 105.000\n",
      "# Iteration  6580 -> Loss: 0.37873530623048696 \t| Accuracy: 105.000\n",
      "# Iteration  6581 -> Loss: 0.37871772547130167 \t| Accuracy: 105.000\n",
      "# Iteration  6582 -> Loss: 0.3787001501322007 \t| Accuracy: 105.000\n",
      "# Iteration  6583 -> Loss: 0.378682580210332 \t| Accuracy: 105.000\n",
      "# Iteration  6584 -> Loss: 0.37866501570284505 \t| Accuracy: 105.000\n",
      "# Iteration  6585 -> Loss: 0.3786474566068916 \t| Accuracy: 105.000\n",
      "# Iteration  6586 -> Loss: 0.37862990291962473 \t| Accuracy: 105.000\n",
      "# Iteration  6587 -> Loss: 0.3786123546382 \t| Accuracy: 105.000\n",
      "# Iteration  6588 -> Loss: 0.37859481175977455 \t| Accuracy: 105.000\n",
      "# Iteration  6589 -> Loss: 0.3785772742815075 \t| Accuracy: 105.000\n",
      "# Iteration  6590 -> Loss: 0.3785597422005599 \t| Accuracy: 105.000\n",
      "# Iteration  6591 -> Loss: 0.3785422155140946 \t| Accuracy: 105.000\n",
      "# Iteration  6592 -> Loss: 0.3785246942192763 \t| Accuracy: 105.000\n",
      "# Iteration  6593 -> Loss: 0.3785071783132718 \t| Accuracy: 105.000\n",
      "# Iteration  6594 -> Loss: 0.3784896677932496 \t| Accuracy: 105.000\n",
      "# Iteration  6595 -> Loss: 0.3784721626563802 \t| Accuracy: 105.000\n",
      "# Iteration  6596 -> Loss: 0.37845466289983576 \t| Accuracy: 105.000\n",
      "# Iteration  6597 -> Loss: 0.3784371685207907 \t| Accuracy: 105.000\n",
      "# Iteration  6598 -> Loss: 0.3784196795164209 \t| Accuracy: 105.000\n",
      "# Iteration  6599 -> Loss: 0.3784021958839044 \t| Accuracy: 105.000\n",
      "# Iteration  6600 -> Loss: 0.3783847176204211 \t| Accuracy: 105.000\n",
      "# Iteration  6601 -> Loss: 0.37836724472315264 \t| Accuracy: 105.000\n",
      "# Iteration  6602 -> Loss: 0.3783497771892826 \t| Accuracy: 105.000\n",
      "# Iteration  6603 -> Loss: 0.3783323150159963 \t| Accuracy: 105.000\n",
      "# Iteration  6604 -> Loss: 0.3783148582004814 \t| Accuracy: 105.000\n",
      "# Iteration  6605 -> Loss: 0.37829740673992673 \t| Accuracy: 105.000\n",
      "# Iteration  6606 -> Loss: 0.37827996063152364 \t| Accuracy: 105.000\n",
      "# Iteration  6607 -> Loss: 0.37826251987246484 \t| Accuracy: 105.000\n",
      "# Iteration  6608 -> Loss: 0.37824508445994537 \t| Accuracy: 105.000\n",
      "# Iteration  6609 -> Loss: 0.3782276543911616 \t| Accuracy: 105.000\n",
      "# Iteration  6610 -> Loss: 0.3782102296633123 \t| Accuracy: 105.000\n",
      "# Iteration  6611 -> Loss: 0.37819281027359763 \t| Accuracy: 105.000\n",
      "# Iteration  6612 -> Loss: 0.37817539621922 \t| Accuracy: 105.000\n",
      "# Iteration  6613 -> Loss: 0.37815798749738333 \t| Accuracy: 105.000\n",
      "# Iteration  6614 -> Loss: 0.3781405841052937 \t| Accuracy: 105.000\n",
      "# Iteration  6615 -> Loss: 0.3781231860401589 \t| Accuracy: 105.000\n",
      "# Iteration  6616 -> Loss: 0.37810579329918853 \t| Accuracy: 105.000\n",
      "# Iteration  6617 -> Loss: 0.37808840587959414 \t| Accuracy: 105.000\n",
      "# Iteration  6618 -> Loss: 0.3780710237785891 \t| Accuracy: 105.000\n",
      "# Iteration  6619 -> Loss: 0.3780536469933885 \t| Accuracy: 105.000\n",
      "# Iteration  6620 -> Loss: 0.37803627552120966 \t| Accuracy: 105.000\n",
      "# Iteration  6621 -> Loss: 0.3780189093592712 \t| Accuracy: 105.000\n",
      "# Iteration  6622 -> Loss: 0.37800154850479395 \t| Accuracy: 105.000\n",
      "# Iteration  6623 -> Loss: 0.37798419295500046 \t| Accuracy: 105.000\n",
      "# Iteration  6624 -> Loss: 0.3779668427071153 \t| Accuracy: 105.000\n",
      "# Iteration  6625 -> Loss: 0.37794949775836456 \t| Accuracy: 105.000\n",
      "# Iteration  6626 -> Loss: 0.3779321581059764 \t| Accuracy: 105.000\n",
      "# Iteration  6627 -> Loss: 0.3779148237471809 \t| Accuracy: 105.000\n",
      "# Iteration  6628 -> Loss: 0.3778974946792096 \t| Accuracy: 105.000\n",
      "# Iteration  6629 -> Loss: 0.3778801708992962 \t| Accuracy: 105.000\n",
      "# Iteration  6630 -> Loss: 0.3778628524046762 \t| Accuracy: 105.000\n",
      "# Iteration  6631 -> Loss: 0.3778455391925869 \t| Accuracy: 105.000\n",
      "# Iteration  6632 -> Loss: 0.3778282312602672 \t| Accuracy: 105.000\n",
      "# Iteration  6633 -> Loss: 0.37781092860495824 \t| Accuracy: 105.000\n",
      "# Iteration  6634 -> Loss: 0.37779363122390264 \t| Accuracy: 105.000\n",
      "# Iteration  6635 -> Loss: 0.37777633911434505 \t| Accuracy: 105.000\n",
      "# Iteration  6636 -> Loss: 0.37775905227353185 \t| Accuracy: 105.000\n",
      "# Iteration  6637 -> Loss: 0.37774177069871123 \t| Accuracy: 105.000\n",
      "# Iteration  6638 -> Loss: 0.37772449438713346 \t| Accuracy: 105.000\n",
      "# Iteration  6639 -> Loss: 0.37770722333605006 \t| Accuracy: 105.000\n",
      "# Iteration  6640 -> Loss: 0.3776899575427149 \t| Accuracy: 105.000\n",
      "# Iteration  6641 -> Loss: 0.37767269700438344 \t| Accuracy: 105.000\n",
      "# Iteration  6642 -> Loss: 0.37765544171831306 \t| Accuracy: 105.000\n",
      "# Iteration  6643 -> Loss: 0.3776381916817629 \t| Accuracy: 105.000\n",
      "# Iteration  6644 -> Loss: 0.37762094689199377 \t| Accuracy: 105.000\n",
      "# Iteration  6645 -> Loss: 0.3776037073462685 \t| Accuracy: 105.000\n",
      "# Iteration  6646 -> Loss: 0.37758647304185167 \t| Accuracy: 105.000\n",
      "# Iteration  6647 -> Loss: 0.3775692439760096 \t| Accuracy: 105.000\n",
      "# Iteration  6648 -> Loss: 0.3775520201460105 \t| Accuracy: 105.000\n",
      "# Iteration  6649 -> Loss: 0.3775348015491244 \t| Accuracy: 105.000\n",
      "# Iteration  6650 -> Loss: 0.37751758818262293 \t| Accuracy: 105.000\n",
      "# Iteration  6651 -> Loss: 0.3775003800437799 \t| Accuracy: 105.000\n",
      "# Iteration  6652 -> Loss: 0.37748317712987045 \t| Accuracy: 105.000\n",
      "# Iteration  6653 -> Loss: 0.37746597943817195 \t| Accuracy: 105.000\n",
      "# Iteration  6654 -> Loss: 0.3774487869659633 \t| Accuracy: 105.000\n",
      "# Iteration  6655 -> Loss: 0.37743159971052526 \t| Accuracy: 105.000\n",
      "# Iteration  6656 -> Loss: 0.3774144176691405 \t| Accuracy: 105.000\n",
      "# Iteration  6657 -> Loss: 0.3773972408390933 \t| Accuracy: 105.000\n",
      "# Iteration  6658 -> Loss: 0.37738006921766987 \t| Accuracy: 105.000\n",
      "# Iteration  6659 -> Loss: 0.37736290280215823 \t| Accuracy: 105.000\n",
      "# Iteration  6660 -> Loss: 0.37734574158984796 \t| Accuracy: 105.000\n",
      "# Iteration  6661 -> Loss: 0.37732858557803073 \t| Accuracy: 105.000\n",
      "# Iteration  6662 -> Loss: 0.37731143476399975 \t| Accuracy: 105.000\n",
      "# Iteration  6663 -> Loss: 0.37729428914505014 \t| Accuracy: 105.000\n",
      "# Iteration  6664 -> Loss: 0.37727714871847884 \t| Accuracy: 105.000\n",
      "# Iteration  6665 -> Loss: 0.37726001348158456 \t| Accuracy: 105.000\n",
      "# Iteration  6666 -> Loss: 0.3772428834316676 \t| Accuracy: 105.000\n",
      "# Iteration  6667 -> Loss: 0.37722575856603036 \t| Accuracy: 105.000\n",
      "# Iteration  6668 -> Loss: 0.37720863888197664 \t| Accuracy: 105.000\n",
      "# Iteration  6669 -> Loss: 0.37719152437681236 \t| Accuracy: 105.000\n",
      "# Iteration  6670 -> Loss: 0.3771744150478452 \t| Accuracy: 105.000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Iteration  6671 -> Loss: 0.37715731089238425 \t| Accuracy: 105.000\n",
      "# Iteration  6672 -> Loss: 0.3771402119077407 \t| Accuracy: 105.000\n",
      "# Iteration  6673 -> Loss: 0.3771231180912274 \t| Accuracy: 105.000\n",
      "# Iteration  6674 -> Loss: 0.3771060294401591 \t| Accuracy: 105.000\n",
      "# Iteration  6675 -> Loss: 0.3770889459518523 \t| Accuracy: 105.000\n",
      "# Iteration  6676 -> Loss: 0.3770718676236247 \t| Accuracy: 105.000\n",
      "# Iteration  6677 -> Loss: 0.3770547944527968 \t| Accuracy: 105.000\n",
      "# Iteration  6678 -> Loss: 0.3770377264366901 \t| Accuracy: 105.000\n",
      "# Iteration  6679 -> Loss: 0.37702066357262803 \t| Accuracy: 105.000\n",
      "# Iteration  6680 -> Loss: 0.37700360585793585 \t| Accuracy: 105.000\n",
      "# Iteration  6681 -> Loss: 0.3769865532899406 \t| Accuracy: 105.000\n",
      "# Iteration  6682 -> Loss: 0.37696950586597106 \t| Accuracy: 105.000\n",
      "# Iteration  6683 -> Loss: 0.37695246358335754 \t| Accuracy: 105.000\n",
      "# Iteration  6684 -> Loss: 0.3769354264394325 \t| Accuracy: 105.000\n",
      "# Iteration  6685 -> Loss: 0.37691839443153 \t| Accuracy: 105.000\n",
      "# Iteration  6686 -> Loss: 0.37690136755698567 \t| Accuracy: 105.000\n",
      "# Iteration  6687 -> Loss: 0.37688434581313707 \t| Accuracy: 105.000\n",
      "# Iteration  6688 -> Loss: 0.3768673291973235 \t| Accuracy: 105.000\n",
      "# Iteration  6689 -> Loss: 0.3768503177068861 \t| Accuracy: 105.000\n",
      "# Iteration  6690 -> Loss: 0.3768333113391674 \t| Accuracy: 105.000\n",
      "# Iteration  6691 -> Loss: 0.37681631009151223 \t| Accuracy: 105.000\n",
      "# Iteration  6692 -> Loss: 0.3767993139612664 \t| Accuracy: 105.000\n",
      "# Iteration  6693 -> Loss: 0.37678232294577846 \t| Accuracy: 105.000\n",
      "# Iteration  6694 -> Loss: 0.3767653370423979 \t| Accuracy: 105.000\n",
      "# Iteration  6695 -> Loss: 0.3767483562484761 \t| Accuracy: 105.000\n",
      "# Iteration  6696 -> Loss: 0.3767313805613665 \t| Accuracy: 105.000\n",
      "# Iteration  6697 -> Loss: 0.376714409978424 \t| Accuracy: 105.000\n",
      "# Iteration  6698 -> Loss: 0.3766974444970052 \t| Accuracy: 105.000\n",
      "# Iteration  6699 -> Loss: 0.37668048411446864 \t| Accuracy: 105.000\n",
      "# Iteration  6700 -> Loss: 0.3766635288281745 \t| Accuracy: 105.000\n",
      "# Iteration  6701 -> Loss: 0.3766465786354846 \t| Accuracy: 105.000\n",
      "# Iteration  6702 -> Loss: 0.3766296335337627 \t| Accuracy: 105.000\n",
      "# Iteration  6703 -> Loss: 0.37661269352037424 \t| Accuracy: 105.000\n",
      "# Iteration  6704 -> Loss: 0.3765957585926861 \t| Accuracy: 105.000\n",
      "# Iteration  6705 -> Loss: 0.37657882874806714 \t| Accuracy: 105.000\n",
      "# Iteration  6706 -> Loss: 0.37656190398388817 \t| Accuracy: 105.000\n",
      "# Iteration  6707 -> Loss: 0.37654498429752115 \t| Accuracy: 105.000\n",
      "# Iteration  6708 -> Loss: 0.3765280696863401 \t| Accuracy: 105.000\n",
      "# Iteration  6709 -> Loss: 0.376511160147721 \t| Accuracy: 105.000\n",
      "# Iteration  6710 -> Loss: 0.3764942556790411 \t| Accuracy: 105.000\n",
      "# Iteration  6711 -> Loss: 0.37647735627767964 \t| Accuracy: 105.000\n",
      "# Iteration  6712 -> Loss: 0.37646046194101745 \t| Accuracy: 105.000\n",
      "# Iteration  6713 -> Loss: 0.376443572666437 \t| Accuracy: 105.000\n",
      "# Iteration  6714 -> Loss: 0.37642668845132293 \t| Accuracy: 105.000\n",
      "# Iteration  6715 -> Loss: 0.3764098092930609 \t| Accuracy: 105.000\n",
      "# Iteration  6716 -> Loss: 0.37639293518903894 \t| Accuracy: 105.000\n",
      "# Iteration  6717 -> Loss: 0.37637606613664626 \t| Accuracy: 105.000\n",
      "# Iteration  6718 -> Loss: 0.37635920213327423 \t| Accuracy: 105.000\n",
      "# Iteration  6719 -> Loss: 0.37634234317631554 \t| Accuracy: 105.000\n",
      "# Iteration  6720 -> Loss: 0.3763254892631649 \t| Accuracy: 105.000\n",
      "# Iteration  6721 -> Loss: 0.3763086403912185 \t| Accuracy: 105.000\n",
      "# Iteration  6722 -> Loss: 0.37629179655787437 \t| Accuracy: 105.000\n",
      "# Iteration  6723 -> Loss: 0.37627495776053216 \t| Accuracy: 105.000\n",
      "# Iteration  6724 -> Loss: 0.37625812399659325 \t| Accuracy: 105.000\n",
      "# Iteration  6725 -> Loss: 0.3762412952634608 \t| Accuracy: 105.000\n",
      "# Iteration  6726 -> Loss: 0.37622447155853955 \t| Accuracy: 105.000\n",
      "# Iteration  6727 -> Loss: 0.3762076528792359 \t| Accuracy: 105.000\n",
      "# Iteration  6728 -> Loss: 0.3761908392229582 \t| Accuracy: 105.000\n",
      "# Iteration  6729 -> Loss: 0.3761740305871162 \t| Accuracy: 105.000\n",
      "# Iteration  6730 -> Loss: 0.3761572269691215 \t| Accuracy: 105.000\n",
      "# Iteration  6731 -> Loss: 0.37614042836638745 \t| Accuracy: 105.000\n",
      "# Iteration  6732 -> Loss: 0.37612363477632893 \t| Accuracy: 105.000\n",
      "# Iteration  6733 -> Loss: 0.37610684619636253 \t| Accuracy: 105.000\n",
      "# Iteration  6734 -> Loss: 0.3760900626239066 \t| Accuracy: 105.000\n",
      "# Iteration  6735 -> Loss: 0.3760732840563813 \t| Accuracy: 105.000\n",
      "# Iteration  6736 -> Loss: 0.37605651049120825 \t| Accuracy: 105.000\n",
      "# Iteration  6737 -> Loss: 0.37603974192581074 \t| Accuracy: 105.000\n",
      "# Iteration  6738 -> Loss: 0.3760229783576139 \t| Accuracy: 105.000\n",
      "# Iteration  6739 -> Loss: 0.37600621978404464 \t| Accuracy: 105.000\n",
      "# Iteration  6740 -> Loss: 0.37598946620253115 \t| Accuracy: 105.000\n",
      "# Iteration  6741 -> Loss: 0.3759727176105037 \t| Accuracy: 105.000\n",
      "# Iteration  6742 -> Loss: 0.3759559740053941 \t| Accuracy: 105.000\n",
      "# Iteration  6743 -> Loss: 0.37593923538463575 \t| Accuracy: 105.000\n",
      "# Iteration  6744 -> Loss: 0.37592250174566383 \t| Accuracy: 105.000\n",
      "# Iteration  6745 -> Loss: 0.37590577308591505 \t| Accuracy: 105.000\n",
      "# Iteration  6746 -> Loss: 0.37588904940282813 \t| Accuracy: 105.000\n",
      "# Iteration  6747 -> Loss: 0.37587233069384307 \t| Accuracy: 105.000\n",
      "# Iteration  6748 -> Loss: 0.3758556169564017 \t| Accuracy: 105.000\n",
      "# Iteration  6749 -> Loss: 0.3758389081879475 \t| Accuracy: 105.000\n",
      "# Iteration  6750 -> Loss: 0.3758222043859258 \t| Accuracy: 105.000\n",
      "# Iteration  6751 -> Loss: 0.37580550554778325 \t| Accuracy: 105.000\n",
      "# Iteration  6752 -> Loss: 0.37578881167096856 \t| Accuracy: 105.000\n",
      "# Iteration  6753 -> Loss: 0.37577212275293176 \t| Accuracy: 105.000\n",
      "# Iteration  6754 -> Loss: 0.3757554387911245 \t| Accuracy: 105.000\n",
      "# Iteration  6755 -> Loss: 0.37573875978300064 \t| Accuracy: 105.000\n",
      "# Iteration  6756 -> Loss: 0.3757220857260152 \t| Accuracy: 105.000\n",
      "# Iteration  6757 -> Loss: 0.3757054166176248 \t| Accuracy: 105.000\n",
      "# Iteration  6758 -> Loss: 0.37568875245528804 \t| Accuracy: 105.000\n",
      "# Iteration  6759 -> Loss: 0.3756720932364652 \t| Accuracy: 105.000\n",
      "# Iteration  6760 -> Loss: 0.3756554389586178 \t| Accuracy: 105.000\n",
      "# Iteration  6761 -> Loss: 0.3756387896192094 \t| Accuracy: 105.000\n",
      "# Iteration  6762 -> Loss: 0.37562214521570503 \t| Accuracy: 105.000\n",
      "# Iteration  6763 -> Loss: 0.37560550574557133 \t| Accuracy: 105.000\n",
      "# Iteration  6764 -> Loss: 0.3755888712062769 \t| Accuracy: 105.000\n",
      "# Iteration  6765 -> Loss: 0.3755722415952917 \t| Accuracy: 105.000\n",
      "# Iteration  6766 -> Loss: 0.3755556169100873 \t| Accuracy: 105.000\n",
      "# Iteration  6767 -> Loss: 0.37553899714813715 \t| Accuracy: 105.000\n",
      "# Iteration  6768 -> Loss: 0.3755223823069161 \t| Accuracy: 105.000\n",
      "# Iteration  6769 -> Loss: 0.3755057723839009 \t| Accuracy: 105.000\n",
      "# Iteration  6770 -> Loss: 0.3754891673765696 \t| Accuracy: 105.000\n",
      "# Iteration  6771 -> Loss: 0.3754725672824024 \t| Accuracy: 105.000\n",
      "# Iteration  6772 -> Loss: 0.3754559720988804 \t| Accuracy: 105.000\n",
      "# Iteration  6773 -> Loss: 0.37543938182348713 \t| Accuracy: 105.000\n",
      "# Iteration  6774 -> Loss: 0.37542279645370735 \t| Accuracy: 105.000\n",
      "# Iteration  6775 -> Loss: 0.3754062159870274 \t| Accuracy: 105.000\n",
      "# Iteration  6776 -> Loss: 0.3753896404209353 \t| Accuracy: 105.000\n",
      "# Iteration  6777 -> Loss: 0.37537306975292095 \t| Accuracy: 105.000\n",
      "# Iteration  6778 -> Loss: 0.37535650398047554 \t| Accuracy: 105.000\n",
      "# Iteration  6779 -> Loss: 0.3753399431010921 \t| Accuracy: 105.000\n",
      "# Iteration  6780 -> Loss: 0.37532338711226515 \t| Accuracy: 105.000\n",
      "# Iteration  6781 -> Loss: 0.3753068360114911 \t| Accuracy: 105.000\n",
      "# Iteration  6782 -> Loss: 0.3752902897962677 \t| Accuracy: 105.000\n",
      "# Iteration  6783 -> Loss: 0.37527374846409434 \t| Accuracy: 105.000\n",
      "# Iteration  6784 -> Loss: 0.3752572120124722 \t| Accuracy: 105.000\n",
      "# Iteration  6785 -> Loss: 0.3752406804389041 \t| Accuracy: 105.000\n",
      "# Iteration  6786 -> Loss: 0.3752241537408942 \t| Accuracy: 105.000\n",
      "# Iteration  6787 -> Loss: 0.3752076319159486 \t| Accuracy: 105.000\n",
      "# Iteration  6788 -> Loss: 0.3751911149615749 \t| Accuracy: 105.000\n",
      "# Iteration  6789 -> Loss: 0.3751746028752823 \t| Accuracy: 105.000\n",
      "# Iteration  6790 -> Loss: 0.3751580956545816 \t| Accuracy: 105.000\n",
      "# Iteration  6791 -> Loss: 0.375141593296985 \t| Accuracy: 105.000\n",
      "# Iteration  6792 -> Loss: 0.37512509580000697 \t| Accuracy: 105.000\n",
      "# Iteration  6793 -> Loss: 0.37510860316116296 \t| Accuracy: 105.000\n",
      "# Iteration  6794 -> Loss: 0.3750921153779702 \t| Accuracy: 105.000\n",
      "# Iteration  6795 -> Loss: 0.3750756324479478 \t| Accuracy: 105.000\n",
      "# Iteration  6796 -> Loss: 0.37505915436861603 \t| Accuracy: 105.000\n",
      "# Iteration  6797 -> Loss: 0.37504268113749695 \t| Accuracy: 105.000\n",
      "# Iteration  6798 -> Loss: 0.37502621275211456 \t| Accuracy: 105.000\n",
      "# Iteration  6799 -> Loss: 0.3750097492099939 \t| Accuracy: 105.000\n",
      "# Iteration  6800 -> Loss: 0.37499329050866204 \t| Accuracy: 105.000\n",
      "# Iteration  6801 -> Loss: 0.3749768366456474 \t| Accuracy: 105.000\n",
      "# Iteration  6802 -> Loss: 0.3749603876184803 \t| Accuracy: 105.000\n",
      "# Iteration  6803 -> Loss: 0.3749439434246922 \t| Accuracy: 105.000\n",
      "# Iteration  6804 -> Loss: 0.3749275040618166 \t| Accuracy: 105.000\n",
      "# Iteration  6805 -> Loss: 0.37491106952738834 \t| Accuracy: 105.000\n",
      "# Iteration  6806 -> Loss: 0.3748946398189439 \t| Accuracy: 105.000\n",
      "# Iteration  6807 -> Loss: 0.37487821493402157 \t| Accuracy: 105.000\n",
      "# Iteration  6808 -> Loss: 0.37486179487016097 \t| Accuracy: 105.000\n",
      "# Iteration  6809 -> Loss: 0.37484537962490333 \t| Accuracy: 105.000\n",
      "# Iteration  6810 -> Loss: 0.37482896919579156 \t| Accuracy: 105.000\n",
      "# Iteration  6811 -> Loss: 0.3748125635803702 \t| Accuracy: 105.000\n",
      "# Iteration  6812 -> Loss: 0.3747961627761854 \t| Accuracy: 105.000\n",
      "# Iteration  6813 -> Loss: 0.3747797667807846 \t| Accuracy: 105.000\n",
      "# Iteration  6814 -> Loss: 0.37476337559171724 \t| Accuracy: 105.000\n",
      "# Iteration  6815 -> Loss: 0.37474698920653404 \t| Accuracy: 105.000\n",
      "# Iteration  6816 -> Loss: 0.37473060762278754 \t| Accuracy: 105.000\n",
      "# Iteration  6817 -> Loss: 0.3747142308380316 \t| Accuracy: 105.000\n",
      "# Iteration  6818 -> Loss: 0.3746978588498218 \t| Accuracy: 105.000\n",
      "# Iteration  6819 -> Loss: 0.37468149165571546 \t| Accuracy: 105.000\n",
      "# Iteration  6820 -> Loss: 0.37466512925327133 \t| Accuracy: 105.000\n",
      "# Iteration  6821 -> Loss: 0.37464877164004945 \t| Accuracy: 105.000\n",
      "# Iteration  6822 -> Loss: 0.37463241881361203 \t| Accuracy: 105.000\n",
      "# Iteration  6823 -> Loss: 0.37461607077152237 \t| Accuracy: 105.000\n",
      "# Iteration  6824 -> Loss: 0.3745997275113455 \t| Accuracy: 105.000\n",
      "# Iteration  6825 -> Loss: 0.37458338903064825 \t| Accuracy: 105.000\n",
      "# Iteration  6826 -> Loss: 0.37456705532699863 \t| Accuracy: 105.000\n",
      "# Iteration  6827 -> Loss: 0.3745507263979663 \t| Accuracy: 105.000\n",
      "# Iteration  6828 -> Loss: 0.3745344022411229 \t| Accuracy: 105.000\n",
      "# Iteration  6829 -> Loss: 0.3745180828540412 \t| Accuracy: 105.000\n",
      "# Iteration  6830 -> Loss: 0.3745017682342956 \t| Accuracy: 105.000\n",
      "# Iteration  6831 -> Loss: 0.37448545837946223 \t| Accuracy: 105.000\n",
      "# Iteration  6832 -> Loss: 0.3744691532871186 \t| Accuracy: 105.000\n",
      "# Iteration  6833 -> Loss: 0.374452852954844 \t| Accuracy: 105.000\n",
      "# Iteration  6834 -> Loss: 0.374436557380219 \t| Accuracy: 105.000\n",
      "# Iteration  6835 -> Loss: 0.37442026656082594 \t| Accuracy: 105.000\n",
      "# Iteration  6836 -> Loss: 0.37440398049424883 \t| Accuracy: 105.000\n",
      "# Iteration  6837 -> Loss: 0.37438769917807296 \t| Accuracy: 105.000\n",
      "# Iteration  6838 -> Loss: 0.3743714226098852 \t| Accuracy: 105.000\n",
      "# Iteration  6839 -> Loss: 0.37435515078727416 \t| Accuracy: 105.000\n",
      "# Iteration  6840 -> Loss: 0.3743388837078301 \t| Accuracy: 105.000\n",
      "# Iteration  6841 -> Loss: 0.3743226213691443 \t| Accuracy: 105.000\n",
      "# Iteration  6842 -> Loss: 0.37430636376881027 \t| Accuracy: 105.000\n",
      "# Iteration  6843 -> Loss: 0.3742901109044225 \t| Accuracy: 105.000\n",
      "# Iteration  6844 -> Loss: 0.37427386277357744 \t| Accuracy: 105.000\n",
      "# Iteration  6845 -> Loss: 0.3742576193738728 \t| Accuracy: 105.000\n",
      "# Iteration  6846 -> Loss: 0.3742413807029082 \t| Accuracy: 105.000\n",
      "# Iteration  6847 -> Loss: 0.3742251467582842 \t| Accuracy: 105.000\n",
      "# Iteration  6848 -> Loss: 0.3742089175376037 \t| Accuracy: 105.000\n",
      "# Iteration  6849 -> Loss: 0.37419269303847036 \t| Accuracy: 105.000\n",
      "# Iteration  6850 -> Loss: 0.37417647325848996 \t| Accuracy: 105.000\n",
      "# Iteration  6851 -> Loss: 0.3741602581952696 \t| Accuracy: 105.000\n",
      "# Iteration  6852 -> Loss: 0.37414404784641786 \t| Accuracy: 105.000\n",
      "# Iteration  6853 -> Loss: 0.374127842209545 \t| Accuracy: 105.000\n",
      "# Iteration  6854 -> Loss: 0.37411164128226265 \t| Accuracy: 105.000\n",
      "# Iteration  6855 -> Loss: 0.3740954450621841 \t| Accuracy: 105.000\n",
      "# Iteration  6856 -> Loss: 0.3740792535469242 \t| Accuracy: 105.000\n",
      "# Iteration  6857 -> Loss: 0.3740630667340993 \t| Accuracy: 105.000\n",
      "# Iteration  6858 -> Loss: 0.3740468846213272 \t| Accuracy: 105.000\n",
      "# Iteration  6859 -> Loss: 0.3740307072062274 \t| Accuracy: 105.000\n",
      "# Iteration  6860 -> Loss: 0.3740145344864207 \t| Accuracy: 105.000\n",
      "# Iteration  6861 -> Loss: 0.3739983664595298 \t| Accuracy: 105.000\n",
      "# Iteration  6862 -> Loss: 0.3739822031231784 \t| Accuracy: 105.000\n",
      "# Iteration  6863 -> Loss: 0.37396604447499227 \t| Accuracy: 105.000\n",
      "# Iteration  6864 -> Loss: 0.3739498905125983 \t| Accuracy: 105.000\n",
      "# Iteration  6865 -> Loss: 0.37393374123362516 \t| Accuracy: 105.000\n",
      "# Iteration  6866 -> Loss: 0.3739175966357029 \t| Accuracy: 105.000\n",
      "# Iteration  6867 -> Loss: 0.3739014567164632 \t| Accuracy: 105.000\n",
      "# Iteration  6868 -> Loss: 0.37388532147353915 \t| Accuracy: 105.000\n",
      "# Iteration  6869 -> Loss: 0.37386919090456544 \t| Accuracy: 105.000\n",
      "# Iteration  6870 -> Loss: 0.37385306500717824 \t| Accuracy: 105.000\n",
      "# Iteration  6871 -> Loss: 0.3738369437790154 \t| Accuracy: 105.000\n",
      "# Iteration  6872 -> Loss: 0.37382082721771587 \t| Accuracy: 105.000\n",
      "# Iteration  6873 -> Loss: 0.37380471532092063 \t| Accuracy: 105.000\n",
      "# Iteration  6874 -> Loss: 0.3737886080862719 \t| Accuracy: 105.000\n",
      "# Iteration  6875 -> Loss: 0.3737725055114133 \t| Accuracy: 105.000\n",
      "# Iteration  6876 -> Loss: 0.37375640759399026 \t| Accuracy: 105.000\n",
      "# Iteration  6877 -> Loss: 0.3737403143316495 \t| Accuracy: 105.000\n",
      "# Iteration  6878 -> Loss: 0.37372422572203945 \t| Accuracy: 105.000\n",
      "# Iteration  6879 -> Loss: 0.37370814176280964 \t| Accuracy: 105.000\n",
      "# Iteration  6880 -> Loss: 0.3736920624516118 \t| Accuracy: 105.000\n",
      "# Iteration  6881 -> Loss: 0.37367598778609845 \t| Accuracy: 105.000\n",
      "# Iteration  6882 -> Loss: 0.37365991776392393 \t| Accuracy: 105.000\n",
      "# Iteration  6883 -> Loss: 0.3736438523827444 \t| Accuracy: 105.000\n",
      "# Iteration  6884 -> Loss: 0.37362779164021676 \t| Accuracy: 105.000\n",
      "# Iteration  6885 -> Loss: 0.3736117355340002 \t| Accuracy: 105.000\n",
      "# Iteration  6886 -> Loss: 0.3735956840617549 \t| Accuracy: 105.000\n",
      "# Iteration  6887 -> Loss: 0.37357963722114274 \t| Accuracy: 105.000\n",
      "# Iteration  6888 -> Loss: 0.37356359500982717 \t| Accuracy: 105.000\n",
      "# Iteration  6889 -> Loss: 0.37354755742547274 \t| Accuracy: 105.000\n",
      "# Iteration  6890 -> Loss: 0.3735315244657463 \t| Accuracy: 105.000\n",
      "# Iteration  6891 -> Loss: 0.37351549612831514 \t| Accuracy: 105.000\n",
      "# Iteration  6892 -> Loss: 0.373499472410849 \t| Accuracy: 105.000\n",
      "# Iteration  6893 -> Loss: 0.37348345331101857 \t| Accuracy: 105.000\n",
      "# Iteration  6894 -> Loss: 0.3734674388264961 \t| Accuracy: 105.000\n",
      "# Iteration  6895 -> Loss: 0.3734514289549554 \t| Accuracy: 105.000\n",
      "# Iteration  6896 -> Loss: 0.37343542369407196 \t| Accuracy: 105.000\n",
      "# Iteration  6897 -> Loss: 0.37341942304152226 \t| Accuracy: 105.000\n",
      "# Iteration  6898 -> Loss: 0.37340342699498486 \t| Accuracy: 105.000\n",
      "# Iteration  6899 -> Loss: 0.3733874355521394 \t| Accuracy: 105.000\n",
      "# Iteration  6900 -> Loss: 0.3733714487106672 \t| Accuracy: 105.000\n",
      "# Iteration  6901 -> Loss: 0.3733554664682508 \t| Accuracy: 105.000\n",
      "# Iteration  6902 -> Loss: 0.3733394888225746 \t| Accuracy: 105.000\n",
      "# Iteration  6903 -> Loss: 0.3733235157713243 \t| Accuracy: 105.000\n",
      "# Iteration  6904 -> Loss: 0.373307547312187 \t| Accuracy: 105.000\n",
      "# Iteration  6905 -> Loss: 0.3732915834428514 \t| Accuracy: 105.000\n",
      "# Iteration  6906 -> Loss: 0.3732756241610076 \t| Accuracy: 105.000\n",
      "# Iteration  6907 -> Loss: 0.3732596694643472 \t| Accuracy: 105.000\n",
      "# Iteration  6908 -> Loss: 0.3732437193505634 \t| Accuracy: 105.000\n",
      "# Iteration  6909 -> Loss: 0.3732277738173507 \t| Accuracy: 105.000\n",
      "# Iteration  6910 -> Loss: 0.37321183286240517 \t| Accuracy: 105.000\n",
      "# Iteration  6911 -> Loss: 0.37319589648342427 \t| Accuracy: 105.000\n",
      "# Iteration  6912 -> Loss: 0.3731799646781069 \t| Accuracy: 105.000\n",
      "# Iteration  6913 -> Loss: 0.3731640374441537 \t| Accuracy: 105.000\n",
      "# Iteration  6914 -> Loss: 0.37314811477926657 \t| Accuracy: 105.000\n",
      "# Iteration  6915 -> Loss: 0.3731321966811488 \t| Accuracy: 105.000\n",
      "# Iteration  6916 -> Loss: 0.37311628314750533 \t| Accuracy: 105.000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Iteration  6917 -> Loss: 0.3731003741760424 \t| Accuracy: 105.000\n",
      "# Iteration  6918 -> Loss: 0.37308446976446796 \t| Accuracy: 105.000\n",
      "# Iteration  6919 -> Loss: 0.37306856991049114 \t| Accuracy: 105.000\n",
      "# Iteration  6920 -> Loss: 0.3730526746118226 \t| Accuracy: 105.000\n",
      "# Iteration  6921 -> Loss: 0.3730367838661748 \t| Accuracy: 105.000\n",
      "# Iteration  6922 -> Loss: 0.37302089767126106 \t| Accuracy: 105.000\n",
      "# Iteration  6923 -> Loss: 0.37300501602479663 \t| Accuracy: 105.000\n",
      "# Iteration  6924 -> Loss: 0.3729891389244981 \t| Accuracy: 105.000\n",
      "# Iteration  6925 -> Loss: 0.3729732663680834 \t| Accuracy: 105.000\n",
      "# Iteration  6926 -> Loss: 0.3729573983532721 \t| Accuracy: 105.000\n",
      "# Iteration  6927 -> Loss: 0.3729415348777851 \t| Accuracy: 105.000\n",
      "# Iteration  6928 -> Loss: 0.37292567593934484 \t| Accuracy: 105.000\n",
      "# Iteration  6929 -> Loss: 0.37290982153567487 \t| Accuracy: 105.000\n",
      "# Iteration  6930 -> Loss: 0.3728939716645008 \t| Accuracy: 105.000\n",
      "# Iteration  6931 -> Loss: 0.3728781263235493 \t| Accuracy: 105.000\n",
      "# Iteration  6932 -> Loss: 0.37286228551054845 \t| Accuracy: 105.000\n",
      "# Iteration  6933 -> Loss: 0.37284644922322807 \t| Accuracy: 105.000\n",
      "# Iteration  6934 -> Loss: 0.37283061745931895 \t| Accuracy: 105.000\n",
      "# Iteration  6935 -> Loss: 0.37281479021655395 \t| Accuracy: 105.000\n",
      "# Iteration  6936 -> Loss: 0.37279896749266683 \t| Accuracy: 105.000\n",
      "# Iteration  6937 -> Loss: 0.372783149285393 \t| Accuracy: 105.000\n",
      "# Iteration  6938 -> Loss: 0.3727673355924696 \t| Accuracy: 105.000\n",
      "# Iteration  6939 -> Loss: 0.3727515264116346 \t| Accuracy: 105.000\n",
      "# Iteration  6940 -> Loss: 0.3727357217406278 \t| Accuracy: 105.000\n",
      "# Iteration  6941 -> Loss: 0.37271992157719064 \t| Accuracy: 105.000\n",
      "# Iteration  6942 -> Loss: 0.3727041259190654 \t| Accuracy: 105.000\n",
      "# Iteration  6943 -> Loss: 0.37268833476399643 \t| Accuracy: 105.000\n",
      "# Iteration  6944 -> Loss: 0.372672548109729 \t| Accuracy: 105.000\n",
      "# Iteration  6945 -> Loss: 0.3726567659540103 \t| Accuracy: 105.000\n",
      "# Iteration  6946 -> Loss: 0.37264098829458847 \t| Accuracy: 105.000\n",
      "# Iteration  6947 -> Loss: 0.3726252151292133 \t| Accuracy: 105.000\n",
      "# Iteration  6948 -> Loss: 0.3726094464556362 \t| Accuracy: 105.000\n",
      "# Iteration  6949 -> Loss: 0.3725936822716098 \t| Accuracy: 105.000\n",
      "# Iteration  6950 -> Loss: 0.37257792257488814 \t| Accuracy: 105.000\n",
      "# Iteration  6951 -> Loss: 0.3725621673632265 \t| Accuracy: 105.208\n",
      "# Iteration  6952 -> Loss: 0.37254641663438226 \t| Accuracy: 105.208\n",
      "# Iteration  6953 -> Loss: 0.37253067038611365 \t| Accuracy: 105.208\n",
      "# Iteration  6954 -> Loss: 0.37251492861618035 \t| Accuracy: 105.208\n",
      "# Iteration  6955 -> Loss: 0.3724991913223436 \t| Accuracy: 105.208\n",
      "# Iteration  6956 -> Loss: 0.3724834585023662 \t| Accuracy: 105.208\n",
      "# Iteration  6957 -> Loss: 0.37246773015401213 \t| Accuracy: 105.208\n",
      "# Iteration  6958 -> Loss: 0.37245200627504693 \t| Accuracy: 105.208\n",
      "# Iteration  6959 -> Loss: 0.3724362868632375 \t| Accuracy: 105.208\n",
      "# Iteration  6960 -> Loss: 0.372420571916352 \t| Accuracy: 105.208\n",
      "# Iteration  6961 -> Loss: 0.37240486143216045 \t| Accuracy: 105.208\n",
      "# Iteration  6962 -> Loss: 0.3723891554084338 \t| Accuracy: 105.208\n",
      "# Iteration  6963 -> Loss: 0.37237345384294485 \t| Accuracy: 105.208\n",
      "# Iteration  6964 -> Loss: 0.3723577567334673 \t| Accuracy: 105.208\n",
      "# Iteration  6965 -> Loss: 0.37234206407777687 \t| Accuracy: 105.208\n",
      "# Iteration  6966 -> Loss: 0.37232637587365025 \t| Accuracy: 105.208\n",
      "# Iteration  6967 -> Loss: 0.3723106921188657 \t| Accuracy: 105.208\n",
      "# Iteration  6968 -> Loss: 0.3722950128112028 \t| Accuracy: 105.208\n",
      "# Iteration  6969 -> Loss: 0.37227933794844265 \t| Accuracy: 105.208\n",
      "# Iteration  6970 -> Loss: 0.37226366752836765 \t| Accuracy: 105.208\n",
      "# Iteration  6971 -> Loss: 0.37224800154876186 \t| Accuracy: 105.208\n",
      "# Iteration  6972 -> Loss: 0.3722323400074104 \t| Accuracy: 105.208\n",
      "# Iteration  6973 -> Loss: 0.3722166829021 \t| Accuracy: 105.208\n",
      "# Iteration  6974 -> Loss: 0.3722010302306187 \t| Accuracy: 105.208\n",
      "# Iteration  6975 -> Loss: 0.3721853819907561 \t| Accuracy: 105.208\n",
      "# Iteration  6976 -> Loss: 0.372169738180303 \t| Accuracy: 105.208\n",
      "# Iteration  6977 -> Loss: 0.37215409879705175 \t| Accuracy: 105.208\n",
      "# Iteration  6978 -> Loss: 0.372138463838796 \t| Accuracy: 105.208\n",
      "# Iteration  6979 -> Loss: 0.37212283330333074 \t| Accuracy: 105.208\n",
      "# Iteration  6980 -> Loss: 0.3721072071884527 \t| Accuracy: 105.208\n",
      "# Iteration  6981 -> Loss: 0.3720915854919595 \t| Accuracy: 105.208\n",
      "# Iteration  6982 -> Loss: 0.37207596821165073 \t| Accuracy: 105.208\n",
      "# Iteration  6983 -> Loss: 0.37206035534532683 \t| Accuracy: 105.208\n",
      "# Iteration  6984 -> Loss: 0.3720447468907899 \t| Accuracy: 105.208\n",
      "# Iteration  6985 -> Loss: 0.37202914284584343 \t| Accuracy: 105.208\n",
      "# Iteration  6986 -> Loss: 0.37201354320829233 \t| Accuracy: 105.208\n",
      "# Iteration  6987 -> Loss: 0.37199794797594277 \t| Accuracy: 105.208\n",
      "# Iteration  6988 -> Loss: 0.3719823571466024 \t| Accuracy: 105.208\n",
      "# Iteration  6989 -> Loss: 0.37196677071808026 \t| Accuracy: 105.208\n",
      "# Iteration  6990 -> Loss: 0.37195118868818683 \t| Accuracy: 105.208\n",
      "# Iteration  6991 -> Loss: 0.3719356110547337 \t| Accuracy: 105.208\n",
      "# Iteration  6992 -> Loss: 0.37192003781553423 \t| Accuracy: 105.208\n",
      "# Iteration  6993 -> Loss: 0.3719044689684029 \t| Accuracy: 105.208\n",
      "# Iteration  6994 -> Loss: 0.37188890451115586 \t| Accuracy: 105.208\n",
      "# Iteration  6995 -> Loss: 0.3718733444416101 \t| Accuracy: 105.208\n",
      "# Iteration  6996 -> Loss: 0.37185778875758446 \t| Accuracy: 105.208\n",
      "# Iteration  6997 -> Loss: 0.3718422374568992 \t| Accuracy: 105.208\n",
      "# Iteration  6998 -> Loss: 0.3718266905373756 \t| Accuracy: 105.208\n",
      "# Iteration  6999 -> Loss: 0.3718111479968365 \t| Accuracy: 105.208\n",
      "# Iteration  7000 -> Loss: 0.3717956098331064 \t| Accuracy: 105.208\n",
      "# Iteration  7001 -> Loss: 0.3717800760440105 \t| Accuracy: 105.208\n",
      "# Iteration  7002 -> Loss: 0.37176454662737596 \t| Accuracy: 105.208\n",
      "# Iteration  7003 -> Loss: 0.37174902158103124 \t| Accuracy: 105.208\n",
      "# Iteration  7004 -> Loss: 0.37173350090280577 \t| Accuracy: 105.208\n",
      "# Iteration  7005 -> Loss: 0.371717984590531 \t| Accuracy: 105.208\n",
      "# Iteration  7006 -> Loss: 0.3717024726420392 \t| Accuracy: 105.208\n",
      "# Iteration  7007 -> Loss: 0.3716869650551641 \t| Accuracy: 105.208\n",
      "# Iteration  7008 -> Loss: 0.37167146182774113 \t| Accuracy: 105.208\n",
      "# Iteration  7009 -> Loss: 0.37165596295760667 \t| Accuracy: 105.208\n",
      "# Iteration  7010 -> Loss: 0.37164046844259874 \t| Accuracy: 105.208\n",
      "# Iteration  7011 -> Loss: 0.37162497828055663 \t| Accuracy: 105.208\n",
      "# Iteration  7012 -> Loss: 0.371609492469321 \t| Accuracy: 105.208\n",
      "# Iteration  7013 -> Loss: 0.3715940110067339 \t| Accuracy: 105.208\n",
      "# Iteration  7014 -> Loss: 0.3715785338906387 \t| Accuracy: 105.208\n",
      "# Iteration  7015 -> Loss: 0.3715630611188801 \t| Accuracy: 105.208\n",
      "# Iteration  7016 -> Loss: 0.3715475926893043 \t| Accuracy: 105.208\n",
      "# Iteration  7017 -> Loss: 0.3715321285997587 \t| Accuracy: 105.208\n",
      "# Iteration  7018 -> Loss: 0.3715166688480922 \t| Accuracy: 105.208\n",
      "# Iteration  7019 -> Loss: 0.371501213432155 \t| Accuracy: 105.208\n",
      "# Iteration  7020 -> Loss: 0.3714857623497984 \t| Accuracy: 105.208\n",
      "# Iteration  7021 -> Loss: 0.37147031559887567 \t| Accuracy: 105.208\n",
      "# Iteration  7022 -> Loss: 0.3714548731772408 \t| Accuracy: 105.208\n",
      "# Iteration  7023 -> Loss: 0.37143943508274946 \t| Accuracy: 105.208\n",
      "# Iteration  7024 -> Loss: 0.3714240013132586 \t| Accuracy: 105.208\n",
      "# Iteration  7025 -> Loss: 0.3714085718666266 \t| Accuracy: 105.208\n",
      "# Iteration  7026 -> Loss: 0.371393146740713 \t| Accuracy: 105.208\n",
      "# Iteration  7027 -> Loss: 0.37137772593337887 \t| Accuracy: 105.208\n",
      "# Iteration  7028 -> Loss: 0.37136230944248655 \t| Accuracy: 105.208\n",
      "# Iteration  7029 -> Loss: 0.37134689726589976 \t| Accuracy: 105.208\n",
      "# Iteration  7030 -> Loss: 0.3713314894014835 \t| Accuracy: 105.208\n",
      "# Iteration  7031 -> Loss: 0.37131608584710424 \t| Accuracy: 105.208\n",
      "# Iteration  7032 -> Loss: 0.37130068660062965 \t| Accuracy: 105.208\n",
      "# Iteration  7033 -> Loss: 0.37128529165992885 \t| Accuracy: 105.208\n",
      "# Iteration  7034 -> Loss: 0.3712699010228723 \t| Accuracy: 105.208\n",
      "# Iteration  7035 -> Loss: 0.3712545146873316 \t| Accuracy: 105.208\n",
      "# Iteration  7036 -> Loss: 0.37123913265118 \t| Accuracy: 105.208\n",
      "# Iteration  7037 -> Loss: 0.37122375491229187 \t| Accuracy: 105.208\n",
      "# Iteration  7038 -> Loss: 0.3712083814685432 \t| Accuracy: 105.208\n",
      "# Iteration  7039 -> Loss: 0.37119301231781077 \t| Accuracy: 105.208\n",
      "# Iteration  7040 -> Loss: 0.37117764745797316 \t| Accuracy: 105.208\n",
      "# Iteration  7041 -> Loss: 0.37116228688691016 \t| Accuracy: 105.208\n",
      "# Iteration  7042 -> Loss: 0.371146930602503 \t| Accuracy: 105.208\n",
      "# Iteration  7043 -> Loss: 0.3711315786026339 \t| Accuracy: 105.208\n",
      "# Iteration  7044 -> Loss: 0.3711162308851869 \t| Accuracy: 105.208\n",
      "# Iteration  7045 -> Loss: 0.37110088744804703 \t| Accuracy: 105.208\n",
      "# Iteration  7046 -> Loss: 0.37108554828910056 \t| Accuracy: 105.208\n",
      "# Iteration  7047 -> Loss: 0.37107021340623564 \t| Accuracy: 105.208\n",
      "# Iteration  7048 -> Loss: 0.37105488279734095 \t| Accuracy: 105.208\n",
      "# Iteration  7049 -> Loss: 0.3710395564603072 \t| Accuracy: 105.208\n",
      "# Iteration  7050 -> Loss: 0.37102423439302606 \t| Accuracy: 105.208\n",
      "# Iteration  7051 -> Loss: 0.3710089165933906 \t| Accuracy: 105.208\n",
      "# Iteration  7052 -> Loss: 0.3709936030592952 \t| Accuracy: 105.208\n",
      "# Iteration  7053 -> Loss: 0.37097829378863556 \t| Accuracy: 105.208\n",
      "# Iteration  7054 -> Loss: 0.37096298877930894 \t| Accuracy: 105.208\n",
      "# Iteration  7055 -> Loss: 0.3709476880292134 \t| Accuracy: 105.208\n",
      "# Iteration  7056 -> Loss: 0.3709323915362489 \t| Accuracy: 105.208\n",
      "# Iteration  7057 -> Loss: 0.3709170992983163 \t| Accuracy: 105.208\n",
      "# Iteration  7058 -> Loss: 0.37090181131331795 \t| Accuracy: 105.208\n",
      "# Iteration  7059 -> Loss: 0.3708865275791576 \t| Accuracy: 105.208\n",
      "# Iteration  7060 -> Loss: 0.3708712480937399 \t| Accuracy: 105.208\n",
      "# Iteration  7061 -> Loss: 0.37085597285497157 \t| Accuracy: 105.208\n",
      "# Iteration  7062 -> Loss: 0.3708407018607598 \t| Accuracy: 105.208\n",
      "# Iteration  7063 -> Loss: 0.3708254351090137 \t| Accuracy: 105.208\n",
      "# Iteration  7064 -> Loss: 0.37081017259764343 \t| Accuracy: 105.208\n",
      "# Iteration  7065 -> Loss: 0.3707949143245605 \t| Accuracy: 105.208\n",
      "# Iteration  7066 -> Loss: 0.3707796602876779 \t| Accuracy: 105.208\n",
      "# Iteration  7067 -> Loss: 0.3707644104849096 \t| Accuracy: 105.208\n",
      "# Iteration  7068 -> Loss: 0.37074916491417104 \t| Accuracy: 105.208\n",
      "# Iteration  7069 -> Loss: 0.370733923573379 \t| Accuracy: 105.208\n",
      "# Iteration  7070 -> Loss: 0.3707186864604516 \t| Accuracy: 105.208\n",
      "# Iteration  7071 -> Loss: 0.3707034535733082 \t| Accuracy: 105.208\n",
      "# Iteration  7072 -> Loss: 0.3706882249098695 \t| Accuracy: 105.208\n",
      "# Iteration  7073 -> Loss: 0.3706730004680574 \t| Accuracy: 105.208\n",
      "# Iteration  7074 -> Loss: 0.37065778024579527 \t| Accuracy: 105.208\n",
      "# Iteration  7075 -> Loss: 0.37064256424100755 \t| Accuracy: 105.208\n",
      "# Iteration  7076 -> Loss: 0.37062735245162026 \t| Accuracy: 105.208\n",
      "# Iteration  7077 -> Loss: 0.37061214487556055 \t| Accuracy: 105.208\n",
      "# Iteration  7078 -> Loss: 0.3705969415107567 \t| Accuracy: 105.208\n",
      "# Iteration  7079 -> Loss: 0.3705817423551389 \t| Accuracy: 105.208\n",
      "# Iteration  7080 -> Loss: 0.3705665474066379 \t| Accuracy: 105.208\n",
      "# Iteration  7081 -> Loss: 0.37055135666318617 \t| Accuracy: 105.208\n",
      "# Iteration  7082 -> Loss: 0.37053617012271733 \t| Accuracy: 105.208\n",
      "# Iteration  7083 -> Loss: 0.3705209877831664 \t| Accuracy: 105.208\n",
      "# Iteration  7084 -> Loss: 0.3705058096424696 \t| Accuracy: 105.208\n",
      "# Iteration  7085 -> Loss: 0.3704906356985644 \t| Accuracy: 105.208\n",
      "# Iteration  7086 -> Loss: 0.3704754659493896 \t| Accuracy: 105.208\n",
      "# Iteration  7087 -> Loss: 0.37046030039288563 \t| Accuracy: 105.208\n",
      "# Iteration  7088 -> Loss: 0.3704451390269936 \t| Accuracy: 105.208\n",
      "# Iteration  7089 -> Loss: 0.3704299818496563 \t| Accuracy: 105.208\n",
      "# Iteration  7090 -> Loss: 0.37041482885881766 \t| Accuracy: 105.208\n",
      "# Iteration  7091 -> Loss: 0.370399680052423 \t| Accuracy: 105.208\n",
      "# Iteration  7092 -> Loss: 0.3703845354284188 \t| Accuracy: 105.208\n",
      "# Iteration  7093 -> Loss: 0.3703693949847531 \t| Accuracy: 105.208\n",
      "# Iteration  7094 -> Loss: 0.37035425871937483 \t| Accuracy: 105.208\n",
      "# Iteration  7095 -> Loss: 0.3703391266302344 \t| Accuracy: 105.208\n",
      "# Iteration  7096 -> Loss: 0.3703239987152836 \t| Accuracy: 105.208\n",
      "# Iteration  7097 -> Loss: 0.3703088749724753 \t| Accuracy: 105.208\n",
      "# Iteration  7098 -> Loss: 0.37029375539976367 \t| Accuracy: 105.208\n",
      "# Iteration  7099 -> Loss: 0.37027863999510446 \t| Accuracy: 105.208\n",
      "# Iteration  7100 -> Loss: 0.37026352875645435 \t| Accuracy: 105.208\n",
      "# Iteration  7101 -> Loss: 0.37024842168177136 \t| Accuracy: 105.208\n",
      "# Iteration  7102 -> Loss: 0.3702333187690149 \t| Accuracy: 105.208\n",
      "# Iteration  7103 -> Loss: 0.3702182200161455 \t| Accuracy: 105.208\n",
      "# Iteration  7104 -> Loss: 0.37020312542112527 \t| Accuracy: 105.208\n",
      "# Iteration  7105 -> Loss: 0.3701880349819172 \t| Accuracy: 105.208\n",
      "# Iteration  7106 -> Loss: 0.3701729486964857 \t| Accuracy: 105.208\n",
      "# Iteration  7107 -> Loss: 0.37015786656279664 \t| Accuracy: 105.208\n",
      "# Iteration  7108 -> Loss: 0.37014278857881694 \t| Accuracy: 105.208\n",
      "# Iteration  7109 -> Loss: 0.3701277147425148 \t| Accuracy: 105.208\n",
      "# Iteration  7110 -> Loss: 0.3701126450518598 \t| Accuracy: 105.208\n",
      "# Iteration  7111 -> Loss: 0.3700975795048225 \t| Accuracy: 105.208\n",
      "# Iteration  7112 -> Loss: 0.37008251809937537 \t| Accuracy: 105.208\n",
      "# Iteration  7113 -> Loss: 0.37006746083349135 \t| Accuracy: 105.208\n",
      "# Iteration  7114 -> Loss: 0.37005240770514525 \t| Accuracy: 105.208\n",
      "# Iteration  7115 -> Loss: 0.3700373587123128 \t| Accuracy: 105.208\n",
      "# Iteration  7116 -> Loss: 0.370022313852971 \t| Accuracy: 105.208\n",
      "# Iteration  7117 -> Loss: 0.3700072731250985 \t| Accuracy: 105.208\n",
      "# Iteration  7118 -> Loss: 0.36999223652667473 \t| Accuracy: 105.208\n",
      "# Iteration  7119 -> Loss: 0.36997720405568063 \t| Accuracy: 105.208\n",
      "# Iteration  7120 -> Loss: 0.3699621757100982 \t| Accuracy: 105.208\n",
      "# Iteration  7121 -> Loss: 0.3699471514879111 \t| Accuracy: 105.208\n",
      "# Iteration  7122 -> Loss: 0.36993213138710374 \t| Accuracy: 105.208\n",
      "# Iteration  7123 -> Loss: 0.3699171154056622 \t| Accuracy: 105.208\n",
      "# Iteration  7124 -> Loss: 0.3699021035415735 \t| Accuracy: 105.208\n",
      "# Iteration  7125 -> Loss: 0.3698870957928262 \t| Accuracy: 105.208\n",
      "# Iteration  7126 -> Loss: 0.3698720921574099 \t| Accuracy: 105.208\n",
      "# Iteration  7127 -> Loss: 0.36985709263331557 \t| Accuracy: 105.208\n",
      "# Iteration  7128 -> Loss: 0.36984209721853534 \t| Accuracy: 105.208\n",
      "# Iteration  7129 -> Loss: 0.3698271059110625 \t| Accuracy: 105.208\n",
      "# Iteration  7130 -> Loss: 0.369812118708892 \t| Accuracy: 105.208\n",
      "# Iteration  7131 -> Loss: 0.3697971356100196 \t| Accuracy: 105.208\n",
      "# Iteration  7132 -> Loss: 0.3697821566124425 \t| Accuracy: 105.208\n",
      "# Iteration  7133 -> Loss: 0.369767181714159 \t| Accuracy: 105.208\n",
      "# Iteration  7134 -> Loss: 0.36975221091316895 \t| Accuracy: 105.208\n",
      "# Iteration  7135 -> Loss: 0.3697372442074733 \t| Accuracy: 105.208\n",
      "# Iteration  7136 -> Loss: 0.36972228159507387 \t| Accuracy: 105.208\n",
      "# Iteration  7137 -> Loss: 0.36970732307397436 \t| Accuracy: 105.208\n",
      "# Iteration  7138 -> Loss: 0.36969236864217914 \t| Accuracy: 105.208\n",
      "# Iteration  7139 -> Loss: 0.36967741829769435 \t| Accuracy: 105.208\n",
      "# Iteration  7140 -> Loss: 0.369662472038527 \t| Accuracy: 105.208\n",
      "# Iteration  7141 -> Loss: 0.36964752986268523 \t| Accuracy: 105.208\n",
      "# Iteration  7142 -> Loss: 0.369632591768179 \t| Accuracy: 105.208\n",
      "# Iteration  7143 -> Loss: 0.3696176577530189 \t| Accuracy: 105.208\n",
      "# Iteration  7144 -> Loss: 0.3696027278152171 \t| Accuracy: 105.208\n",
      "# Iteration  7145 -> Loss: 0.3695878019527867 \t| Accuracy: 105.208\n",
      "# Iteration  7146 -> Loss: 0.3695728801637424 \t| Accuracy: 105.208\n",
      "# Iteration  7147 -> Loss: 0.36955796244609995 \t| Accuracy: 105.208\n",
      "# Iteration  7148 -> Loss: 0.36954304879787636 \t| Accuracy: 105.208\n",
      "# Iteration  7149 -> Loss: 0.3695281392170898 \t| Accuracy: 105.208\n",
      "# Iteration  7150 -> Loss: 0.3695132337017597 \t| Accuracy: 105.208\n",
      "# Iteration  7151 -> Loss: 0.3694983322499068 \t| Accuracy: 105.208\n",
      "# Iteration  7152 -> Loss: 0.369483434859553 \t| Accuracy: 105.208\n",
      "# Iteration  7153 -> Loss: 0.3694685415287215 \t| Accuracy: 105.208\n",
      "# Iteration  7154 -> Loss: 0.36945365225543647 \t| Accuracy: 105.208\n",
      "# Iteration  7155 -> Loss: 0.36943876703772377 \t| Accuracy: 105.208\n",
      "# Iteration  7156 -> Loss: 0.3694238858736101 \t| Accuracy: 105.208\n",
      "# Iteration  7157 -> Loss: 0.36940900876112354 \t| Accuracy: 105.208\n",
      "# Iteration  7158 -> Loss: 0.36939413569829327 \t| Accuracy: 105.208\n",
      "# Iteration  7159 -> Loss: 0.36937926668314974 \t| Accuracy: 105.208\n",
      "# Iteration  7160 -> Loss: 0.3693644017137249 \t| Accuracy: 105.208\n",
      "# Iteration  7161 -> Loss: 0.3693495407880515 \t| Accuracy: 105.208\n",
      "# Iteration  7162 -> Loss: 0.3693346839041637 \t| Accuracy: 105.208\n",
      "# Iteration  7163 -> Loss: 0.36931983106009686 \t| Accuracy: 105.208\n",
      "# Iteration  7164 -> Loss: 0.3693049822538877 \t| Accuracy: 105.208\n",
      "# Iteration  7165 -> Loss: 0.3692901374835739 \t| Accuracy: 105.208\n",
      "# Iteration  7166 -> Loss: 0.3692752967471944 \t| Accuracy: 105.208\n",
      "# Iteration  7167 -> Loss: 0.3692604600427896 \t| Accuracy: 105.208\n",
      "# Iteration  7168 -> Loss: 0.369245627368401 \t| Accuracy: 105.208\n",
      "# Iteration  7169 -> Loss: 0.36923079872207104 \t| Accuracy: 105.208\n",
      "# Iteration  7170 -> Loss: 0.3692159741018438 \t| Accuracy: 105.208\n",
      "# Iteration  7171 -> Loss: 0.36920115350576427 \t| Accuracy: 105.208\n",
      "# Iteration  7172 -> Loss: 0.36918633693187874 \t| Accuracy: 105.208\n",
      "# Iteration  7173 -> Loss: 0.36917152437823464 \t| Accuracy: 105.208\n",
      "# Iteration  7174 -> Loss: 0.3691567158428809 \t| Accuracy: 105.208\n",
      "# Iteration  7175 -> Loss: 0.3691419113238673 \t| Accuracy: 105.208\n",
      "# Iteration  7176 -> Loss: 0.369127110819245 \t| Accuracy: 105.208\n",
      "# Iteration  7177 -> Loss: 0.36911231432706626 \t| Accuracy: 105.208\n",
      "# Iteration  7178 -> Loss: 0.3690975218453847 \t| Accuracy: 105.208\n",
      "# Iteration  7179 -> Loss: 0.3690827333722551 \t| Accuracy: 105.208\n",
      "# Iteration  7180 -> Loss: 0.36906794890573347 \t| Accuracy: 105.208\n",
      "# Iteration  7181 -> Loss: 0.36905316844387664 \t| Accuracy: 105.208\n",
      "# Iteration  7182 -> Loss: 0.3690383919847433 \t| Accuracy: 105.208\n",
      "# Iteration  7183 -> Loss: 0.36902361952639295 \t| Accuracy: 105.208\n",
      "# Iteration  7184 -> Loss: 0.36900885106688613 \t| Accuracy: 105.208\n",
      "# Iteration  7185 -> Loss: 0.3689940866042851 \t| Accuracy: 105.208\n",
      "# Iteration  7186 -> Loss: 0.3689793261366527 \t| Accuracy: 105.208\n",
      "# Iteration  7187 -> Loss: 0.3689645696620535 \t| Accuracy: 105.208\n",
      "# Iteration  7188 -> Loss: 0.368949817178553 \t| Accuracy: 105.208\n",
      "# Iteration  7189 -> Loss: 0.3689350686842178 \t| Accuracy: 105.208\n",
      "# Iteration  7190 -> Loss: 0.3689203241771161 \t| Accuracy: 105.208\n",
      "# Iteration  7191 -> Loss: 0.36890558365531684 \t| Accuracy: 105.208\n",
      "# Iteration  7192 -> Loss: 0.3688908471168903 \t| Accuracy: 105.208\n",
      "# Iteration  7193 -> Loss: 0.3688761145599082 \t| Accuracy: 105.208\n",
      "# Iteration  7194 -> Loss: 0.368861385982443 \t| Accuracy: 105.208\n",
      "# Iteration  7195 -> Loss: 0.36884666138256883 \t| Accuracy: 105.208\n",
      "# Iteration  7196 -> Loss: 0.36883194075836057 \t| Accuracy: 105.208\n",
      "# Iteration  7197 -> Loss: 0.36881722410789464 \t| Accuracy: 105.208\n",
      "# Iteration  7198 -> Loss: 0.36880251142924836 \t| Accuracy: 105.208\n",
      "# Iteration  7199 -> Loss: 0.36878780272050043 \t| Accuracy: 105.208\n",
      "# Iteration  7200 -> Loss: 0.3687730979797308 \t| Accuracy: 105.208\n",
      "# Iteration  7201 -> Loss: 0.36875839720502035 \t| Accuracy: 105.208\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Iteration  7202 -> Loss: 0.3687437003944512 \t| Accuracy: 105.208\n",
      "# Iteration  7203 -> Loss: 0.368729007546107 \t| Accuracy: 105.208\n",
      "# Iteration  7204 -> Loss: 0.3687143186580721 \t| Accuracy: 105.208\n",
      "# Iteration  7205 -> Loss: 0.36869963372843223 \t| Accuracy: 105.208\n",
      "# Iteration  7206 -> Loss: 0.3686849527552745 \t| Accuracy: 105.208\n",
      "# Iteration  7207 -> Loss: 0.368670275736687 \t| Accuracy: 105.208\n",
      "# Iteration  7208 -> Loss: 0.36865560267075886 \t| Accuracy: 105.208\n",
      "# Iteration  7209 -> Loss: 0.36864093355558053 \t| Accuracy: 105.208\n",
      "# Iteration  7210 -> Loss: 0.36862626838924384 \t| Accuracy: 105.208\n",
      "# Iteration  7211 -> Loss: 0.3686116071698414 \t| Accuracy: 105.208\n",
      "# Iteration  7212 -> Loss: 0.3685969498954674 \t| Accuracy: 105.208\n",
      "# Iteration  7213 -> Loss: 0.3685822965642168 \t| Accuracy: 105.208\n",
      "# Iteration  7214 -> Loss: 0.36856764717418616 \t| Accuracy: 105.208\n",
      "# Iteration  7215 -> Loss: 0.3685530017234728 \t| Accuracy: 105.208\n",
      "# Iteration  7216 -> Loss: 0.3685383602101756 \t| Accuracy: 105.208\n",
      "# Iteration  7217 -> Loss: 0.3685237226323942 \t| Accuracy: 105.208\n",
      "# Iteration  7218 -> Loss: 0.36850908898822976 \t| Accuracy: 105.208\n",
      "# Iteration  7219 -> Loss: 0.36849445927578434 \t| Accuracy: 105.208\n",
      "# Iteration  7220 -> Loss: 0.36847983349316155 \t| Accuracy: 105.208\n",
      "# Iteration  7221 -> Loss: 0.3684652116384657 \t| Accuracy: 105.208\n",
      "# Iteration  7222 -> Loss: 0.36845059370980254 \t| Accuracy: 105.208\n",
      "# Iteration  7223 -> Loss: 0.36843597970527897 \t| Accuracy: 105.208\n",
      "# Iteration  7224 -> Loss: 0.36842136962300304 \t| Accuracy: 105.208\n",
      "# Iteration  7225 -> Loss: 0.36840676346108386 \t| Accuracy: 105.208\n",
      "# Iteration  7226 -> Loss: 0.3683921612176319 \t| Accuracy: 105.208\n",
      "# Iteration  7227 -> Loss: 0.3683775628907585 \t| Accuracy: 105.208\n",
      "# Iteration  7228 -> Loss: 0.3683629684785765 \t| Accuracy: 105.208\n",
      "# Iteration  7229 -> Loss: 0.3683483779791997 \t| Accuracy: 105.208\n",
      "# Iteration  7230 -> Loss: 0.36833379139074307 \t| Accuracy: 105.208\n",
      "# Iteration  7231 -> Loss: 0.36831920871132273 \t| Accuracy: 105.208\n",
      "# Iteration  7232 -> Loss: 0.36830462993905605 \t| Accuracy: 105.208\n",
      "# Iteration  7233 -> Loss: 0.36829005507206153 \t| Accuracy: 105.208\n",
      "# Iteration  7234 -> Loss: 0.3682754841084588 \t| Accuracy: 105.208\n",
      "# Iteration  7235 -> Loss: 0.36826091704636854 \t| Accuracy: 105.208\n",
      "# Iteration  7236 -> Loss: 0.3682463538839128 \t| Accuracy: 105.208\n",
      "# Iteration  7237 -> Loss: 0.3682317946192146 \t| Accuracy: 105.208\n",
      "# Iteration  7238 -> Loss: 0.36821723925039834 \t| Accuracy: 105.208\n",
      "# Iteration  7239 -> Loss: 0.3682026877755892 \t| Accuracy: 105.208\n",
      "# Iteration  7240 -> Loss: 0.36818814019291385 \t| Accuracy: 105.208\n",
      "# Iteration  7241 -> Loss: 0.3681735965005 \t| Accuracy: 105.208\n",
      "# Iteration  7242 -> Loss: 0.3681590566964764 \t| Accuracy: 105.208\n",
      "# Iteration  7243 -> Loss: 0.3681445207789732 \t| Accuracy: 105.208\n",
      "# Iteration  7244 -> Loss: 0.36812998874612135 \t| Accuracy: 105.208\n",
      "# Iteration  7245 -> Loss: 0.3681154605960534 \t| Accuracy: 105.208\n",
      "# Iteration  7246 -> Loss: 0.36810093632690255 \t| Accuracy: 105.208\n",
      "# Iteration  7247 -> Loss: 0.3680864159368035 \t| Accuracy: 105.208\n",
      "# Iteration  7248 -> Loss: 0.36807189942389207 \t| Accuracy: 105.208\n",
      "# Iteration  7249 -> Loss: 0.3680573867863049 \t| Accuracy: 105.208\n",
      "# Iteration  7250 -> Loss: 0.3680428780221803 \t| Accuracy: 105.208\n",
      "# Iteration  7251 -> Loss: 0.368028373129657 \t| Accuracy: 105.208\n",
      "# Iteration  7252 -> Loss: 0.3680138721068758 \t| Accuracy: 105.208\n",
      "# Iteration  7253 -> Loss: 0.36799937495197776 \t| Accuracy: 105.208\n",
      "# Iteration  7254 -> Loss: 0.36798488166310567 \t| Accuracy: 105.208\n",
      "# Iteration  7255 -> Loss: 0.36797039223840317 \t| Accuracy: 105.208\n",
      "# Iteration  7256 -> Loss: 0.36795590667601513 \t| Accuracy: 105.208\n",
      "# Iteration  7257 -> Loss: 0.3679414249740875 \t| Accuracy: 105.208\n",
      "# Iteration  7258 -> Loss: 0.36792694713076746 \t| Accuracy: 105.208\n",
      "# Iteration  7259 -> Loss: 0.3679124731442033 \t| Accuracy: 105.208\n",
      "# Iteration  7260 -> Loss: 0.36789800301254433 \t| Accuracy: 105.208\n",
      "# Iteration  7261 -> Loss: 0.3678835367339412 \t| Accuracy: 105.208\n",
      "# Iteration  7262 -> Loss: 0.36786907430654536 \t| Accuracy: 105.208\n",
      "# Iteration  7263 -> Loss: 0.36785461572850986 \t| Accuracy: 105.208\n",
      "# Iteration  7264 -> Loss: 0.3678401609979885 \t| Accuracy: 105.208\n",
      "# Iteration  7265 -> Loss: 0.36782571011313636 \t| Accuracy: 105.208\n",
      "# Iteration  7266 -> Loss: 0.36781126307210965 \t| Accuracy: 105.208\n",
      "# Iteration  7267 -> Loss: 0.3677968198730657 \t| Accuracy: 105.208\n",
      "# Iteration  7268 -> Loss: 0.3677823805141628 \t| Accuracy: 105.208\n",
      "# Iteration  7269 -> Loss: 0.36776794499356075 \t| Accuracy: 105.208\n",
      "# Iteration  7270 -> Loss: 0.3677535133094201 \t| Accuracy: 105.208\n",
      "# Iteration  7271 -> Loss: 0.3677390854599027 \t| Accuracy: 105.208\n",
      "# Iteration  7272 -> Loss: 0.36772466144317145 \t| Accuracy: 105.208\n",
      "# Iteration  7273 -> Loss: 0.36771024125739055 \t| Accuracy: 105.208\n",
      "# Iteration  7274 -> Loss: 0.36769582490072517 \t| Accuracy: 105.208\n",
      "# Iteration  7275 -> Loss: 0.3676814123713415 \t| Accuracy: 105.208\n",
      "# Iteration  7276 -> Loss: 0.36766700366740707 \t| Accuracy: 105.208\n",
      "# Iteration  7277 -> Loss: 0.36765259878709045 \t| Accuracy: 105.208\n",
      "# Iteration  7278 -> Loss: 0.3676381977285612 \t| Accuracy: 105.208\n",
      "# Iteration  7279 -> Loss: 0.3676238004899902 \t| Accuracy: 105.208\n",
      "# Iteration  7280 -> Loss: 0.3676094070695495 \t| Accuracy: 105.208\n",
      "# Iteration  7281 -> Loss: 0.36759501746541196 \t| Accuracy: 105.208\n",
      "# Iteration  7282 -> Loss: 0.3675806316757518 \t| Accuracy: 105.208\n",
      "# Iteration  7283 -> Loss: 0.3675662496987442 \t| Accuracy: 105.208\n",
      "# Iteration  7284 -> Loss: 0.3675518715325655 \t| Accuracy: 105.208\n",
      "# Iteration  7285 -> Loss: 0.36753749717539347 \t| Accuracy: 105.208\n",
      "# Iteration  7286 -> Loss: 0.3675231266254064 \t| Accuracy: 105.208\n",
      "# Iteration  7287 -> Loss: 0.3675087598807842 \t| Accuracy: 105.208\n",
      "# Iteration  7288 -> Loss: 0.36749439693970753 \t| Accuracy: 105.208\n",
      "# Iteration  7289 -> Loss: 0.3674800378003584 \t| Accuracy: 105.208\n",
      "# Iteration  7290 -> Loss: 0.3674656824609201 \t| Accuracy: 105.208\n",
      "# Iteration  7291 -> Loss: 0.36745133091957644 \t| Accuracy: 105.208\n",
      "# Iteration  7292 -> Loss: 0.36743698317451284 \t| Accuracy: 105.208\n",
      "# Iteration  7293 -> Loss: 0.36742263922391566 \t| Accuracy: 105.208\n",
      "# Iteration  7294 -> Loss: 0.36740829906597233 \t| Accuracy: 105.208\n",
      "# Iteration  7295 -> Loss: 0.3673939626988715 \t| Accuracy: 105.208\n",
      "# Iteration  7296 -> Loss: 0.3673796301208029 \t| Accuracy: 105.208\n",
      "# Iteration  7297 -> Loss: 0.3673653013299572 \t| Accuracy: 105.208\n",
      "# Iteration  7298 -> Loss: 0.36735097632452635 \t| Accuracy: 105.208\n",
      "# Iteration  7299 -> Loss: 0.36733665510270347 \t| Accuracy: 105.208\n",
      "# Iteration  7300 -> Loss: 0.36732233766268246 \t| Accuracy: 105.208\n",
      "# Iteration  7301 -> Loss: 0.36730802400265866 \t| Accuracy: 105.208\n",
      "# Iteration  7302 -> Loss: 0.36729371412082834 \t| Accuracy: 105.208\n",
      "# Iteration  7303 -> Loss: 0.36727940801538883 \t| Accuracy: 105.208\n",
      "# Iteration  7304 -> Loss: 0.3672651056845388 \t| Accuracy: 105.208\n",
      "# Iteration  7305 -> Loss: 0.3672508071264776 \t| Accuracy: 105.208\n",
      "# Iteration  7306 -> Loss: 0.36723651233940624 \t| Accuracy: 105.208\n",
      "# Iteration  7307 -> Loss: 0.36722222132152627 \t| Accuracy: 105.208\n",
      "# Iteration  7308 -> Loss: 0.36720793407104063 \t| Accuracy: 105.208\n",
      "# Iteration  7309 -> Loss: 0.3671936505861533 \t| Accuracy: 105.208\n",
      "# Iteration  7310 -> Loss: 0.3671793708650694 \t| Accuracy: 105.208\n",
      "# Iteration  7311 -> Loss: 0.367165094905995 \t| Accuracy: 105.208\n",
      "# Iteration  7312 -> Loss: 0.36715082270713756 \t| Accuracy: 105.208\n",
      "# Iteration  7313 -> Loss: 0.3671365542667052 \t| Accuracy: 105.208\n",
      "# Iteration  7314 -> Loss: 0.36712228958290755 \t| Accuracy: 105.208\n",
      "# Iteration  7315 -> Loss: 0.3671080286539551 \t| Accuracy: 105.208\n",
      "# Iteration  7316 -> Loss: 0.3670937714780593 \t| Accuracy: 105.208\n",
      "# Iteration  7317 -> Loss: 0.3670795180534331 \t| Accuracy: 105.208\n",
      "# Iteration  7318 -> Loss: 0.3670652683782901 \t| Accuracy: 105.208\n",
      "# Iteration  7319 -> Loss: 0.3670510224508453 \t| Accuracy: 105.208\n",
      "# Iteration  7320 -> Loss: 0.36703678026931463 \t| Accuracy: 105.208\n",
      "# Iteration  7321 -> Loss: 0.3670225418319151 \t| Accuracy: 105.208\n",
      "# Iteration  7322 -> Loss: 0.3670083071368649 \t| Accuracy: 105.208\n",
      "# Iteration  7323 -> Loss: 0.36699407618238333 \t| Accuracy: 105.208\n",
      "# Iteration  7324 -> Loss: 0.36697984896669056 \t| Accuracy: 105.208\n",
      "# Iteration  7325 -> Loss: 0.366965625488008 \t| Accuracy: 105.208\n",
      "# Iteration  7326 -> Loss: 0.3669514057445582 \t| Accuracy: 105.208\n",
      "# Iteration  7327 -> Loss: 0.3669371897345647 \t| Accuracy: 105.208\n",
      "# Iteration  7328 -> Loss: 0.3669229774562519 \t| Accuracy: 105.208\n",
      "# Iteration  7329 -> Loss: 0.3669087689078458 \t| Accuracy: 105.208\n",
      "# Iteration  7330 -> Loss: 0.36689456408757304 \t| Accuracy: 105.208\n",
      "# Iteration  7331 -> Loss: 0.36688036299366156 \t| Accuracy: 105.208\n",
      "# Iteration  7332 -> Loss: 0.36686616562434005 \t| Accuracy: 105.208\n",
      "# Iteration  7333 -> Loss: 0.3668519719778389 \t| Accuracy: 105.208\n",
      "# Iteration  7334 -> Loss: 0.36683778205238887 \t| Accuracy: 105.208\n",
      "# Iteration  7335 -> Loss: 0.3668235958462223 \t| Accuracy: 105.208\n",
      "# Iteration  7336 -> Loss: 0.3668094133575724 \t| Accuracy: 105.208\n",
      "# Iteration  7337 -> Loss: 0.3667952345846734 \t| Accuracy: 105.208\n",
      "# Iteration  7338 -> Loss: 0.3667810595257608 \t| Accuracy: 105.208\n",
      "# Iteration  7339 -> Loss: 0.3667668881790708 \t| Accuracy: 105.208\n",
      "# Iteration  7340 -> Loss: 0.36675272054284136 \t| Accuracy: 105.208\n",
      "# Iteration  7341 -> Loss: 0.3667385566153105 \t| Accuracy: 105.208\n",
      "# Iteration  7342 -> Loss: 0.3667243963947185 \t| Accuracy: 105.208\n",
      "# Iteration  7343 -> Loss: 0.3667102398793056 \t| Accuracy: 105.208\n",
      "# Iteration  7344 -> Loss: 0.3666960870673137 \t| Accuracy: 105.208\n",
      "# Iteration  7345 -> Loss: 0.3666819379569858 \t| Accuracy: 105.208\n",
      "# Iteration  7346 -> Loss: 0.3666677925465656 \t| Accuracy: 105.208\n",
      "# Iteration  7347 -> Loss: 0.3666536508342983 \t| Accuracy: 105.208\n",
      "# Iteration  7348 -> Loss: 0.3666395128184298 \t| Accuracy: 105.208\n",
      "# Iteration  7349 -> Loss: 0.36662537849720744 \t| Accuracy: 105.208\n",
      "# Iteration  7350 -> Loss: 0.3666112478688791 \t| Accuracy: 105.208\n",
      "# Iteration  7351 -> Loss: 0.36659712093169416 \t| Accuracy: 105.208\n",
      "# Iteration  7352 -> Loss: 0.36658299768390284 \t| Accuracy: 105.208\n",
      "# Iteration  7353 -> Loss: 0.3665688781237567 \t| Accuracy: 105.208\n",
      "# Iteration  7354 -> Loss: 0.36655476224950795 \t| Accuracy: 105.208\n",
      "# Iteration  7355 -> Loss: 0.3665406500594101 \t| Accuracy: 105.208\n",
      "# Iteration  7356 -> Loss: 0.3665265415517178 \t| Accuracy: 105.208\n",
      "# Iteration  7357 -> Loss: 0.3665124367246866 \t| Accuracy: 105.208\n",
      "# Iteration  7358 -> Loss: 0.36649833557657296 \t| Accuracy: 105.208\n",
      "# Iteration  7359 -> Loss: 0.3664842381056348 \t| Accuracy: 105.208\n",
      "# Iteration  7360 -> Loss: 0.3664701443101308 \t| Accuracy: 105.208\n",
      "# Iteration  7361 -> Loss: 0.36645605418832083 \t| Accuracy: 105.208\n",
      "# Iteration  7362 -> Loss: 0.36644196773846555 \t| Accuracy: 105.208\n",
      "# Iteration  7363 -> Loss: 0.36642788495882717 \t| Accuracy: 105.208\n",
      "# Iteration  7364 -> Loss: 0.3664138058476684 \t| Accuracy: 105.208\n",
      "# Iteration  7365 -> Loss: 0.36639973040325347 \t| Accuracy: 105.208\n",
      "# Iteration  7366 -> Loss: 0.36638565862384725 \t| Accuracy: 105.208\n",
      "# Iteration  7367 -> Loss: 0.366371590507716 \t| Accuracy: 105.208\n",
      "# Iteration  7368 -> Loss: 0.36635752605312677 \t| Accuracy: 105.208\n",
      "# Iteration  7369 -> Loss: 0.36634346525834793 \t| Accuracy: 105.208\n",
      "# Iteration  7370 -> Loss: 0.3663294081216486 \t| Accuracy: 105.208\n",
      "# Iteration  7371 -> Loss: 0.3663153546412991 \t| Accuracy: 105.208\n",
      "# Iteration  7372 -> Loss: 0.3663013048155708 \t| Accuracy: 105.208\n",
      "# Iteration  7373 -> Loss: 0.3662872586427362 \t| Accuracy: 105.208\n",
      "# Iteration  7374 -> Loss: 0.3662732161210686 \t| Accuracy: 105.208\n",
      "# Iteration  7375 -> Loss: 0.3662591772488426 \t| Accuracy: 105.208\n",
      "# Iteration  7376 -> Loss: 0.3662451420243336 \t| Accuracy: 105.208\n",
      "# Iteration  7377 -> Loss: 0.36623111044581824 \t| Accuracy: 105.208\n",
      "# Iteration  7378 -> Loss: 0.36621708251157414 \t| Accuracy: 105.208\n",
      "# Iteration  7379 -> Loss: 0.3662030582198799 \t| Accuracy: 105.208\n",
      "# Iteration  7380 -> Loss: 0.36618903756901516 \t| Accuracy: 105.208\n",
      "# Iteration  7381 -> Loss: 0.3661750205572608 \t| Accuracy: 105.208\n",
      "# Iteration  7382 -> Loss: 0.3661610071828985 \t| Accuracy: 105.208\n",
      "# Iteration  7383 -> Loss: 0.36614699744421114 \t| Accuracy: 105.208\n",
      "# Iteration  7384 -> Loss: 0.36613299133948235 \t| Accuracy: 105.208\n",
      "# Iteration  7385 -> Loss: 0.36611898886699723 \t| Accuracy: 105.208\n",
      "# Iteration  7386 -> Loss: 0.3661049900250415 \t| Accuracy: 105.208\n",
      "# Iteration  7387 -> Loss: 0.3660909948119022 \t| Accuracy: 105.208\n",
      "# Iteration  7388 -> Loss: 0.36607700322586745 \t| Accuracy: 105.208\n",
      "# Iteration  7389 -> Loss: 0.366063015265226 \t| Accuracy: 105.208\n",
      "# Iteration  7390 -> Loss: 0.366049030928268 \t| Accuracy: 105.208\n",
      "# Iteration  7391 -> Loss: 0.3660350502132846 \t| Accuracy: 105.208\n",
      "# Iteration  7392 -> Loss: 0.36602107311856774 \t| Accuracy: 105.208\n",
      "# Iteration  7393 -> Loss: 0.3660070996424107 \t| Accuracy: 105.208\n",
      "# Iteration  7394 -> Loss: 0.36599312978310755 \t| Accuracy: 105.208\n",
      "# Iteration  7395 -> Loss: 0.3659791635389536 \t| Accuracy: 105.208\n",
      "# Iteration  7396 -> Loss: 0.36596520090824486 \t| Accuracy: 105.208\n",
      "# Iteration  7397 -> Loss: 0.3659512418892787 \t| Accuracy: 105.208\n",
      "# Iteration  7398 -> Loss: 0.3659372864803534 \t| Accuracy: 105.208\n",
      "# Iteration  7399 -> Loss: 0.3659233346797683 \t| Accuracy: 105.208\n",
      "# Iteration  7400 -> Loss: 0.3659093864858235 \t| Accuracy: 105.208\n",
      "# Iteration  7401 -> Loss: 0.36589544189682066 \t| Accuracy: 105.208\n",
      "# Iteration  7402 -> Loss: 0.365881500911062 \t| Accuracy: 105.208\n",
      "# Iteration  7403 -> Loss: 0.36586756352685074 \t| Accuracy: 105.208\n",
      "# Iteration  7404 -> Loss: 0.3658536297424915 \t| Accuracy: 105.208\n",
      "# Iteration  7405 -> Loss: 0.36583969955628964 \t| Accuracy: 105.208\n",
      "# Iteration  7406 -> Loss: 0.36582577296655167 \t| Accuracy: 105.208\n",
      "# Iteration  7407 -> Loss: 0.3658118499715849 \t| Accuracy: 105.208\n",
      "# Iteration  7408 -> Loss: 0.365797930569698 \t| Accuracy: 105.208\n",
      "# Iteration  7409 -> Loss: 0.3657840147592004 \t| Accuracy: 105.208\n",
      "# Iteration  7410 -> Loss: 0.3657701025384026 \t| Accuracy: 105.208\n",
      "# Iteration  7411 -> Loss: 0.36575619390561614 \t| Accuracy: 105.208\n",
      "# Iteration  7412 -> Loss: 0.36574228885915366 \t| Accuracy: 105.208\n",
      "# Iteration  7413 -> Loss: 0.36572838739732866 \t| Accuracy: 105.208\n",
      "# Iteration  7414 -> Loss: 0.3657144895184557 \t| Accuracy: 105.208\n",
      "# Iteration  7415 -> Loss: 0.36570059522085047 \t| Accuracy: 105.208\n",
      "# Iteration  7416 -> Loss: 0.3656867045028294 \t| Accuracy: 105.208\n",
      "# Iteration  7417 -> Loss: 0.36567281736271035 \t| Accuracy: 105.208\n",
      "# Iteration  7418 -> Loss: 0.3656589337988117 \t| Accuracy: 105.208\n",
      "# Iteration  7419 -> Loss: 0.3656450538094534 \t| Accuracy: 105.208\n",
      "# Iteration  7420 -> Loss: 0.3656311773929558 \t| Accuracy: 105.208\n",
      "# Iteration  7421 -> Loss: 0.3656173045476408 \t| Accuracy: 105.208\n",
      "# Iteration  7422 -> Loss: 0.3656034352718309 \t| Accuracy: 105.208\n",
      "# Iteration  7423 -> Loss: 0.3655895695638499 \t| Accuracy: 105.208\n",
      "# Iteration  7424 -> Loss: 0.36557570742202233 \t| Accuracy: 105.208\n",
      "# Iteration  7425 -> Loss: 0.365561848844674 \t| Accuracy: 105.208\n",
      "# Iteration  7426 -> Loss: 0.36554799383013165 \t| Accuracy: 105.208\n",
      "# Iteration  7427 -> Loss: 0.3655341423767229 \t| Accuracy: 105.208\n",
      "# Iteration  7428 -> Loss: 0.3655202944827764 \t| Accuracy: 105.208\n",
      "# Iteration  7429 -> Loss: 0.36550645014662203 \t| Accuracy: 105.208\n",
      "# Iteration  7430 -> Loss: 0.3654926093665904 \t| Accuracy: 105.208\n",
      "# Iteration  7431 -> Loss: 0.3654787721410132 \t| Accuracy: 105.208\n",
      "# Iteration  7432 -> Loss: 0.3654649384682232 \t| Accuracy: 105.208\n",
      "# Iteration  7433 -> Loss: 0.36545110834655403 \t| Accuracy: 105.208\n",
      "# Iteration  7434 -> Loss: 0.36543728177434054 \t| Accuracy: 105.208\n",
      "# Iteration  7435 -> Loss: 0.3654234587499182 \t| Accuracy: 105.208\n",
      "# Iteration  7436 -> Loss: 0.36540963927162406 \t| Accuracy: 105.208\n",
      "# Iteration  7437 -> Loss: 0.36539582333779563 \t| Accuracy: 105.208\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Iteration  7438 -> Loss: 0.36538201094677164 \t| Accuracy: 105.208\n",
      "# Iteration  7439 -> Loss: 0.3653682020968917 \t| Accuracy: 105.208\n",
      "# Iteration  7440 -> Loss: 0.3653543967864967 \t| Accuracy: 105.208\n",
      "# Iteration  7441 -> Loss: 0.3653405950139281 \t| Accuracy: 105.208\n",
      "# Iteration  7442 -> Loss: 0.36532679677752894 \t| Accuracy: 105.208\n",
      "# Iteration  7443 -> Loss: 0.36531300207564255 \t| Accuracy: 105.208\n",
      "# Iteration  7444 -> Loss: 0.3652992109066137 \t| Accuracy: 105.208\n",
      "# Iteration  7445 -> Loss: 0.36528542326878816 \t| Accuracy: 105.208\n",
      "# Iteration  7446 -> Loss: 0.3652716391605124 \t| Accuracy: 105.208\n",
      "# Iteration  7447 -> Loss: 0.36525785858013426 \t| Accuracy: 105.208\n",
      "# Iteration  7448 -> Loss: 0.3652440815260023 \t| Accuracy: 105.208\n",
      "# Iteration  7449 -> Loss: 0.36523030799646605 \t| Accuracy: 105.208\n",
      "# Iteration  7450 -> Loss: 0.3652165379898762 \t| Accuracy: 105.208\n",
      "# Iteration  7451 -> Loss: 0.36520277150458447 \t| Accuracy: 105.208\n",
      "# Iteration  7452 -> Loss: 0.3651890085389432 \t| Accuracy: 105.208\n",
      "# Iteration  7453 -> Loss: 0.36517524909130616 \t| Accuracy: 105.208\n",
      "# Iteration  7454 -> Loss: 0.3651614931600277 \t| Accuracy: 105.208\n",
      "# Iteration  7455 -> Loss: 0.36514774074346357 \t| Accuracy: 105.208\n",
      "# Iteration  7456 -> Loss: 0.3651339918399703 \t| Accuracy: 105.208\n",
      "# Iteration  7457 -> Loss: 0.3651202464479052 \t| Accuracy: 105.208\n",
      "# Iteration  7458 -> Loss: 0.3651065045656268 \t| Accuracy: 105.208\n",
      "# Iteration  7459 -> Loss: 0.3650927661914949 \t| Accuracy: 105.208\n",
      "# Iteration  7460 -> Loss: 0.36507903132386954 \t| Accuracy: 105.208\n",
      "# Iteration  7461 -> Loss: 0.36506529996111237 \t| Accuracy: 105.208\n",
      "# Iteration  7462 -> Loss: 0.36505157210158573 \t| Accuracy: 105.208\n",
      "# Iteration  7463 -> Loss: 0.36503784774365305 \t| Accuracy: 105.208\n",
      "# Iteration  7464 -> Loss: 0.3650241268856787 \t| Accuracy: 105.208\n",
      "# Iteration  7465 -> Loss: 0.36501040952602803 \t| Accuracy: 105.208\n",
      "# Iteration  7466 -> Loss: 0.3649966956630673 \t| Accuracy: 105.208\n",
      "# Iteration  7467 -> Loss: 0.36498298529516393 \t| Accuracy: 105.208\n",
      "# Iteration  7468 -> Loss: 0.36496927842068605 \t| Accuracy: 105.208\n",
      "# Iteration  7469 -> Loss: 0.36495557503800313 \t| Accuracy: 105.208\n",
      "# Iteration  7470 -> Loss: 0.36494187514548515 \t| Accuracy: 105.208\n",
      "# Iteration  7471 -> Loss: 0.36492817874150346 \t| Accuracy: 105.208\n",
      "# Iteration  7472 -> Loss: 0.3649144858244302 \t| Accuracy: 105.208\n",
      "# Iteration  7473 -> Loss: 0.36490079639263856 \t| Accuracy: 105.208\n",
      "# Iteration  7474 -> Loss: 0.36488711044450256 \t| Accuracy: 105.208\n",
      "# Iteration  7475 -> Loss: 0.3648734279783973 \t| Accuracy: 105.208\n",
      "# Iteration  7476 -> Loss: 0.36485974899269896 \t| Accuracy: 105.208\n",
      "# Iteration  7477 -> Loss: 0.36484607348578446 \t| Accuracy: 105.208\n",
      "# Iteration  7478 -> Loss: 0.3648324014560318 \t| Accuracy: 105.208\n",
      "# Iteration  7479 -> Loss: 0.36481873290181993 \t| Accuracy: 105.208\n",
      "# Iteration  7480 -> Loss: 0.36480506782152883 \t| Accuracy: 105.208\n",
      "# Iteration  7481 -> Loss: 0.36479140621353945 \t| Accuracy: 105.208\n",
      "# Iteration  7482 -> Loss: 0.3647777480762335 \t| Accuracy: 105.208\n",
      "# Iteration  7483 -> Loss: 0.364764093407994 \t| Accuracy: 105.208\n",
      "# Iteration  7484 -> Loss: 0.36475044220720443 \t| Accuracy: 105.208\n",
      "# Iteration  7485 -> Loss: 0.36473679447224977 \t| Accuracy: 105.208\n",
      "# Iteration  7486 -> Loss: 0.36472315020151586 \t| Accuracy: 105.208\n",
      "# Iteration  7487 -> Loss: 0.3647095093933892 \t| Accuracy: 105.208\n",
      "# Iteration  7488 -> Loss: 0.3646958720462574 \t| Accuracy: 105.208\n",
      "# Iteration  7489 -> Loss: 0.36468223815850914 \t| Accuracy: 105.208\n",
      "# Iteration  7490 -> Loss: 0.36466860772853393 \t| Accuracy: 105.208\n",
      "# Iteration  7491 -> Loss: 0.3646549807547224 \t| Accuracy: 105.208\n",
      "# Iteration  7492 -> Loss: 0.3646413572354659 \t| Accuracy: 105.208\n",
      "# Iteration  7493 -> Loss: 0.36462773716915703 \t| Accuracy: 105.208\n",
      "# Iteration  7494 -> Loss: 0.364614120554189 \t| Accuracy: 105.208\n",
      "# Iteration  7495 -> Loss: 0.3646005073889563 \t| Accuracy: 105.208\n",
      "# Iteration  7496 -> Loss: 0.3645868976718542 \t| Accuracy: 105.208\n",
      "# Iteration  7497 -> Loss: 0.364573291401279 \t| Accuracy: 105.208\n",
      "# Iteration  7498 -> Loss: 0.36455968857562776 \t| Accuracy: 105.208\n",
      "# Iteration  7499 -> Loss: 0.36454608919329884 \t| Accuracy: 105.208\n",
      "# Iteration  7500 -> Loss: 0.3645324932526914 \t| Accuracy: 105.208\n",
      "# Iteration  7501 -> Loss: 0.36451890075220533 \t| Accuracy: 105.208\n",
      "# Iteration  7502 -> Loss: 0.36450531169024186 \t| Accuracy: 105.208\n",
      "# Iteration  7503 -> Loss: 0.3644917260652027 \t| Accuracy: 105.208\n",
      "# Iteration  7504 -> Loss: 0.3644781438754911 \t| Accuracy: 105.208\n",
      "# Iteration  7505 -> Loss: 0.3644645651195108 \t| Accuracy: 105.208\n",
      "# Iteration  7506 -> Loss: 0.36445098979566665 \t| Accuracy: 105.208\n",
      "# Iteration  7507 -> Loss: 0.3644374179023643 \t| Accuracy: 105.208\n",
      "# Iteration  7508 -> Loss: 0.3644238494380107 \t| Accuracy: 105.208\n",
      "# Iteration  7509 -> Loss: 0.3644102844010135 \t| Accuracy: 105.208\n",
      "# Iteration  7510 -> Loss: 0.3643967227897812 \t| Accuracy: 105.208\n",
      "# Iteration  7511 -> Loss: 0.3643831646027234 \t| Accuracy: 105.208\n",
      "# Iteration  7512 -> Loss: 0.36436960983825073 \t| Accuracy: 105.208\n",
      "# Iteration  7513 -> Loss: 0.3643560584947745 \t| Accuracy: 105.208\n",
      "# Iteration  7514 -> Loss: 0.36434251057070727 \t| Accuracy: 105.208\n",
      "# Iteration  7515 -> Loss: 0.36432896606446213 \t| Accuracy: 105.208\n",
      "# Iteration  7516 -> Loss: 0.3643154249744537 \t| Accuracy: 105.208\n",
      "# Iteration  7517 -> Loss: 0.364301887299097 \t| Accuracy: 105.208\n",
      "# Iteration  7518 -> Loss: 0.36428835303680834 \t| Accuracy: 105.208\n",
      "# Iteration  7519 -> Loss: 0.36427482218600465 \t| Accuracy: 105.208\n",
      "# Iteration  7520 -> Loss: 0.3642612947451041 \t| Accuracy: 105.208\n",
      "# Iteration  7521 -> Loss: 0.36424777071252573 \t| Accuracy: 105.208\n",
      "# Iteration  7522 -> Loss: 0.36423425008668947 \t| Accuracy: 105.208\n",
      "# Iteration  7523 -> Loss: 0.3642207328660161 \t| Accuracy: 105.208\n",
      "# Iteration  7524 -> Loss: 0.3642072190489273 \t| Accuracy: 105.208\n",
      "# Iteration  7525 -> Loss: 0.3641937086338462 \t| Accuracy: 105.208\n",
      "# Iteration  7526 -> Loss: 0.3641802016191961 \t| Accuracy: 105.208\n",
      "# Iteration  7527 -> Loss: 0.3641666980034019 \t| Accuracy: 105.208\n",
      "# Iteration  7528 -> Loss: 0.36415319778488897 \t| Accuracy: 105.208\n",
      "# Iteration  7529 -> Loss: 0.36413970096208387 \t| Accuracy: 105.208\n",
      "# Iteration  7530 -> Loss: 0.364126207533414 \t| Accuracy: 105.208\n",
      "# Iteration  7531 -> Loss: 0.36411271749730767 \t| Accuracy: 105.208\n",
      "# Iteration  7532 -> Loss: 0.3640992308521942 \t| Accuracy: 105.208\n",
      "# Iteration  7533 -> Loss: 0.36408574759650386 \t| Accuracy: 105.208\n",
      "# Iteration  7534 -> Loss: 0.36407226772866774 \t| Accuracy: 105.208\n",
      "# Iteration  7535 -> Loss: 0.3640587912471178 \t| Accuracy: 105.208\n",
      "# Iteration  7536 -> Loss: 0.3640453181502873 \t| Accuracy: 105.208\n",
      "# Iteration  7537 -> Loss: 0.3640318484366099 \t| Accuracy: 105.208\n",
      "# Iteration  7538 -> Loss: 0.3640183821045206 \t| Accuracy: 105.208\n",
      "# Iteration  7539 -> Loss: 0.36400491915245525 \t| Accuracy: 105.208\n",
      "# Iteration  7540 -> Loss: 0.3639914595788504 \t| Accuracy: 105.208\n",
      "# Iteration  7541 -> Loss: 0.3639780033821437 \t| Accuracy: 105.208\n",
      "# Iteration  7542 -> Loss: 0.3639645505607739 \t| Accuracy: 105.208\n",
      "# Iteration  7543 -> Loss: 0.36395110111318035 \t| Accuracy: 105.208\n",
      "# Iteration  7544 -> Loss: 0.3639376550378035 \t| Accuracy: 105.208\n",
      "# Iteration  7545 -> Loss: 0.36392421233308464 \t| Accuracy: 105.208\n",
      "# Iteration  7546 -> Loss: 0.363910772997466 \t| Accuracy: 105.208\n",
      "# Iteration  7547 -> Loss: 0.363897337029391 \t| Accuracy: 105.208\n",
      "# Iteration  7548 -> Loss: 0.3638839044273034 \t| Accuracy: 105.208\n",
      "# Iteration  7549 -> Loss: 0.36387047518964843 \t| Accuracy: 105.208\n",
      "# Iteration  7550 -> Loss: 0.36385704931487206 \t| Accuracy: 105.208\n",
      "# Iteration  7551 -> Loss: 0.363843626801421 \t| Accuracy: 105.208\n",
      "# Iteration  7552 -> Loss: 0.36383020764774326 \t| Accuracy: 105.208\n",
      "# Iteration  7553 -> Loss: 0.3638167918522872 \t| Accuracy: 105.208\n",
      "# Iteration  7554 -> Loss: 0.3638033794135027 \t| Accuracy: 105.208\n",
      "# Iteration  7555 -> Loss: 0.36378997032984034 \t| Accuracy: 105.208\n",
      "# Iteration  7556 -> Loss: 0.36377656459975144 \t| Accuracy: 105.208\n",
      "# Iteration  7557 -> Loss: 0.3637631622216882 \t| Accuracy: 105.208\n",
      "# Iteration  7558 -> Loss: 0.3637497631941042 \t| Accuracy: 105.208\n",
      "# Iteration  7559 -> Loss: 0.3637363675154536 \t| Accuracy: 105.208\n",
      "# Iteration  7560 -> Loss: 0.3637229751841912 \t| Accuracy: 105.208\n",
      "# Iteration  7561 -> Loss: 0.3637095861987734 \t| Accuracy: 105.208\n",
      "# Iteration  7562 -> Loss: 0.36369620055765695 \t| Accuracy: 105.208\n",
      "# Iteration  7563 -> Loss: 0.36368281825929966 \t| Accuracy: 105.208\n",
      "# Iteration  7564 -> Loss: 0.3636694393021605 \t| Accuracy: 105.208\n",
      "# Iteration  7565 -> Loss: 0.3636560636846989 \t| Accuracy: 105.208\n",
      "# Iteration  7566 -> Loss: 0.36364269140537553 \t| Accuracy: 105.208\n",
      "# Iteration  7567 -> Loss: 0.36362932246265184 \t| Accuracy: 105.208\n",
      "# Iteration  7568 -> Loss: 0.3636159568549903 \t| Accuracy: 105.208\n",
      "# Iteration  7569 -> Loss: 0.3636025945808541 \t| Accuracy: 105.208\n",
      "# Iteration  7570 -> Loss: 0.36358923563870754 \t| Accuracy: 105.208\n",
      "# Iteration  7571 -> Loss: 0.3635758800270157 \t| Accuracy: 105.208\n",
      "# Iteration  7572 -> Loss: 0.3635625277442446 \t| Accuracy: 105.208\n",
      "# Iteration  7573 -> Loss: 0.3635491787888611 \t| Accuracy: 105.208\n",
      "# Iteration  7574 -> Loss: 0.3635358331593333 \t| Accuracy: 105.208\n",
      "# Iteration  7575 -> Loss: 0.3635224908541295 \t| Accuracy: 105.208\n",
      "# Iteration  7576 -> Loss: 0.3635091518717197 \t| Accuracy: 105.208\n",
      "# Iteration  7577 -> Loss: 0.3634958162105743 \t| Accuracy: 105.208\n",
      "# Iteration  7578 -> Loss: 0.3634824838691648 \t| Accuracy: 105.208\n",
      "# Iteration  7579 -> Loss: 0.36346915484596354 \t| Accuracy: 105.208\n",
      "# Iteration  7580 -> Loss: 0.3634558291394436 \t| Accuracy: 105.208\n",
      "# Iteration  7581 -> Loss: 0.36344250674807915 \t| Accuracy: 105.208\n",
      "# Iteration  7582 -> Loss: 0.3634291876703455 \t| Accuracy: 105.208\n",
      "# Iteration  7583 -> Loss: 0.36341587190471836 \t| Accuracy: 105.208\n",
      "# Iteration  7584 -> Loss: 0.36340255944967464 \t| Accuracy: 105.208\n",
      "# Iteration  7585 -> Loss: 0.36338925030369207 \t| Accuracy: 105.208\n",
      "# Iteration  7586 -> Loss: 0.3633759444652493 \t| Accuracy: 105.208\n",
      "# Iteration  7587 -> Loss: 0.36336264193282586 \t| Accuracy: 105.208\n",
      "# Iteration  7588 -> Loss: 0.36334934270490216 \t| Accuracy: 105.208\n",
      "# Iteration  7589 -> Loss: 0.36333604677995957 \t| Accuracy: 105.208\n",
      "# Iteration  7590 -> Loss: 0.3633227541564801 \t| Accuracy: 105.208\n",
      "# Iteration  7591 -> Loss: 0.3633094648329471 \t| Accuracy: 105.208\n",
      "# Iteration  7592 -> Loss: 0.3632961788078445 \t| Accuracy: 105.208\n",
      "# Iteration  7593 -> Loss: 0.3632828960796573 \t| Accuracy: 105.208\n",
      "# Iteration  7594 -> Loss: 0.3632696166468711 \t| Accuracy: 105.208\n",
      "# Iteration  7595 -> Loss: 0.36325634050797256 \t| Accuracy: 105.208\n",
      "# Iteration  7596 -> Loss: 0.3632430676614494 \t| Accuracy: 105.208\n",
      "# Iteration  7597 -> Loss: 0.36322979810579015 \t| Accuracy: 105.208\n",
      "# Iteration  7598 -> Loss: 0.3632165318394839 \t| Accuracy: 105.208\n",
      "# Iteration  7599 -> Loss: 0.36320326886102106 \t| Accuracy: 105.208\n",
      "# Iteration  7600 -> Loss: 0.36319000916889266 \t| Accuracy: 105.208\n",
      "# Iteration  7601 -> Loss: 0.3631767527615909 \t| Accuracy: 105.208\n",
      "# Iteration  7602 -> Loss: 0.36316349963760847 \t| Accuracy: 105.208\n",
      "# Iteration  7603 -> Loss: 0.36315024979543925 \t| Accuracy: 105.208\n",
      "# Iteration  7604 -> Loss: 0.36313700323357784 \t| Accuracy: 105.208\n",
      "# Iteration  7605 -> Loss: 0.36312375995051993 \t| Accuracy: 105.208\n",
      "# Iteration  7606 -> Loss: 0.36311051994476184 \t| Accuracy: 105.208\n",
      "# Iteration  7607 -> Loss: 0.36309728321480084 \t| Accuracy: 105.208\n",
      "# Iteration  7608 -> Loss: 0.3630840497591354 \t| Accuracy: 105.208\n",
      "# Iteration  7609 -> Loss: 0.36307081957626436 \t| Accuracy: 105.208\n",
      "# Iteration  7610 -> Loss: 0.3630575926646878 \t| Accuracy: 105.833\n",
      "# Iteration  7611 -> Loss: 0.3630443690229066 \t| Accuracy: 105.833\n",
      "# Iteration  7612 -> Loss: 0.3630311486494224 \t| Accuracy: 105.833\n",
      "# Iteration  7613 -> Loss: 0.36301793154273776 \t| Accuracy: 105.833\n",
      "# Iteration  7614 -> Loss: 0.36300471770135645 \t| Accuracy: 105.833\n",
      "# Iteration  7615 -> Loss: 0.3629915071237825 \t| Accuracy: 105.833\n",
      "# Iteration  7616 -> Loss: 0.3629782998085214 \t| Accuracy: 105.833\n",
      "# Iteration  7617 -> Loss: 0.36296509575407926 \t| Accuracy: 105.833\n",
      "# Iteration  7618 -> Loss: 0.36295189495896296 \t| Accuracy: 105.833\n",
      "# Iteration  7619 -> Loss: 0.36293869742168045 \t| Accuracy: 105.833\n",
      "# Iteration  7620 -> Loss: 0.3629255031407404 \t| Accuracy: 105.833\n",
      "# Iteration  7621 -> Loss: 0.36291231211465264 \t| Accuracy: 105.833\n",
      "# Iteration  7622 -> Loss: 0.3628991243419275 \t| Accuracy: 105.833\n",
      "# Iteration  7623 -> Loss: 0.3628859398210764 \t| Accuracy: 105.833\n",
      "# Iteration  7624 -> Loss: 0.36287275855061163 \t| Accuracy: 105.833\n",
      "# Iteration  7625 -> Loss: 0.36285958052904627 \t| Accuracy: 105.833\n",
      "# Iteration  7626 -> Loss: 0.36284640575489435 \t| Accuracy: 105.833\n",
      "# Iteration  7627 -> Loss: 0.3628332342266708 \t| Accuracy: 105.833\n",
      "# Iteration  7628 -> Loss: 0.36282006594289123 \t| Accuracy: 105.833\n",
      "# Iteration  7629 -> Loss: 0.3628069009020723 \t| Accuracy: 105.833\n",
      "# Iteration  7630 -> Loss: 0.36279373910273155 \t| Accuracy: 105.833\n",
      "# Iteration  7631 -> Loss: 0.3627805805433873 \t| Accuracy: 105.833\n",
      "# Iteration  7632 -> Loss: 0.3627674252225586 \t| Accuracy: 105.833\n",
      "# Iteration  7633 -> Loss: 0.36275427313876574 \t| Accuracy: 105.833\n",
      "# Iteration  7634 -> Loss: 0.3627411242905296 \t| Accuracy: 105.833\n",
      "# Iteration  7635 -> Loss: 0.3627279786763721 \t| Accuracy: 105.833\n",
      "# Iteration  7636 -> Loss: 0.3627148362948157 \t| Accuracy: 105.833\n",
      "# Iteration  7637 -> Loss: 0.36270169714438416 \t| Accuracy: 105.833\n",
      "# Iteration  7638 -> Loss: 0.3626885612236017 \t| Accuracy: 105.833\n",
      "# Iteration  7639 -> Loss: 0.3626754285309938 \t| Accuracy: 105.833\n",
      "# Iteration  7640 -> Loss: 0.36266229906508646 \t| Accuracy: 105.833\n",
      "# Iteration  7641 -> Loss: 0.3626491728244068 \t| Accuracy: 105.833\n",
      "# Iteration  7642 -> Loss: 0.36263604980748254 \t| Accuracy: 105.833\n",
      "# Iteration  7643 -> Loss: 0.36262293001284246 \t| Accuracy: 105.833\n",
      "# Iteration  7644 -> Loss: 0.36260981343901627 \t| Accuracy: 105.833\n",
      "# Iteration  7645 -> Loss: 0.36259670008453426 \t| Accuracy: 105.833\n",
      "# Iteration  7646 -> Loss: 0.36258358994792783 \t| Accuracy: 105.833\n",
      "# Iteration  7647 -> Loss: 0.36257048302772904 \t| Accuracy: 105.833\n",
      "# Iteration  7648 -> Loss: 0.362557379322471 \t| Accuracy: 105.833\n",
      "# Iteration  7649 -> Loss: 0.36254427883068774 \t| Accuracy: 105.833\n",
      "# Iteration  7650 -> Loss: 0.3625311815509137 \t| Accuracy: 105.833\n",
      "# Iteration  7651 -> Loss: 0.3625180874816846 \t| Accuracy: 105.833\n",
      "# Iteration  7652 -> Loss: 0.36250499662153707 \t| Accuracy: 105.833\n",
      "# Iteration  7653 -> Loss: 0.3624919089690081 \t| Accuracy: 105.833\n",
      "# Iteration  7654 -> Loss: 0.36247882452263613 \t| Accuracy: 105.833\n",
      "# Iteration  7655 -> Loss: 0.3624657432809601 \t| Accuracy: 105.833\n",
      "# Iteration  7656 -> Loss: 0.36245266524251984 \t| Accuracy: 105.833\n",
      "# Iteration  7657 -> Loss: 0.36243959040585605 \t| Accuracy: 105.833\n",
      "# Iteration  7658 -> Loss: 0.3624265187695105 \t| Accuracy: 105.833\n",
      "# Iteration  7659 -> Loss: 0.36241345033202527 \t| Accuracy: 105.833\n",
      "# Iteration  7660 -> Loss: 0.362400385091944 \t| Accuracy: 105.833\n",
      "# Iteration  7661 -> Loss: 0.36238732304781063 \t| Accuracy: 105.833\n",
      "# Iteration  7662 -> Loss: 0.3623742641981702 \t| Accuracy: 105.833\n",
      "# Iteration  7663 -> Loss: 0.3623612085415685 \t| Accuracy: 105.833\n",
      "# Iteration  7664 -> Loss: 0.36234815607655224 \t| Accuracy: 105.833\n",
      "# Iteration  7665 -> Loss: 0.36233510680166897 \t| Accuracy: 105.833\n",
      "# Iteration  7666 -> Loss: 0.3623220607154671 \t| Accuracy: 105.833\n",
      "# Iteration  7667 -> Loss: 0.3623090178164957 \t| Accuracy: 105.833\n",
      "# Iteration  7668 -> Loss: 0.36229597810330505 \t| Accuracy: 105.833\n",
      "# Iteration  7669 -> Loss: 0.36228294157444596 \t| Accuracy: 105.833\n",
      "# Iteration  7670 -> Loss: 0.36226990822847016 \t| Accuracy: 105.833\n",
      "# Iteration  7671 -> Loss: 0.3622568780639304 \t| Accuracy: 105.833\n",
      "# Iteration  7672 -> Loss: 0.36224385107938 \t| Accuracy: 105.833\n",
      "# Iteration  7673 -> Loss: 0.3622308272733733 \t| Accuracy: 105.833\n",
      "# Iteration  7674 -> Loss: 0.3622178066444654 \t| Accuracy: 105.833\n",
      "# Iteration  7675 -> Loss: 0.36220478919121235 \t| Accuracy: 105.833\n",
      "# Iteration  7676 -> Loss: 0.362191774912171 \t| Accuracy: 105.833\n",
      "# Iteration  7677 -> Loss: 0.36217876380589886 \t| Accuracy: 105.833\n",
      "# Iteration  7678 -> Loss: 0.3621657558709547 \t| Accuracy: 105.833\n",
      "# Iteration  7679 -> Loss: 0.3621527511058975 \t| Accuracy: 105.833\n",
      "# Iteration  7680 -> Loss: 0.36213974950928784 \t| Accuracy: 105.833\n",
      "# Iteration  7681 -> Loss: 0.36212675107968645 \t| Accuracy: 105.833\n",
      "# Iteration  7682 -> Loss: 0.3621137558156553 \t| Accuracy: 105.833\n",
      "# Iteration  7683 -> Loss: 0.36210076371575717 \t| Accuracy: 105.833\n",
      "# Iteration  7684 -> Loss: 0.36208777477855547 \t| Accuracy: 105.833\n",
      "# Iteration  7685 -> Loss: 0.3620747890026147 \t| Accuracy: 105.833\n",
      "# Iteration  7686 -> Loss: 0.3620618063864999 \t| Accuracy: 105.833\n",
      "# Iteration  7687 -> Loss: 0.3620488269287774 \t| Accuracy: 105.833\n",
      "# Iteration  7688 -> Loss: 0.36203585062801374 \t| Accuracy: 105.833\n",
      "# Iteration  7689 -> Loss: 0.3620228774827768 \t| Accuracy: 105.833\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Iteration  7690 -> Loss: 0.36200990749163525 \t| Accuracy: 105.833\n",
      "# Iteration  7691 -> Loss: 0.36199694065315824 \t| Accuracy: 105.833\n",
      "# Iteration  7692 -> Loss: 0.3619839769659162 \t| Accuracy: 105.833\n",
      "# Iteration  7693 -> Loss: 0.36197101642848006 \t| Accuracy: 105.833\n",
      "# Iteration  7694 -> Loss: 0.3619580590394218 \t| Accuracy: 105.833\n",
      "# Iteration  7695 -> Loss: 0.36194510479731384 \t| Accuracy: 105.833\n",
      "# Iteration  7696 -> Loss: 0.3619321537007301 \t| Accuracy: 105.833\n",
      "# Iteration  7697 -> Loss: 0.36191920574824477 \t| Accuracy: 105.833\n",
      "# Iteration  7698 -> Loss: 0.3619062609384331 \t| Accuracy: 105.833\n",
      "# Iteration  7699 -> Loss: 0.36189331926987106 \t| Accuracy: 105.833\n",
      "# Iteration  7700 -> Loss: 0.3618803807411355 \t| Accuracy: 105.833\n",
      "# Iteration  7701 -> Loss: 0.3618674453508042 \t| Accuracy: 105.833\n",
      "# Iteration  7702 -> Loss: 0.36185451309745553 \t| Accuracy: 105.833\n",
      "# Iteration  7703 -> Loss: 0.361841583979669 \t| Accuracy: 105.833\n",
      "# Iteration  7704 -> Loss: 0.36182865799602465 \t| Accuracy: 105.833\n",
      "# Iteration  7705 -> Loss: 0.3618157351451034 \t| Accuracy: 105.833\n",
      "# Iteration  7706 -> Loss: 0.3618028154254873 \t| Accuracy: 105.833\n",
      "# Iteration  7707 -> Loss: 0.3617898988357588 \t| Accuracy: 105.833\n",
      "# Iteration  7708 -> Loss: 0.3617769853745014 \t| Accuracy: 105.833\n",
      "# Iteration  7709 -> Loss: 0.36176407504029945 \t| Accuracy: 105.833\n",
      "# Iteration  7710 -> Loss: 0.3617511678317379 \t| Accuracy: 105.833\n",
      "# Iteration  7711 -> Loss: 0.36173826374740276 \t| Accuracy: 105.833\n",
      "# Iteration  7712 -> Loss: 0.36172536278588097 \t| Accuracy: 105.833\n",
      "# Iteration  7713 -> Loss: 0.36171246494575976 \t| Accuracy: 105.833\n",
      "# Iteration  7714 -> Loss: 0.3616995702256277 \t| Accuracy: 105.833\n",
      "# Iteration  7715 -> Loss: 0.36168667862407405 \t| Accuracy: 105.833\n",
      "# Iteration  7716 -> Loss: 0.36167379013968876 \t| Accuracy: 105.833\n",
      "# Iteration  7717 -> Loss: 0.3616609047710626 \t| Accuracy: 105.833\n",
      "# Iteration  7718 -> Loss: 0.36164802251678735 \t| Accuracy: 105.833\n",
      "# Iteration  7719 -> Loss: 0.36163514337545544 \t| Accuracy: 105.833\n",
      "# Iteration  7720 -> Loss: 0.3616222673456602 \t| Accuracy: 105.833\n",
      "# Iteration  7721 -> Loss: 0.3616093944259957 \t| Accuracy: 105.833\n",
      "# Iteration  7722 -> Loss: 0.3615965246150569 \t| Accuracy: 105.833\n",
      "# Iteration  7723 -> Loss: 0.3615836579114396 \t| Accuracy: 105.833\n",
      "# Iteration  7724 -> Loss: 0.3615707943137403 \t| Accuracy: 105.833\n",
      "# Iteration  7725 -> Loss: 0.36155793382055623 \t| Accuracy: 105.833\n",
      "# Iteration  7726 -> Loss: 0.3615450764304858 \t| Accuracy: 105.833\n",
      "# Iteration  7727 -> Loss: 0.3615322221421279 \t| Accuracy: 105.833\n",
      "# Iteration  7728 -> Loss: 0.3615193709540823 \t| Accuracy: 105.833\n",
      "# Iteration  7729 -> Loss: 0.36150652286494966 \t| Accuracy: 105.833\n",
      "# Iteration  7730 -> Loss: 0.3614936778733315 \t| Accuracy: 105.833\n",
      "# Iteration  7731 -> Loss: 0.36148083597783 \t| Accuracy: 105.833\n",
      "# Iteration  7732 -> Loss: 0.3614679971770481 \t| Accuracy: 105.833\n",
      "# Iteration  7733 -> Loss: 0.3614551614695898 \t| Accuracy: 105.833\n",
      "# Iteration  7734 -> Loss: 0.36144232885405975 \t| Accuracy: 105.833\n",
      "# Iteration  7735 -> Loss: 0.3614294993290634 \t| Accuracy: 105.833\n",
      "# Iteration  7736 -> Loss: 0.36141667289320695 \t| Accuracy: 105.833\n",
      "# Iteration  7737 -> Loss: 0.3614038495450976 \t| Accuracy: 105.833\n",
      "# Iteration  7738 -> Loss: 0.36139102928334327 \t| Accuracy: 105.833\n",
      "# Iteration  7739 -> Loss: 0.36137821210655263 \t| Accuracy: 105.833\n",
      "# Iteration  7740 -> Loss: 0.36136539801333517 \t| Accuracy: 105.833\n",
      "# Iteration  7741 -> Loss: 0.3613525870023011 \t| Accuracy: 105.833\n",
      "# Iteration  7742 -> Loss: 0.3613397790720619 \t| Accuracy: 105.833\n",
      "# Iteration  7743 -> Loss: 0.36132697422122906 \t| Accuracy: 105.833\n",
      "# Iteration  7744 -> Loss: 0.3613141724484156 \t| Accuracy: 105.833\n",
      "# Iteration  7745 -> Loss: 0.3613013737522349 \t| Accuracy: 105.833\n",
      "# Iteration  7746 -> Loss: 0.36128857813130133 \t| Accuracy: 105.833\n",
      "# Iteration  7747 -> Loss: 0.36127578558423 \t| Accuracy: 105.833\n",
      "# Iteration  7748 -> Loss: 0.3612629961096369 \t| Accuracy: 105.833\n",
      "# Iteration  7749 -> Loss: 0.3612502097061387 \t| Accuracy: 105.833\n",
      "# Iteration  7750 -> Loss: 0.36123742637235295 \t| Accuracy: 105.833\n",
      "# Iteration  7751 -> Loss: 0.361224646106898 \t| Accuracy: 105.833\n",
      "# Iteration  7752 -> Loss: 0.36121186890839296 \t| Accuracy: 105.833\n",
      "# Iteration  7753 -> Loss: 0.36119909477545786 \t| Accuracy: 105.833\n",
      "# Iteration  7754 -> Loss: 0.3611863237067132 \t| Accuracy: 105.833\n",
      "# Iteration  7755 -> Loss: 0.36117355570078075 \t| Accuracy: 105.833\n",
      "# Iteration  7756 -> Loss: 0.3611607907562827 \t| Accuracy: 105.833\n",
      "# Iteration  7757 -> Loss: 0.3611480288718421 \t| Accuracy: 105.833\n",
      "# Iteration  7758 -> Loss: 0.3611352700460831 \t| Accuracy: 105.833\n",
      "# Iteration  7759 -> Loss: 0.3611225142776302 \t| Accuracy: 105.833\n",
      "# Iteration  7760 -> Loss: 0.3611097615651089 \t| Accuracy: 105.833\n",
      "# Iteration  7761 -> Loss: 0.36109701190714555 \t| Accuracy: 105.833\n",
      "# Iteration  7762 -> Loss: 0.36108426530236715 \t| Accuracy: 105.833\n",
      "# Iteration  7763 -> Loss: 0.3610715217494018 \t| Accuracy: 105.833\n",
      "# Iteration  7764 -> Loss: 0.361058781246878 \t| Accuracy: 105.833\n",
      "# Iteration  7765 -> Loss: 0.3610460437934254 \t| Accuracy: 105.833\n",
      "# Iteration  7766 -> Loss: 0.3610333093876739 \t| Accuracy: 105.833\n",
      "# Iteration  7767 -> Loss: 0.3610205780282548 \t| Accuracy: 105.833\n",
      "# Iteration  7768 -> Loss: 0.3610078497137999 \t| Accuracy: 105.833\n",
      "# Iteration  7769 -> Loss: 0.3609951244429418 \t| Accuracy: 105.833\n",
      "# Iteration  7770 -> Loss: 0.36098240221431394 \t| Accuracy: 105.833\n",
      "# Iteration  7771 -> Loss: 0.36096968302655047 \t| Accuracy: 105.833\n",
      "# Iteration  7772 -> Loss: 0.3609569668782865 \t| Accuracy: 105.833\n",
      "# Iteration  7773 -> Loss: 0.3609442537681576 \t| Accuracy: 105.833\n",
      "# Iteration  7774 -> Loss: 0.3609315436948005 \t| Accuracy: 105.833\n",
      "# Iteration  7775 -> Loss: 0.36091883665685254 \t| Accuracy: 105.833\n",
      "# Iteration  7776 -> Loss: 0.36090613265295174 \t| Accuracy: 105.833\n",
      "# Iteration  7777 -> Loss: 0.3608934316817371 \t| Accuracy: 105.833\n",
      "# Iteration  7778 -> Loss: 0.3608807337418484 \t| Accuracy: 105.833\n",
      "# Iteration  7779 -> Loss: 0.36086803883192603 \t| Accuracy: 105.833\n",
      "# Iteration  7780 -> Loss: 0.3608553469506113 \t| Accuracy: 105.833\n",
      "# Iteration  7781 -> Loss: 0.36084265809654614 \t| Accuracy: 105.833\n",
      "# Iteration  7782 -> Loss: 0.36082997226837354 \t| Accuracy: 105.833\n",
      "# Iteration  7783 -> Loss: 0.3608172894647371 \t| Accuracy: 105.833\n",
      "# Iteration  7784 -> Loss: 0.36080460968428113 \t| Accuracy: 105.833\n",
      "# Iteration  7785 -> Loss: 0.3607919329256511 \t| Accuracy: 105.833\n",
      "# Iteration  7786 -> Loss: 0.3607792591874925 \t| Accuracy: 105.833\n",
      "# Iteration  7787 -> Loss: 0.3607665884684524 \t| Accuracy: 105.833\n",
      "# Iteration  7788 -> Loss: 0.36075392076717816 \t| Accuracy: 105.833\n",
      "# Iteration  7789 -> Loss: 0.3607412560823183 \t| Accuracy: 105.833\n",
      "# Iteration  7790 -> Loss: 0.36072859441252164 \t| Accuracy: 105.833\n",
      "# Iteration  7791 -> Loss: 0.3607159357564381 \t| Accuracy: 105.833\n",
      "# Iteration  7792 -> Loss: 0.3607032801127185 \t| Accuracy: 105.833\n",
      "# Iteration  7793 -> Loss: 0.36069062748001396 \t| Accuracy: 105.833\n",
      "# Iteration  7794 -> Loss: 0.36067797785697686 \t| Accuracy: 105.833\n",
      "# Iteration  7795 -> Loss: 0.36066533124226013 \t| Accuracy: 105.833\n",
      "# Iteration  7796 -> Loss: 0.3606526876345175 \t| Accuracy: 105.833\n",
      "# Iteration  7797 -> Loss: 0.3606400470324034 \t| Accuracy: 105.833\n",
      "# Iteration  7798 -> Loss: 0.3606274094345732 \t| Accuracy: 105.833\n",
      "# Iteration  7799 -> Loss: 0.360614774839683 \t| Accuracy: 105.833\n",
      "# Iteration  7800 -> Loss: 0.3606021432463894 \t| Accuracy: 105.833\n",
      "# Iteration  7801 -> Loss: 0.3605895146533503 \t| Accuracy: 105.833\n",
      "# Iteration  7802 -> Loss: 0.36057688905922386 \t| Accuracy: 105.833\n",
      "# Iteration  7803 -> Loss: 0.36056426646266926 \t| Accuracy: 105.833\n",
      "# Iteration  7804 -> Loss: 0.36055164686234653 \t| Accuracy: 105.833\n",
      "# Iteration  7805 -> Loss: 0.36053903025691625 \t| Accuracy: 105.833\n",
      "# Iteration  7806 -> Loss: 0.36052641664503987 \t| Accuracy: 105.833\n",
      "# Iteration  7807 -> Loss: 0.36051380602537964 \t| Accuracy: 105.833\n",
      "# Iteration  7808 -> Loss: 0.36050119839659844 \t| Accuracy: 105.833\n",
      "# Iteration  7809 -> Loss: 0.36048859375736025 \t| Accuracy: 105.833\n",
      "# Iteration  7810 -> Loss: 0.36047599210632947 \t| Accuracy: 105.833\n",
      "# Iteration  7811 -> Loss: 0.3604633934421713 \t| Accuracy: 105.833\n",
      "# Iteration  7812 -> Loss: 0.3604507977635521 \t| Accuracy: 105.833\n",
      "# Iteration  7813 -> Loss: 0.3604382050691384 \t| Accuracy: 105.833\n",
      "# Iteration  7814 -> Loss: 0.36042561535759793 \t| Accuracy: 105.833\n",
      "# Iteration  7815 -> Loss: 0.36041302862759894 \t| Accuracy: 105.833\n",
      "# Iteration  7816 -> Loss: 0.3604004448778106 \t| Accuracy: 105.833\n",
      "# Iteration  7817 -> Loss: 0.36038786410690293 \t| Accuracy: 105.833\n",
      "# Iteration  7818 -> Loss: 0.36037528631354643 \t| Accuracy: 105.833\n",
      "# Iteration  7819 -> Loss: 0.3603627114964125 \t| Accuracy: 105.833\n",
      "# Iteration  7820 -> Loss: 0.36035013965417334 \t| Accuracy: 105.833\n",
      "# Iteration  7821 -> Loss: 0.360337570785502 \t| Accuracy: 105.833\n",
      "# Iteration  7822 -> Loss: 0.36032500488907204 \t| Accuracy: 105.833\n",
      "# Iteration  7823 -> Loss: 0.36031244196355794 \t| Accuracy: 105.833\n",
      "# Iteration  7824 -> Loss: 0.36029988200763485 \t| Accuracy: 105.833\n",
      "# Iteration  7825 -> Loss: 0.36028732501997884 \t| Accuracy: 105.833\n",
      "# Iteration  7826 -> Loss: 0.3602747709992668 \t| Accuracy: 105.833\n",
      "# Iteration  7827 -> Loss: 0.36026221994417595 \t| Accuracy: 105.833\n",
      "# Iteration  7828 -> Loss: 0.36024967185338463 \t| Accuracy: 105.833\n",
      "# Iteration  7829 -> Loss: 0.36023712672557184 \t| Accuracy: 105.833\n",
      "# Iteration  7830 -> Loss: 0.3602245845594174 \t| Accuracy: 105.833\n",
      "# Iteration  7831 -> Loss: 0.3602120453536019 \t| Accuracy: 105.833\n",
      "# Iteration  7832 -> Loss: 0.3601995091068065 \t| Accuracy: 105.833\n",
      "# Iteration  7833 -> Loss: 0.3601869758177132 \t| Accuracy: 105.833\n",
      "# Iteration  7834 -> Loss: 0.3601744454850049 \t| Accuracy: 105.833\n",
      "# Iteration  7835 -> Loss: 0.36016191810736503 \t| Accuracy: 105.833\n",
      "# Iteration  7836 -> Loss: 0.3601493936834779 \t| Accuracy: 105.833\n",
      "# Iteration  7837 -> Loss: 0.3601368722120288 \t| Accuracy: 105.833\n",
      "# Iteration  7838 -> Loss: 0.3601243536917033 \t| Accuracy: 105.833\n",
      "# Iteration  7839 -> Loss: 0.36011183812118797 \t| Accuracy: 105.833\n",
      "# Iteration  7840 -> Loss: 0.3600993254991702 \t| Accuracy: 105.833\n",
      "# Iteration  7841 -> Loss: 0.360086815824338 \t| Accuracy: 105.833\n",
      "# Iteration  7842 -> Loss: 0.3600743090953802 \t| Accuracy: 105.833\n",
      "# Iteration  7843 -> Loss: 0.3600618053109863 \t| Accuracy: 105.833\n",
      "# Iteration  7844 -> Loss: 0.3600493044698467 \t| Accuracy: 105.833\n",
      "# Iteration  7845 -> Loss: 0.36003680657065246 \t| Accuracy: 105.833\n",
      "# Iteration  7846 -> Loss: 0.36002431161209536 \t| Accuracy: 105.833\n",
      "# Iteration  7847 -> Loss: 0.3600118195928679 \t| Accuracy: 105.833\n",
      "# Iteration  7848 -> Loss: 0.3599993305116635 \t| Accuracy: 105.833\n",
      "# Iteration  7849 -> Loss: 0.3599868443671762 \t| Accuracy: 105.833\n",
      "# Iteration  7850 -> Loss: 0.3599743611581007 \t| Accuracy: 105.833\n",
      "# Iteration  7851 -> Loss: 0.3599618808831326 \t| Accuracy: 105.833\n",
      "# Iteration  7852 -> Loss: 0.35994940354096816 \t| Accuracy: 105.833\n",
      "# Iteration  7853 -> Loss: 0.3599369291303045 \t| Accuracy: 105.833\n",
      "# Iteration  7854 -> Loss: 0.3599244576498394 \t| Accuracy: 105.833\n",
      "# Iteration  7855 -> Loss: 0.3599119890982713 \t| Accuracy: 105.833\n",
      "# Iteration  7856 -> Loss: 0.3598995234742996 \t| Accuracy: 105.833\n",
      "# Iteration  7857 -> Loss: 0.3598870607766242 \t| Accuracy: 105.833\n",
      "# Iteration  7858 -> Loss: 0.3598746010039458 \t| Accuracy: 105.833\n",
      "# Iteration  7859 -> Loss: 0.35986214415496603 \t| Accuracy: 105.833\n",
      "# Iteration  7860 -> Loss: 0.3598496902283872 \t| Accuracy: 105.833\n",
      "# Iteration  7861 -> Loss: 0.3598372392229121 \t| Accuracy: 105.833\n",
      "# Iteration  7862 -> Loss: 0.3598247911372445 \t| Accuracy: 105.833\n",
      "# Iteration  7863 -> Loss: 0.359812345970089 \t| Accuracy: 105.833\n",
      "# Iteration  7864 -> Loss: 0.3597999037201507 \t| Accuracy: 105.833\n",
      "# Iteration  7865 -> Loss: 0.3597874643861355 \t| Accuracy: 105.833\n",
      "# Iteration  7866 -> Loss: 0.3597750279667501 \t| Accuracy: 105.833\n",
      "# Iteration  7867 -> Loss: 0.35976259446070197 \t| Accuracy: 105.833\n",
      "# Iteration  7868 -> Loss: 0.3597501638666993 \t| Accuracy: 105.833\n",
      "# Iteration  7869 -> Loss: 0.359737736183451 \t| Accuracy: 105.833\n",
      "# Iteration  7870 -> Loss: 0.3597253114096665 \t| Accuracy: 105.833\n",
      "# Iteration  7871 -> Loss: 0.35971288954405634 \t| Accuracy: 105.833\n",
      "# Iteration  7872 -> Loss: 0.35970047058533156 \t| Accuracy: 105.833\n",
      "# Iteration  7873 -> Loss: 0.35968805453220415 \t| Accuracy: 105.833\n",
      "# Iteration  7874 -> Loss: 0.3596756413833864 \t| Accuracy: 105.833\n",
      "# Iteration  7875 -> Loss: 0.3596632311375919 \t| Accuracy: 105.833\n",
      "# Iteration  7876 -> Loss: 0.35965082379353447 \t| Accuracy: 105.833\n",
      "# Iteration  7877 -> Loss: 0.35963841934992913 \t| Accuracy: 105.833\n",
      "# Iteration  7878 -> Loss: 0.35962601780549114 \t| Accuracy: 105.833\n",
      "# Iteration  7879 -> Loss: 0.35961361915893686 \t| Accuracy: 105.833\n",
      "# Iteration  7880 -> Loss: 0.35960122340898326 \t| Accuracy: 105.833\n",
      "# Iteration  7881 -> Loss: 0.359588830554348 \t| Accuracy: 105.833\n",
      "# Iteration  7882 -> Loss: 0.35957644059374955 \t| Accuracy: 105.833\n",
      "# Iteration  7883 -> Loss: 0.35956405352590703 \t| Accuracy: 105.833\n",
      "# Iteration  7884 -> Loss: 0.3595516693495405 \t| Accuracy: 105.833\n",
      "# Iteration  7885 -> Loss: 0.3595392880633703 \t| Accuracy: 105.833\n",
      "# Iteration  7886 -> Loss: 0.35952690966611806 \t| Accuracy: 105.833\n",
      "# Iteration  7887 -> Loss: 0.35951453415650575 \t| Accuracy: 105.833\n",
      "# Iteration  7888 -> Loss: 0.35950216153325615 \t| Accuracy: 105.833\n",
      "# Iteration  7889 -> Loss: 0.3594897917950927 \t| Accuracy: 105.833\n",
      "# Iteration  7890 -> Loss: 0.3594774249407399 \t| Accuracy: 105.833\n",
      "# Iteration  7891 -> Loss: 0.3594650609689226 \t| Accuracy: 105.833\n",
      "# Iteration  7892 -> Loss: 0.3594526998783666 \t| Accuracy: 105.833\n",
      "# Iteration  7893 -> Loss: 0.3594403416677982 \t| Accuracy: 105.833\n",
      "# Iteration  7894 -> Loss: 0.35942798633594464 \t| Accuracy: 105.833\n",
      "# Iteration  7895 -> Loss: 0.35941563388153386 \t| Accuracy: 105.833\n",
      "# Iteration  7896 -> Loss: 0.3594032843032943 \t| Accuracy: 105.833\n",
      "# Iteration  7897 -> Loss: 0.3593909375999555 \t| Accuracy: 105.833\n",
      "# Iteration  7898 -> Loss: 0.3593785937702475 \t| Accuracy: 105.833\n",
      "# Iteration  7899 -> Loss: 0.359366252812901 \t| Accuracy: 105.833\n",
      "# Iteration  7900 -> Loss: 0.35935391472664757 \t| Accuracy: 105.833\n",
      "# Iteration  7901 -> Loss: 0.3593415795102193 \t| Accuracy: 105.833\n",
      "# Iteration  7902 -> Loss: 0.35932924716234926 \t| Accuracy: 105.833\n",
      "# Iteration  7903 -> Loss: 0.35931691768177115 \t| Accuracy: 105.833\n",
      "# Iteration  7904 -> Loss: 0.35930459106721924 \t| Accuracy: 105.833\n",
      "# Iteration  7905 -> Loss: 0.35929226731742875 \t| Accuracy: 105.833\n",
      "# Iteration  7906 -> Loss: 0.3592799464311355 \t| Accuracy: 105.833\n",
      "# Iteration  7907 -> Loss: 0.35926762840707593 \t| Accuracy: 105.833\n",
      "# Iteration  7908 -> Loss: 0.35925531324398735 \t| Accuracy: 105.833\n",
      "# Iteration  7909 -> Loss: 0.35924300094060774 \t| Accuracy: 105.833\n",
      "# Iteration  7910 -> Loss: 0.3592306914956759 \t| Accuracy: 105.833\n",
      "# Iteration  7911 -> Loss: 0.3592183849079311 \t| Accuracy: 105.833\n",
      "# Iteration  7912 -> Loss: 0.35920608117611363 \t| Accuracy: 105.833\n",
      "# Iteration  7913 -> Loss: 0.35919378029896426 \t| Accuracy: 105.833\n",
      "# Iteration  7914 -> Loss: 0.35918148227522434 \t| Accuracy: 105.833\n",
      "# Iteration  7915 -> Loss: 0.3591691871036365 \t| Accuracy: 105.833\n",
      "# Iteration  7916 -> Loss: 0.3591568947829435 \t| Accuracy: 105.833\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Iteration  7917 -> Loss: 0.35914460531188913 \t| Accuracy: 105.833\n",
      "# Iteration  7918 -> Loss: 0.3591323186892178 \t| Accuracy: 105.833\n",
      "# Iteration  7919 -> Loss: 0.3591200349136746 \t| Accuracy: 105.833\n",
      "# Iteration  7920 -> Loss: 0.3591077539840055 \t| Accuracy: 105.833\n",
      "# Iteration  7921 -> Loss: 0.3590954758989569 \t| Accuracy: 105.833\n",
      "# Iteration  7922 -> Loss: 0.35908320065727617 \t| Accuracy: 105.833\n",
      "# Iteration  7923 -> Loss: 0.35907092825771125 \t| Accuracy: 105.833\n",
      "# Iteration  7924 -> Loss: 0.3590586586990109 \t| Accuracy: 105.833\n",
      "# Iteration  7925 -> Loss: 0.35904639197992433 \t| Accuracy: 105.833\n",
      "# Iteration  7926 -> Loss: 0.35903412809920177 \t| Accuracy: 105.833\n",
      "# Iteration  7927 -> Loss: 0.359021867055594 \t| Accuracy: 105.833\n",
      "# Iteration  7928 -> Loss: 0.3590096088478527 \t| Accuracy: 105.833\n",
      "# Iteration  7929 -> Loss: 0.35899735347472994 \t| Accuracy: 105.833\n",
      "# Iteration  7930 -> Loss: 0.3589851009349786 \t| Accuracy: 105.833\n",
      "# Iteration  7931 -> Loss: 0.3589728512273525 \t| Accuracy: 105.833\n",
      "# Iteration  7932 -> Loss: 0.358960604350606 \t| Accuracy: 105.833\n",
      "# Iteration  7933 -> Loss: 0.358948360303494 \t| Accuracy: 105.833\n",
      "# Iteration  7934 -> Loss: 0.3589361190847724 \t| Accuracy: 105.833\n",
      "# Iteration  7935 -> Loss: 0.3589238806931976 \t| Accuracy: 105.833\n",
      "# Iteration  7936 -> Loss: 0.35891164512752677 \t| Accuracy: 105.833\n",
      "# Iteration  7937 -> Loss: 0.35889941238651796 \t| Accuracy: 105.833\n",
      "# Iteration  7938 -> Loss: 0.3588871824689294 \t| Accuracy: 105.833\n",
      "# Iteration  7939 -> Loss: 0.35887495537352077 \t| Accuracy: 105.833\n",
      "# Iteration  7940 -> Loss: 0.35886273109905176 \t| Accuracy: 105.833\n",
      "# Iteration  7941 -> Loss: 0.3588505096442833 \t| Accuracy: 105.833\n",
      "# Iteration  7942 -> Loss: 0.3588382910079766 \t| Accuracy: 105.833\n",
      "# Iteration  7943 -> Loss: 0.35882607518889387 \t| Accuracy: 105.833\n",
      "# Iteration  7944 -> Loss: 0.3588138621857978 \t| Accuracy: 105.833\n",
      "# Iteration  7945 -> Loss: 0.358801651997452 \t| Accuracy: 105.833\n",
      "# Iteration  7946 -> Loss: 0.35878944462262063 \t| Accuracy: 105.833\n",
      "# Iteration  7947 -> Loss: 0.3587772400600686 \t| Accuracy: 105.833\n",
      "# Iteration  7948 -> Loss: 0.35876503830856143 \t| Accuracy: 105.833\n",
      "# Iteration  7949 -> Loss: 0.35875283936686553 \t| Accuracy: 105.833\n",
      "# Iteration  7950 -> Loss: 0.35874064323374777 \t| Accuracy: 105.833\n",
      "# Iteration  7951 -> Loss: 0.358728449907976 \t| Accuracy: 105.833\n",
      "# Iteration  7952 -> Loss: 0.3587162593883184 \t| Accuracy: 105.833\n",
      "# Iteration  7953 -> Loss: 0.3587040716735443 \t| Accuracy: 105.833\n",
      "# Iteration  7954 -> Loss: 0.3586918867624233 \t| Accuracy: 105.833\n",
      "# Iteration  7955 -> Loss: 0.3586797046537259 \t| Accuracy: 105.833\n",
      "# Iteration  7956 -> Loss: 0.3586675253462233 \t| Accuracy: 105.833\n",
      "# Iteration  7957 -> Loss: 0.3586553488386875 \t| Accuracy: 105.833\n",
      "# Iteration  7958 -> Loss: 0.3586431751298908 \t| Accuracy: 105.833\n",
      "# Iteration  7959 -> Loss: 0.3586310042186066 \t| Accuracy: 105.833\n",
      "# Iteration  7960 -> Loss: 0.358618836103609 \t| Accuracy: 105.833\n",
      "# Iteration  7961 -> Loss: 0.35860667078367225 \t| Accuracy: 105.833\n",
      "# Iteration  7962 -> Loss: 0.3585945082575721 \t| Accuracy: 105.833\n",
      "# Iteration  7963 -> Loss: 0.3585823485240844 \t| Accuracy: 105.833\n",
      "# Iteration  7964 -> Loss: 0.3585701915819858 \t| Accuracy: 105.833\n",
      "# Iteration  7965 -> Loss: 0.35855803743005377 \t| Accuracy: 105.833\n",
      "# Iteration  7966 -> Loss: 0.35854588606706644 \t| Accuracy: 105.833\n",
      "# Iteration  7967 -> Loss: 0.3585337374918025 \t| Accuracy: 105.833\n",
      "# Iteration  7968 -> Loss: 0.35852159170304165 \t| Accuracy: 105.833\n",
      "# Iteration  7969 -> Loss: 0.35850944869956386 \t| Accuracy: 105.833\n",
      "# Iteration  7970 -> Loss: 0.35849730848015005 \t| Accuracy: 105.833\n",
      "# Iteration  7971 -> Loss: 0.35848517104358185 \t| Accuracy: 105.833\n",
      "# Iteration  7972 -> Loss: 0.35847303638864136 \t| Accuracy: 105.833\n",
      "# Iteration  7973 -> Loss: 0.35846090451411156 \t| Accuracy: 105.833\n",
      "# Iteration  7974 -> Loss: 0.3584487754187761 \t| Accuracy: 105.833\n",
      "# Iteration  7975 -> Loss: 0.3584366491014192 \t| Accuracy: 105.833\n",
      "# Iteration  7976 -> Loss: 0.35842452556082594 \t| Accuracy: 105.833\n",
      "# Iteration  7977 -> Loss: 0.35841240479578196 \t| Accuracy: 105.833\n",
      "# Iteration  7978 -> Loss: 0.35840028680507346 \t| Accuracy: 105.833\n",
      "# Iteration  7979 -> Loss: 0.3583881715874878 \t| Accuracy: 105.833\n",
      "# Iteration  7980 -> Loss: 0.3583760591418123 \t| Accuracy: 105.833\n",
      "# Iteration  7981 -> Loss: 0.35836394946683575 \t| Accuracy: 105.833\n",
      "# Iteration  7982 -> Loss: 0.35835184256134706 \t| Accuracy: 105.833\n",
      "# Iteration  7983 -> Loss: 0.358339738424136 \t| Accuracy: 105.833\n",
      "# Iteration  7984 -> Loss: 0.35832763705399306 \t| Accuracy: 105.833\n",
      "# Iteration  7985 -> Loss: 0.3583155384497093 \t| Accuracy: 105.833\n",
      "# Iteration  7986 -> Loss: 0.3583034426100767 \t| Accuracy: 105.833\n",
      "# Iteration  7987 -> Loss: 0.3582913495338877 \t| Accuracy: 105.833\n",
      "# Iteration  7988 -> Loss: 0.3582792592199354 \t| Accuracy: 105.833\n",
      "# Iteration  7989 -> Loss: 0.3582671716670137 \t| Accuracy: 105.833\n",
      "# Iteration  7990 -> Loss: 0.3582550868739173 \t| Accuracy: 105.833\n",
      "# Iteration  7991 -> Loss: 0.35824300483944105 \t| Accuracy: 105.833\n",
      "# Iteration  7992 -> Loss: 0.3582309255623812 \t| Accuracy: 105.833\n",
      "# Iteration  7993 -> Loss: 0.3582188490415343 \t| Accuracy: 105.833\n",
      "# Iteration  7994 -> Loss: 0.3582067752756974 \t| Accuracy: 105.833\n",
      "# Iteration  7995 -> Loss: 0.35819470426366856 \t| Accuracy: 105.833\n",
      "# Iteration  7996 -> Loss: 0.3581826360042465 \t| Accuracy: 105.833\n",
      "# Iteration  7997 -> Loss: 0.35817057049623025 \t| Accuracy: 105.833\n",
      "# Iteration  7998 -> Loss: 0.35815850773842 \t| Accuracy: 105.833\n",
      "# Iteration  7999 -> Loss: 0.35814644772961646 \t| Accuracy: 105.833\n",
      "# Iteration  8000 -> Loss: 0.3581343904686206 \t| Accuracy: 105.833\n",
      "# Iteration  8001 -> Loss: 0.3581223359542347 \t| Accuracy: 105.833\n",
      "# Iteration  8002 -> Loss: 0.3581102841852613 \t| Accuracy: 105.833\n",
      "# Iteration  8003 -> Loss: 0.35809823516050393 \t| Accuracy: 105.833\n",
      "# Iteration  8004 -> Loss: 0.35808618887876625 \t| Accuracy: 105.833\n",
      "# Iteration  8005 -> Loss: 0.35807414533885334 \t| Accuracy: 105.833\n",
      "# Iteration  8006 -> Loss: 0.35806210453957027 \t| Accuracy: 105.833\n",
      "# Iteration  8007 -> Loss: 0.3580500664797232 \t| Accuracy: 105.833\n",
      "# Iteration  8008 -> Loss: 0.3580380311581188 \t| Accuracy: 105.833\n",
      "# Iteration  8009 -> Loss: 0.35802599857356443 \t| Accuracy: 105.833\n",
      "# Iteration  8010 -> Loss: 0.3580139687248682 \t| Accuracy: 105.833\n",
      "# Iteration  8011 -> Loss: 0.3580019416108388 \t| Accuracy: 105.833\n",
      "# Iteration  8012 -> Loss: 0.35798991723028556 \t| Accuracy: 105.833\n",
      "# Iteration  8013 -> Loss: 0.3579778955820186 \t| Accuracy: 105.833\n",
      "# Iteration  8014 -> Loss: 0.35796587666484864 \t| Accuracy: 105.833\n",
      "# Iteration  8015 -> Loss: 0.3579538604775871 \t| Accuracy: 105.833\n",
      "# Iteration  8016 -> Loss: 0.35794184701904586 \t| Accuracy: 105.833\n",
      "# Iteration  8017 -> Loss: 0.3579298362880378 \t| Accuracy: 105.833\n",
      "# Iteration  8018 -> Loss: 0.3579178282833764 \t| Accuracy: 105.833\n",
      "# Iteration  8019 -> Loss: 0.35790582300387563 \t| Accuracy: 105.833\n",
      "# Iteration  8020 -> Loss: 0.3578938204483502 \t| Accuracy: 105.833\n",
      "# Iteration  8021 -> Loss: 0.3578818206156156 \t| Accuracy: 105.833\n",
      "# Iteration  8022 -> Loss: 0.35786982350448765 \t| Accuracy: 105.833\n",
      "# Iteration  8023 -> Loss: 0.3578578291137834 \t| Accuracy: 105.833\n",
      "# Iteration  8024 -> Loss: 0.35784583744232 \t| Accuracy: 105.833\n",
      "# Iteration  8025 -> Loss: 0.3578338484889157 \t| Accuracy: 105.833\n",
      "# Iteration  8026 -> Loss: 0.35782186225238893 \t| Accuracy: 105.833\n",
      "# Iteration  8027 -> Loss: 0.3578098787315594 \t| Accuracy: 105.833\n",
      "# Iteration  8028 -> Loss: 0.35779789792524697 \t| Accuracy: 105.833\n",
      "# Iteration  8029 -> Loss: 0.3577859198322725 \t| Accuracy: 105.833\n",
      "# Iteration  8030 -> Loss: 0.35777394445145716 \t| Accuracy: 105.833\n",
      "# Iteration  8031 -> Loss: 0.35776197178162306 \t| Accuracy: 105.833\n",
      "# Iteration  8032 -> Loss: 0.35775000182159294 \t| Accuracy: 105.833\n",
      "# Iteration  8033 -> Loss: 0.3577380345701901 \t| Accuracy: 105.833\n",
      "# Iteration  8034 -> Loss: 0.35772607002623863 \t| Accuracy: 105.833\n",
      "# Iteration  8035 -> Loss: 0.3577141081885632 \t| Accuracy: 105.833\n",
      "# Iteration  8036 -> Loss: 0.35770214905598896 \t| Accuracy: 105.833\n",
      "# Iteration  8037 -> Loss: 0.3576901926273422 \t| Accuracy: 105.833\n",
      "# Iteration  8038 -> Loss: 0.35767823890144945 \t| Accuracy: 105.833\n",
      "# Iteration  8039 -> Loss: 0.35766628787713783 \t| Accuracy: 105.833\n",
      "# Iteration  8040 -> Loss: 0.3576543395532355 \t| Accuracy: 105.833\n",
      "# Iteration  8041 -> Loss: 0.35764239392857106 \t| Accuracy: 105.833\n",
      "# Iteration  8042 -> Loss: 0.3576304510019739 \t| Accuracy: 105.833\n",
      "# Iteration  8043 -> Loss: 0.35761851077227375 \t| Accuracy: 105.833\n",
      "# Iteration  8044 -> Loss: 0.3576065732383013 \t| Accuracy: 105.833\n",
      "# Iteration  8045 -> Loss: 0.35759463839888794 \t| Accuracy: 105.833\n",
      "# Iteration  8046 -> Loss: 0.3575827062528652 \t| Accuracy: 105.833\n",
      "# Iteration  8047 -> Loss: 0.35757077679906607 \t| Accuracy: 105.833\n",
      "# Iteration  8048 -> Loss: 0.35755885003632343 \t| Accuracy: 105.833\n",
      "# Iteration  8049 -> Loss: 0.3575469259634715 \t| Accuracy: 105.833\n",
      "# Iteration  8050 -> Loss: 0.3575350045793444 \t| Accuracy: 105.833\n",
      "# Iteration  8051 -> Loss: 0.35752308588277765 \t| Accuracy: 105.833\n",
      "# Iteration  8052 -> Loss: 0.3575111698726067 \t| Accuracy: 105.833\n",
      "# Iteration  8053 -> Loss: 0.3574992565476685 \t| Accuracy: 105.833\n",
      "# Iteration  8054 -> Loss: 0.3574873459067999 \t| Accuracy: 105.833\n",
      "# Iteration  8055 -> Loss: 0.3574754379488386 \t| Accuracy: 105.833\n",
      "# Iteration  8056 -> Loss: 0.3574635326726233 \t| Accuracy: 105.833\n",
      "# Iteration  8057 -> Loss: 0.35745163007699293 \t| Accuracy: 105.833\n",
      "# Iteration  8058 -> Loss: 0.35743973016078723 \t| Accuracy: 105.833\n",
      "# Iteration  8059 -> Loss: 0.35742783292284663 \t| Accuracy: 105.833\n",
      "# Iteration  8060 -> Loss: 0.357415938362012 \t| Accuracy: 105.833\n",
      "# Iteration  8061 -> Loss: 0.3574040464771252 \t| Accuracy: 105.833\n",
      "# Iteration  8062 -> Loss: 0.3573921572670286 \t| Accuracy: 105.833\n",
      "# Iteration  8063 -> Loss: 0.35738027073056505 \t| Accuracy: 105.833\n",
      "# Iteration  8064 -> Loss: 0.3573683868665781 \t| Accuracy: 105.833\n",
      "# Iteration  8065 -> Loss: 0.35735650567391225 \t| Accuracy: 105.833\n",
      "# Iteration  8066 -> Loss: 0.3573446271514123 \t| Accuracy: 105.833\n",
      "# Iteration  8067 -> Loss: 0.3573327512979238 \t| Accuracy: 105.833\n",
      "# Iteration  8068 -> Loss: 0.357320878112293 \t| Accuracy: 105.833\n",
      "# Iteration  8069 -> Loss: 0.3573090075933668 \t| Accuracy: 105.833\n",
      "# Iteration  8070 -> Loss: 0.3572971397399926 \t| Accuracy: 105.833\n",
      "# Iteration  8071 -> Loss: 0.3572852745510185 \t| Accuracy: 105.833\n",
      "# Iteration  8072 -> Loss: 0.3572734120252935 \t| Accuracy: 105.833\n",
      "# Iteration  8073 -> Loss: 0.3572615521616669 \t| Accuracy: 105.833\n",
      "# Iteration  8074 -> Loss: 0.3572496949589888 \t| Accuracy: 105.833\n",
      "# Iteration  8075 -> Loss: 0.35723784041610984 \t| Accuracy: 105.833\n",
      "# Iteration  8076 -> Loss: 0.3572259885318816 \t| Accuracy: 105.833\n",
      "# Iteration  8077 -> Loss: 0.3572141393051558 \t| Accuracy: 105.833\n",
      "# Iteration  8078 -> Loss: 0.3572022927347853 \t| Accuracy: 105.833\n",
      "# Iteration  8079 -> Loss: 0.3571904488196233 \t| Accuracy: 105.833\n",
      "# Iteration  8080 -> Loss: 0.3571786075585238 \t| Accuracy: 105.833\n",
      "# Iteration  8081 -> Loss: 0.35716676895034133 \t| Accuracy: 105.833\n",
      "# Iteration  8082 -> Loss: 0.357154932993931 \t| Accuracy: 105.833\n",
      "# Iteration  8083 -> Loss: 0.3571430996881488 \t| Accuracy: 105.833\n",
      "# Iteration  8084 -> Loss: 0.35713126903185116 \t| Accuracy: 105.833\n",
      "# Iteration  8085 -> Loss: 0.35711944102389515 \t| Accuracy: 105.833\n",
      "# Iteration  8086 -> Loss: 0.35710761566313864 \t| Accuracy: 105.833\n",
      "# Iteration  8087 -> Loss: 0.35709579294844007 \t| Accuracy: 105.833\n",
      "# Iteration  8088 -> Loss: 0.35708397287865823 \t| Accuracy: 105.833\n",
      "# Iteration  8089 -> Loss: 0.3570721554526531 \t| Accuracy: 105.833\n",
      "# Iteration  8090 -> Loss: 0.35706034066928477 \t| Accuracy: 105.833\n",
      "# Iteration  8091 -> Loss: 0.3570485285274143 \t| Accuracy: 105.833\n",
      "# Iteration  8092 -> Loss: 0.35703671902590317 \t| Accuracy: 105.833\n",
      "# Iteration  8093 -> Loss: 0.3570249121636138 \t| Accuracy: 105.833\n",
      "# Iteration  8094 -> Loss: 0.3570131079394089 \t| Accuracy: 105.833\n",
      "# Iteration  8095 -> Loss: 0.35700130635215205 \t| Accuracy: 105.833\n",
      "# Iteration  8096 -> Loss: 0.3569895074007072 \t| Accuracy: 105.833\n",
      "# Iteration  8097 -> Loss: 0.35697771108393916 \t| Accuracy: 105.833\n",
      "# Iteration  8098 -> Loss: 0.35696591740071354 \t| Accuracy: 105.833\n",
      "# Iteration  8099 -> Loss: 0.35695412634989615 \t| Accuracy: 105.833\n",
      "# Iteration  8100 -> Loss: 0.3569423379303538 \t| Accuracy: 105.833\n",
      "# Iteration  8101 -> Loss: 0.3569305521409535 \t| Accuracy: 105.833\n",
      "# Iteration  8102 -> Loss: 0.3569187689805634 \t| Accuracy: 105.833\n",
      "# Iteration  8103 -> Loss: 0.356906988448052 \t| Accuracy: 105.833\n",
      "# Iteration  8104 -> Loss: 0.3568952105422885 \t| Accuracy: 105.833\n",
      "# Iteration  8105 -> Loss: 0.3568834352621428 \t| Accuracy: 105.833\n",
      "# Iteration  8106 -> Loss: 0.3568716626064851 \t| Accuracy: 105.833\n",
      "# Iteration  8107 -> Loss: 0.3568598925741867 \t| Accuracy: 105.833\n",
      "# Iteration  8108 -> Loss: 0.3568481251641191 \t| Accuracy: 105.833\n",
      "# Iteration  8109 -> Loss: 0.35683636037515487 \t| Accuracy: 105.833\n",
      "# Iteration  8110 -> Loss: 0.3568245982061668 \t| Accuracy: 105.833\n",
      "# Iteration  8111 -> Loss: 0.3568128386560285 \t| Accuracy: 105.833\n",
      "# Iteration  8112 -> Loss: 0.35680108172361424 \t| Accuracy: 105.833\n",
      "# Iteration  8113 -> Loss: 0.35678932740779873 \t| Accuracy: 105.833\n",
      "# Iteration  8114 -> Loss: 0.35677757570745766 \t| Accuracy: 105.833\n",
      "# Iteration  8115 -> Loss: 0.3567658266214669 \t| Accuracy: 105.833\n",
      "# Iteration  8116 -> Loss: 0.3567540801487033 \t| Accuracy: 105.833\n",
      "# Iteration  8117 -> Loss: 0.3567423362880442 \t| Accuracy: 105.833\n",
      "# Iteration  8118 -> Loss: 0.35673059503836757 \t| Accuracy: 105.833\n",
      "# Iteration  8119 -> Loss: 0.3567188563985519 \t| Accuracy: 105.833\n",
      "# Iteration  8120 -> Loss: 0.35670712036747654 \t| Accuracy: 105.833\n",
      "# Iteration  8121 -> Loss: 0.35669538694402125 \t| Accuracy: 105.833\n",
      "# Iteration  8122 -> Loss: 0.35668365612706665 \t| Accuracy: 105.833\n",
      "# Iteration  8123 -> Loss: 0.35667192791549357 \t| Accuracy: 105.833\n",
      "# Iteration  8124 -> Loss: 0.35666020230818396 \t| Accuracy: 105.833\n",
      "# Iteration  8125 -> Loss: 0.35664847930402 \t| Accuracy: 105.833\n",
      "# Iteration  8126 -> Loss: 0.3566367589018847 \t| Accuracy: 105.833\n",
      "# Iteration  8127 -> Loss: 0.3566250411006617 \t| Accuracy: 105.833\n",
      "# Iteration  8128 -> Loss: 0.3566133258992351 \t| Accuracy: 105.833\n",
      "# Iteration  8129 -> Loss: 0.3566016132964898 \t| Accuracy: 105.833\n",
      "# Iteration  8130 -> Loss: 0.35658990329131124 \t| Accuracy: 105.833\n",
      "# Iteration  8131 -> Loss: 0.35657819588258544 \t| Accuracy: 105.833\n",
      "# Iteration  8132 -> Loss: 0.35656649106919913 \t| Accuracy: 105.833\n",
      "# Iteration  8133 -> Loss: 0.3565547888500395 \t| Accuracy: 105.833\n",
      "# Iteration  8134 -> Loss: 0.3565430892239945 \t| Accuracy: 105.833\n",
      "# Iteration  8135 -> Loss: 0.35653139218995283 \t| Accuracy: 105.833\n",
      "# Iteration  8136 -> Loss: 0.35651969774680337 \t| Accuracy: 105.833\n",
      "# Iteration  8137 -> Loss: 0.3565080058934362 \t| Accuracy: 105.833\n",
      "# Iteration  8138 -> Loss: 0.35649631662874154 \t| Accuracy: 105.833\n",
      "# Iteration  8139 -> Loss: 0.3564846299516103 \t| Accuracy: 105.833\n",
      "# Iteration  8140 -> Loss: 0.35647294586093436 \t| Accuracy: 105.833\n",
      "# Iteration  8141 -> Loss: 0.3564612643556056 \t| Accuracy: 105.833\n",
      "# Iteration  8142 -> Loss: 0.35644958543451727 \t| Accuracy: 105.833\n",
      "# Iteration  8143 -> Loss: 0.35643790909656253 \t| Accuracy: 105.833\n",
      "# Iteration  8144 -> Loss: 0.3564262353406357 \t| Accuracy: 105.833\n",
      "# Iteration  8145 -> Loss: 0.3564145641656313 \t| Accuracy: 105.833\n",
      "# Iteration  8146 -> Loss: 0.35640289557044463 \t| Accuracy: 105.833\n",
      "# Iteration  8147 -> Loss: 0.35639122955397173 \t| Accuracy: 105.833\n",
      "# Iteration  8148 -> Loss: 0.3563795661151091 \t| Accuracy: 105.833\n",
      "# Iteration  8149 -> Loss: 0.35636790525275397 \t| Accuracy: 105.833\n",
      "# Iteration  8150 -> Loss: 0.356356246965804 \t| Accuracy: 105.833\n",
      "# Iteration  8151 -> Loss: 0.3563445912531575 \t| Accuracy: 105.833\n",
      "# Iteration  8152 -> Loss: 0.35633293811371347 \t| Accuracy: 105.833\n",
      "# Iteration  8153 -> Loss: 0.3563212875463718 \t| Accuracy: 105.833\n",
      "# Iteration  8154 -> Loss: 0.3563096395500323 \t| Accuracy: 105.833\n",
      "# Iteration  8155 -> Loss: 0.356297994123596 \t| Accuracy: 105.833\n",
      "# Iteration  8156 -> Loss: 0.3562863512659644 \t| Accuracy: 105.833\n",
      "# Iteration  8157 -> Loss: 0.35627471097603924 \t| Accuracy: 105.833\n",
      "# Iteration  8158 -> Loss: 0.35626307325272355 \t| Accuracy: 105.833\n",
      "# Iteration  8159 -> Loss: 0.3562514380949203 \t| Accuracy: 105.833\n",
      "# Iteration  8160 -> Loss: 0.3562398055015335 \t| Accuracy: 105.833\n",
      "# Iteration  8161 -> Loss: 0.3562281754714675 \t| Accuracy: 105.833\n",
      "# Iteration  8162 -> Loss: 0.3562165480036275 \t| Accuracy: 105.833\n",
      "# Iteration  8163 -> Loss: 0.35620492309691915 \t| Accuracy: 105.833\n",
      "# Iteration  8164 -> Loss: 0.3561933007502487 \t| Accuracy: 105.833\n",
      "# Iteration  8165 -> Loss: 0.3561816809625231 \t| Accuracy: 105.833\n",
      "# Iteration  8166 -> Loss: 0.3561700637326499 \t| Accuracy: 105.833\n",
      "# Iteration  8167 -> Loss: 0.3561584490595371 \t| Accuracy: 105.833\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Iteration  8168 -> Loss: 0.3561468369420935 \t| Accuracy: 105.833\n",
      "# Iteration  8169 -> Loss: 0.35613522737922837 \t| Accuracy: 105.833\n",
      "# Iteration  8170 -> Loss: 0.3561236203698517 \t| Accuracy: 105.833\n",
      "# Iteration  8171 -> Loss: 0.35611201591287395 \t| Accuracy: 105.833\n",
      "# Iteration  8172 -> Loss: 0.35610041400720627 \t| Accuracy: 105.833\n",
      "# Iteration  8173 -> Loss: 0.35608881465176057 \t| Accuracy: 105.833\n",
      "# Iteration  8174 -> Loss: 0.3560772178454489 \t| Accuracy: 105.833\n",
      "# Iteration  8175 -> Loss: 0.3560656235871844 \t| Accuracy: 105.833\n",
      "# Iteration  8176 -> Loss: 0.3560540318758805 \t| Accuracy: 105.833\n",
      "# Iteration  8177 -> Loss: 0.3560424427104515 \t| Accuracy: 105.833\n",
      "# Iteration  8178 -> Loss: 0.35603085608981194 \t| Accuracy: 105.833\n",
      "# Iteration  8179 -> Loss: 0.35601927201287736 \t| Accuracy: 105.833\n",
      "# Iteration  8180 -> Loss: 0.3560076904785636 \t| Accuracy: 105.833\n",
      "# Iteration  8181 -> Loss: 0.35599611148578736 \t| Accuracy: 105.833\n",
      "# Iteration  8182 -> Loss: 0.3559845350334656 \t| Accuracy: 105.833\n",
      "# Iteration  8183 -> Loss: 0.35597296112051624 \t| Accuracy: 105.833\n",
      "# Iteration  8184 -> Loss: 0.3559613897458575 \t| Accuracy: 105.833\n",
      "# Iteration  8185 -> Loss: 0.3559498209084083 \t| Accuracy: 105.833\n",
      "# Iteration  8186 -> Loss: 0.35593825460708833 \t| Accuracy: 105.833\n",
      "# Iteration  8187 -> Loss: 0.35592669084081763 \t| Accuracy: 105.833\n",
      "# Iteration  8188 -> Loss: 0.355915129608517 \t| Accuracy: 105.833\n",
      "# Iteration  8189 -> Loss: 0.35590357090910774 \t| Accuracy: 105.833\n",
      "# Iteration  8190 -> Loss: 0.35589201474151183 \t| Accuracy: 105.833\n",
      "# Iteration  8191 -> Loss: 0.3558804611046517 \t| Accuracy: 105.833\n",
      "# Iteration  8192 -> Loss: 0.3558689099974507 \t| Accuracy: 105.833\n",
      "# Iteration  8193 -> Loss: 0.3558573614188323 \t| Accuracy: 105.833\n",
      "# Iteration  8194 -> Loss: 0.35584581536772103 \t| Accuracy: 105.833\n",
      "# Iteration  8195 -> Loss: 0.35583427184304156 \t| Accuracy: 105.833\n",
      "# Iteration  8196 -> Loss: 0.3558227308437196 \t| Accuracy: 105.833\n",
      "# Iteration  8197 -> Loss: 0.35581119236868125 \t| Accuracy: 105.833\n",
      "# Iteration  8198 -> Loss: 0.3557996564168532 \t| Accuracy: 105.833\n",
      "# Iteration  8199 -> Loss: 0.35578812298716256 \t| Accuracy: 105.833\n",
      "# Iteration  8200 -> Loss: 0.35577659207853735 \t| Accuracy: 105.833\n",
      "# Iteration  8201 -> Loss: 0.3557650636899061 \t| Accuracy: 105.833\n",
      "# Iteration  8202 -> Loss: 0.35575353782019786 \t| Accuracy: 105.833\n",
      "# Iteration  8203 -> Loss: 0.35574201446834225 \t| Accuracy: 105.833\n",
      "# Iteration  8204 -> Loss: 0.3557304936332694 \t| Accuracy: 105.833\n",
      "# Iteration  8205 -> Loss: 0.35571897531391045 \t| Accuracy: 105.833\n",
      "# Iteration  8206 -> Loss: 0.3557074595091966 \t| Accuracy: 105.833\n",
      "# Iteration  8207 -> Loss: 0.35569594621805994 \t| Accuracy: 105.833\n",
      "# Iteration  8208 -> Loss: 0.355684435439433 \t| Accuracy: 105.833\n",
      "# Iteration  8209 -> Loss: 0.35567292717224924 \t| Accuracy: 105.833\n",
      "# Iteration  8210 -> Loss: 0.35566142141544205 \t| Accuracy: 105.833\n",
      "# Iteration  8211 -> Loss: 0.35564991816794633 \t| Accuracy: 105.833\n",
      "# Iteration  8212 -> Loss: 0.3556384174286965 \t| Accuracy: 105.833\n",
      "# Iteration  8213 -> Loss: 0.3556269191966284 \t| Accuracy: 105.833\n",
      "# Iteration  8214 -> Loss: 0.3556154234706782 \t| Accuracy: 105.833\n",
      "# Iteration  8215 -> Loss: 0.35560393024978265 \t| Accuracy: 105.833\n",
      "# Iteration  8216 -> Loss: 0.355592439532879 \t| Accuracy: 105.833\n",
      "# Iteration  8217 -> Loss: 0.35558095131890516 \t| Accuracy: 105.833\n",
      "# Iteration  8218 -> Loss: 0.35556946560679953 \t| Accuracy: 105.833\n",
      "# Iteration  8219 -> Loss: 0.3555579823955013 \t| Accuracy: 105.833\n",
      "# Iteration  8220 -> Loss: 0.3555465016839502 \t| Accuracy: 105.833\n",
      "# Iteration  8221 -> Loss: 0.3555350234710864 \t| Accuracy: 105.833\n",
      "# Iteration  8222 -> Loss: 0.3555235477558508 \t| Accuracy: 105.833\n",
      "# Iteration  8223 -> Loss: 0.3555120745371846 \t| Accuracy: 105.833\n",
      "# Iteration  8224 -> Loss: 0.3555006038140301 \t| Accuracy: 105.833\n",
      "# Iteration  8225 -> Loss: 0.3554891355853297 \t| Accuracy: 105.833\n",
      "# Iteration  8226 -> Loss: 0.35547766985002655 \t| Accuracy: 105.833\n",
      "# Iteration  8227 -> Loss: 0.35546620660706457 \t| Accuracy: 105.833\n",
      "# Iteration  8228 -> Loss: 0.35545474585538794 \t| Accuracy: 105.833\n",
      "# Iteration  8229 -> Loss: 0.3554432875939416 \t| Accuracy: 105.833\n",
      "# Iteration  8230 -> Loss: 0.35543183182167115 \t| Accuracy: 105.833\n",
      "# Iteration  8231 -> Loss: 0.35542037853752256 \t| Accuracy: 105.833\n",
      "# Iteration  8232 -> Loss: 0.35540892774044247 \t| Accuracy: 105.833\n",
      "# Iteration  8233 -> Loss: 0.35539747942937827 \t| Accuracy: 105.833\n",
      "# Iteration  8234 -> Loss: 0.3553860336032777 \t| Accuracy: 105.833\n",
      "# Iteration  8235 -> Loss: 0.35537459026108914 \t| Accuracy: 105.833\n",
      "# Iteration  8236 -> Loss: 0.3553631494017616 \t| Accuracy: 105.833\n",
      "# Iteration  8237 -> Loss: 0.3553517110242446 \t| Accuracy: 105.833\n",
      "# Iteration  8238 -> Loss: 0.35534027512748834 \t| Accuracy: 105.833\n",
      "# Iteration  8239 -> Loss: 0.35532884171044354 \t| Accuracy: 105.833\n",
      "# Iteration  8240 -> Loss: 0.35531741077206136 \t| Accuracy: 105.833\n",
      "# Iteration  8241 -> Loss: 0.3553059823112939 \t| Accuracy: 105.833\n",
      "# Iteration  8242 -> Loss: 0.3552945563270935 \t| Accuracy: 105.833\n",
      "# Iteration  8243 -> Loss: 0.3552831328184132 \t| Accuracy: 105.833\n",
      "# Iteration  8244 -> Loss: 0.3552717117842066 \t| Accuracy: 105.833\n",
      "# Iteration  8245 -> Loss: 0.35526029322342784 \t| Accuracy: 105.833\n",
      "# Iteration  8246 -> Loss: 0.35524887713503167 \t| Accuracy: 105.833\n",
      "# Iteration  8247 -> Loss: 0.3552374635179734 \t| Accuracy: 105.833\n",
      "# Iteration  8248 -> Loss: 0.3552260523712091 \t| Accuracy: 105.833\n",
      "# Iteration  8249 -> Loss: 0.35521464369369515 \t| Accuracy: 105.833\n",
      "# Iteration  8250 -> Loss: 0.3552032374843887 \t| Accuracy: 105.833\n",
      "# Iteration  8251 -> Loss: 0.3551918337422472 \t| Accuracy: 105.833\n",
      "# Iteration  8252 -> Loss: 0.35518043246622893 \t| Accuracy: 105.833\n",
      "# Iteration  8253 -> Loss: 0.3551690336552927 \t| Accuracy: 105.833\n",
      "# Iteration  8254 -> Loss: 0.355157637308398 \t| Accuracy: 105.833\n",
      "# Iteration  8255 -> Loss: 0.35514624342450446 \t| Accuracy: 105.833\n",
      "# Iteration  8256 -> Loss: 0.3551348520025728 \t| Accuracy: 105.833\n",
      "# Iteration  8257 -> Loss: 0.35512346304156395 \t| Accuracy: 105.833\n",
      "# Iteration  8258 -> Loss: 0.3551120765404396 \t| Accuracy: 105.833\n",
      "# Iteration  8259 -> Loss: 0.355100692498162 \t| Accuracy: 105.833\n",
      "# Iteration  8260 -> Loss: 0.355089310913694 \t| Accuracy: 105.833\n",
      "# Iteration  8261 -> Loss: 0.3550779317859988 \t| Accuracy: 105.833\n",
      "# Iteration  8262 -> Loss: 0.3550665551140403 \t| Accuracy: 105.833\n",
      "# Iteration  8263 -> Loss: 0.3550551808967832 \t| Accuracy: 105.833\n",
      "# Iteration  8264 -> Loss: 0.35504380913319233 \t| Accuracy: 105.833\n",
      "# Iteration  8265 -> Loss: 0.35503243982223354 \t| Accuracy: 105.833\n",
      "# Iteration  8266 -> Loss: 0.35502107296287283 \t| Accuracy: 105.833\n",
      "# Iteration  8267 -> Loss: 0.3550097085540771 \t| Accuracy: 105.833\n",
      "# Iteration  8268 -> Loss: 0.35499834659481366 \t| Accuracy: 105.833\n",
      "# Iteration  8269 -> Loss: 0.35498698708405046 \t| Accuracy: 105.833\n",
      "# Iteration  8270 -> Loss: 0.3549756300207559 \t| Accuracy: 105.833\n",
      "# Iteration  8271 -> Loss: 0.3549642754038989 \t| Accuracy: 105.833\n",
      "# Iteration  8272 -> Loss: 0.35495292323244937 \t| Accuracy: 105.833\n",
      "# Iteration  8273 -> Loss: 0.35494157350537714 \t| Accuracy: 105.833\n",
      "# Iteration  8274 -> Loss: 0.35493022622165327 \t| Accuracy: 105.833\n",
      "# Iteration  8275 -> Loss: 0.35491888138024874 \t| Accuracy: 105.833\n",
      "# Iteration  8276 -> Loss: 0.3549075389801357 \t| Accuracy: 105.833\n",
      "# Iteration  8277 -> Loss: 0.3548961990202864 \t| Accuracy: 105.833\n",
      "# Iteration  8278 -> Loss: 0.3548848614996739 \t| Accuracy: 105.833\n",
      "# Iteration  8279 -> Loss: 0.3548735264172719 \t| Accuracy: 105.833\n",
      "# Iteration  8280 -> Loss: 0.35486219377205414 \t| Accuracy: 105.833\n",
      "# Iteration  8281 -> Loss: 0.3548508635629958 \t| Accuracy: 105.833\n",
      "# Iteration  8282 -> Loss: 0.3548395357890717 \t| Accuracy: 105.833\n",
      "# Iteration  8283 -> Loss: 0.35482821044925794 \t| Accuracy: 105.833\n",
      "# Iteration  8284 -> Loss: 0.35481688754253066 \t| Accuracy: 105.833\n",
      "# Iteration  8285 -> Loss: 0.3548055670678671 \t| Accuracy: 105.833\n",
      "# Iteration  8286 -> Loss: 0.3547942490242445 \t| Accuracy: 105.833\n",
      "# Iteration  8287 -> Loss: 0.35478293341064093 \t| Accuracy: 105.833\n",
      "# Iteration  8288 -> Loss: 0.35477162022603526 \t| Accuracy: 105.833\n",
      "# Iteration  8289 -> Loss: 0.35476030946940634 \t| Accuracy: 105.833\n",
      "# Iteration  8290 -> Loss: 0.3547490011397342 \t| Accuracy: 105.833\n",
      "# Iteration  8291 -> Loss: 0.3547376952359989 \t| Accuracy: 105.833\n",
      "# Iteration  8292 -> Loss: 0.3547263917571815 \t| Accuracy: 105.833\n",
      "# Iteration  8293 -> Loss: 0.3547150907022632 \t| Accuracy: 105.833\n",
      "# Iteration  8294 -> Loss: 0.35470379207022623 \t| Accuracy: 105.833\n",
      "# Iteration  8295 -> Loss: 0.35469249586005297 \t| Accuracy: 105.833\n",
      "# Iteration  8296 -> Loss: 0.35468120207072656 \t| Accuracy: 105.833\n",
      "# Iteration  8297 -> Loss: 0.3546699107012306 \t| Accuracy: 105.833\n",
      "# Iteration  8298 -> Loss: 0.3546586217505494 \t| Accuracy: 105.833\n",
      "# Iteration  8299 -> Loss: 0.35464733521766767 \t| Accuracy: 105.833\n",
      "# Iteration  8300 -> Loss: 0.35463605110157065 \t| Accuracy: 105.833\n",
      "# Iteration  8301 -> Loss: 0.35462476940124443 \t| Accuracy: 105.833\n",
      "# Iteration  8302 -> Loss: 0.3546134901156752 \t| Accuracy: 105.833\n",
      "# Iteration  8303 -> Loss: 0.35460221324385016 \t| Accuracy: 105.833\n",
      "# Iteration  8304 -> Loss: 0.35459093878475667 \t| Accuracy: 105.833\n",
      "# Iteration  8305 -> Loss: 0.35457966673738306 \t| Accuracy: 105.833\n",
      "# Iteration  8306 -> Loss: 0.3545683971007178 \t| Accuracy: 105.833\n",
      "# Iteration  8307 -> Loss: 0.3545571298737501 \t| Accuracy: 105.833\n",
      "# Iteration  8308 -> Loss: 0.35454586505546987 \t| Accuracy: 105.833\n",
      "# Iteration  8309 -> Loss: 0.3545346026448673 \t| Accuracy: 105.833\n",
      "# Iteration  8310 -> Loss: 0.35452334264093327 \t| Accuracy: 105.833\n",
      "# Iteration  8311 -> Loss: 0.3545120850426592 \t| Accuracy: 105.833\n",
      "# Iteration  8312 -> Loss: 0.3545008298490373 \t| Accuracy: 105.833\n",
      "# Iteration  8313 -> Loss: 0.3544895770590597 \t| Accuracy: 105.833\n",
      "# Iteration  8314 -> Loss: 0.35447832667171975 \t| Accuracy: 105.833\n",
      "# Iteration  8315 -> Loss: 0.35446707868601096 \t| Accuracy: 105.833\n",
      "# Iteration  8316 -> Loss: 0.3544558331009276 \t| Accuracy: 105.833\n",
      "# Iteration  8317 -> Loss: 0.35444458991546435 \t| Accuracy: 105.833\n",
      "# Iteration  8318 -> Loss: 0.35443334912861646 \t| Accuracy: 105.833\n",
      "# Iteration  8319 -> Loss: 0.3544221107393799 \t| Accuracy: 105.833\n",
      "# Iteration  8320 -> Loss: 0.35441087474675104 \t| Accuracy: 105.833\n",
      "# Iteration  8321 -> Loss: 0.35439964114972655 \t| Accuracy: 105.833\n",
      "# Iteration  8322 -> Loss: 0.3543884099473042 \t| Accuracy: 105.833\n",
      "# Iteration  8323 -> Loss: 0.3543771811384819 \t| Accuracy: 105.833\n",
      "# Iteration  8324 -> Loss: 0.3543659547222583 \t| Accuracy: 105.833\n",
      "# Iteration  8325 -> Loss: 0.3543547306976324 \t| Accuracy: 105.833\n",
      "# Iteration  8326 -> Loss: 0.354343509063604 \t| Accuracy: 105.833\n",
      "# Iteration  8327 -> Loss: 0.3543322898191733 \t| Accuracy: 105.833\n",
      "# Iteration  8328 -> Loss: 0.3543210729633409 \t| Accuracy: 105.833\n",
      "# Iteration  8329 -> Loss: 0.35430985849510854 \t| Accuracy: 105.833\n",
      "# Iteration  8330 -> Loss: 0.3542986464134775 \t| Accuracy: 105.833\n",
      "# Iteration  8331 -> Loss: 0.35428743671745067 \t| Accuracy: 105.833\n",
      "# Iteration  8332 -> Loss: 0.35427622940603076 \t| Accuracy: 105.833\n",
      "# Iteration  8333 -> Loss: 0.35426502447822134 \t| Accuracy: 105.833\n",
      "# Iteration  8334 -> Loss: 0.35425382193302646 \t| Accuracy: 105.833\n",
      "# Iteration  8335 -> Loss: 0.35424262176945065 \t| Accuracy: 105.833\n",
      "# Iteration  8336 -> Loss: 0.35423142398649915 \t| Accuracy: 105.833\n",
      "# Iteration  8337 -> Loss: 0.3542202285831775 \t| Accuracy: 105.833\n",
      "# Iteration  8338 -> Loss: 0.35420903555849204 \t| Accuracy: 105.833\n",
      "# Iteration  8339 -> Loss: 0.35419784491144946 \t| Accuracy: 105.833\n",
      "# Iteration  8340 -> Loss: 0.35418665664105714 \t| Accuracy: 105.833\n",
      "# Iteration  8341 -> Loss: 0.3541754707463228 \t| Accuracy: 105.833\n",
      "# Iteration  8342 -> Loss: 0.3541642872262549 \t| Accuracy: 105.833\n",
      "# Iteration  8343 -> Loss: 0.35415310607986256 \t| Accuracy: 105.833\n",
      "# Iteration  8344 -> Loss: 0.35414192730615485 \t| Accuracy: 105.833\n",
      "# Iteration  8345 -> Loss: 0.35413075090414214 \t| Accuracy: 105.833\n",
      "# Iteration  8346 -> Loss: 0.3541195768728348 \t| Accuracy: 105.833\n",
      "# Iteration  8347 -> Loss: 0.3541084052112441 \t| Accuracy: 105.833\n",
      "# Iteration  8348 -> Loss: 0.35409723591838144 \t| Accuracy: 105.833\n",
      "# Iteration  8349 -> Loss: 0.3540860689932592 \t| Accuracy: 105.833\n",
      "# Iteration  8350 -> Loss: 0.35407490443488987 \t| Accuracy: 105.833\n",
      "# Iteration  8351 -> Loss: 0.3540637422422871 \t| Accuracy: 105.833\n",
      "# Iteration  8352 -> Loss: 0.35405258241446425 \t| Accuracy: 105.833\n",
      "# Iteration  8353 -> Loss: 0.35404142495043595 \t| Accuracy: 105.833\n",
      "# Iteration  8354 -> Loss: 0.3540302698492168 \t| Accuracy: 105.833\n",
      "# Iteration  8355 -> Loss: 0.35401911710982253 \t| Accuracy: 105.833\n",
      "# Iteration  8356 -> Loss: 0.35400796673126883 \t| Accuracy: 105.833\n",
      "# Iteration  8357 -> Loss: 0.35399681871257227 \t| Accuracy: 105.833\n",
      "# Iteration  8358 -> Loss: 0.35398567305275 \t| Accuracy: 105.833\n",
      "# Iteration  8359 -> Loss: 0.3539745297508194 \t| Accuracy: 105.833\n",
      "# Iteration  8360 -> Loss: 0.3539633888057986 \t| Accuracy: 105.833\n",
      "# Iteration  8361 -> Loss: 0.3539522502167063 \t| Accuracy: 105.833\n",
      "# Iteration  8362 -> Loss: 0.3539411139825616 \t| Accuracy: 105.833\n",
      "# Iteration  8363 -> Loss: 0.3539299801023842 \t| Accuracy: 105.833\n",
      "# Iteration  8364 -> Loss: 0.3539188485751943 \t| Accuracy: 105.833\n",
      "# Iteration  8365 -> Loss: 0.35390771940001275 \t| Accuracy: 105.833\n",
      "# Iteration  8366 -> Loss: 0.35389659257586087 \t| Accuracy: 105.833\n",
      "# Iteration  8367 -> Loss: 0.3538854681017604 \t| Accuracy: 105.833\n",
      "# Iteration  8368 -> Loss: 0.3538743459767338 \t| Accuracy: 105.833\n",
      "# Iteration  8369 -> Loss: 0.3538632261998039 \t| Accuracy: 105.833\n",
      "# Iteration  8370 -> Loss: 0.3538521087699941 \t| Accuracy: 105.833\n",
      "# Iteration  8371 -> Loss: 0.35384099368632854 \t| Accuracy: 105.833\n",
      "# Iteration  8372 -> Loss: 0.35382988094783163 \t| Accuracy: 105.833\n",
      "# Iteration  8373 -> Loss: 0.35381877055352845 \t| Accuracy: 105.833\n",
      "# Iteration  8374 -> Loss: 0.3538076625024444 \t| Accuracy: 105.833\n",
      "# Iteration  8375 -> Loss: 0.3537965567936058 \t| Accuracy: 105.833\n",
      "# Iteration  8376 -> Loss: 0.35378545342603906 \t| Accuracy: 105.833\n",
      "# Iteration  8377 -> Loss: 0.3537743523987714 \t| Accuracy: 105.833\n",
      "# Iteration  8378 -> Loss: 0.3537632537108307 \t| Accuracy: 105.833\n",
      "# Iteration  8379 -> Loss: 0.353752157361245 \t| Accuracy: 105.833\n",
      "# Iteration  8380 -> Loss: 0.35374106334904304 \t| Accuracy: 105.833\n",
      "# Iteration  8381 -> Loss: 0.3537299716732541 \t| Accuracy: 105.833\n",
      "# Iteration  8382 -> Loss: 0.35371888233290805 \t| Accuracy: 105.833\n",
      "# Iteration  8383 -> Loss: 0.35370779532703517 \t| Accuracy: 105.833\n",
      "# Iteration  8384 -> Loss: 0.3536967106546664 \t| Accuracy: 105.833\n",
      "# Iteration  8385 -> Loss: 0.353685628314833 \t| Accuracy: 105.833\n",
      "# Iteration  8386 -> Loss: 0.3536745483065671 \t| Accuracy: 105.833\n",
      "# Iteration  8387 -> Loss: 0.3536634706289008 \t| Accuracy: 105.833\n",
      "# Iteration  8388 -> Loss: 0.3536523952808674 \t| Accuracy: 105.833\n",
      "# Iteration  8389 -> Loss: 0.35364132226150036 \t| Accuracy: 105.833\n",
      "# Iteration  8390 -> Loss: 0.35363025156983346 \t| Accuracy: 105.833\n",
      "# Iteration  8391 -> Loss: 0.35361918320490154 \t| Accuracy: 105.833\n",
      "# Iteration  8392 -> Loss: 0.3536081171657395 \t| Accuracy: 105.833\n",
      "# Iteration  8393 -> Loss: 0.35359705345138304 \t| Accuracy: 105.833\n",
      "# Iteration  8394 -> Loss: 0.35358599206086816 \t| Accuracy: 105.833\n",
      "# Iteration  8395 -> Loss: 0.3535749329932316 \t| Accuracy: 105.833\n",
      "# Iteration  8396 -> Loss: 0.3535638762475106 \t| Accuracy: 105.833\n",
      "# Iteration  8397 -> Loss: 0.3535528218227428 \t| Accuracy: 105.833\n",
      "# Iteration  8398 -> Loss: 0.3535417697179664 \t| Accuracy: 105.833\n",
      "# Iteration  8399 -> Loss: 0.35353071993222007 \t| Accuracy: 105.833\n",
      "# Iteration  8400 -> Loss: 0.35351967246454324 \t| Accuracy: 105.833\n",
      "# Iteration  8401 -> Loss: 0.35350862731397564 \t| Accuracy: 105.833\n",
      "# Iteration  8402 -> Loss: 0.3534975844795576 \t| Accuracy: 105.833\n",
      "# Iteration  8403 -> Loss: 0.35348654396033 \t| Accuracy: 105.833\n",
      "# Iteration  8404 -> Loss: 0.35347550575533404 \t| Accuracy: 105.833\n",
      "# Iteration  8405 -> Loss: 0.35346446986361174 \t| Accuracy: 105.833\n",
      "# Iteration  8406 -> Loss: 0.3534534362842054 \t| Accuracy: 105.833\n",
      "# Iteration  8407 -> Loss: 0.35344240501615803 \t| Accuracy: 105.833\n",
      "# Iteration  8408 -> Loss: 0.35343137605851305 \t| Accuracy: 105.833\n",
      "# Iteration  8409 -> Loss: 0.35342034941031447 \t| Accuracy: 105.833\n",
      "# Iteration  8410 -> Loss: 0.3534093250706067 \t| Accuracy: 105.833\n",
      "# Iteration  8411 -> Loss: 0.3533983030384347 \t| Accuracy: 105.833\n",
      "# Iteration  8412 -> Loss: 0.3533872833128441 \t| Accuracy: 105.833\n",
      "# Iteration  8413 -> Loss: 0.35337626589288096 \t| Accuracy: 105.833\n",
      "# Iteration  8414 -> Loss: 0.3533652507775917 \t| Accuracy: 105.833\n",
      "# Iteration  8415 -> Loss: 0.3533542379660235 \t| Accuracy: 105.833\n",
      "# Iteration  8416 -> Loss: 0.353343227457224 \t| Accuracy: 105.833\n",
      "# Iteration  8417 -> Loss: 0.35333221925024116 \t| Accuracy: 105.833\n",
      "# Iteration  8418 -> Loss: 0.3533212133441237 \t| Accuracy: 105.833\n",
      "# Iteration  8419 -> Loss: 0.35331020973792077 \t| Accuracy: 105.833\n",
      "# Iteration  8420 -> Loss: 0.35329920843068213 \t| Accuracy: 105.833\n",
      "# Iteration  8421 -> Loss: 0.3532882094214578 \t| Accuracy: 105.833\n",
      "# Iteration  8422 -> Loss: 0.35327721270929846 \t| Accuracy: 105.833\n",
      "# Iteration  8423 -> Loss: 0.35326621829325544 \t| Accuracy: 105.833\n",
      "# Iteration  8424 -> Loss: 0.3532552261723804 \t| Accuracy: 105.833\n",
      "# Iteration  8425 -> Loss: 0.35324423634572566 \t| Accuracy: 105.833\n",
      "# Iteration  8426 -> Loss: 0.35323324881234397 \t| Accuracy: 105.833\n",
      "# Iteration  8427 -> Loss: 0.3532222635712885 \t| Accuracy: 105.833\n",
      "# Iteration  8428 -> Loss: 0.3532112806216131 \t| Accuracy: 105.833\n",
      "# Iteration  8429 -> Loss: 0.3532002999623721 \t| Accuracy: 105.833\n",
      "# Iteration  8430 -> Loss: 0.35318932159262023 \t| Accuracy: 105.833\n",
      "# Iteration  8431 -> Loss: 0.3531783455114128 \t| Accuracy: 105.833\n",
      "# Iteration  8432 -> Loss: 0.3531673717178059 \t| Accuracy: 105.833\n",
      "# Iteration  8433 -> Loss: 0.35315640021085565 \t| Accuracy: 105.833\n",
      "# Iteration  8434 -> Loss: 0.35314543098961887 \t| Accuracy: 105.833\n",
      "# Iteration  8435 -> Loss: 0.3531344640531531 \t| Accuracy: 105.833\n",
      "# Iteration  8436 -> Loss: 0.353123499400516 \t| Accuracy: 105.833\n",
      "# Iteration  8437 -> Loss: 0.3531125370307663 \t| Accuracy: 105.833\n",
      "# Iteration  8438 -> Loss: 0.3531015769429627 \t| Accuracy: 105.833\n",
      "# Iteration  8439 -> Loss: 0.3530906191361646 \t| Accuracy: 105.833\n",
      "# Iteration  8440 -> Loss: 0.353079663609432 \t| Accuracy: 105.833\n",
      "# Iteration  8441 -> Loss: 0.3530687103618253 \t| Accuracy: 105.833\n",
      "# Iteration  8442 -> Loss: 0.3530577593924055 \t| Accuracy: 105.833\n",
      "# Iteration  8443 -> Loss: 0.3530468107002341 \t| Accuracy: 105.833\n",
      "# Iteration  8444 -> Loss: 0.353035864284373 \t| Accuracy: 105.833\n",
      "# Iteration  8445 -> Loss: 0.35302492014388454 \t| Accuracy: 105.833\n",
      "# Iteration  8446 -> Loss: 0.35301397827783204 \t| Accuracy: 105.833\n",
      "# Iteration  8447 -> Loss: 0.3530030386852787 \t| Accuracy: 105.833\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Iteration  8448 -> Loss: 0.3529921013652886 \t| Accuracy: 105.833\n",
      "# Iteration  8449 -> Loss: 0.35298116631692644 \t| Accuracy: 105.833\n",
      "# Iteration  8450 -> Loss: 0.35297023353925694 \t| Accuracy: 105.833\n",
      "# Iteration  8451 -> Loss: 0.3529593030313457 \t| Accuracy: 105.833\n",
      "# Iteration  8452 -> Loss: 0.352948374792259 \t| Accuracy: 105.833\n",
      "# Iteration  8453 -> Loss: 0.35293744882106304 \t| Accuracy: 105.833\n",
      "# Iteration  8454 -> Loss: 0.35292652511682504 \t| Accuracy: 105.833\n",
      "# Iteration  8455 -> Loss: 0.3529156036786126 \t| Accuracy: 105.833\n",
      "# Iteration  8456 -> Loss: 0.3529046845054936 \t| Accuracy: 105.833\n",
      "# Iteration  8457 -> Loss: 0.35289376759653673 \t| Accuracy: 105.833\n",
      "# Iteration  8458 -> Loss: 0.35288285295081095 \t| Accuracy: 105.833\n",
      "# Iteration  8459 -> Loss: 0.352871940567386 \t| Accuracy: 105.833\n",
      "# Iteration  8460 -> Loss: 0.35286103044533174 \t| Accuracy: 105.833\n",
      "# Iteration  8461 -> Loss: 0.352850122583719 \t| Accuracy: 105.833\n",
      "# Iteration  8462 -> Loss: 0.35283921698161863 \t| Accuracy: 105.833\n",
      "# Iteration  8463 -> Loss: 0.3528283136381024 \t| Accuracy: 105.833\n",
      "# Iteration  8464 -> Loss: 0.3528174125522422 \t| Accuracy: 105.833\n",
      "# Iteration  8465 -> Loss: 0.3528065137231106 \t| Accuracy: 105.833\n",
      "# Iteration  8466 -> Loss: 0.352795617149781 \t| Accuracy: 105.833\n",
      "# Iteration  8467 -> Loss: 0.35278472283132667 \t| Accuracy: 105.833\n",
      "# Iteration  8468 -> Loss: 0.3527738307668219 \t| Accuracy: 105.833\n",
      "# Iteration  8469 -> Loss: 0.35276294095534116 \t| Accuracy: 105.833\n",
      "# Iteration  8470 -> Loss: 0.35275205339595966 \t| Accuracy: 105.833\n",
      "# Iteration  8471 -> Loss: 0.35274116808775285 \t| Accuracy: 105.833\n",
      "# Iteration  8472 -> Loss: 0.3527302850297971 \t| Accuracy: 105.833\n",
      "# Iteration  8473 -> Loss: 0.3527194042211687 \t| Accuracy: 105.833\n",
      "# Iteration  8474 -> Loss: 0.3527085256609449 \t| Accuracy: 105.833\n",
      "# Iteration  8475 -> Loss: 0.35269764934820325 \t| Accuracy: 105.833\n",
      "# Iteration  8476 -> Loss: 0.35268677528202197 \t| Accuracy: 105.833\n",
      "# Iteration  8477 -> Loss: 0.3526759034614795 \t| Accuracy: 105.833\n",
      "# Iteration  8478 -> Loss: 0.3526650338856549 \t| Accuracy: 105.833\n",
      "# Iteration  8479 -> Loss: 0.35265416655362797 \t| Accuracy: 105.833\n",
      "# Iteration  8480 -> Loss: 0.35264330146447864 \t| Accuracy: 105.833\n",
      "# Iteration  8481 -> Loss: 0.3526324386172875 \t| Accuracy: 105.833\n",
      "# Iteration  8482 -> Loss: 0.3526215780111358 \t| Accuracy: 105.833\n",
      "# Iteration  8483 -> Loss: 0.3526107196451049 \t| Accuracy: 105.833\n",
      "# Iteration  8484 -> Loss: 0.352599863518277 \t| Accuracy: 105.833\n",
      "# Iteration  8485 -> Loss: 0.3525890096297347 \t| Accuracy: 105.833\n",
      "# Iteration  8486 -> Loss: 0.35257815797856107 \t| Accuracy: 105.833\n",
      "# Iteration  8487 -> Loss: 0.35256730856383944 \t| Accuracy: 105.833\n",
      "# Iteration  8488 -> Loss: 0.35255646138465424 \t| Accuracy: 105.833\n",
      "# Iteration  8489 -> Loss: 0.3525456164400899 \t| Accuracy: 105.833\n",
      "# Iteration  8490 -> Loss: 0.3525347737292313 \t| Accuracy: 105.833\n",
      "# Iteration  8491 -> Loss: 0.3525239332511642 \t| Accuracy: 105.833\n",
      "# Iteration  8492 -> Loss: 0.35251309500497463 \t| Accuracy: 105.833\n",
      "# Iteration  8493 -> Loss: 0.352502258989749 \t| Accuracy: 105.833\n",
      "# Iteration  8494 -> Loss: 0.35249142520457444 \t| Accuracy: 105.833\n",
      "# Iteration  8495 -> Loss: 0.35248059364853834 \t| Accuracy: 105.833\n",
      "# Iteration  8496 -> Loss: 0.3524697643207289 \t| Accuracy: 105.833\n",
      "# Iteration  8497 -> Loss: 0.3524589372202346 \t| Accuracy: 105.833\n",
      "# Iteration  8498 -> Loss: 0.35244811234614426 \t| Accuracy: 105.833\n",
      "# Iteration  8499 -> Loss: 0.3524372896975476 \t| Accuracy: 105.833\n",
      "# Iteration  8500 -> Loss: 0.35242646927353455 \t| Accuracy: 105.833\n",
      "# Iteration  8501 -> Loss: 0.3524156510731955 \t| Accuracy: 105.833\n",
      "# Iteration  8502 -> Loss: 0.3524048350956215 \t| Accuracy: 105.833\n",
      "# Iteration  8503 -> Loss: 0.35239402133990405 \t| Accuracy: 105.833\n",
      "# Iteration  8504 -> Loss: 0.352383209805135 \t| Accuracy: 105.833\n",
      "# Iteration  8505 -> Loss: 0.3523724004904068 \t| Accuracy: 105.833\n",
      "# Iteration  8506 -> Loss: 0.35236159339481243 \t| Accuracy: 105.833\n",
      "# Iteration  8507 -> Loss: 0.3523507885174454 \t| Accuracy: 105.833\n",
      "# Iteration  8508 -> Loss: 0.3523399858573993 \t| Accuracy: 105.833\n",
      "# Iteration  8509 -> Loss: 0.3523291854137689 \t| Accuracy: 105.833\n",
      "# Iteration  8510 -> Loss: 0.352318387185649 \t| Accuracy: 105.833\n",
      "# Iteration  8511 -> Loss: 0.35230759117213484 \t| Accuracy: 105.833\n",
      "# Iteration  8512 -> Loss: 0.35229679737232233 \t| Accuracy: 105.833\n",
      "# Iteration  8513 -> Loss: 0.35228600578530794 \t| Accuracy: 105.833\n",
      "# Iteration  8514 -> Loss: 0.3522752164101885 \t| Accuracy: 105.833\n",
      "# Iteration  8515 -> Loss: 0.35226442924606116 \t| Accuracy: 105.833\n",
      "# Iteration  8516 -> Loss: 0.35225364429202405 \t| Accuracy: 105.833\n",
      "# Iteration  8517 -> Loss: 0.3522428615471752 \t| Accuracy: 105.833\n",
      "# Iteration  8518 -> Loss: 0.3522320810106134 \t| Accuracy: 105.833\n",
      "# Iteration  8519 -> Loss: 0.35222130268143814 \t| Accuracy: 105.833\n",
      "# Iteration  8520 -> Loss: 0.35221052655874896 \t| Accuracy: 105.833\n",
      "# Iteration  8521 -> Loss: 0.3521997526416463 \t| Accuracy: 105.833\n",
      "# Iteration  8522 -> Loss: 0.3521889809292308 \t| Accuracy: 105.833\n",
      "# Iteration  8523 -> Loss: 0.3521782114206037 \t| Accuracy: 105.833\n",
      "# Iteration  8524 -> Loss: 0.3521674441148666 \t| Accuracy: 105.833\n",
      "# Iteration  8525 -> Loss: 0.3521566790111219 \t| Accuracy: 105.833\n",
      "# Iteration  8526 -> Loss: 0.3521459161084721 \t| Accuracy: 105.833\n",
      "# Iteration  8527 -> Loss: 0.35213515540602053 \t| Accuracy: 105.833\n",
      "# Iteration  8528 -> Loss: 0.3521243969028706 \t| Accuracy: 105.833\n",
      "# Iteration  8529 -> Loss: 0.35211364059812666 \t| Accuracy: 105.833\n",
      "# Iteration  8530 -> Loss: 0.35210288649089316 \t| Accuracy: 105.833\n",
      "# Iteration  8531 -> Loss: 0.3520921345802754 \t| Accuracy: 105.833\n",
      "# Iteration  8532 -> Loss: 0.35208138486537854 \t| Accuracy: 105.833\n",
      "# Iteration  8533 -> Loss: 0.3520706373453091 \t| Accuracy: 105.833\n",
      "# Iteration  8534 -> Loss: 0.35205989201917326 \t| Accuracy: 105.833\n",
      "# Iteration  8535 -> Loss: 0.3520491488860784 \t| Accuracy: 105.833\n",
      "# Iteration  8536 -> Loss: 0.3520384079451317 \t| Accuracy: 105.833\n",
      "# Iteration  8537 -> Loss: 0.35202766919544126 \t| Accuracy: 106.250\n",
      "# Iteration  8538 -> Loss: 0.3520169326361156 \t| Accuracy: 106.250\n",
      "# Iteration  8539 -> Loss: 0.35200619826626356 \t| Accuracy: 106.250\n",
      "# Iteration  8540 -> Loss: 0.35199546608499466 \t| Accuracy: 106.250\n",
      "# Iteration  8541 -> Loss: 0.3519847360914188 \t| Accuracy: 106.250\n",
      "# Iteration  8542 -> Loss: 0.3519740082846462 \t| Accuracy: 106.250\n",
      "# Iteration  8543 -> Loss: 0.3519632826637879 \t| Accuracy: 106.250\n",
      "# Iteration  8544 -> Loss: 0.3519525592279552 \t| Accuracy: 106.250\n",
      "# Iteration  8545 -> Loss: 0.35194183797625983 \t| Accuracy: 106.250\n",
      "# Iteration  8546 -> Loss: 0.35193111890781426 \t| Accuracy: 106.250\n",
      "# Iteration  8547 -> Loss: 0.35192040202173114 \t| Accuracy: 106.250\n",
      "# Iteration  8548 -> Loss: 0.3519096873171238 \t| Accuracy: 106.250\n",
      "# Iteration  8549 -> Loss: 0.3518989747931059 \t| Accuracy: 106.250\n",
      "# Iteration  8550 -> Loss: 0.3518882644487917 \t| Accuracy: 106.250\n",
      "# Iteration  8551 -> Loss: 0.3518775562832958 \t| Accuracy: 106.250\n",
      "# Iteration  8552 -> Loss: 0.3518668502957335 \t| Accuracy: 106.250\n",
      "# Iteration  8553 -> Loss: 0.3518561464852203 \t| Accuracy: 106.250\n",
      "# Iteration  8554 -> Loss: 0.35184544485087244 \t| Accuracy: 106.250\n",
      "# Iteration  8555 -> Loss: 0.3518347453918064 \t| Accuracy: 106.250\n",
      "# Iteration  8556 -> Loss: 0.35182404810713935 \t| Accuracy: 106.250\n",
      "# Iteration  8557 -> Loss: 0.35181335299598876 \t| Accuracy: 106.250\n",
      "# Iteration  8558 -> Loss: 0.35180266005747274 \t| Accuracy: 106.250\n",
      "# Iteration  8559 -> Loss: 0.35179196929070966 \t| Accuracy: 106.250\n",
      "# Iteration  8560 -> Loss: 0.3517812806948185 \t| Accuracy: 106.250\n",
      "# Iteration  8561 -> Loss: 0.3517705942689188 \t| Accuracy: 106.250\n",
      "# Iteration  8562 -> Loss: 0.3517599100121304 \t| Accuracy: 106.250\n",
      "# Iteration  8563 -> Loss: 0.35174922792357377 \t| Accuracy: 106.250\n",
      "# Iteration  8564 -> Loss: 0.35173854800236964 \t| Accuracy: 106.250\n",
      "# Iteration  8565 -> Loss: 0.3517278702476394 \t| Accuracy: 106.250\n",
      "# Iteration  8566 -> Loss: 0.35171719465850493 \t| Accuracy: 106.250\n",
      "# Iteration  8567 -> Loss: 0.35170652123408835 \t| Accuracy: 106.250\n",
      "# Iteration  8568 -> Loss: 0.3516958499735126 \t| Accuracy: 106.250\n",
      "# Iteration  8569 -> Loss: 0.35168518087590084 \t| Accuracy: 106.250\n",
      "# Iteration  8570 -> Loss: 0.3516745139403767 \t| Accuracy: 106.250\n",
      "# Iteration  8571 -> Loss: 0.3516638491660644 \t| Accuracy: 106.250\n",
      "# Iteration  8572 -> Loss: 0.3516531865520885 \t| Accuracy: 106.250\n",
      "# Iteration  8573 -> Loss: 0.35164252609757424 \t| Accuracy: 106.250\n",
      "# Iteration  8574 -> Loss: 0.3516318678016471 \t| Accuracy: 106.250\n",
      "# Iteration  8575 -> Loss: 0.3516212116634332 \t| Accuracy: 106.250\n",
      "# Iteration  8576 -> Loss: 0.3516105576820589 \t| Accuracy: 106.250\n",
      "# Iteration  8577 -> Loss: 0.3515999058566514 \t| Accuracy: 106.250\n",
      "# Iteration  8578 -> Loss: 0.3515892561863381 \t| Accuracy: 106.250\n",
      "# Iteration  8579 -> Loss: 0.3515786086702468 \t| Accuracy: 106.250\n",
      "# Iteration  8580 -> Loss: 0.351567963307506 \t| Accuracy: 106.250\n",
      "# Iteration  8581 -> Loss: 0.3515573200972446 \t| Accuracy: 106.250\n",
      "# Iteration  8582 -> Loss: 0.35154667903859194 \t| Accuracy: 106.250\n",
      "# Iteration  8583 -> Loss: 0.3515360401306777 \t| Accuracy: 106.250\n",
      "# Iteration  8584 -> Loss: 0.3515254033726321 \t| Accuracy: 106.250\n",
      "# Iteration  8585 -> Loss: 0.35151476876358606 \t| Accuracy: 106.250\n",
      "# Iteration  8586 -> Loss: 0.35150413630267074 \t| Accuracy: 106.250\n",
      "# Iteration  8587 -> Loss: 0.35149350598901774 \t| Accuracy: 106.250\n",
      "# Iteration  8588 -> Loss: 0.35148287782175913 \t| Accuracy: 106.250\n",
      "# Iteration  8589 -> Loss: 0.3514722518000277 \t| Accuracy: 106.250\n",
      "# Iteration  8590 -> Loss: 0.35146162792295654 \t| Accuracy: 106.250\n",
      "# Iteration  8591 -> Loss: 0.35145100618967895 \t| Accuracy: 106.250\n",
      "# Iteration  8592 -> Loss: 0.351440386599329 \t| Accuracy: 106.250\n",
      "# Iteration  8593 -> Loss: 0.35142976915104124 \t| Accuracy: 106.250\n",
      "# Iteration  8594 -> Loss: 0.35141915384395056 \t| Accuracy: 106.250\n",
      "# Iteration  8595 -> Loss: 0.35140854067719235 \t| Accuracy: 106.250\n",
      "# Iteration  8596 -> Loss: 0.3513979296499024 \t| Accuracy: 106.250\n",
      "# Iteration  8597 -> Loss: 0.3513873207612172 \t| Accuracy: 106.250\n",
      "# Iteration  8598 -> Loss: 0.35137671401027337 \t| Accuracy: 106.250\n",
      "# Iteration  8599 -> Loss: 0.35136610939620816 \t| Accuracy: 106.250\n",
      "# Iteration  8600 -> Loss: 0.3513555069181594 \t| Accuracy: 106.250\n",
      "# Iteration  8601 -> Loss: 0.3513449065752652 \t| Accuracy: 106.250\n",
      "# Iteration  8602 -> Loss: 0.3513343083666643 \t| Accuracy: 106.250\n",
      "# Iteration  8603 -> Loss: 0.3513237122914955 \t| Accuracy: 106.250\n",
      "# Iteration  8604 -> Loss: 0.3513131183488986 \t| Accuracy: 106.250\n",
      "# Iteration  8605 -> Loss: 0.35130252653801364 \t| Accuracy: 106.250\n",
      "# Iteration  8606 -> Loss: 0.351291936857981 \t| Accuracy: 106.250\n",
      "# Iteration  8607 -> Loss: 0.3512813493079417 \t| Accuracy: 106.250\n",
      "# Iteration  8608 -> Loss: 0.35127076388703704 \t| Accuracy: 106.250\n",
      "# Iteration  8609 -> Loss: 0.351260180594409 \t| Accuracy: 106.250\n",
      "# Iteration  8610 -> Loss: 0.3512495994291999 \t| Accuracy: 106.250\n",
      "# Iteration  8611 -> Loss: 0.35123902039055255 \t| Accuracy: 106.250\n",
      "# Iteration  8612 -> Loss: 0.35122844347760995 \t| Accuracy: 106.250\n",
      "# Iteration  8613 -> Loss: 0.3512178686895161 \t| Accuracy: 106.250\n",
      "# Iteration  8614 -> Loss: 0.35120729602541506 \t| Accuracy: 106.250\n",
      "# Iteration  8615 -> Loss: 0.35119672548445136 \t| Accuracy: 106.250\n",
      "# Iteration  8616 -> Loss: 0.3511861570657703 \t| Accuracy: 106.250\n",
      "# Iteration  8617 -> Loss: 0.35117559076851723 \t| Accuracy: 106.250\n",
      "# Iteration  8618 -> Loss: 0.3511650265918381 \t| Accuracy: 106.250\n",
      "# Iteration  8619 -> Loss: 0.35115446453487964 \t| Accuracy: 106.250\n",
      "# Iteration  8620 -> Loss: 0.3511439045967885 \t| Accuracy: 106.250\n",
      "# Iteration  8621 -> Loss: 0.35113334677671226 \t| Accuracy: 106.250\n",
      "# Iteration  8622 -> Loss: 0.3511227910737986 \t| Accuracy: 106.250\n",
      "# Iteration  8623 -> Loss: 0.35111223748719583 \t| Accuracy: 106.250\n",
      "# Iteration  8624 -> Loss: 0.35110168601605274 \t| Accuracy: 106.250\n",
      "# Iteration  8625 -> Loss: 0.3510911366595186 \t| Accuracy: 106.250\n",
      "# Iteration  8626 -> Loss: 0.3510805894167428 \t| Accuracy: 106.250\n",
      "# Iteration  8627 -> Loss: 0.35107004428687566 \t| Accuracy: 106.250\n",
      "# Iteration  8628 -> Loss: 0.35105950126906776 \t| Accuracy: 106.250\n",
      "# Iteration  8629 -> Loss: 0.3510489603624701 \t| Accuracy: 106.250\n",
      "# Iteration  8630 -> Loss: 0.35103842156623405 \t| Accuracy: 106.250\n",
      "# Iteration  8631 -> Loss: 0.3510278848795116 \t| Accuracy: 106.250\n",
      "# Iteration  8632 -> Loss: 0.35101735030145514 \t| Accuracy: 106.250\n",
      "# Iteration  8633 -> Loss: 0.3510068178312177 \t| Accuracy: 106.250\n",
      "# Iteration  8634 -> Loss: 0.3509962874679521 \t| Accuracy: 106.250\n",
      "# Iteration  8635 -> Loss: 0.35098575921081243 \t| Accuracy: 106.250\n",
      "# Iteration  8636 -> Loss: 0.350975233058953 \t| Accuracy: 106.250\n",
      "# Iteration  8637 -> Loss: 0.3509647090115281 \t| Accuracy: 106.250\n",
      "# Iteration  8638 -> Loss: 0.3509541870676932 \t| Accuracy: 106.250\n",
      "# Iteration  8639 -> Loss: 0.35094366722660364 \t| Accuracy: 106.250\n",
      "# Iteration  8640 -> Loss: 0.3509331494874154 \t| Accuracy: 106.250\n",
      "# Iteration  8641 -> Loss: 0.350922633849285 \t| Accuracy: 106.250\n",
      "# Iteration  8642 -> Loss: 0.35091212031136954 \t| Accuracy: 106.250\n",
      "# Iteration  8643 -> Loss: 0.35090160887282607 \t| Accuracy: 106.250\n",
      "# Iteration  8644 -> Loss: 0.3508910995328125 \t| Accuracy: 106.250\n",
      "# Iteration  8645 -> Loss: 0.3508805922904872 \t| Accuracy: 106.250\n",
      "# Iteration  8646 -> Loss: 0.35087008714500895 \t| Accuracy: 106.250\n",
      "# Iteration  8647 -> Loss: 0.3508595840955366 \t| Accuracy: 106.250\n",
      "# Iteration  8648 -> Loss: 0.35084908314123014 \t| Accuracy: 106.250\n",
      "# Iteration  8649 -> Loss: 0.3508385842812494 \t| Accuracy: 106.250\n",
      "# Iteration  8650 -> Loss: 0.3508280875147549 \t| Accuracy: 106.250\n",
      "# Iteration  8651 -> Loss: 0.35081759284090774 \t| Accuracy: 106.250\n",
      "# Iteration  8652 -> Loss: 0.3508071002588692 \t| Accuracy: 106.250\n",
      "# Iteration  8653 -> Loss: 0.35079660976780125 \t| Accuracy: 106.250\n",
      "# Iteration  8654 -> Loss: 0.35078612136686604 \t| Accuracy: 106.250\n",
      "# Iteration  8655 -> Loss: 0.3507756350552265 \t| Accuracy: 106.250\n",
      "# Iteration  8656 -> Loss: 0.35076515083204574 \t| Accuracy: 106.250\n",
      "# Iteration  8657 -> Loss: 0.35075466869648747 \t| Accuracy: 106.250\n",
      "# Iteration  8658 -> Loss: 0.3507441886477158 \t| Accuracy: 106.250\n",
      "# Iteration  8659 -> Loss: 0.3507337106848952 \t| Accuracy: 106.250\n",
      "# Iteration  8660 -> Loss: 0.3507232348071908 \t| Accuracy: 106.250\n",
      "# Iteration  8661 -> Loss: 0.35071276101376786 \t| Accuracy: 106.250\n",
      "# Iteration  8662 -> Loss: 0.3507022893037924 \t| Accuracy: 106.250\n",
      "# Iteration  8663 -> Loss: 0.35069181967643076 \t| Accuracy: 106.250\n",
      "# Iteration  8664 -> Loss: 0.35068135213084967 \t| Accuracy: 106.250\n",
      "# Iteration  8665 -> Loss: 0.35067088666621626 \t| Accuracy: 106.250\n",
      "# Iteration  8666 -> Loss: 0.3506604232816984 \t| Accuracy: 106.250\n",
      "# Iteration  8667 -> Loss: 0.35064996197646403 \t| Accuracy: 106.250\n",
      "# Iteration  8668 -> Loss: 0.35063950274968186 \t| Accuracy: 106.250\n",
      "# Iteration  8669 -> Loss: 0.3506290456005209 \t| Accuracy: 106.250\n",
      "# Iteration  8670 -> Loss: 0.35061859052815036 \t| Accuracy: 106.250\n",
      "# Iteration  8671 -> Loss: 0.35060813753174036 \t| Accuracy: 106.250\n",
      "# Iteration  8672 -> Loss: 0.3505976866104612 \t| Accuracy: 106.250\n",
      "# Iteration  8673 -> Loss: 0.3505872377634836 \t| Accuracy: 106.250\n",
      "# Iteration  8674 -> Loss: 0.3505767909899789 \t| Accuracy: 106.250\n",
      "# Iteration  8675 -> Loss: 0.35056634628911854 \t| Accuracy: 106.250\n",
      "# Iteration  8676 -> Loss: 0.3505559036600749 \t| Accuracy: 106.250\n",
      "# Iteration  8677 -> Loss: 0.3505454631020202 \t| Accuracy: 106.250\n",
      "# Iteration  8678 -> Loss: 0.35053502461412783 \t| Accuracy: 106.250\n",
      "# Iteration  8679 -> Loss: 0.35052458819557086 \t| Accuracy: 106.250\n",
      "# Iteration  8680 -> Loss: 0.3505141538455234 \t| Accuracy: 106.250\n",
      "# Iteration  8681 -> Loss: 0.3505037215631598 \t| Accuracy: 106.250\n",
      "# Iteration  8682 -> Loss: 0.35049329134765456 \t| Accuracy: 106.250\n",
      "# Iteration  8683 -> Loss: 0.35048286319818306 \t| Accuracy: 106.250\n",
      "# Iteration  8684 -> Loss: 0.3504724371139209 \t| Accuracy: 106.250\n",
      "# Iteration  8685 -> Loss: 0.35046201309404423 \t| Accuracy: 106.250\n",
      "# Iteration  8686 -> Loss: 0.3504515911377295 \t| Accuracy: 106.250\n",
      "# Iteration  8687 -> Loss: 0.3504411712441535 \t| Accuracy: 106.250\n",
      "# Iteration  8688 -> Loss: 0.35043075341249397 \t| Accuracy: 106.250\n",
      "# Iteration  8689 -> Loss: 0.3504203376419285 \t| Accuracy: 106.250\n",
      "# Iteration  8690 -> Loss: 0.35040992393163545 \t| Accuracy: 106.250\n",
      "# Iteration  8691 -> Loss: 0.3503995122807936 \t| Accuracy: 106.250\n",
      "# Iteration  8692 -> Loss: 0.350389102688582 \t| Accuracy: 106.250\n",
      "# Iteration  8693 -> Loss: 0.35037869515418024 \t| Accuracy: 106.250\n",
      "# Iteration  8694 -> Loss: 0.35036828967676836 \t| Accuracy: 106.250\n",
      "# Iteration  8695 -> Loss: 0.3503578862555268 \t| Accuracy: 106.250\n",
      "# Iteration  8696 -> Loss: 0.3503474848896365 \t| Accuracy: 106.250\n",
      "# Iteration  8697 -> Loss: 0.350337085578279 \t| Accuracy: 106.250\n",
      "# Iteration  8698 -> Loss: 0.3503266883206355 \t| Accuracy: 106.250\n",
      "# Iteration  8699 -> Loss: 0.3503162931158889 \t| Accuracy: 106.250\n",
      "# Iteration  8700 -> Loss: 0.35030589996322153 \t| Accuracy: 106.250\n",
      "# Iteration  8701 -> Loss: 0.3502955088618164 \t| Accuracy: 106.250\n",
      "# Iteration  8702 -> Loss: 0.35028511981085714 \t| Accuracy: 106.250\n",
      "# Iteration  8703 -> Loss: 0.35027473280952764 \t| Accuracy: 106.250\n",
      "# Iteration  8704 -> Loss: 0.3502643478570124 \t| Accuracy: 106.250\n",
      "# Iteration  8705 -> Loss: 0.3502539649524961 \t| Accuracy: 106.250\n",
      "# Iteration  8706 -> Loss: 0.35024358409516415 \t| Accuracy: 106.250\n",
      "# Iteration  8707 -> Loss: 0.3502332052842022 \t| Accuracy: 106.250\n",
      "# Iteration  8708 -> Loss: 0.3502228285187963 \t| Accuracy: 106.250\n",
      "# Iteration  8709 -> Loss: 0.3502124537981332 \t| Accuracy: 106.250\n",
      "# Iteration  8710 -> Loss: 0.3502020811213996 \t| Accuracy: 106.250\n",
      "# Iteration  8711 -> Loss: 0.35019171048778314 \t| Accuracy: 106.250\n",
      "# Iteration  8712 -> Loss: 0.35018134189647165 \t| Accuracy: 106.250\n",
      "# Iteration  8713 -> Loss: 0.35017097534665353 \t| Accuracy: 106.250\n",
      "# Iteration  8714 -> Loss: 0.3501606108375173 \t| Accuracy: 106.250\n",
      "# Iteration  8715 -> Loss: 0.3501502483682522 \t| Accuracy: 106.250\n",
      "# Iteration  8716 -> Loss: 0.3501398879380479 \t| Accuracy: 106.250\n",
      "# Iteration  8717 -> Loss: 0.3501295295460944 \t| Accuracy: 106.250\n",
      "# Iteration  8718 -> Loss: 0.3501191731915821 \t| Accuracy: 106.250\n",
      "# Iteration  8719 -> Loss: 0.35010881887370193 \t| Accuracy: 106.250\n",
      "# Iteration  8720 -> Loss: 0.3500984665916452 \t| Accuracy: 106.250\n",
      "# Iteration  8721 -> Loss: 0.3500881163446036 \t| Accuracy: 106.250\n",
      "# Iteration  8722 -> Loss: 0.35007776813176944 \t| Accuracy: 106.250\n",
      "# Iteration  8723 -> Loss: 0.3500674219523352 \t| Accuracy: 106.250\n",
      "# Iteration  8724 -> Loss: 0.3500570778054941 \t| Accuracy: 106.250\n",
      "# Iteration  8725 -> Loss: 0.3500467356904393 \t| Accuracy: 106.250\n",
      "# Iteration  8726 -> Loss: 0.3500363956063651 \t| Accuracy: 106.250\n",
      "# Iteration  8727 -> Loss: 0.3500260575524655 \t| Accuracy: 106.250\n",
      "# Iteration  8728 -> Loss: 0.3500157215279355 \t| Accuracy: 106.250\n",
      "# Iteration  8729 -> Loss: 0.3500053875319701 \t| Accuracy: 106.250\n",
      "# Iteration  8730 -> Loss: 0.349995055563765 \t| Accuracy: 106.250\n",
      "# Iteration  8731 -> Loss: 0.34998472562251615 \t| Accuracy: 106.250\n",
      "# Iteration  8732 -> Loss: 0.3499743977074202 \t| Accuracy: 106.250\n",
      "# Iteration  8733 -> Loss: 0.34996407181767397 \t| Accuracy: 106.250\n",
      "# Iteration  8734 -> Loss: 0.3499537479524747 \t| Accuracy: 106.250\n",
      "# Iteration  8735 -> Loss: 0.34994342611102036 \t| Accuracy: 106.250\n",
      "# Iteration  8736 -> Loss: 0.34993310629250896 \t| Accuracy: 106.250\n",
      "# Iteration  8737 -> Loss: 0.34992278849613906 \t| Accuracy: 106.250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Iteration  8738 -> Loss: 0.3499124727211098 \t| Accuracy: 106.250\n",
      "# Iteration  8739 -> Loss: 0.34990215896662064 \t| Accuracy: 106.250\n",
      "# Iteration  8740 -> Loss: 0.3498918472318715 \t| Accuracy: 106.250\n",
      "# Iteration  8741 -> Loss: 0.34988153751606255 \t| Accuracy: 106.250\n",
      "# Iteration  8742 -> Loss: 0.3498712298183947 \t| Accuracy: 106.250\n",
      "# Iteration  8743 -> Loss: 0.3498609241380689 \t| Accuracy: 106.250\n",
      "# Iteration  8744 -> Loss: 0.34985062047428706 \t| Accuracy: 106.250\n",
      "# Iteration  8745 -> Loss: 0.34984031882625094 \t| Accuracy: 106.250\n",
      "# Iteration  8746 -> Loss: 0.34983001919316303 \t| Accuracy: 106.250\n",
      "# Iteration  8747 -> Loss: 0.34981972157422614 \t| Accuracy: 106.250\n",
      "# Iteration  8748 -> Loss: 0.34980942596864373 \t| Accuracy: 106.250\n",
      "# Iteration  8749 -> Loss: 0.3497991323756195 \t| Accuracy: 106.250\n",
      "# Iteration  8750 -> Loss: 0.3497888407943573 \t| Accuracy: 106.250\n",
      "# Iteration  8751 -> Loss: 0.34977855122406193 \t| Accuracy: 106.250\n",
      "# Iteration  8752 -> Loss: 0.3497682636639383 \t| Accuracy: 106.250\n",
      "# Iteration  8753 -> Loss: 0.3497579781131919 \t| Accuracy: 106.250\n",
      "# Iteration  8754 -> Loss: 0.34974769457102844 \t| Accuracy: 106.250\n",
      "# Iteration  8755 -> Loss: 0.34973741303665434 \t| Accuracy: 106.250\n",
      "# Iteration  8756 -> Loss: 0.34972713350927603 \t| Accuracy: 106.250\n",
      "# Iteration  8757 -> Loss: 0.34971685598810076 \t| Accuracy: 106.250\n",
      "# Iteration  8758 -> Loss: 0.34970658047233594 \t| Accuracy: 106.250\n",
      "# Iteration  8759 -> Loss: 0.34969630696118964 \t| Accuracy: 106.250\n",
      "# Iteration  8760 -> Loss: 0.34968603545387 \t| Accuracy: 106.250\n",
      "# Iteration  8761 -> Loss: 0.34967576594958605 \t| Accuracy: 106.250\n",
      "# Iteration  8762 -> Loss: 0.34966549844754685 \t| Accuracy: 106.250\n",
      "# Iteration  8763 -> Loss: 0.349655232946962 \t| Accuracy: 106.250\n",
      "# Iteration  8764 -> Loss: 0.3496449694470416 \t| Accuracy: 106.250\n",
      "# Iteration  8765 -> Loss: 0.3496347079469962 \t| Accuracy: 106.250\n",
      "# Iteration  8766 -> Loss: 0.34962444844603635 \t| Accuracy: 106.250\n",
      "# Iteration  8767 -> Loss: 0.34961419094337365 \t| Accuracy: 106.250\n",
      "# Iteration  8768 -> Loss: 0.34960393543821966 \t| Accuracy: 106.250\n",
      "# Iteration  8769 -> Loss: 0.3495936819297866 \t| Accuracy: 106.250\n",
      "# Iteration  8770 -> Loss: 0.34958343041728696 \t| Accuracy: 106.250\n",
      "# Iteration  8771 -> Loss: 0.3495731808999337 \t| Accuracy: 106.250\n",
      "# Iteration  8772 -> Loss: 0.3495629333769403 \t| Accuracy: 106.250\n",
      "# Iteration  8773 -> Loss: 0.34955268784752047 \t| Accuracy: 106.250\n",
      "# Iteration  8774 -> Loss: 0.34954244431088854 \t| Accuracy: 106.250\n",
      "# Iteration  8775 -> Loss: 0.349532202766259 \t| Accuracy: 106.250\n",
      "# Iteration  8776 -> Loss: 0.349521963212847 \t| Accuracy: 106.250\n",
      "# Iteration  8777 -> Loss: 0.349511725649868 \t| Accuracy: 106.250\n",
      "# Iteration  8778 -> Loss: 0.34950149007653786 \t| Accuracy: 106.250\n",
      "# Iteration  8779 -> Loss: 0.3494912564920731 \t| Accuracy: 106.250\n",
      "# Iteration  8780 -> Loss: 0.3494810248956901 \t| Accuracy: 106.250\n",
      "# Iteration  8781 -> Loss: 0.349470795286606 \t| Accuracy: 106.250\n",
      "# Iteration  8782 -> Loss: 0.34946056766403877 \t| Accuracy: 106.250\n",
      "# Iteration  8783 -> Loss: 0.34945034202720593 \t| Accuracy: 106.250\n",
      "# Iteration  8784 -> Loss: 0.3494401183753261 \t| Accuracy: 106.250\n",
      "# Iteration  8785 -> Loss: 0.3494298967076182 \t| Accuracy: 106.250\n",
      "# Iteration  8786 -> Loss: 0.349419677023301 \t| Accuracy: 106.250\n",
      "# Iteration  8787 -> Loss: 0.34940945932159456 \t| Accuracy: 106.250\n",
      "# Iteration  8788 -> Loss: 0.3493992436017187 \t| Accuracy: 106.250\n",
      "# Iteration  8789 -> Loss: 0.34938902986289394 \t| Accuracy: 106.250\n",
      "# Iteration  8790 -> Loss: 0.34937881810434124 \t| Accuracy: 106.250\n",
      "# Iteration  8791 -> Loss: 0.34936860832528166 \t| Accuracy: 106.250\n",
      "# Iteration  8792 -> Loss: 0.3493584005249371 \t| Accuracy: 106.250\n",
      "# Iteration  8793 -> Loss: 0.34934819470252954 \t| Accuracy: 106.250\n",
      "# Iteration  8794 -> Loss: 0.3493379908572815 \t| Accuracy: 106.250\n",
      "# Iteration  8795 -> Loss: 0.349327788988416 \t| Accuracy: 106.250\n",
      "# Iteration  8796 -> Loss: 0.3493175890951565 \t| Accuracy: 106.250\n",
      "# Iteration  8797 -> Loss: 0.3493073911767263 \t| Accuracy: 106.250\n",
      "# Iteration  8798 -> Loss: 0.34929719523235014 \t| Accuracy: 106.250\n",
      "# Iteration  8799 -> Loss: 0.3492870012612521 \t| Accuracy: 106.250\n",
      "# Iteration  8800 -> Loss: 0.34927680926265753 \t| Accuracy: 106.250\n",
      "# Iteration  8801 -> Loss: 0.3492666192357917 \t| Accuracy: 106.250\n",
      "# Iteration  8802 -> Loss: 0.34925643117988053 \t| Accuracy: 106.250\n",
      "# Iteration  8803 -> Loss: 0.34924624509415 \t| Accuracy: 106.250\n",
      "# Iteration  8804 -> Loss: 0.3492360609778269 \t| Accuracy: 106.250\n",
      "# Iteration  8805 -> Loss: 0.34922587883013834 \t| Accuracy: 106.250\n",
      "# Iteration  8806 -> Loss: 0.34921569865031155 \t| Accuracy: 106.250\n",
      "# Iteration  8807 -> Loss: 0.3492055204375746 \t| Accuracy: 106.250\n",
      "# Iteration  8808 -> Loss: 0.3491953441911556 \t| Accuracy: 106.250\n",
      "# Iteration  8809 -> Loss: 0.3491851699102835 \t| Accuracy: 106.250\n",
      "# Iteration  8810 -> Loss: 0.3491749975941872 \t| Accuracy: 106.250\n",
      "# Iteration  8811 -> Loss: 0.349164827242096 \t| Accuracy: 106.250\n",
      "# Iteration  8812 -> Loss: 0.34915465885324015 \t| Accuracy: 106.250\n",
      "# Iteration  8813 -> Loss: 0.34914449242684986 \t| Accuracy: 106.250\n",
      "# Iteration  8814 -> Loss: 0.34913432796215577 \t| Accuracy: 106.250\n",
      "# Iteration  8815 -> Loss: 0.34912416545838904 \t| Accuracy: 106.250\n",
      "# Iteration  8816 -> Loss: 0.3491140049147813 \t| Accuracy: 106.250\n",
      "# Iteration  8817 -> Loss: 0.34910384633056424 \t| Accuracy: 106.250\n",
      "# Iteration  8818 -> Loss: 0.34909368970497046 \t| Accuracy: 106.250\n",
      "# Iteration  8819 -> Loss: 0.3490835350372327 \t| Accuracy: 106.250\n",
      "# Iteration  8820 -> Loss: 0.3490733823265841 \t| Accuracy: 106.250\n",
      "# Iteration  8821 -> Loss: 0.3490632315722581 \t| Accuracy: 106.250\n",
      "# Iteration  8822 -> Loss: 0.3490530827734888 \t| Accuracy: 106.250\n",
      "# Iteration  8823 -> Loss: 0.3490429359295105 \t| Accuracy: 106.250\n",
      "# Iteration  8824 -> Loss: 0.34903279103955803 \t| Accuracy: 106.250\n",
      "# Iteration  8825 -> Loss: 0.3490226481028666 \t| Accuracy: 106.250\n",
      "# Iteration  8826 -> Loss: 0.3490125071186719 \t| Accuracy: 106.250\n",
      "# Iteration  8827 -> Loss: 0.3490023680862097 \t| Accuracy: 106.250\n",
      "# Iteration  8828 -> Loss: 0.34899223100471655 \t| Accuracy: 106.250\n",
      "# Iteration  8829 -> Loss: 0.3489820958734292 \t| Accuracy: 106.250\n",
      "# Iteration  8830 -> Loss: 0.3489719626915849 \t| Accuracy: 106.250\n",
      "# Iteration  8831 -> Loss: 0.34896183145842125 \t| Accuracy: 106.250\n",
      "# Iteration  8832 -> Loss: 0.34895170217317634 \t| Accuracy: 106.250\n",
      "# Iteration  8833 -> Loss: 0.34894157483508853 \t| Accuracy: 106.250\n",
      "# Iteration  8834 -> Loss: 0.34893144944339655 \t| Accuracy: 106.250\n",
      "# Iteration  8835 -> Loss: 0.3489213259973398 \t| Accuracy: 106.250\n",
      "# Iteration  8836 -> Loss: 0.3489112044961578 \t| Accuracy: 106.250\n",
      "# Iteration  8837 -> Loss: 0.34890108493909067 \t| Accuracy: 106.250\n",
      "# Iteration  8838 -> Loss: 0.3488909673253788 \t| Accuracy: 106.250\n",
      "# Iteration  8839 -> Loss: 0.3488808516542629 \t| Accuracy: 106.250\n",
      "# Iteration  8840 -> Loss: 0.3488707379249844 \t| Accuracy: 106.250\n",
      "# Iteration  8841 -> Loss: 0.3488606261367849 \t| Accuracy: 106.250\n",
      "# Iteration  8842 -> Loss: 0.34885051628890634 \t| Accuracy: 106.250\n",
      "# Iteration  8843 -> Loss: 0.3488404083805913 \t| Accuracy: 106.250\n",
      "# Iteration  8844 -> Loss: 0.3488303024110825 \t| Accuracy: 106.250\n",
      "# Iteration  8845 -> Loss: 0.34882019837962325 \t| Accuracy: 106.250\n",
      "# Iteration  8846 -> Loss: 0.3488100962854572 \t| Accuracy: 106.250\n",
      "# Iteration  8847 -> Loss: 0.3487999961278283 \t| Accuracy: 106.250\n",
      "# Iteration  8848 -> Loss: 0.34878989790598114 \t| Accuracy: 106.250\n",
      "# Iteration  8849 -> Loss: 0.3487798016191605 \t| Accuracy: 106.250\n",
      "# Iteration  8850 -> Loss: 0.3487697072666115 \t| Accuracy: 106.250\n",
      "# Iteration  8851 -> Loss: 0.34875961484757995 \t| Accuracy: 106.250\n",
      "# Iteration  8852 -> Loss: 0.3487495243613118 \t| Accuracy: 106.250\n",
      "# Iteration  8853 -> Loss: 0.3487394358070535 \t| Accuracy: 106.250\n",
      "# Iteration  8854 -> Loss: 0.3487293491840519 \t| Accuracy: 106.250\n",
      "# Iteration  8855 -> Loss: 0.34871926449155427 \t| Accuracy: 106.250\n",
      "# Iteration  8856 -> Loss: 0.3487091817288082 \t| Accuracy: 106.250\n",
      "# Iteration  8857 -> Loss: 0.3486991008950617 \t| Accuracy: 106.250\n",
      "# Iteration  8858 -> Loss: 0.3486890219895632 \t| Accuracy: 106.250\n",
      "# Iteration  8859 -> Loss: 0.34867894501156144 \t| Accuracy: 106.250\n",
      "# Iteration  8860 -> Loss: 0.3486688699603059 \t| Accuracy: 106.250\n",
      "# Iteration  8861 -> Loss: 0.3486587968350458 \t| Accuracy: 106.250\n",
      "# Iteration  8862 -> Loss: 0.3486487256350317 \t| Accuracy: 106.250\n",
      "# Iteration  8863 -> Loss: 0.3486386563595135 \t| Accuracy: 106.250\n",
      "# Iteration  8864 -> Loss: 0.3486285890077422 \t| Accuracy: 106.250\n",
      "# Iteration  8865 -> Loss: 0.34861852357896894 \t| Accuracy: 106.250\n",
      "# Iteration  8866 -> Loss: 0.3486084600724455 \t| Accuracy: 106.250\n",
      "# Iteration  8867 -> Loss: 0.3485983984874236 \t| Accuracy: 106.250\n",
      "# Iteration  8868 -> Loss: 0.3485883388231559 \t| Accuracy: 106.250\n",
      "# Iteration  8869 -> Loss: 0.348578281078895 \t| Accuracy: 106.250\n",
      "# Iteration  8870 -> Loss: 0.3485682252538941 \t| Accuracy: 106.250\n",
      "# Iteration  8871 -> Loss: 0.34855817134740674 \t| Accuracy: 106.250\n",
      "# Iteration  8872 -> Loss: 0.3485481193586871 \t| Accuracy: 106.250\n",
      "# Iteration  8873 -> Loss: 0.3485380692869892 \t| Accuracy: 106.250\n",
      "# Iteration  8874 -> Loss: 0.348528021131568 \t| Accuracy: 106.250\n",
      "# Iteration  8875 -> Loss: 0.3485179748916787 \t| Accuracy: 106.250\n",
      "# Iteration  8876 -> Loss: 0.3485079305665766 \t| Accuracy: 106.250\n",
      "# Iteration  8877 -> Loss: 0.3484978881555179 \t| Accuracy: 106.250\n",
      "# Iteration  8878 -> Loss: 0.3484878476577587 \t| Accuracy: 106.250\n",
      "# Iteration  8879 -> Loss: 0.3484778090725558 \t| Accuracy: 106.250\n",
      "# Iteration  8880 -> Loss: 0.3484677723991664 \t| Accuracy: 106.250\n",
      "# Iteration  8881 -> Loss: 0.34845773763684784 \t| Accuracy: 106.250\n",
      "# Iteration  8882 -> Loss: 0.3484477047848581 \t| Accuracy: 106.250\n",
      "# Iteration  8883 -> Loss: 0.3484376738424555 \t| Accuracy: 106.250\n",
      "# Iteration  8884 -> Loss: 0.34842764480889865 \t| Accuracy: 106.250\n",
      "# Iteration  8885 -> Loss: 0.34841761768344653 \t| Accuracy: 106.250\n",
      "# Iteration  8886 -> Loss: 0.3484075924653588 \t| Accuracy: 106.250\n",
      "# Iteration  8887 -> Loss: 0.34839756915389514 \t| Accuracy: 106.250\n",
      "# Iteration  8888 -> Loss: 0.3483875477483159 \t| Accuracy: 106.250\n",
      "# Iteration  8889 -> Loss: 0.34837752824788154 \t| Accuracy: 106.250\n",
      "# Iteration  8890 -> Loss: 0.34836751065185334 \t| Accuracy: 106.250\n",
      "# Iteration  8891 -> Loss: 0.3483574949594924 \t| Accuracy: 106.250\n",
      "# Iteration  8892 -> Loss: 0.34834748117006076 \t| Accuracy: 106.250\n",
      "# Iteration  8893 -> Loss: 0.34833746928282044 \t| Accuracy: 106.250\n",
      "# Iteration  8894 -> Loss: 0.3483274592970342 \t| Accuracy: 106.250\n",
      "# Iteration  8895 -> Loss: 0.3483174512119648 \t| Accuracy: 106.250\n",
      "# Iteration  8896 -> Loss: 0.3483074450268757 \t| Accuracy: 106.250\n",
      "# Iteration  8897 -> Loss: 0.34829744074103064 \t| Accuracy: 106.250\n",
      "# Iteration  8898 -> Loss: 0.34828743835369363 \t| Accuracy: 106.250\n",
      "# Iteration  8899 -> Loss: 0.3482774378641293 \t| Accuracy: 106.250\n",
      "# Iteration  8900 -> Loss: 0.34826743927160253 \t| Accuracy: 106.250\n",
      "\n",
      "!!! Convergence reached !!!\n",
      "# Iteration 8899 #\n",
      "Cross-Entropy Loss:      0.34826743927160253\n",
      "Accuracy (Training Set): 106.250%\n",
      "Weights\n",
      "S -> H:\n",
      " [[-0.42162271 -4.8315122   4.86336538  2.75767799 -3.84853384  0.34793311\n",
      "   6.3843904  -5.96098794 -2.21497504  3.08512446]\n",
      " [-4.03172861 -0.15899844 -1.52256232 -1.93761044 -1.93797296 -0.13792446\n",
      "   0.19953485  1.50979697  1.84994719  1.47920725]\n",
      " [ 1.47959167  1.18550557 -1.46809122  0.43637036 -1.4269925  -3.82618393\n",
      "  -1.59092377 -0.26656353  1.24740843 -0.96658015]] \n",
      "H -> O:\n",
      " [[ 0.20719207 -0.25240827 -2.02696276 -1.29429201]\n",
      " [ 2.64316782 -3.82893282  3.57389041 -3.30993211]\n",
      " [-3.03909471  3.99447684 -1.00667319 -5.03681678]\n",
      " [-1.61002904  7.05367655 -6.52794856 -2.38124965]\n",
      " [ 0.18911036 -6.30082628  1.89929226  2.44855799]\n",
      " [-1.64420049 -0.72054005 -3.83089922  6.91000025]\n",
      " [-7.24065088  2.10348405  8.15725968 -1.30358395]\n",
      " [ 3.48000067 -6.45813159 -2.21086742  1.39289419]\n",
      " [ 4.89582182 -1.95877964  2.24855288 -6.97401321]\n",
      " [-5.02672418  0.96933616  3.05293882  1.34310975]\n",
      " [ 0.05981627  1.62299445 -5.30545358  1.09855182]]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Creates the Neural Network with 2 Input Neurons, 10 Hidden Neurons and 4 Output Neuron\n",
    "# (The weights are randomnly initiated)\n",
    "brain = MultilayerPerceptron(n_neurons=[2,10,4])\n",
    "\n",
    "# Run the Gradient Descent and returns the Error History for the training.\n",
    "errorHist = brain.train(X_train, y_train, alpha=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then can classify the entire training data space in order to see the pattern that our Neural Network learned:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tl.plotClassContour(X_train, y_train, brain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
