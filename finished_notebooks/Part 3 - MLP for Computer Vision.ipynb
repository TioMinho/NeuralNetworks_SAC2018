{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks - A Practical Introduction\n",
    "by _Minho Menezes_  \n",
    "\n",
    "---\n",
    "\n",
    "## Multilayer Perceptron for Computer Vision\n",
    "\n",
    "In this third and last notebook, we will get an experience about how to work with image data and use our Neural models for Computer Vision tasks. This will give you a brief introduction in one of the fields where Neural Networks are most widely used, and will show you that this Multilayer Perceptron class that we built really works. \n",
    "\n",
    "* [1. Image Data Extraction](#1.-Image-Data-Extraction)  \n",
    "* [2. Loading the MNIST Dataset](#2.-Loading-the-MNIST-Dataset)  \n",
    "* [3. Training the Network](#3.-Training-the-Network)  \n",
    "* [4. Testing the Network](#4.-Testing-the-Network)  \n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "### Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## LIBRARIES ##\n",
    "import numpy as np                         # Library for Numerical and Matricial Operations\n",
    "import matplotlib.pyplot as plt            # Library for Generating Visualizations\n",
    "import pandas as pd                        # Library for Handling Datasets\n",
    "from tools import Tools as tl              # Library for some Utilitary Tools\n",
    "from glob import glob                      # Library for File Searching\n",
    "\n",
    "# Function for loading the MNIST dataset into a Numpy Matrix\n",
    "import pickle\n",
    "\n",
    "def loadMNIST():\n",
    "    with open(\"../data/mnist/mnist.pkl\",'rb') as f:\n",
    "        mnist = pickle.load(f)\n",
    "    return mnist[\"training_images\"].T, mnist[\"test_images\"].T, mnist[\"training_labels\"].T, mnist[\"test_labels\"].T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Neural Networks Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "## CLASS: Multilayer Perceptron ##\n",
    "class MultilayerPerceptron:\n",
    "    \n",
    "    # CLASS CONSTRUCTOR\n",
    "    def __init__(self, n_neurons=[2, 5, 1]):\n",
    "        if(len(n_neurons) < 2):\n",
    "            raise ValueError(\"The network must have at least two layers! (The input and the output layers)\")\n",
    "        \n",
    "        # Network Architecture\n",
    "        self.hidden_layers = len(n_neurons)-2\n",
    "        self.n_neurons = n_neurons\n",
    "        self.W = []\n",
    "        \n",
    "        # Adjusting the Network architecture\n",
    "        for i in range(1, len(n_neurons)):\n",
    "            self.W.append( np.random.randn(self.n_neurons[i-1]+1 , self.n_neurons[i]) )\n",
    "        \n",
    "    # ACTIVATION FUNCTION\n",
    "    def activate(self,Z):\n",
    "        return 1 / (1 + np.exp(-Z))\n",
    "    \n",
    "    # FORWARD PROPAGATION\n",
    "    def forward(self, X):\n",
    "        # Activation List\n",
    "        A = []\n",
    "        \n",
    "        # Input Layer Activation\n",
    "        A.append( np.vstack([np.ones([1, X.shape[1]]), X]) )\n",
    "        \n",
    "        # Hidden Layer Activation\n",
    "        for i in range(0, self.hidden_layers):\n",
    "            Z = np.matmul(self.W[i].T, A[-1])\n",
    "            Z = self.activate(Z)\n",
    "            \n",
    "            A.append( np.vstack([np.ones([1, Z.shape[1]]), Z]) )\n",
    "        \n",
    "        # Output Layer Activation\n",
    "        Z = np.matmul(self.W[-1].T, A[-1])\n",
    "        Z = self.activate(Z)\n",
    "\n",
    "        A.append(Z)\n",
    "        \n",
    "        return A\n",
    "    \n",
    "    # CLASSIFICATION PREDICTION\n",
    "    def predict(self, X):\n",
    "        A = self.forward(X)\n",
    "        \n",
    "        if(self.n_neurons[-1] > 1):\n",
    "            return A[-1].argmax(axis=0)\n",
    "        else:\n",
    "            return (A[-1] > 0.5).astype(int)\n",
    "    \n",
    "    # LOSS FUNCTION\n",
    "    def loss(self, y, y_hat):\n",
    "        m = y.shape[1]\n",
    "        return -(1/m) * np.sum(y * np.log(y_hat) + (1-y) * np.log(1 - y_hat))\n",
    "    \n",
    "    # ACCURACY FUNCTION\n",
    "    def accuracy(self, y, y_hat):\n",
    "        m = y.shape[1]\n",
    "        if(y.shape[0] > 1):\n",
    "            y = y.argmax(axis=0)\n",
    "        \n",
    "        return (1/m) * np.sum(y == y_hat) * 100\n",
    "    \n",
    "    # BACKPROPAGATION\n",
    "    def backpropagate(self, A, y):\n",
    "        # Calculates the error in the Output Layer (difference between the real and the predicted)\n",
    "        E = []\n",
    "        E.append( A[-1] - y )\n",
    "\n",
    "        # Backpropagates the error to all the Hidden Layers\n",
    "        for i in range(self.hidden_layers, 0, -1):\n",
    "            E.append( np.matmul(self.W[i], E[-1]) * A[i] * (1-A[i]) )\n",
    "            E[-1] = E[-1][1:,:]\n",
    "\n",
    "        # Returns the list of Error Matrices\n",
    "        return E[::-1]\n",
    "    \n",
    "    # GRADIENT DESCENT TRAINING\n",
    "    def train(self, X_train, y_train, alpha=1e-3, maxIt=50000, tol=1e-5, verbose=True):\n",
    "        # Returns the total number of samples in the training\n",
    "        m = X_train.shape[1]\n",
    "\n",
    "        # Defines the Error History and other auxiliary variables\n",
    "        errorHist = []\n",
    "\n",
    "        # Gradient Descent Loop\n",
    "        for it in range(0, maxIt):\n",
    "            # 1. Calculates all the activations and prediction (Forward Propagation) and \n",
    "            #    the errors (Backpropagation), using the current weights self.WË†(i)\n",
    "            A = self.forward(X_train)\n",
    "            E = self.backpropagate(A, y_train)\n",
    "            P = self.predict(X_train)\n",
    "\n",
    "            # 2. Calculates the Evaluation Metrics\n",
    "            actualLoss = self.loss(y_train, A[-1])\n",
    "            actualAcc = self.accuracy(y_train, P)\n",
    "            errorHist.append(actualLoss)\n",
    "\n",
    "            # 3. Updates the Neural Networks weights\n",
    "            for i in range(0, self.hidden_layers+1):\n",
    "                self.W[i] = self.W[i] - (alpha/m) * np.matmul(A[i], E[i].T)\n",
    "\n",
    "            # 4. Prints the training results\n",
    "            if(verbose): \n",
    "                print(\"# Iteration {0:5} -> Loss: {1:} \\t| Accuracy: {2:.3f}\".format(it+1, actualLoss, actualAcc))\n",
    "\n",
    "            # 5. Check for convergence and prints the final result for the training.\n",
    "            if(it > 1 and abs(errorHist[-1] - errorHist[-2]) <= tol):\n",
    "                print(\"\\n!!! Convergence reached !!!\")\n",
    "                print(\"# Iteration\", it, \"#\")\n",
    "                print(\"Cross-Entropy Loss:      {}\".format(actualLoss))\n",
    "                print(\"Accuracy (Training Set): {0:.3f}%\".format(actualAcc))\n",
    "                print(\"Weights\\nS -> H:\\n\", self.W[0], \"\\nH -> O:\\n\", self.W[1])\n",
    "                print(\"\\n\")\n",
    "                break;\n",
    "\n",
    "        # End of the Training\n",
    "        return errorHist\n",
    "        \n",
    "## ---------------------------- ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 1. Image Data Extraction\n",
    "\n",
    "When working with Computer Vision, the first thing you need, of course, is to be able to open image data and format it to some shape that your Neural Network can understand. This is a step in this field known as **Feature Extraction**. Consider the case of the following image:\n",
    "\n",
    "<img src=\"../data/mnist/mnist1.png\" alt=\"binary multilayer perceptron\" width=\"100px\"/>\n",
    "\n",
    "There are, mainly, two ways that we can feed this image to a Neural Network:\n",
    "\n",
    "1. We can extract, in advance, some informations that descripts the image (mean of colors, borders positions, center of mass, etc.) and build a vector comprising these descriptions. This vector is then used as a Input Layer to the Neural Network.\n",
    "\n",
    "2. The entire image (28x28 pixels, in this case) is arranged into a vector shape. This vector (that will have dimension 784x1) is then used as a Input Layer to the Neural Network.\n",
    "\n",
    "Of course, in the first case it is possible that we form a set of features that is much smaller than 784 (in the second case), hence performing faster. However, this process can be time-consuming and maybe some information will still be missing. The second case requires more memory and processing power, but usually yields a better performance. We will stick to the second approach, now, because this are actually one of the reasons that Neural Networks are popular right now.\n",
    "\n",
    "**To build your own dataset from image data, first use the function _plt.imread()_ to capture the matrices of a image and the function _plt.imshow()_ to visualize it:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Open an image and returns the pixel matrices\n",
    "X = plt.imread(\"../data/mnist/mnist1.png\")\n",
    "\n",
    "# Selects only the pixel intensity for one of the RGBA channels\n",
    "X = X[:,:,0]\n",
    "\n",
    "# Exhibits the image loaded in such matrix\n",
    "plt.figure()\n",
    "plt.imshow(X)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To flatten the entire image to an array of specific dimensions, we use the method _.reshape()_ from the Numpy matrices. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensions of X: (784, 1)\n"
     ]
    }
   ],
   "source": [
    "# Flatten the image to a column vector\n",
    "X = X.reshape(-1, 1)\n",
    "\n",
    "print(\"Dimensions of X:\", X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is only one image. In order to built an entire dataset, we need to iteratively load and reshape all the images that we have in our database:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset:\n",
      " [[0.26666668 0.26666668 0.26666668 ... 0.26666668 0.26666668 0.26666668]\n",
      " [0.26666668 0.26666668 0.26666668 ... 0.26666668 0.26666668 0.26666668]\n",
      " [0.26666668 0.26666668 0.26666668 ... 0.26666668 0.26666668 0.26666668]\n",
      " ...\n",
      " [0.26666668 0.26666668 0.26666668 ... 0.26666668 0.26666668 0.26666668]\n",
      " [0.26666668 0.26666668 0.26666668 ... 0.26666668 0.26666668 0.26666668]\n",
      " [0.26666668 0.26666668 0.26666668 ... 0.26666668 0.26666668 0.26666668]]\n",
      "\n",
      "Dimensions: (784, 9)\n"
     ]
    }
   ],
   "source": [
    "# Returns the name of all PNG files in the MNIST folder\n",
    "filenames = glob(\"../data/mnist/*.png\")\n",
    "\n",
    "# Creates the dataset by horizontally stacking the individual images\n",
    "X = np.zeros([28*28, len(filenames)])\n",
    "for i,f in enumerate(filenames):\n",
    "    img = plt.imread(f)\n",
    "    img = img[:,:,0]\n",
    "    X[:,i] = img.reshape(1,-1)\n",
    "    \n",
    "# Prints part of the dataset\n",
    "print(\"Dataset:\\n\", X)\n",
    "print(\"\\nDimensions:\", X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we need to indicate the real class of each image. This work is usually done manually to label each image by actually looking at it.\n",
    "\n",
    "In this case, we first visualize each example of our dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 9 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Exhibits the images in the dataset\n",
    "plt.figure()\n",
    "for i in range(0,3):\n",
    "    for j in range(0,3):\n",
    "        plt.subplot(3,3,j + 3*i +1)\n",
    "        plt.imshow(X[:,j + 3*i].reshape(28,28))\n",
    "        plt.axis('off')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we manually create the labels vector $Y$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defines the label classes:\n",
    "y = np.array([\"0\", \"4\", \"1\", \"9\", \"2\", \"1\", \"3\", \"1\", \"4\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Loading the MNIST Dataset\n",
    "\n",
    "You could be worried if a dataset with only 9 samples is enough to train a Neural Network. You are right.\n",
    "\n",
    "The number of samples needed to train a Neural Network varies from the problem and the architecture of the network. The intention is to get as much data as possible, but they also must have great variance to expose the network to several possible cases. In the Computer Vision case, however, the number of input is too great, and the Networks tends to be proportionally as big. In this case we need very big datasets.\n",
    "\n",
    "The **[MNIST dataset](http://yann.lecun.com/exdb/mnist/)** is a very popular dataset of 70,000 images of handwritten digits that are used as benchmark data for several machine learning models. Actually the images that you used before are samples of this dataset.\n",
    "\n",
    "**We will then, work with this data:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loads the entire MNIST dataset\n",
    "X_train, X_test, y_train, y_test = loadMNIST()\n",
    "\n",
    "# Creating the One-Hot Encoding labels\n",
    "y_train_ohe = np.zeros([10, y_train.shape[0]])\n",
    "for i in range(0, y_train.shape[0]):\n",
    "    y_train_ohe[y_train[i], i] = 1\n",
    "\n",
    "y_test_ohe = np.zeros([10, y_test.shape[0]])\n",
    "for i in range(0, y_test.shape[0]):\n",
    "    y_test_ohe[y_test[i], i] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also visualize samples from this dataset by using the _.reshape()_ and the _plt.imshow()_ functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels:\n",
      "\n",
      "\t7\t6\t2\t5\t0\t\n",
      "\t4\t0\t3\t2\t5\t\n",
      "\t9\t2\t4\t5\t9\t\n",
      "\n",
      "Images:\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 15 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Exhibits the images in the dataset\n",
    "print(\"Labels:\")\n",
    "\n",
    "plt.figure()\n",
    "for i in range(0,3):\n",
    "    print(end=\"\\n\\t\")\n",
    "    for j in range(0,5):\n",
    "        plt.subplot(3,5,j + 5*i +1)\n",
    "        \n",
    "        # Selects a random example from the dataset\n",
    "        idx = np.random.randint(0, 60000)\n",
    "        print(y_train[idx], end=\"\\t\")\n",
    "        plt.imshow(X_train[:,idx].reshape(28,28))\n",
    "        \n",
    "        plt.axis('off')\n",
    "\n",
    "\n",
    "print(\"\\n\\nImages:\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Training the Network\n",
    "\n",
    "Now, we have a entire dataset and we already built our Neural Network class. We are ready to train a network to work with this data.\n",
    "\n",
    "A quick reminder: the Network will have _748_ Input Labels, a arbitrary number _N_ of Random Neurons and _10_ Output Neurons. This means that we will have _748xN + N*10_ weights to update! Furthermore, we have 60000 training examples. So, you can imagine that this training will be a lot slower (and harder!) than those of the previous notebooks.\n",
    "\n",
    "But fear not! The network can still learn something if we choose a significantly higher number of Hidden Neurons; it will work, but will take a lot of time. Use a small number of Hidden Neurons in order to visualize the training, and (if you computer is fast enough) you can use more neurons later.\n",
    "\n",
    "**The training itself should be equal to the previous cases, just remember to match the dimensions:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Iteration     1 -> Loss: 9.26188408371542 \t| Accuracy: 13.947\n",
      "# Iteration     2 -> Loss: 8.44249905565116 \t| Accuracy: 13.535\n",
      "# Iteration     3 -> Loss: 7.744467943322247 \t| Accuracy: 13.362\n",
      "# Iteration     4 -> Loss: 7.172812125934414 \t| Accuracy: 12.563\n",
      "# Iteration     5 -> Loss: 6.683869920854961 \t| Accuracy: 12.613\n",
      "# Iteration     6 -> Loss: 6.274769190549216 \t| Accuracy: 12.615\n",
      "# Iteration     7 -> Loss: 5.933065060184531 \t| Accuracy: 12.555\n",
      "# Iteration     8 -> Loss: 5.6470850013137435 \t| Accuracy: 12.657\n",
      "# Iteration     9 -> Loss: 5.408483130371432 \t| Accuracy: 12.675\n",
      "# Iteration    10 -> Loss: 5.203568716650019 \t| Accuracy: 12.640\n",
      "# Iteration    11 -> Loss: 5.029578821559296 \t| Accuracy: 12.052\n",
      "# Iteration    12 -> Loss: 4.88539063895556 \t| Accuracy: 12.047\n",
      "# Iteration    13 -> Loss: 4.759755322327465 \t| Accuracy: 12.110\n",
      "# Iteration    14 -> Loss: 4.647163250742028 \t| Accuracy: 12.152\n",
      "# Iteration    15 -> Loss: 4.548439144660528 \t| Accuracy: 12.172\n",
      "# Iteration    16 -> Loss: 4.461448735554254 \t| Accuracy: 12.192\n",
      "# Iteration    17 -> Loss: 4.385515096859018 \t| Accuracy: 12.193\n",
      "# Iteration    18 -> Loss: 4.317100796174055 \t| Accuracy: 12.998\n",
      "# Iteration    19 -> Loss: 4.256507178783222 \t| Accuracy: 12.952\n",
      "# Iteration    20 -> Loss: 4.202458328733353 \t| Accuracy: 12.927\n",
      "# Iteration    21 -> Loss: 4.154369329391909 \t| Accuracy: 12.948\n",
      "# Iteration    22 -> Loss: 4.111827555478646 \t| Accuracy: 12.958\n",
      "# Iteration    23 -> Loss: 4.074211625076135 \t| Accuracy: 13.100\n",
      "# Iteration    24 -> Loss: 4.037987269937743 \t| Accuracy: 13.135\n",
      "# Iteration    25 -> Loss: 4.004937794521101 \t| Accuracy: 13.270\n",
      "# Iteration    26 -> Loss: 3.975943384959828 \t| Accuracy: 13.725\n",
      "# Iteration    27 -> Loss: 3.9477726746733994 \t| Accuracy: 13.782\n",
      "# Iteration    28 -> Loss: 3.9189856195251798 \t| Accuracy: 13.857\n",
      "# Iteration    29 -> Loss: 3.8937439209602265 \t| Accuracy: 13.875\n",
      "# Iteration    30 -> Loss: 3.86920359626086 \t| Accuracy: 14.705\n",
      "# Iteration    31 -> Loss: 3.8464313144534157 \t| Accuracy: 14.858\n",
      "# Iteration    32 -> Loss: 3.825454158854803 \t| Accuracy: 14.875\n",
      "# Iteration    33 -> Loss: 3.80529396586834 \t| Accuracy: 14.920\n",
      "# Iteration    34 -> Loss: 3.7860241981899123 \t| Accuracy: 14.930\n",
      "# Iteration    35 -> Loss: 3.7670331509922237 \t| Accuracy: 14.930\n",
      "# Iteration    36 -> Loss: 3.750392775319979 \t| Accuracy: 15.112\n",
      "# Iteration    37 -> Loss: 3.733457319969492 \t| Accuracy: 15.127\n",
      "# Iteration    38 -> Loss: 3.718500502760677 \t| Accuracy: 15.128\n",
      "# Iteration    39 -> Loss: 3.7039299871880913 \t| Accuracy: 15.283\n",
      "# Iteration    40 -> Loss: 3.689479883977623 \t| Accuracy: 15.380\n",
      "# Iteration    41 -> Loss: 3.6750978744807554 \t| Accuracy: 15.530\n",
      "# Iteration    42 -> Loss: 3.6621899953734154 \t| Accuracy: 15.533\n",
      "# Iteration    43 -> Loss: 3.650114917491286 \t| Accuracy: 15.582\n",
      "# Iteration    44 -> Loss: 3.6387690941804496 \t| Accuracy: 15.595\n",
      "# Iteration    45 -> Loss: 3.6279113134168584 \t| Accuracy: 15.645\n",
      "# Iteration    46 -> Loss: 3.6171775900074365 \t| Accuracy: 15.988\n",
      "# Iteration    47 -> Loss: 3.6062348484887408 \t| Accuracy: 16.025\n",
      "# Iteration    48 -> Loss: 3.5960729363132238 \t| Accuracy: 16.013\n",
      "# Iteration    49 -> Loss: 3.584989601879622 \t| Accuracy: 16.088\n",
      "# Iteration    50 -> Loss: 3.574391515412167 \t| Accuracy: 16.125\n",
      "# Iteration    51 -> Loss: 3.563687240991531 \t| Accuracy: 16.148\n",
      "# Iteration    52 -> Loss: 3.5530651075872015 \t| Accuracy: 16.278\n",
      "# Iteration    53 -> Loss: 3.541702443053546 \t| Accuracy: 16.348\n",
      "# Iteration    54 -> Loss: 3.5325290949929906 \t| Accuracy: 16.375\n",
      "# Iteration    55 -> Loss: 3.52271804378913 \t| Accuracy: 16.427\n",
      "# Iteration    56 -> Loss: 3.5130154909781934 \t| Accuracy: 16.492\n",
      "# Iteration    57 -> Loss: 3.504217222924841 \t| Accuracy: 16.527\n",
      "# Iteration    58 -> Loss: 3.4960521721537137 \t| Accuracy: 16.498\n",
      "# Iteration    59 -> Loss: 3.4878738774124876 \t| Accuracy: 16.343\n",
      "# Iteration    60 -> Loss: 3.4800286329859853 \t| Accuracy: 16.333\n",
      "# Iteration    61 -> Loss: 3.472329947645219 \t| Accuracy: 15.820\n",
      "# Iteration    62 -> Loss: 3.463703443596061 \t| Accuracy: 15.850\n",
      "# Iteration    63 -> Loss: 3.4560476105457654 \t| Accuracy: 15.910\n",
      "# Iteration    64 -> Loss: 3.4489763776167424 \t| Accuracy: 15.910\n",
      "# Iteration    65 -> Loss: 3.4418489689611422 \t| Accuracy: 15.915\n",
      "# Iteration    66 -> Loss: 3.4350190289097604 \t| Accuracy: 15.977\n",
      "# Iteration    67 -> Loss: 3.428214069178301 \t| Accuracy: 16.938\n",
      "# Iteration    68 -> Loss: 3.42172370237271 \t| Accuracy: 17.065\n",
      "# Iteration    69 -> Loss: 3.4153957911273665 \t| Accuracy: 17.052\n",
      "# Iteration    70 -> Loss: 3.4087674942532833 \t| Accuracy: 17.215\n",
      "# Iteration    71 -> Loss: 3.4027213343156935 \t| Accuracy: 17.238\n",
      "# Iteration    72 -> Loss: 3.39693702138027 \t| Accuracy: 17.282\n",
      "# Iteration    73 -> Loss: 3.390941315368857 \t| Accuracy: 17.307\n",
      "# Iteration    74 -> Loss: 3.3845580250168745 \t| Accuracy: 17.337\n",
      "# Iteration    75 -> Loss: 3.3783994728462328 \t| Accuracy: 17.430\n",
      "# Iteration    76 -> Loss: 3.372595920696603 \t| Accuracy: 17.460\n",
      "# Iteration    77 -> Loss: 3.367159181219563 \t| Accuracy: 17.577\n",
      "# Iteration    78 -> Loss: 3.361291589439226 \t| Accuracy: 17.605\n",
      "# Iteration    79 -> Loss: 3.3556756883679415 \t| Accuracy: 17.695\n",
      "# Iteration    80 -> Loss: 3.350497539903328 \t| Accuracy: 17.710\n",
      "# Iteration    81 -> Loss: 3.346105157817403 \t| Accuracy: 17.797\n",
      "# Iteration    82 -> Loss: 3.3409772131269464 \t| Accuracy: 17.758\n",
      "# Iteration    83 -> Loss: 3.335573438703576 \t| Accuracy: 17.830\n",
      "# Iteration    84 -> Loss: 3.3303889726363924 \t| Accuracy: 17.938\n",
      "# Iteration    85 -> Loss: 3.3257923168721053 \t| Accuracy: 17.953\n",
      "# Iteration    86 -> Loss: 3.3214725434491466 \t| Accuracy: 17.957\n",
      "# Iteration    87 -> Loss: 3.3168697322184606 \t| Accuracy: 17.873\n",
      "# Iteration    88 -> Loss: 3.3128268830609464 \t| Accuracy: 17.947\n",
      "# Iteration    89 -> Loss: 3.3085897898006125 \t| Accuracy: 18.253\n",
      "# Iteration    90 -> Loss: 3.304669919226533 \t| Accuracy: 18.265\n",
      "# Iteration    91 -> Loss: 3.300709745706383 \t| Accuracy: 18.270\n",
      "# Iteration    92 -> Loss: 3.2969082492343924 \t| Accuracy: 18.295\n",
      "# Iteration    93 -> Loss: 3.2932947573644773 \t| Accuracy: 18.405\n",
      "# Iteration    94 -> Loss: 3.2894084423279 \t| Accuracy: 18.555\n",
      "# Iteration    95 -> Loss: 3.285367491434286 \t| Accuracy: 18.617\n",
      "# Iteration    96 -> Loss: 3.2813440412364274 \t| Accuracy: 18.652\n",
      "# Iteration    97 -> Loss: 3.2775853748313915 \t| Accuracy: 18.630\n",
      "# Iteration    98 -> Loss: 3.2735320319379757 \t| Accuracy: 18.660\n",
      "# Iteration    99 -> Loss: 3.269682410218242 \t| Accuracy: 18.682\n",
      "# Iteration   100 -> Loss: 3.266045916161725 \t| Accuracy: 18.720\n",
      "# Iteration   101 -> Loss: 3.2623457919022756 \t| Accuracy: 18.550\n",
      "# Iteration   102 -> Loss: 3.25863617233063 \t| Accuracy: 18.650\n",
      "# Iteration   103 -> Loss: 3.255154572129228 \t| Accuracy: 18.623\n",
      "# Iteration   104 -> Loss: 3.251567492659862 \t| Accuracy: 18.642\n",
      "# Iteration   105 -> Loss: 3.248239338049994 \t| Accuracy: 18.663\n",
      "# Iteration   106 -> Loss: 3.2448671468300088 \t| Accuracy: 18.685\n",
      "# Iteration   107 -> Loss: 3.2415185904030945 \t| Accuracy: 18.723\n",
      "# Iteration   108 -> Loss: 3.2380411340480815 \t| Accuracy: 18.757\n",
      "# Iteration   109 -> Loss: 3.2348600637422216 \t| Accuracy: 18.765\n",
      "# Iteration   110 -> Loss: 3.231964807231815 \t| Accuracy: 18.758\n",
      "# Iteration   111 -> Loss: 3.2290005559421098 \t| Accuracy: 18.767\n",
      "# Iteration   112 -> Loss: 3.2259881214199413 \t| Accuracy: 18.815\n",
      "# Iteration   113 -> Loss: 3.223290119508348 \t| Accuracy: 18.815\n",
      "# Iteration   114 -> Loss: 3.220502553400706 \t| Accuracy: 18.855\n",
      "# Iteration   115 -> Loss: 3.217676437404962 \t| Accuracy: 18.923\n",
      "# Iteration   116 -> Loss: 3.215057445112365 \t| Accuracy: 18.915\n",
      "# Iteration   117 -> Loss: 3.212017471940042 \t| Accuracy: 18.943\n",
      "# Iteration   118 -> Loss: 3.209598190093168 \t| Accuracy: 19.013\n",
      "# Iteration   119 -> Loss: 3.206643980336699 \t| Accuracy: 19.040\n",
      "# Iteration   120 -> Loss: 3.203697349670896 \t| Accuracy: 19.075\n",
      "# Iteration   121 -> Loss: 3.200699444529083 \t| Accuracy: 19.098\n",
      "# Iteration   122 -> Loss: 3.1979656537337204 \t| Accuracy: 19.125\n",
      "# Iteration   123 -> Loss: 3.195273861418837 \t| Accuracy: 19.130\n",
      "# Iteration   124 -> Loss: 3.1930979851776065 \t| Accuracy: 19.128\n",
      "# Iteration   125 -> Loss: 3.1903747752466787 \t| Accuracy: 19.160\n",
      "# Iteration   126 -> Loss: 3.18791762267588 \t| Accuracy: 19.097\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Iteration   127 -> Loss: 3.184833951423863 \t| Accuracy: 19.105\n",
      "# Iteration   128 -> Loss: 3.182081952748148 \t| Accuracy: 19.165\n",
      "# Iteration   129 -> Loss: 3.1797174424098102 \t| Accuracy: 20.493\n",
      "# Iteration   130 -> Loss: 3.1773259027334246 \t| Accuracy: 20.528\n",
      "# Iteration   131 -> Loss: 3.17495259982761 \t| Accuracy: 20.558\n",
      "# Iteration   132 -> Loss: 3.1725537412809395 \t| Accuracy: 20.580\n",
      "# Iteration   133 -> Loss: 3.1699152389577474 \t| Accuracy: 20.585\n",
      "# Iteration   134 -> Loss: 3.1672665575324017 \t| Accuracy: 20.583\n",
      "# Iteration   135 -> Loss: 3.1651369323708125 \t| Accuracy: 20.623\n",
      "# Iteration   136 -> Loss: 3.1629860215124594 \t| Accuracy: 20.617\n",
      "# Iteration   137 -> Loss: 3.1608976061453617 \t| Accuracy: 20.660\n",
      "# Iteration   138 -> Loss: 3.1587855239537648 \t| Accuracy: 20.645\n",
      "# Iteration   139 -> Loss: 3.156582702190251 \t| Accuracy: 20.680\n",
      "# Iteration   140 -> Loss: 3.154761025783033 \t| Accuracy: 20.695\n",
      "# Iteration   141 -> Loss: 3.152485035631606 \t| Accuracy: 20.717\n",
      "# Iteration   142 -> Loss: 3.1504051546257736 \t| Accuracy: 20.737\n",
      "# Iteration   143 -> Loss: 3.1483432060474477 \t| Accuracy: 20.727\n",
      "# Iteration   144 -> Loss: 3.1461773639687594 \t| Accuracy: 20.745\n",
      "# Iteration   145 -> Loss: 3.144484222210845 \t| Accuracy: 20.755\n",
      "# Iteration   146 -> Loss: 3.14241074677389 \t| Accuracy: 20.785\n",
      "# Iteration   147 -> Loss: 3.1404194076529746 \t| Accuracy: 20.802\n",
      "# Iteration   148 -> Loss: 3.138609764843153 \t| Accuracy: 20.813\n",
      "# Iteration   149 -> Loss: 3.136548158654854 \t| Accuracy: 20.812\n",
      "# Iteration   150 -> Loss: 3.1345113352442273 \t| Accuracy: 20.830\n",
      "# Iteration   151 -> Loss: 3.132479165046624 \t| Accuracy: 20.848\n",
      "# Iteration   152 -> Loss: 3.130359659501474 \t| Accuracy: 20.897\n",
      "# Iteration   153 -> Loss: 3.128620465727708 \t| Accuracy: 20.740\n",
      "# Iteration   154 -> Loss: 3.126619796426323 \t| Accuracy: 20.762\n",
      "# Iteration   155 -> Loss: 3.1248734402558807 \t| Accuracy: 20.783\n",
      "# Iteration   156 -> Loss: 3.122333479662446 \t| Accuracy: 20.820\n",
      "# Iteration   157 -> Loss: 3.12039753207296 \t| Accuracy: 20.825\n",
      "# Iteration   158 -> Loss: 3.1181844757149775 \t| Accuracy: 20.887\n",
      "# Iteration   159 -> Loss: 3.1164167787859234 \t| Accuracy: 20.902\n",
      "# Iteration   160 -> Loss: 3.11479098760644 \t| Accuracy: 21.712\n",
      "# Iteration   161 -> Loss: 3.1126145884376957 \t| Accuracy: 21.827\n",
      "# Iteration   162 -> Loss: 3.1107289354665975 \t| Accuracy: 21.847\n",
      "# Iteration   163 -> Loss: 3.108913882706369 \t| Accuracy: 22.585\n",
      "# Iteration   164 -> Loss: 3.1071554056255857 \t| Accuracy: 22.612\n",
      "# Iteration   165 -> Loss: 3.1055624753846507 \t| Accuracy: 22.617\n",
      "# Iteration   166 -> Loss: 3.1039677782060853 \t| Accuracy: 22.632\n",
      "# Iteration   167 -> Loss: 3.102299452142019 \t| Accuracy: 22.643\n",
      "# Iteration   168 -> Loss: 3.100573822707472 \t| Accuracy: 22.655\n",
      "# Iteration   169 -> Loss: 3.098810783550199 \t| Accuracy: 22.658\n",
      "# Iteration   170 -> Loss: 3.097288333253834 \t| Accuracy: 22.685\n",
      "# Iteration   171 -> Loss: 3.095718298827017 \t| Accuracy: 22.707\n",
      "# Iteration   172 -> Loss: 3.094101082365707 \t| Accuracy: 23.503\n",
      "# Iteration   173 -> Loss: 3.0923493636386525 \t| Accuracy: 23.593\n",
      "# Iteration   174 -> Loss: 3.0906327343282456 \t| Accuracy: 23.583\n",
      "# Iteration   175 -> Loss: 3.0888750169992547 \t| Accuracy: 23.607\n",
      "# Iteration   176 -> Loss: 3.0871056982658938 \t| Accuracy: 23.613\n",
      "# Iteration   177 -> Loss: 3.0856207838860947 \t| Accuracy: 23.622\n",
      "# Iteration   178 -> Loss: 3.0839401410363556 \t| Accuracy: 23.685\n",
      "# Iteration   179 -> Loss: 3.082097181297475 \t| Accuracy: 23.702\n",
      "# Iteration   180 -> Loss: 3.080447279247615 \t| Accuracy: 24.263\n",
      "# Iteration   181 -> Loss: 3.078807193291272 \t| Accuracy: 24.297\n",
      "# Iteration   182 -> Loss: 3.0771614861691257 \t| Accuracy: 24.317\n",
      "# Iteration   183 -> Loss: 3.075633382969913 \t| Accuracy: 24.315\n",
      "# Iteration   184 -> Loss: 3.0740031271051027 \t| Accuracy: 24.347\n",
      "# Iteration   185 -> Loss: 3.0725148787853973 \t| Accuracy: 24.360\n",
      "# Iteration   186 -> Loss: 3.0709585408543076 \t| Accuracy: 24.377\n",
      "# Iteration   187 -> Loss: 3.069399558975954 \t| Accuracy: 24.387\n",
      "# Iteration   188 -> Loss: 3.067805955877234 \t| Accuracy: 24.403\n",
      "# Iteration   189 -> Loss: 3.0661727377755748 \t| Accuracy: 24.420\n",
      "# Iteration   190 -> Loss: 3.0644861424239953 \t| Accuracy: 24.430\n",
      "# Iteration   191 -> Loss: 3.062748987980409 \t| Accuracy: 24.450\n",
      "# Iteration   192 -> Loss: 3.061119317826186 \t| Accuracy: 24.453\n",
      "# Iteration   193 -> Loss: 3.05941963737727 \t| Accuracy: 24.475\n",
      "# Iteration   194 -> Loss: 3.0579121200910553 \t| Accuracy: 24.493\n",
      "# Iteration   195 -> Loss: 3.056272580354902 \t| Accuracy: 24.515\n",
      "# Iteration   196 -> Loss: 3.0544521771035864 \t| Accuracy: 24.505\n",
      "# Iteration   197 -> Loss: 3.0528594065350787 \t| Accuracy: 24.537\n",
      "# Iteration   198 -> Loss: 3.051312071271633 \t| Accuracy: 24.552\n",
      "# Iteration   199 -> Loss: 3.0496393487826734 \t| Accuracy: 24.558\n",
      "# Iteration   200 -> Loss: 3.0478749478012754 \t| Accuracy: 24.597\n",
      "# Iteration   201 -> Loss: 3.0463747061253184 \t| Accuracy: 24.648\n",
      "# Iteration   202 -> Loss: 3.0444588816343163 \t| Accuracy: 24.653\n",
      "# Iteration   203 -> Loss: 3.0425737951201475 \t| Accuracy: 25.702\n",
      "# Iteration   204 -> Loss: 3.0407767539906936 \t| Accuracy: 25.712\n",
      "# Iteration   205 -> Loss: 3.0393278526028986 \t| Accuracy: 25.728\n",
      "# Iteration   206 -> Loss: 3.037826631670163 \t| Accuracy: 25.735\n",
      "# Iteration   207 -> Loss: 3.036299642025572 \t| Accuracy: 25.735\n",
      "# Iteration   208 -> Loss: 3.0347531956619194 \t| Accuracy: 25.750\n",
      "# Iteration   209 -> Loss: 3.033326137740988 \t| Accuracy: 25.768\n",
      "# Iteration   210 -> Loss: 3.031939683231165 \t| Accuracy: 25.775\n",
      "# Iteration   211 -> Loss: 3.030402326204139 \t| Accuracy: 25.802\n",
      "# Iteration   212 -> Loss: 3.028813486960541 \t| Accuracy: 27.813\n",
      "# Iteration   213 -> Loss: 3.027011279794626 \t| Accuracy: 27.830\n",
      "# Iteration   214 -> Loss: 3.025594149343581 \t| Accuracy: 27.880\n",
      "# Iteration   215 -> Loss: 3.024198877667199 \t| Accuracy: 27.882\n",
      "# Iteration   216 -> Loss: 3.022788069668764 \t| Accuracy: 27.888\n",
      "# Iteration   217 -> Loss: 3.021013082303862 \t| Accuracy: 27.913\n",
      "# Iteration   218 -> Loss: 3.0191779889426176 \t| Accuracy: 27.933\n",
      "# Iteration   219 -> Loss: 3.0176234360874385 \t| Accuracy: 27.960\n",
      "# Iteration   220 -> Loss: 3.0161855981022114 \t| Accuracy: 27.962\n",
      "# Iteration   221 -> Loss: 3.0148446004287233 \t| Accuracy: 28.085\n",
      "# Iteration   222 -> Loss: 3.013501786267031 \t| Accuracy: 28.117\n",
      "# Iteration   223 -> Loss: 3.011761094031475 \t| Accuracy: 28.173\n",
      "# Iteration   224 -> Loss: 3.0101010259628156 \t| Accuracy: 28.200\n",
      "# Iteration   225 -> Loss: 3.008479989873178 \t| Accuracy: 28.550\n",
      "# Iteration   226 -> Loss: 3.0074095158266267 \t| Accuracy: 28.592\n",
      "# Iteration   227 -> Loss: 3.0061917469925143 \t| Accuracy: 28.980\n",
      "# Iteration   228 -> Loss: 3.0047561211984837 \t| Accuracy: 28.995\n",
      "# Iteration   229 -> Loss: 3.0027434020352683 \t| Accuracy: 29.038\n",
      "# Iteration   230 -> Loss: 3.001394000928893 \t| Accuracy: 29.015\n",
      "# Iteration   231 -> Loss: 3.0001051169951904 \t| Accuracy: 29.045\n",
      "# Iteration   232 -> Loss: 2.999035990367353 \t| Accuracy: 29.045\n",
      "# Iteration   233 -> Loss: 2.997758380734526 \t| Accuracy: 29.122\n",
      "# Iteration   234 -> Loss: 2.9961497148474576 \t| Accuracy: 29.142\n",
      "# Iteration   235 -> Loss: 2.9948541440855965 \t| Accuracy: 29.162\n",
      "# Iteration   236 -> Loss: 2.9935487075031553 \t| Accuracy: 29.165\n",
      "# Iteration   237 -> Loss: 2.9919721878092087 \t| Accuracy: 29.212\n",
      "# Iteration   238 -> Loss: 2.9907938891061283 \t| Accuracy: 29.273\n",
      "# Iteration   239 -> Loss: 2.9893542699873046 \t| Accuracy: 29.345\n",
      "# Iteration   240 -> Loss: 2.988133193267785 \t| Accuracy: 29.300\n",
      "# Iteration   241 -> Loss: 2.9863643937951356 \t| Accuracy: 29.367\n",
      "# Iteration   242 -> Loss: 2.985118313418007 \t| Accuracy: 29.372\n",
      "# Iteration   243 -> Loss: 2.9837936365955255 \t| Accuracy: 29.398\n",
      "# Iteration   244 -> Loss: 2.9823462301376513 \t| Accuracy: 30.077\n",
      "# Iteration   245 -> Loss: 2.9810426984358886 \t| Accuracy: 30.107\n",
      "# Iteration   246 -> Loss: 2.9796284853584605 \t| Accuracy: 30.130\n",
      "# Iteration   247 -> Loss: 2.9784341741674987 \t| Accuracy: 30.155\n",
      "# Iteration   248 -> Loss: 2.9771194000363246 \t| Accuracy: 30.168\n",
      "# Iteration   249 -> Loss: 2.9756308652112464 \t| Accuracy: 30.187\n",
      "# Iteration   250 -> Loss: 2.9743580845717066 \t| Accuracy: 30.228\n",
      "# Iteration   251 -> Loss: 2.9731254027679808 \t| Accuracy: 30.232\n",
      "# Iteration   252 -> Loss: 2.9722401539984715 \t| Accuracy: 30.240\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Iteration   253 -> Loss: 2.9709073035875466 \t| Accuracy: 30.255\n",
      "# Iteration   254 -> Loss: 2.969437738602198 \t| Accuracy: 30.282\n",
      "# Iteration   255 -> Loss: 2.9679872991280027 \t| Accuracy: 30.288\n",
      "# Iteration   256 -> Loss: 2.966793029952509 \t| Accuracy: 30.277\n",
      "# Iteration   257 -> Loss: 2.9657342005804406 \t| Accuracy: 30.278\n",
      "# Iteration   258 -> Loss: 2.964407794578188 \t| Accuracy: 30.303\n",
      "# Iteration   259 -> Loss: 2.962904327211876 \t| Accuracy: 30.337\n",
      "# Iteration   260 -> Loss: 2.9615778131248702 \t| Accuracy: 30.368\n",
      "# Iteration   261 -> Loss: 2.960278147534624 \t| Accuracy: 30.460\n",
      "# Iteration   262 -> Loss: 2.9588199629358396 \t| Accuracy: 31.040\n",
      "# Iteration   263 -> Loss: 2.957466346578415 \t| Accuracy: 31.037\n",
      "# Iteration   264 -> Loss: 2.956194528016811 \t| Accuracy: 31.082\n",
      "# Iteration   265 -> Loss: 2.954700815134397 \t| Accuracy: 31.088\n",
      "# Iteration   266 -> Loss: 2.95368744095322 \t| Accuracy: 31.097\n",
      "# Iteration   267 -> Loss: 2.952625801974519 \t| Accuracy: 31.087\n",
      "# Iteration   268 -> Loss: 2.95146637270961 \t| Accuracy: 31.077\n",
      "# Iteration   269 -> Loss: 2.950067349488899 \t| Accuracy: 31.123\n",
      "# Iteration   270 -> Loss: 2.9488894581144027 \t| Accuracy: 31.125\n",
      "# Iteration   271 -> Loss: 2.947801779876024 \t| Accuracy: 31.148\n",
      "# Iteration   272 -> Loss: 2.946768883944021 \t| Accuracy: 31.122\n",
      "# Iteration   273 -> Loss: 2.9456827653373647 \t| Accuracy: 32.362\n",
      "# Iteration   274 -> Loss: 2.944189559880354 \t| Accuracy: 32.370\n",
      "# Iteration   275 -> Loss: 2.9430447982033443 \t| Accuracy: 32.400\n",
      "# Iteration   276 -> Loss: 2.941638487779638 \t| Accuracy: 32.400\n",
      "# Iteration   277 -> Loss: 2.9404563454397596 \t| Accuracy: 32.437\n",
      "# Iteration   278 -> Loss: 2.939385252794904 \t| Accuracy: 32.417\n",
      "# Iteration   279 -> Loss: 2.938228743372587 \t| Accuracy: 32.455\n",
      "# Iteration   280 -> Loss: 2.937069941717081 \t| Accuracy: 32.475\n",
      "# Iteration   281 -> Loss: 2.9358951583045667 \t| Accuracy: 32.475\n",
      "# Iteration   282 -> Loss: 2.9348584438750995 \t| Accuracy: 32.502\n",
      "# Iteration   283 -> Loss: 2.9339455213756516 \t| Accuracy: 32.560\n",
      "# Iteration   284 -> Loss: 2.932612397260917 \t| Accuracy: 32.577\n",
      "# Iteration   285 -> Loss: 2.9316021891755883 \t| Accuracy: 32.583\n",
      "# Iteration   286 -> Loss: 2.930488082233129 \t| Accuracy: 32.587\n",
      "# Iteration   287 -> Loss: 2.9291103877083584 \t| Accuracy: 32.665\n",
      "# Iteration   288 -> Loss: 2.9278672216741177 \t| Accuracy: 32.667\n",
      "# Iteration   289 -> Loss: 2.926781918349401 \t| Accuracy: 32.703\n",
      "# Iteration   290 -> Loss: 2.925628770422344 \t| Accuracy: 32.682\n",
      "# Iteration   291 -> Loss: 2.9240206985558777 \t| Accuracy: 32.740\n",
      "# Iteration   292 -> Loss: 2.9227445301091906 \t| Accuracy: 32.743\n",
      "# Iteration   293 -> Loss: 2.921546662104903 \t| Accuracy: 32.758\n",
      "# Iteration   294 -> Loss: 2.9202431183174515 \t| Accuracy: 32.773\n",
      "# Iteration   295 -> Loss: 2.9188280578573336 \t| Accuracy: 32.800\n",
      "# Iteration   296 -> Loss: 2.917592241801238 \t| Accuracy: 32.865\n",
      "# Iteration   297 -> Loss: 2.9165243325180694 \t| Accuracy: 32.837\n",
      "# Iteration   298 -> Loss: 2.915137676987312 \t| Accuracy: 32.852\n",
      "# Iteration   299 -> Loss: 2.9138411937844033 \t| Accuracy: 32.863\n",
      "# Iteration   300 -> Loss: 2.912769935516753 \t| Accuracy: 32.885\n",
      "# Iteration   301 -> Loss: 2.911691412910073 \t| Accuracy: 32.902\n",
      "# Iteration   302 -> Loss: 2.9106686362027774 \t| Accuracy: 32.925\n",
      "# Iteration   303 -> Loss: 2.9095546322952925 \t| Accuracy: 32.913\n",
      "# Iteration   304 -> Loss: 2.9085204282945174 \t| Accuracy: 32.982\n",
      "# Iteration   305 -> Loss: 2.907423316208292 \t| Accuracy: 32.988\n",
      "# Iteration   306 -> Loss: 2.9064954893990715 \t| Accuracy: 32.997\n",
      "# Iteration   307 -> Loss: 2.905294493922427 \t| Accuracy: 33.055\n",
      "# Iteration   308 -> Loss: 2.904216774429348 \t| Accuracy: 33.053\n",
      "# Iteration   309 -> Loss: 2.903279429564661 \t| Accuracy: 33.063\n",
      "# Iteration   310 -> Loss: 2.9023098246510943 \t| Accuracy: 33.102\n",
      "# Iteration   311 -> Loss: 2.9013301419701683 \t| Accuracy: 33.102\n",
      "# Iteration   312 -> Loss: 2.9001997856625494 \t| Accuracy: 33.115\n",
      "# Iteration   313 -> Loss: 2.8988773076139536 \t| Accuracy: 33.148\n",
      "# Iteration   314 -> Loss: 2.8976577973592006 \t| Accuracy: 33.108\n",
      "# Iteration   315 -> Loss: 2.8967478816591075 \t| Accuracy: 33.128\n",
      "# Iteration   316 -> Loss: 2.895584672822895 \t| Accuracy: 33.177\n",
      "# Iteration   317 -> Loss: 2.894422432481977 \t| Accuracy: 33.172\n",
      "# Iteration   318 -> Loss: 2.8932785855915784 \t| Accuracy: 33.242\n",
      "# Iteration   319 -> Loss: 2.892365816449811 \t| Accuracy: 33.245\n",
      "# Iteration   320 -> Loss: 2.891167743219846 \t| Accuracy: 33.253\n",
      "# Iteration   321 -> Loss: 2.8902363255717187 \t| Accuracy: 33.367\n",
      "# Iteration   322 -> Loss: 2.889251970687046 \t| Accuracy: 33.362\n",
      "# Iteration   323 -> Loss: 2.888138344591818 \t| Accuracy: 33.380\n",
      "# Iteration   324 -> Loss: 2.887086402533711 \t| Accuracy: 33.393\n",
      "# Iteration   325 -> Loss: 2.886193476593174 \t| Accuracy: 33.410\n",
      "# Iteration   326 -> Loss: 2.8849998131947467 \t| Accuracy: 33.430\n",
      "# Iteration   327 -> Loss: 2.883912186801116 \t| Accuracy: 33.475\n",
      "# Iteration   328 -> Loss: 2.8828602479130017 \t| Accuracy: 33.485\n",
      "# Iteration   329 -> Loss: 2.8821555267701613 \t| Accuracy: 33.500\n",
      "# Iteration   330 -> Loss: 2.880963762230932 \t| Accuracy: 33.518\n",
      "# Iteration   331 -> Loss: 2.8796342791861593 \t| Accuracy: 33.555\n",
      "# Iteration   332 -> Loss: 2.878658825163172 \t| Accuracy: 33.583\n",
      "# Iteration   333 -> Loss: 2.8777069272574334 \t| Accuracy: 33.602\n",
      "# Iteration   334 -> Loss: 2.8762216840326604 \t| Accuracy: 33.632\n",
      "# Iteration   335 -> Loss: 2.8750303779891877 \t| Accuracy: 33.655\n",
      "# Iteration   336 -> Loss: 2.8736274379517415 \t| Accuracy: 33.678\n",
      "# Iteration   337 -> Loss: 2.8723408641661528 \t| Accuracy: 33.665\n",
      "# Iteration   338 -> Loss: 2.871219956897804 \t| Accuracy: 33.690\n",
      "# Iteration   339 -> Loss: 2.8699594516121905 \t| Accuracy: 33.708\n",
      "# Iteration   340 -> Loss: 2.868862409362936 \t| Accuracy: 33.762\n",
      "# Iteration   341 -> Loss: 2.867474400487905 \t| Accuracy: 33.792\n",
      "# Iteration   342 -> Loss: 2.8663862919279937 \t| Accuracy: 33.777\n",
      "# Iteration   343 -> Loss: 2.8651785947688233 \t| Accuracy: 33.820\n",
      "# Iteration   344 -> Loss: 2.864105631498116 \t| Accuracy: 33.837\n",
      "# Iteration   345 -> Loss: 2.863082770628027 \t| Accuracy: 33.850\n",
      "# Iteration   346 -> Loss: 2.862271730603302 \t| Accuracy: 33.865\n",
      "# Iteration   347 -> Loss: 2.861105270099894 \t| Accuracy: 33.878\n",
      "# Iteration   348 -> Loss: 2.8600233114341873 \t| Accuracy: 33.897\n",
      "# Iteration   349 -> Loss: 2.8591116181130736 \t| Accuracy: 33.925\n",
      "# Iteration   350 -> Loss: 2.858114089537412 \t| Accuracy: 33.940\n",
      "# Iteration   351 -> Loss: 2.85706276193627 \t| Accuracy: 33.958\n",
      "# Iteration   352 -> Loss: 2.8560748500057653 \t| Accuracy: 33.968\n",
      "# Iteration   353 -> Loss: 2.855170922355905 \t| Accuracy: 33.992\n",
      "# Iteration   354 -> Loss: 2.8539669393680054 \t| Accuracy: 34.012\n",
      "# Iteration   355 -> Loss: 2.853062904010653 \t| Accuracy: 34.018\n",
      "# Iteration   356 -> Loss: 2.852036113090671 \t| Accuracy: 34.055\n",
      "# Iteration   357 -> Loss: 2.8509883646779484 \t| Accuracy: 34.075\n",
      "# Iteration   358 -> Loss: 2.8499965942809977 \t| Accuracy: 34.095\n",
      "# Iteration   359 -> Loss: 2.849355226409344 \t| Accuracy: 34.103\n",
      "# Iteration   360 -> Loss: 2.8479317710496637 \t| Accuracy: 34.117\n",
      "# Iteration   361 -> Loss: 2.8467603251582383 \t| Accuracy: 34.150\n",
      "# Iteration   362 -> Loss: 2.8457988292186647 \t| Accuracy: 34.162\n",
      "# Iteration   363 -> Loss: 2.8448123740017115 \t| Accuracy: 34.225\n",
      "# Iteration   364 -> Loss: 2.843883784639163 \t| Accuracy: 34.230\n",
      "# Iteration   365 -> Loss: 2.8428897676436007 \t| Accuracy: 34.267\n",
      "# Iteration   366 -> Loss: 2.8420501962866487 \t| Accuracy: 34.278\n",
      "# Iteration   367 -> Loss: 2.841222801580753 \t| Accuracy: 34.278\n",
      "# Iteration   368 -> Loss: 2.840097151380713 \t| Accuracy: 34.295\n",
      "# Iteration   369 -> Loss: 2.839111075485366 \t| Accuracy: 34.307\n",
      "# Iteration   370 -> Loss: 2.838330113180842 \t| Accuracy: 34.327\n",
      "# Iteration   371 -> Loss: 2.837467357831168 \t| Accuracy: 34.327\n",
      "# Iteration   372 -> Loss: 2.8366183274812062 \t| Accuracy: 34.362\n",
      "# Iteration   373 -> Loss: 2.8357239209212732 \t| Accuracy: 34.365\n",
      "# Iteration   374 -> Loss: 2.8347867609964514 \t| Accuracy: 34.388\n",
      "# Iteration   375 -> Loss: 2.8337415750653157 \t| Accuracy: 34.395\n",
      "# Iteration   376 -> Loss: 2.8327261254776066 \t| Accuracy: 34.412\n",
      "# Iteration   377 -> Loss: 2.83163625048131 \t| Accuracy: 34.447\n",
      "# Iteration   378 -> Loss: 2.8307532951109278 \t| Accuracy: 34.508\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Iteration   379 -> Loss: 2.830006520542479 \t| Accuracy: 34.490\n",
      "# Iteration   380 -> Loss: 2.829182363102595 \t| Accuracy: 34.512\n",
      "# Iteration   381 -> Loss: 2.8278930038990286 \t| Accuracy: 34.553\n",
      "# Iteration   382 -> Loss: 2.826702166252524 \t| Accuracy: 34.558\n",
      "# Iteration   383 -> Loss: 2.825713914916387 \t| Accuracy: 34.607\n",
      "# Iteration   384 -> Loss: 2.824545137867665 \t| Accuracy: 34.613\n",
      "# Iteration   385 -> Loss: 2.823325229177649 \t| Accuracy: 34.643\n",
      "# Iteration   386 -> Loss: 2.8222900462719034 \t| Accuracy: 34.673\n",
      "# Iteration   387 -> Loss: 2.8212931218101227 \t| Accuracy: 34.697\n",
      "# Iteration   388 -> Loss: 2.820385872786067 \t| Accuracy: 34.713\n",
      "# Iteration   389 -> Loss: 2.8197221178927396 \t| Accuracy: 34.712\n",
      "# Iteration   390 -> Loss: 2.8190947282298673 \t| Accuracy: 34.717\n",
      "# Iteration   391 -> Loss: 2.817839263263855 \t| Accuracy: 34.745\n",
      "# Iteration   392 -> Loss: 2.8168654282310572 \t| Accuracy: 34.763\n",
      "# Iteration   393 -> Loss: 2.815937774311823 \t| Accuracy: 34.803\n",
      "# Iteration   394 -> Loss: 2.8151639732846925 \t| Accuracy: 34.787\n",
      "# Iteration   395 -> Loss: 2.8139527348130025 \t| Accuracy: 34.837\n",
      "# Iteration   396 -> Loss: 2.8131984195889896 \t| Accuracy: 34.902\n",
      "# Iteration   397 -> Loss: 2.8124475938705773 \t| Accuracy: 34.902\n",
      "# Iteration   398 -> Loss: 2.8112585499261264 \t| Accuracy: 34.913\n",
      "# Iteration   399 -> Loss: 2.8103806239164864 \t| Accuracy: 34.940\n",
      "# Iteration   400 -> Loss: 2.809512724687558 \t| Accuracy: 34.948\n",
      "# Iteration   401 -> Loss: 2.808693308454447 \t| Accuracy: 34.948\n",
      "# Iteration   402 -> Loss: 2.8078523841168823 \t| Accuracy: 34.975\n",
      "# Iteration   403 -> Loss: 2.807091572418844 \t| Accuracy: 34.947\n",
      "# Iteration   404 -> Loss: 2.805911435357583 \t| Accuracy: 34.950\n",
      "# Iteration   405 -> Loss: 2.804697125183642 \t| Accuracy: 34.987\n",
      "# Iteration   406 -> Loss: 2.8035761080219377 \t| Accuracy: 35.022\n",
      "# Iteration   407 -> Loss: 2.80242220080835 \t| Accuracy: 35.045\n",
      "# Iteration   408 -> Loss: 2.8014751327938243 \t| Accuracy: 35.070\n",
      "# Iteration   409 -> Loss: 2.8005636802757023 \t| Accuracy: 35.078\n",
      "# Iteration   410 -> Loss: 2.7999372652326557 \t| Accuracy: 35.092\n",
      "# Iteration   411 -> Loss: 2.7987861120773116 \t| Accuracy: 35.093\n",
      "# Iteration   412 -> Loss: 2.7980867948739423 \t| Accuracy: 35.117\n",
      "# Iteration   413 -> Loss: 2.797167744378235 \t| Accuracy: 35.123\n",
      "# Iteration   414 -> Loss: 2.796356938112381 \t| Accuracy: 35.128\n",
      "# Iteration   415 -> Loss: 2.795355412521561 \t| Accuracy: 35.268\n",
      "# Iteration   416 -> Loss: 2.794505033597239 \t| Accuracy: 35.290\n",
      "# Iteration   417 -> Loss: 2.7937230146793657 \t| Accuracy: 35.290\n",
      "# Iteration   418 -> Loss: 2.7927973503733825 \t| Accuracy: 35.295\n",
      "# Iteration   419 -> Loss: 2.791802224385052 \t| Accuracy: 35.318\n",
      "# Iteration   420 -> Loss: 2.7906434514144403 \t| Accuracy: 35.347\n",
      "# Iteration   421 -> Loss: 2.7898203318103443 \t| Accuracy: 35.368\n",
      "# Iteration   422 -> Loss: 2.7891742319148087 \t| Accuracy: 35.363\n",
      "# Iteration   423 -> Loss: 2.788218621137591 \t| Accuracy: 35.397\n",
      "# Iteration   424 -> Loss: 2.787230302909406 \t| Accuracy: 35.402\n",
      "# Iteration   425 -> Loss: 2.7863538956128293 \t| Accuracy: 35.438\n",
      "# Iteration   426 -> Loss: 2.785257410175704 \t| Accuracy: 35.453\n",
      "# Iteration   427 -> Loss: 2.784463352235537 \t| Accuracy: 35.468\n",
      "# Iteration   428 -> Loss: 2.7833188800218647 \t| Accuracy: 35.485\n",
      "# Iteration   429 -> Loss: 2.7827596268345522 \t| Accuracy: 35.497\n",
      "# Iteration   430 -> Loss: 2.782248208586994 \t| Accuracy: 35.502\n",
      "# Iteration   431 -> Loss: 2.7812202895895544 \t| Accuracy: 35.513\n",
      "# Iteration   432 -> Loss: 2.7803532733785854 \t| Accuracy: 35.525\n",
      "# Iteration   433 -> Loss: 2.7794438370813643 \t| Accuracy: 35.540\n",
      "# Iteration   434 -> Loss: 2.778461980069632 \t| Accuracy: 35.573\n",
      "# Iteration   435 -> Loss: 2.7776016456219295 \t| Accuracy: 35.593\n",
      "# Iteration   436 -> Loss: 2.7767734130786557 \t| Accuracy: 35.605\n",
      "# Iteration   437 -> Loss: 2.775985361541919 \t| Accuracy: 35.613\n",
      "# Iteration   438 -> Loss: 2.774890199324994 \t| Accuracy: 35.668\n",
      "# Iteration   439 -> Loss: 2.7740859827698667 \t| Accuracy: 35.668\n",
      "# Iteration   440 -> Loss: 2.773296913601184 \t| Accuracy: 35.703\n",
      "# Iteration   441 -> Loss: 2.772537507471764 \t| Accuracy: 35.700\n",
      "# Iteration   442 -> Loss: 2.7717211952292353 \t| Accuracy: 35.717\n",
      "# Iteration   443 -> Loss: 2.771036476452949 \t| Accuracy: 35.720\n",
      "# Iteration   444 -> Loss: 2.7698776133619534 \t| Accuracy: 35.727\n",
      "# Iteration   445 -> Loss: 2.7691368893893764 \t| Accuracy: 35.758\n",
      "# Iteration   446 -> Loss: 2.768257793399199 \t| Accuracy: 35.873\n",
      "# Iteration   447 -> Loss: 2.767340463695753 \t| Accuracy: 35.895\n",
      "# Iteration   448 -> Loss: 2.766389523619554 \t| Accuracy: 35.905\n",
      "# Iteration   449 -> Loss: 2.7656142459523974 \t| Accuracy: 35.968\n",
      "# Iteration   450 -> Loss: 2.7645815547052552 \t| Accuracy: 35.948\n",
      "# Iteration   451 -> Loss: 2.7638518528766602 \t| Accuracy: 36.490\n",
      "# Iteration   452 -> Loss: 2.762727198104999 \t| Accuracy: 36.495\n",
      "# Iteration   453 -> Loss: 2.7619962607043607 \t| Accuracy: 36.503\n",
      "# Iteration   454 -> Loss: 2.761102676315286 \t| Accuracy: 36.505\n",
      "# Iteration   455 -> Loss: 2.7600807816465145 \t| Accuracy: 36.530\n",
      "# Iteration   456 -> Loss: 2.7593084490663284 \t| Accuracy: 36.607\n",
      "# Iteration   457 -> Loss: 2.7583752135967674 \t| Accuracy: 36.638\n",
      "# Iteration   458 -> Loss: 2.7578326137526257 \t| Accuracy: 36.628\n",
      "# Iteration   459 -> Loss: 2.757232451281693 \t| Accuracy: 36.647\n",
      "# Iteration   460 -> Loss: 2.756431820402081 \t| Accuracy: 36.648\n",
      "# Iteration   461 -> Loss: 2.755369450890242 \t| Accuracy: 36.693\n",
      "# Iteration   462 -> Loss: 2.754455144708253 \t| Accuracy: 36.698\n",
      "# Iteration   463 -> Loss: 2.7540168053394027 \t| Accuracy: 36.688\n",
      "# Iteration   464 -> Loss: 2.7534634614378284 \t| Accuracy: 36.660\n",
      "# Iteration   465 -> Loss: 2.752580959387835 \t| Accuracy: 36.697\n",
      "# Iteration   466 -> Loss: 2.751562090087658 \t| Accuracy: 36.730\n",
      "# Iteration   467 -> Loss: 2.7506969090721984 \t| Accuracy: 36.747\n",
      "# Iteration   468 -> Loss: 2.7497183477827747 \t| Accuracy: 36.767\n",
      "# Iteration   469 -> Loss: 2.7491270427162666 \t| Accuracy: 36.732\n",
      "# Iteration   470 -> Loss: 2.7485214940059994 \t| Accuracy: 36.752\n",
      "# Iteration   471 -> Loss: 2.7476559784751697 \t| Accuracy: 36.775\n",
      "# Iteration   472 -> Loss: 2.746655232113202 \t| Accuracy: 36.785\n",
      "# Iteration   473 -> Loss: 2.7456749721106144 \t| Accuracy: 36.813\n",
      "# Iteration   474 -> Loss: 2.745112384121181 \t| Accuracy: 36.792\n",
      "# Iteration   475 -> Loss: 2.744219589242945 \t| Accuracy: 36.815\n",
      "# Iteration   476 -> Loss: 2.743302062999117 \t| Accuracy: 36.848\n",
      "# Iteration   477 -> Loss: 2.7424906165092744 \t| Accuracy: 36.852\n",
      "# Iteration   478 -> Loss: 2.7417923422684685 \t| Accuracy: 36.875\n",
      "# Iteration   479 -> Loss: 2.741243518996947 \t| Accuracy: 36.877\n",
      "# Iteration   480 -> Loss: 2.739995078547581 \t| Accuracy: 36.907\n",
      "# Iteration   481 -> Loss: 2.739378170111771 \t| Accuracy: 36.932\n",
      "# Iteration   482 -> Loss: 2.7386099812211135 \t| Accuracy: 36.938\n",
      "# Iteration   483 -> Loss: 2.7376950873247607 \t| Accuracy: 36.945\n",
      "# Iteration   484 -> Loss: 2.736931662077546 \t| Accuracy: 36.963\n",
      "# Iteration   485 -> Loss: 2.7361617968689065 \t| Accuracy: 36.988\n",
      "# Iteration   486 -> Loss: 2.7352571003841675 \t| Accuracy: 37.012\n",
      "# Iteration   487 -> Loss: 2.7346279504138016 \t| Accuracy: 37.012\n",
      "# Iteration   488 -> Loss: 2.733860475212484 \t| Accuracy: 37.037\n",
      "# Iteration   489 -> Loss: 2.7329783532431104 \t| Accuracy: 37.498\n",
      "# Iteration   490 -> Loss: 2.732166814935146 \t| Accuracy: 37.527\n",
      "# Iteration   491 -> Loss: 2.731498174607458 \t| Accuracy: 37.532\n",
      "# Iteration   492 -> Loss: 2.7308969404657595 \t| Accuracy: 37.532\n",
      "# Iteration   493 -> Loss: 2.7297639348142257 \t| Accuracy: 37.633\n",
      "# Iteration   494 -> Loss: 2.7290015325521657 \t| Accuracy: 37.678\n",
      "# Iteration   495 -> Loss: 2.7283440723458035 \t| Accuracy: 37.703\n",
      "# Iteration   496 -> Loss: 2.7280257585743883 \t| Accuracy: 37.672\n",
      "# Iteration   497 -> Loss: 2.7268856230076084 \t| Accuracy: 37.702\n",
      "# Iteration   498 -> Loss: 2.726163775727226 \t| Accuracy: 37.712\n",
      "# Iteration   499 -> Loss: 2.7254276832135242 \t| Accuracy: 37.718\n",
      "# Iteration   500 -> Loss: 2.7247181243309284 \t| Accuracy: 37.732\n",
      "# Iteration   501 -> Loss: 2.7240932768849535 \t| Accuracy: 37.750\n",
      "# Iteration   502 -> Loss: 2.7232332216537753 \t| Accuracy: 37.768\n",
      "# Iteration   503 -> Loss: 2.7226723608517003 \t| Accuracy: 37.792\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Iteration   504 -> Loss: 2.7217216461039895 \t| Accuracy: 37.813\n",
      "# Iteration   505 -> Loss: 2.720896045875295 \t| Accuracy: 37.828\n",
      "# Iteration   506 -> Loss: 2.7200390451048038 \t| Accuracy: 37.858\n",
      "# Iteration   507 -> Loss: 2.7195923946111145 \t| Accuracy: 37.848\n",
      "# Iteration   508 -> Loss: 2.718971392553575 \t| Accuracy: 37.885\n",
      "# Iteration   509 -> Loss: 2.7179683543056483 \t| Accuracy: 37.878\n",
      "# Iteration   510 -> Loss: 2.7172234102183266 \t| Accuracy: 37.885\n",
      "# Iteration   511 -> Loss: 2.7166113691695646 \t| Accuracy: 37.927\n",
      "# Iteration   512 -> Loss: 2.715741029042677 \t| Accuracy: 37.945\n",
      "# Iteration   513 -> Loss: 2.71497116925865 \t| Accuracy: 37.902\n",
      "# Iteration   514 -> Loss: 2.7138297868045975 \t| Accuracy: 37.938\n",
      "# Iteration   515 -> Loss: 2.713247639261488 \t| Accuracy: 37.962\n",
      "# Iteration   516 -> Loss: 2.7122901710564844 \t| Accuracy: 37.962\n",
      "# Iteration   517 -> Loss: 2.711419519167547 \t| Accuracy: 38.017\n",
      "# Iteration   518 -> Loss: 2.7107453415341833 \t| Accuracy: 38.010\n",
      "# Iteration   519 -> Loss: 2.7103274545854243 \t| Accuracy: 38.027\n",
      "# Iteration   520 -> Loss: 2.7092051661790206 \t| Accuracy: 38.030\n",
      "# Iteration   521 -> Loss: 2.7082690237086227 \t| Accuracy: 38.063\n",
      "# Iteration   522 -> Loss: 2.707458580979258 \t| Accuracy: 38.080\n",
      "# Iteration   523 -> Loss: 2.706673675226117 \t| Accuracy: 38.092\n",
      "# Iteration   524 -> Loss: 2.706010958360154 \t| Accuracy: 38.117\n",
      "# Iteration   525 -> Loss: 2.7050541415004363 \t| Accuracy: 38.140\n",
      "# Iteration   526 -> Loss: 2.7041261080711614 \t| Accuracy: 38.145\n",
      "# Iteration   527 -> Loss: 2.7035200552806575 \t| Accuracy: 38.280\n",
      "# Iteration   528 -> Loss: 2.703113022560719 \t| Accuracy: 38.295\n",
      "# Iteration   529 -> Loss: 2.702232257193628 \t| Accuracy: 38.297\n",
      "# Iteration   530 -> Loss: 2.7012437208096416 \t| Accuracy: 38.293\n",
      "# Iteration   531 -> Loss: 2.7003024619881306 \t| Accuracy: 38.337\n",
      "# Iteration   532 -> Loss: 2.6996257471846836 \t| Accuracy: 38.050\n",
      "# Iteration   533 -> Loss: 2.699020213888808 \t| Accuracy: 38.085\n",
      "# Iteration   534 -> Loss: 2.69833484890487 \t| Accuracy: 38.078\n",
      "# Iteration   535 -> Loss: 2.697527066768496 \t| Accuracy: 38.107\n",
      "# Iteration   536 -> Loss: 2.696650770747144 \t| Accuracy: 38.267\n",
      "# Iteration   537 -> Loss: 2.6959312105926707 \t| Accuracy: 38.250\n",
      "# Iteration   538 -> Loss: 2.695109942216858 \t| Accuracy: 38.283\n",
      "# Iteration   539 -> Loss: 2.6942948108208746 \t| Accuracy: 38.302\n",
      "# Iteration   540 -> Loss: 2.6936082532368144 \t| Accuracy: 38.330\n",
      "# Iteration   541 -> Loss: 2.6928882874570483 \t| Accuracy: 38.325\n",
      "# Iteration   542 -> Loss: 2.6920890913372726 \t| Accuracy: 38.355\n",
      "# Iteration   543 -> Loss: 2.691539360309428 \t| Accuracy: 38.385\n",
      "# Iteration   544 -> Loss: 2.690491937432794 \t| Accuracy: 38.400\n",
      "# Iteration   545 -> Loss: 2.6898425441941756 \t| Accuracy: 38.392\n",
      "# Iteration   546 -> Loss: 2.6890754049714283 \t| Accuracy: 38.427\n",
      "# Iteration   547 -> Loss: 2.6886923574037387 \t| Accuracy: 38.403\n",
      "# Iteration   548 -> Loss: 2.687925362401353 \t| Accuracy: 38.422\n",
      "# Iteration   549 -> Loss: 2.686927289162274 \t| Accuracy: 38.453\n",
      "# Iteration   550 -> Loss: 2.686098366370166 \t| Accuracy: 38.478\n",
      "# Iteration   551 -> Loss: 2.6852466193018465 \t| Accuracy: 38.487\n",
      "# Iteration   552 -> Loss: 2.684569038366442 \t| Accuracy: 38.493\n",
      "# Iteration   553 -> Loss: 2.683869931579373 \t| Accuracy: 38.522\n",
      "# Iteration   554 -> Loss: 2.6833744078267894 \t| Accuracy: 38.573\n",
      "# Iteration   555 -> Loss: 2.6827977401529406 \t| Accuracy: 38.578\n",
      "# Iteration   556 -> Loss: 2.682087130646627 \t| Accuracy: 38.597\n",
      "# Iteration   557 -> Loss: 2.6810057494636927 \t| Accuracy: 38.628\n",
      "# Iteration   558 -> Loss: 2.6801706115982755 \t| Accuracy: 38.637\n",
      "# Iteration   559 -> Loss: 2.679346396059827 \t| Accuracy: 38.673\n",
      "# Iteration   560 -> Loss: 2.678727535076939 \t| Accuracy: 38.680\n",
      "# Iteration   561 -> Loss: 2.6781891523678336 \t| Accuracy: 38.707\n",
      "# Iteration   562 -> Loss: 2.6773961528082713 \t| Accuracy: 38.720\n",
      "# Iteration   563 -> Loss: 2.6769082365921557 \t| Accuracy: 38.715\n",
      "# Iteration   564 -> Loss: 2.6762820728867163 \t| Accuracy: 38.735\n",
      "# Iteration   565 -> Loss: 2.6755086354230397 \t| Accuracy: 38.787\n",
      "# Iteration   566 -> Loss: 2.6747365226233706 \t| Accuracy: 38.772\n",
      "# Iteration   567 -> Loss: 2.6743188394129165 \t| Accuracy: 38.798\n",
      "# Iteration   568 -> Loss: 2.6734444426632393 \t| Accuracy: 38.792\n",
      "# Iteration   569 -> Loss: 2.672570850914132 \t| Accuracy: 38.840\n",
      "# Iteration   570 -> Loss: 2.6717736322643866 \t| Accuracy: 38.840\n",
      "# Iteration   571 -> Loss: 2.6708611673230034 \t| Accuracy: 38.828\n",
      "# Iteration   572 -> Loss: 2.670024670420374 \t| Accuracy: 38.843\n",
      "# Iteration   573 -> Loss: 2.6694011241313134 \t| Accuracy: 38.898\n",
      "# Iteration   574 -> Loss: 2.6688084504078216 \t| Accuracy: 38.887\n",
      "# Iteration   575 -> Loss: 2.6681352084098084 \t| Accuracy: 38.948\n",
      "# Iteration   576 -> Loss: 2.6668886497432833 \t| Accuracy: 38.945\n",
      "# Iteration   577 -> Loss: 2.6661040398178444 \t| Accuracy: 38.975\n",
      "# Iteration   578 -> Loss: 2.66517478463406 \t| Accuracy: 38.935\n",
      "# Iteration   579 -> Loss: 2.664513566363477 \t| Accuracy: 38.962\n",
      "# Iteration   580 -> Loss: 2.6637794255411102 \t| Accuracy: 38.978\n",
      "# Iteration   581 -> Loss: 2.663187846476163 \t| Accuracy: 39.027\n",
      "# Iteration   582 -> Loss: 2.662179897212202 \t| Accuracy: 39.043\n",
      "# Iteration   583 -> Loss: 2.6617808549854716 \t| Accuracy: 39.045\n",
      "# Iteration   584 -> Loss: 2.660884590225954 \t| Accuracy: 39.060\n",
      "# Iteration   585 -> Loss: 2.660043216868294 \t| Accuracy: 39.088\n",
      "# Iteration   586 -> Loss: 2.659501401521997 \t| Accuracy: 39.095\n",
      "# Iteration   587 -> Loss: 2.6588006206756374 \t| Accuracy: 39.125\n",
      "# Iteration   588 -> Loss: 2.657913882366913 \t| Accuracy: 39.147\n",
      "# Iteration   589 -> Loss: 2.6569831711330796 \t| Accuracy: 39.173\n",
      "# Iteration   590 -> Loss: 2.6562950638357554 \t| Accuracy: 39.205\n",
      "# Iteration   591 -> Loss: 2.6557181755982637 \t| Accuracy: 39.197\n",
      "# Iteration   592 -> Loss: 2.6552413321218378 \t| Accuracy: 39.210\n",
      "# Iteration   593 -> Loss: 2.654578038629213 \t| Accuracy: 39.207\n",
      "# Iteration   594 -> Loss: 2.6538150221002548 \t| Accuracy: 39.247\n",
      "# Iteration   595 -> Loss: 2.6533909762189523 \t| Accuracy: 39.237\n",
      "# Iteration   596 -> Loss: 2.6526299182339415 \t| Accuracy: 39.257\n",
      "# Iteration   597 -> Loss: 2.652230305940432 \t| Accuracy: 39.255\n",
      "# Iteration   598 -> Loss: 2.6514679973113218 \t| Accuracy: 39.275\n",
      "# Iteration   599 -> Loss: 2.650364213868105 \t| Accuracy: 39.307\n",
      "# Iteration   600 -> Loss: 2.6496139101161833 \t| Accuracy: 39.327\n",
      "# Iteration   601 -> Loss: 2.6490473173259454 \t| Accuracy: 39.348\n",
      "# Iteration   602 -> Loss: 2.648230672913047 \t| Accuracy: 39.348\n",
      "# Iteration   603 -> Loss: 2.6475317253458277 \t| Accuracy: 39.390\n",
      "# Iteration   604 -> Loss: 2.646822296737378 \t| Accuracy: 39.382\n",
      "# Iteration   605 -> Loss: 2.646043886983359 \t| Accuracy: 39.435\n",
      "# Iteration   606 -> Loss: 2.6453653430570823 \t| Accuracy: 39.470\n",
      "# Iteration   607 -> Loss: 2.6443782454509313 \t| Accuracy: 39.483\n",
      "# Iteration   608 -> Loss: 2.6435696278696814 \t| Accuracy: 39.525\n",
      "# Iteration   609 -> Loss: 2.642953200644898 \t| Accuracy: 39.522\n",
      "# Iteration   610 -> Loss: 2.642144664858521 \t| Accuracy: 39.555\n",
      "# Iteration   611 -> Loss: 2.641388188951971 \t| Accuracy: 39.577\n",
      "# Iteration   612 -> Loss: 2.6406653795915283 \t| Accuracy: 39.583\n",
      "# Iteration   613 -> Loss: 2.6400362826965145 \t| Accuracy: 39.605\n",
      "# Iteration   614 -> Loss: 2.6393928163642153 \t| Accuracy: 39.632\n",
      "# Iteration   615 -> Loss: 2.6385947555686675 \t| Accuracy: 39.648\n",
      "# Iteration   616 -> Loss: 2.6381742786308977 \t| Accuracy: 39.627\n",
      "# Iteration   617 -> Loss: 2.6375685994375995 \t| Accuracy: 39.675\n",
      "# Iteration   618 -> Loss: 2.636604064069064 \t| Accuracy: 39.667\n",
      "# Iteration   619 -> Loss: 2.6358568072360256 \t| Accuracy: 39.705\n",
      "# Iteration   620 -> Loss: 2.6349530023293966 \t| Accuracy: 39.742\n",
      "# Iteration   621 -> Loss: 2.63411321756986 \t| Accuracy: 39.733\n",
      "# Iteration   622 -> Loss: 2.633453853996189 \t| Accuracy: 39.775\n",
      "# Iteration   623 -> Loss: 2.6329611171327096 \t| Accuracy: 39.780\n",
      "# Iteration   624 -> Loss: 2.6321486085118067 \t| Accuracy: 39.815\n",
      "# Iteration   625 -> Loss: 2.631076876878043 \t| Accuracy: 39.850\n",
      "# Iteration   626 -> Loss: 2.630473067157774 \t| Accuracy: 39.868\n",
      "# Iteration   627 -> Loss: 2.629969381711451 \t| Accuracy: 39.873\n",
      "# Iteration   628 -> Loss: 2.6292052758494795 \t| Accuracy: 39.882\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Iteration   629 -> Loss: 2.628566583831533 \t| Accuracy: 39.920\n",
      "# Iteration   630 -> Loss: 2.6274801630430615 \t| Accuracy: 39.952\n",
      "# Iteration   631 -> Loss: 2.6266288942175793 \t| Accuracy: 39.977\n",
      "# Iteration   632 -> Loss: 2.625985126156779 \t| Accuracy: 39.982\n",
      "# Iteration   633 -> Loss: 2.625688381870895 \t| Accuracy: 39.978\n",
      "# Iteration   634 -> Loss: 2.625034124012005 \t| Accuracy: 39.972\n",
      "# Iteration   635 -> Loss: 2.6243869673994196 \t| Accuracy: 39.985\n",
      "# Iteration   636 -> Loss: 2.6241837982202063 \t| Accuracy: 39.980\n",
      "# Iteration   637 -> Loss: 2.6232968745511567 \t| Accuracy: 40.007\n",
      "# Iteration   638 -> Loss: 2.622722701386512 \t| Accuracy: 40.027\n",
      "# Iteration   639 -> Loss: 2.6220428682128842 \t| Accuracy: 40.022\n",
      "# Iteration   640 -> Loss: 2.6215048214214254 \t| Accuracy: 40.042\n",
      "# Iteration   641 -> Loss: 2.6206877197019165 \t| Accuracy: 40.045\n",
      "# Iteration   642 -> Loss: 2.6201028618934843 \t| Accuracy: 40.070\n",
      "# Iteration   643 -> Loss: 2.619410532740993 \t| Accuracy: 40.075\n",
      "# Iteration   644 -> Loss: 2.6185156780667227 \t| Accuracy: 40.118\n",
      "# Iteration   645 -> Loss: 2.618042862969414 \t| Accuracy: 40.120\n",
      "# Iteration   646 -> Loss: 2.6172122820520034 \t| Accuracy: 40.147\n",
      "# Iteration   647 -> Loss: 2.6165749237435376 \t| Accuracy: 40.160\n",
      "# Iteration   648 -> Loss: 2.6158182639527516 \t| Accuracy: 40.175\n",
      "# Iteration   649 -> Loss: 2.61523345739464 \t| Accuracy: 40.197\n",
      "# Iteration   650 -> Loss: 2.6141304224997706 \t| Accuracy: 40.213\n",
      "# Iteration   651 -> Loss: 2.6134587368397417 \t| Accuracy: 40.233\n",
      "# Iteration   652 -> Loss: 2.612977136015508 \t| Accuracy: 40.242\n",
      "# Iteration   653 -> Loss: 2.6126511702662536 \t| Accuracy: 40.247\n",
      "# Iteration   654 -> Loss: 2.6119108200013144 \t| Accuracy: 40.270\n",
      "# Iteration   655 -> Loss: 2.6107399749039937 \t| Accuracy: 40.273\n",
      "# Iteration   656 -> Loss: 2.6101659931279255 \t| Accuracy: 40.303\n",
      "# Iteration   657 -> Loss: 2.6091119975488577 \t| Accuracy: 40.317\n",
      "# Iteration   658 -> Loss: 2.608481912559066 \t| Accuracy: 40.345\n",
      "# Iteration   659 -> Loss: 2.6076986708693544 \t| Accuracy: 40.348\n",
      "# Iteration   660 -> Loss: 2.606784131162634 \t| Accuracy: 40.367\n",
      "# Iteration   661 -> Loss: 2.6062771815321404 \t| Accuracy: 40.400\n",
      "# Iteration   662 -> Loss: 2.6052070541051258 \t| Accuracy: 40.405\n",
      "# Iteration   663 -> Loss: 2.6043157291841985 \t| Accuracy: 40.427\n",
      "# Iteration   664 -> Loss: 2.6035772552482155 \t| Accuracy: 41.542\n",
      "# Iteration   665 -> Loss: 2.602853604460307 \t| Accuracy: 41.553\n",
      "# Iteration   666 -> Loss: 2.6021671459792803 \t| Accuracy: 41.570\n",
      "# Iteration   667 -> Loss: 2.6014917809048743 \t| Accuracy: 41.588\n",
      "# Iteration   668 -> Loss: 2.6006965619657065 \t| Accuracy: 41.557\n",
      "# Iteration   669 -> Loss: 2.5998827276126306 \t| Accuracy: 41.550\n",
      "# Iteration   670 -> Loss: 2.5989150200509306 \t| Accuracy: 41.590\n",
      "# Iteration   671 -> Loss: 2.5982624439072937 \t| Accuracy: 41.730\n",
      "# Iteration   672 -> Loss: 2.597437913264065 \t| Accuracy: 41.765\n",
      "# Iteration   673 -> Loss: 2.5968058272462664 \t| Accuracy: 41.785\n",
      "# Iteration   674 -> Loss: 2.5961164346939594 \t| Accuracy: 41.807\n",
      "# Iteration   675 -> Loss: 2.5954194971390763 \t| Accuracy: 41.815\n",
      "# Iteration   676 -> Loss: 2.594621309896676 \t| Accuracy: 41.820\n",
      "# Iteration   677 -> Loss: 2.5939305696494634 \t| Accuracy: 41.835\n",
      "# Iteration   678 -> Loss: 2.5932663195648766 \t| Accuracy: 41.852\n",
      "# Iteration   679 -> Loss: 2.5921926928916563 \t| Accuracy: 41.887\n",
      "# Iteration   680 -> Loss: 2.5917382433824354 \t| Accuracy: 41.910\n",
      "# Iteration   681 -> Loss: 2.5909897008027607 \t| Accuracy: 41.910\n",
      "# Iteration   682 -> Loss: 2.590451968791371 \t| Accuracy: 41.940\n",
      "# Iteration   683 -> Loss: 2.589497457671817 \t| Accuracy: 41.975\n",
      "# Iteration   684 -> Loss: 2.5886775060901868 \t| Accuracy: 41.985\n",
      "# Iteration   685 -> Loss: 2.5879654770563176 \t| Accuracy: 42.013\n",
      "# Iteration   686 -> Loss: 2.587105177105084 \t| Accuracy: 42.033\n",
      "# Iteration   687 -> Loss: 2.5865880856726027 \t| Accuracy: 42.040\n",
      "# Iteration   688 -> Loss: 2.5858539078084077 \t| Accuracy: 42.058\n",
      "# Iteration   689 -> Loss: 2.585307790721822 \t| Accuracy: 42.098\n",
      "# Iteration   690 -> Loss: 2.5846708027674925 \t| Accuracy: 42.092\n",
      "# Iteration   691 -> Loss: 2.5840571775200853 \t| Accuracy: 42.125\n",
      "# Iteration   692 -> Loss: 2.583219951854657 \t| Accuracy: 42.125\n",
      "# Iteration   693 -> Loss: 2.5827562054297615 \t| Accuracy: 42.145\n",
      "# Iteration   694 -> Loss: 2.5820531165886593 \t| Accuracy: 42.368\n",
      "# Iteration   695 -> Loss: 2.5813758095531334 \t| Accuracy: 42.392\n",
      "# Iteration   696 -> Loss: 2.580607351329109 \t| Accuracy: 42.380\n",
      "# Iteration   697 -> Loss: 2.5797243081280676 \t| Accuracy: 42.415\n",
      "# Iteration   698 -> Loss: 2.579309969827425 \t| Accuracy: 42.430\n",
      "# Iteration   699 -> Loss: 2.5789444597090765 \t| Accuracy: 42.425\n",
      "# Iteration   700 -> Loss: 2.5784226377413013 \t| Accuracy: 42.458\n",
      "# Iteration   701 -> Loss: 2.577387165608679 \t| Accuracy: 42.463\n",
      "# Iteration   702 -> Loss: 2.5767273105020965 \t| Accuracy: 42.487\n",
      "# Iteration   703 -> Loss: 2.575993574479101 \t| Accuracy: 42.508\n",
      "# Iteration   704 -> Loss: 2.5750912554126737 \t| Accuracy: 42.538\n",
      "# Iteration   705 -> Loss: 2.5743573886651068 \t| Accuracy: 42.552\n",
      "# Iteration   706 -> Loss: 2.573817677991667 \t| Accuracy: 42.558\n",
      "# Iteration   707 -> Loss: 2.5731462609356077 \t| Accuracy: 42.578\n",
      "# Iteration   708 -> Loss: 2.5725497991189092 \t| Accuracy: 42.588\n",
      "# Iteration   709 -> Loss: 2.571612652676069 \t| Accuracy: 42.602\n",
      "# Iteration   710 -> Loss: 2.5709067254731797 \t| Accuracy: 42.618\n",
      "# Iteration   711 -> Loss: 2.570266515759484 \t| Accuracy: 42.627\n",
      "# Iteration   712 -> Loss: 2.5698953124125437 \t| Accuracy: 42.637\n",
      "# Iteration   713 -> Loss: 2.5693754527842096 \t| Accuracy: 42.648\n",
      "# Iteration   714 -> Loss: 2.568516015600824 \t| Accuracy: 42.668\n",
      "# Iteration   715 -> Loss: 2.5677967298712474 \t| Accuracy: 42.685\n",
      "# Iteration   716 -> Loss: 2.5671415422538755 \t| Accuracy: 42.717\n",
      "# Iteration   717 -> Loss: 2.5666562732135096 \t| Accuracy: 42.713\n",
      "# Iteration   718 -> Loss: 2.5660208356679144 \t| Accuracy: 42.737\n",
      "# Iteration   719 -> Loss: 2.5655648776321778 \t| Accuracy: 42.718\n",
      "# Iteration   720 -> Loss: 2.564884614385859 \t| Accuracy: 42.742\n",
      "# Iteration   721 -> Loss: 2.56396812404227 \t| Accuracy: 42.762\n",
      "# Iteration   722 -> Loss: 2.5631682341315694 \t| Accuracy: 42.782\n",
      "# Iteration   723 -> Loss: 2.562786689672181 \t| Accuracy: 42.778\n",
      "# Iteration   724 -> Loss: 2.561879050064325 \t| Accuracy: 42.818\n",
      "# Iteration   725 -> Loss: 2.561085725302371 \t| Accuracy: 42.828\n",
      "# Iteration   726 -> Loss: 2.56031987356148 \t| Accuracy: 42.855\n",
      "# Iteration   727 -> Loss: 2.560066353531098 \t| Accuracy: 42.840\n",
      "# Iteration   728 -> Loss: 2.5593936688010137 \t| Accuracy: 42.868\n",
      "# Iteration   729 -> Loss: 2.558301123797112 \t| Accuracy: 42.882\n",
      "# Iteration   730 -> Loss: 2.5575802210416385 \t| Accuracy: 42.902\n",
      "# Iteration   731 -> Loss: 2.5572378537889873 \t| Accuracy: 42.905\n",
      "# Iteration   732 -> Loss: 2.556835370544651 \t| Accuracy: 42.907\n",
      "# Iteration   733 -> Loss: 2.556003003975749 \t| Accuracy: 42.950\n",
      "# Iteration   734 -> Loss: 2.555396309669867 \t| Accuracy: 42.948\n",
      "# Iteration   735 -> Loss: 2.554743644159457 \t| Accuracy: 42.977\n",
      "# Iteration   736 -> Loss: 2.554002301275608 \t| Accuracy: 42.965\n",
      "# Iteration   737 -> Loss: 2.5533493385509898 \t| Accuracy: 43.058\n",
      "# Iteration   738 -> Loss: 2.552947719450648 \t| Accuracy: 43.058\n",
      "# Iteration   739 -> Loss: 2.5521392930384152 \t| Accuracy: 43.098\n",
      "# Iteration   740 -> Loss: 2.5514221477866186 \t| Accuracy: 43.182\n",
      "# Iteration   741 -> Loss: 2.5508184244143948 \t| Accuracy: 43.215\n",
      "# Iteration   742 -> Loss: 2.5500197343736297 \t| Accuracy: 43.228\n",
      "# Iteration   743 -> Loss: 2.5495716957291434 \t| Accuracy: 43.222\n",
      "# Iteration   744 -> Loss: 2.5487780302610443 \t| Accuracy: 43.243\n",
      "# Iteration   745 -> Loss: 2.54844626744096 \t| Accuracy: 43.232\n",
      "# Iteration   746 -> Loss: 2.5476266804138765 \t| Accuracy: 43.255\n",
      "# Iteration   747 -> Loss: 2.5470684151848797 \t| Accuracy: 43.292\n",
      "# Iteration   748 -> Loss: 2.5466566994068702 \t| Accuracy: 43.272\n",
      "# Iteration   749 -> Loss: 2.546152894308507 \t| Accuracy: 43.305\n",
      "# Iteration   750 -> Loss: 2.5454913304515734 \t| Accuracy: 43.288\n",
      "# Iteration   751 -> Loss: 2.545339515456124 \t| Accuracy: 43.302\n",
      "# Iteration   752 -> Loss: 2.544408540593901 \t| Accuracy: 43.320\n",
      "# Iteration   753 -> Loss: 2.5440600686559844 \t| Accuracy: 43.327\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Iteration   754 -> Loss: 2.543163837146969 \t| Accuracy: 43.343\n",
      "# Iteration   755 -> Loss: 2.542924196097196 \t| Accuracy: 43.343\n",
      "# Iteration   756 -> Loss: 2.5423989395637014 \t| Accuracy: 43.342\n",
      "# Iteration   757 -> Loss: 2.542065938061623 \t| Accuracy: 43.370\n",
      "# Iteration   758 -> Loss: 2.5410726196709024 \t| Accuracy: 43.382\n",
      "# Iteration   759 -> Loss: 2.540667986556846 \t| Accuracy: 43.408\n",
      "# Iteration   760 -> Loss: 2.5397568051593487 \t| Accuracy: 43.418\n",
      "# Iteration   761 -> Loss: 2.539381116334543 \t| Accuracy: 43.442\n",
      "# Iteration   762 -> Loss: 2.539179249947746 \t| Accuracy: 43.420\n",
      "# Iteration   763 -> Loss: 2.5388072359315514 \t| Accuracy: 43.445\n",
      "# Iteration   764 -> Loss: 2.5382993266758453 \t| Accuracy: 43.422\n",
      "# Iteration   765 -> Loss: 2.5372643785550193 \t| Accuracy: 43.502\n",
      "# Iteration   766 -> Loss: 2.5366985008120864 \t| Accuracy: 43.537\n",
      "# Iteration   767 -> Loss: 2.535977254597311 \t| Accuracy: 43.552\n",
      "# Iteration   768 -> Loss: 2.5356670880944634 \t| Accuracy: 43.565\n",
      "# Iteration   769 -> Loss: 2.5353290572063654 \t| Accuracy: 43.553\n",
      "# Iteration   770 -> Loss: 2.5347163554778094 \t| Accuracy: 43.577\n",
      "# Iteration   771 -> Loss: 2.5337338682283104 \t| Accuracy: 43.598\n",
      "# Iteration   772 -> Loss: 2.5332821883701433 \t| Accuracy: 43.613\n",
      "# Iteration   773 -> Loss: 2.5332928995457404 \t| Accuracy: 43.613\n",
      "# Iteration   774 -> Loss: 2.5325568437229937 \t| Accuracy: 43.640\n",
      "# Iteration   775 -> Loss: 2.5320179443660784 \t| Accuracy: 43.658\n",
      "# Iteration   776 -> Loss: 2.5312204417429185 \t| Accuracy: 43.663\n",
      "# Iteration   777 -> Loss: 2.531017139916413 \t| Accuracy: 43.683\n",
      "# Iteration   778 -> Loss: 2.5300801291268677 \t| Accuracy: 43.693\n",
      "# Iteration   779 -> Loss: 2.5297563485404586 \t| Accuracy: 43.710\n",
      "# Iteration   780 -> Loss: 2.529620544015191 \t| Accuracy: 43.700\n",
      "# Iteration   781 -> Loss: 2.5285816943844943 \t| Accuracy: 43.722\n",
      "# Iteration   782 -> Loss: 2.528016343313015 \t| Accuracy: 43.757\n",
      "# Iteration   783 -> Loss: 2.527746492967181 \t| Accuracy: 43.738\n",
      "# Iteration   784 -> Loss: 2.527223125136371 \t| Accuracy: 43.743\n",
      "# Iteration   785 -> Loss: 2.5270700909368036 \t| Accuracy: 43.745\n",
      "# Iteration   786 -> Loss: 2.526939337900627 \t| Accuracy: 43.738\n",
      "# Iteration   787 -> Loss: 2.526013230359146 \t| Accuracy: 43.755\n",
      "# Iteration   788 -> Loss: 2.525670091370108 \t| Accuracy: 43.770\n",
      "# Iteration   789 -> Loss: 2.5251364242585486 \t| Accuracy: 43.772\n",
      "# Iteration   790 -> Loss: 2.524679288640959 \t| Accuracy: 43.785\n",
      "# Iteration   791 -> Loss: 2.524463030054137 \t| Accuracy: 43.777\n",
      "# Iteration   792 -> Loss: 2.5238208913702604 \t| Accuracy: 43.793\n",
      "# Iteration   793 -> Loss: 2.5233205224075155 \t| Accuracy: 44.103\n",
      "# Iteration   794 -> Loss: 2.5227285841841485 \t| Accuracy: 44.117\n",
      "# Iteration   795 -> Loss: 2.52266273630088 \t| Accuracy: 44.125\n",
      "# Iteration   796 -> Loss: 2.52238134126409 \t| Accuracy: 44.118\n",
      "# Iteration   797 -> Loss: 2.5218333300496343 \t| Accuracy: 44.105\n",
      "# Iteration   798 -> Loss: 2.521472067536864 \t| Accuracy: 44.143\n",
      "# Iteration   799 -> Loss: 2.5206262907745622 \t| Accuracy: 44.158\n",
      "# Iteration   800 -> Loss: 2.520094973000759 \t| Accuracy: 44.165\n",
      "# Iteration   801 -> Loss: 2.519822830244159 \t| Accuracy: 44.165\n",
      "# Iteration   802 -> Loss: 2.519553636200252 \t| Accuracy: 44.178\n",
      "# Iteration   803 -> Loss: 2.519125300971998 \t| Accuracy: 44.177\n",
      "# Iteration   804 -> Loss: 2.518617521766719 \t| Accuracy: 44.195\n",
      "# Iteration   805 -> Loss: 2.51792892209609 \t| Accuracy: 44.207\n",
      "# Iteration   806 -> Loss: 2.517435542464699 \t| Accuracy: 44.222\n",
      "# Iteration   807 -> Loss: 2.5169615308649034 \t| Accuracy: 44.248\n",
      "# Iteration   808 -> Loss: 2.5167216638137186 \t| Accuracy: 44.242\n",
      "# Iteration   809 -> Loss: 2.515993891499508 \t| Accuracy: 44.238\n",
      "# Iteration   810 -> Loss: 2.5160877404222326 \t| Accuracy: 44.250\n",
      "# Iteration   811 -> Loss: 2.5154696117123683 \t| Accuracy: 44.237\n",
      "# Iteration   812 -> Loss: 2.514645536398719 \t| Accuracy: 44.257\n",
      "# Iteration   813 -> Loss: 2.5142006614201042 \t| Accuracy: 44.275\n",
      "# Iteration   814 -> Loss: 2.5136082783271587 \t| Accuracy: 44.298\n",
      "# Iteration   815 -> Loss: 2.5128222739247925 \t| Accuracy: 44.305\n",
      "# Iteration   816 -> Loss: 2.512348213019967 \t| Accuracy: 44.303\n",
      "# Iteration   817 -> Loss: 2.512242002029379 \t| Accuracy: 44.292\n",
      "# Iteration   818 -> Loss: 2.5116912977753096 \t| Accuracy: 44.338\n",
      "# Iteration   819 -> Loss: 2.5111726795041474 \t| Accuracy: 44.325\n",
      "# Iteration   820 -> Loss: 2.5105320351354745 \t| Accuracy: 44.335\n",
      "# Iteration   821 -> Loss: 2.510308078697018 \t| Accuracy: 44.325\n",
      "# Iteration   822 -> Loss: 2.5095997519795774 \t| Accuracy: 44.367\n",
      "# Iteration   823 -> Loss: 2.509330795202001 \t| Accuracy: 44.357\n",
      "# Iteration   824 -> Loss: 2.5092135059653335 \t| Accuracy: 44.368\n",
      "# Iteration   825 -> Loss: 2.5084870869564004 \t| Accuracy: 44.358\n",
      "# Iteration   826 -> Loss: 2.507925378075809 \t| Accuracy: 44.368\n",
      "# Iteration   827 -> Loss: 2.5073870593861955 \t| Accuracy: 44.373\n",
      "# Iteration   828 -> Loss: 2.506915338753983 \t| Accuracy: 44.377\n",
      "# Iteration   829 -> Loss: 2.5067922404836893 \t| Accuracy: 44.395\n",
      "# Iteration   830 -> Loss: 2.5059829727241394 \t| Accuracy: 44.397\n",
      "# Iteration   831 -> Loss: 2.5057703753821206 \t| Accuracy: 44.430\n",
      "# Iteration   832 -> Loss: 2.505181487596291 \t| Accuracy: 44.417\n",
      "# Iteration   833 -> Loss: 2.505301505406277 \t| Accuracy: 44.422\n",
      "# Iteration   834 -> Loss: 2.5045859361206846 \t| Accuracy: 44.413\n",
      "# Iteration   835 -> Loss: 2.504240366380642 \t| Accuracy: 44.423\n",
      "# Iteration   836 -> Loss: 2.5035876800450976 \t| Accuracy: 44.445\n",
      "# Iteration   837 -> Loss: 2.5032197215753746 \t| Accuracy: 44.430\n",
      "# Iteration   838 -> Loss: 2.5029232721841983 \t| Accuracy: 44.457\n",
      "# Iteration   839 -> Loss: 2.502229536144384 \t| Accuracy: 44.477\n",
      "# Iteration   840 -> Loss: 2.502144309036276 \t| Accuracy: 44.467\n",
      "# Iteration   841 -> Loss: 2.5014704232783176 \t| Accuracy: 44.482\n",
      "# Iteration   842 -> Loss: 2.500752034615167 \t| Accuracy: 44.492\n",
      "# Iteration   843 -> Loss: 2.500050034298514 \t| Accuracy: 44.500\n",
      "# Iteration   844 -> Loss: 2.4996314441801064 \t| Accuracy: 44.495\n",
      "# Iteration   845 -> Loss: 2.498921451288729 \t| Accuracy: 44.513\n",
      "# Iteration   846 -> Loss: 2.498110275548837 \t| Accuracy: 44.517\n",
      "# Iteration   847 -> Loss: 2.4972634812339995 \t| Accuracy: 44.538\n",
      "# Iteration   848 -> Loss: 2.4968601950233835 \t| Accuracy: 44.542\n",
      "# Iteration   849 -> Loss: 2.4962009112647103 \t| Accuracy: 44.563\n",
      "# Iteration   850 -> Loss: 2.495897096388928 \t| Accuracy: 44.560\n",
      "# Iteration   851 -> Loss: 2.495395739227455 \t| Accuracy: 44.565\n",
      "# Iteration   852 -> Loss: 2.494908149255479 \t| Accuracy: 44.572\n",
      "# Iteration   853 -> Loss: 2.494292947051096 \t| Accuracy: 44.543\n",
      "# Iteration   854 -> Loss: 2.4938734961228373 \t| Accuracy: 44.557\n",
      "# Iteration   855 -> Loss: 2.4937003060659517 \t| Accuracy: 44.595\n",
      "# Iteration   856 -> Loss: 2.493009107250185 \t| Accuracy: 44.583\n",
      "# Iteration   857 -> Loss: 2.4923835270067416 \t| Accuracy: 44.625\n",
      "# Iteration   858 -> Loss: 2.492072804189761 \t| Accuracy: 44.618\n",
      "# Iteration   859 -> Loss: 2.4916099890009016 \t| Accuracy: 44.638\n",
      "# Iteration   860 -> Loss: 2.4912888224284715 \t| Accuracy: 44.640\n",
      "# Iteration   861 -> Loss: 2.4904350980869405 \t| Accuracy: 44.665\n",
      "# Iteration   862 -> Loss: 2.489766297983419 \t| Accuracy: 44.688\n",
      "# Iteration   863 -> Loss: 2.4896083029536906 \t| Accuracy: 44.682\n",
      "# Iteration   864 -> Loss: 2.489298400660442 \t| Accuracy: 44.677\n",
      "# Iteration   865 -> Loss: 2.4884293218706266 \t| Accuracy: 44.713\n",
      "# Iteration   866 -> Loss: 2.4876204358874596 \t| Accuracy: 44.727\n",
      "# Iteration   867 -> Loss: 2.4872791752653183 \t| Accuracy: 44.725\n",
      "# Iteration   868 -> Loss: 2.4868419488932547 \t| Accuracy: 44.798\n",
      "# Iteration   869 -> Loss: 2.486236797804947 \t| Accuracy: 44.825\n",
      "# Iteration   870 -> Loss: 2.485877676437815 \t| Accuracy: 44.822\n",
      "# Iteration   871 -> Loss: 2.4850668078181153 \t| Accuracy: 44.870\n",
      "# Iteration   872 -> Loss: 2.4844976827289185 \t| Accuracy: 44.877\n",
      "# Iteration   873 -> Loss: 2.4837712529592055 \t| Accuracy: 44.893\n",
      "# Iteration   874 -> Loss: 2.48294176282591 \t| Accuracy: 44.927\n",
      "# Iteration   875 -> Loss: 2.4824541147036023 \t| Accuracy: 44.932\n",
      "# Iteration   876 -> Loss: 2.482099273776182 \t| Accuracy: 44.923\n",
      "# Iteration   877 -> Loss: 2.4813555669580345 \t| Accuracy: 44.940\n",
      "# Iteration   878 -> Loss: 2.4806990497268098 \t| Accuracy: 44.947\n",
      "# Iteration   879 -> Loss: 2.480334663242383 \t| Accuracy: 45.510\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Iteration   880 -> Loss: 2.479527506193371 \t| Accuracy: 45.538\n",
      "# Iteration   881 -> Loss: 2.4791212117768193 \t| Accuracy: 45.543\n",
      "# Iteration   882 -> Loss: 2.4784682906757136 \t| Accuracy: 45.548\n",
      "# Iteration   883 -> Loss: 2.477878448528554 \t| Accuracy: 45.552\n",
      "# Iteration   884 -> Loss: 2.476974214886534 \t| Accuracy: 45.577\n",
      "# Iteration   885 -> Loss: 2.4765273233619665 \t| Accuracy: 45.588\n",
      "# Iteration   886 -> Loss: 2.4761956202513753 \t| Accuracy: 45.600\n",
      "# Iteration   887 -> Loss: 2.475676020851685 \t| Accuracy: 45.582\n",
      "# Iteration   888 -> Loss: 2.4753695341009414 \t| Accuracy: 45.620\n",
      "# Iteration   889 -> Loss: 2.474782396146242 \t| Accuracy: 45.627\n",
      "# Iteration   890 -> Loss: 2.473973678432129 \t| Accuracy: 45.673\n",
      "# Iteration   891 -> Loss: 2.4734400555389997 \t| Accuracy: 45.675\n",
      "# Iteration   892 -> Loss: 2.4729972179653585 \t| Accuracy: 45.693\n",
      "# Iteration   893 -> Loss: 2.472939776693261 \t| Accuracy: 45.697\n",
      "# Iteration   894 -> Loss: 2.4719603892214925 \t| Accuracy: 45.703\n",
      "# Iteration   895 -> Loss: 2.4714462604428116 \t| Accuracy: 45.712\n",
      "# Iteration   896 -> Loss: 2.4711066311117067 \t| Accuracy: 45.722\n",
      "# Iteration   897 -> Loss: 2.4704515552769 \t| Accuracy: 45.745\n",
      "# Iteration   898 -> Loss: 2.4700241985025886 \t| Accuracy: 45.745\n",
      "# Iteration   899 -> Loss: 2.469562948111694 \t| Accuracy: 45.760\n",
      "# Iteration   900 -> Loss: 2.469176918826304 \t| Accuracy: 45.770\n",
      "# Iteration   901 -> Loss: 2.468730262743006 \t| Accuracy: 45.785\n",
      "# Iteration   902 -> Loss: 2.468360300784973 \t| Accuracy: 45.787\n",
      "# Iteration   903 -> Loss: 2.4679195851951277 \t| Accuracy: 45.798\n",
      "# Iteration   904 -> Loss: 2.4680740638785705 \t| Accuracy: 45.755\n",
      "# Iteration   905 -> Loss: 2.4669403062405326 \t| Accuracy: 45.817\n",
      "# Iteration   906 -> Loss: 2.466590453181759 \t| Accuracy: 45.828\n",
      "# Iteration   907 -> Loss: 2.4663183401671267 \t| Accuracy: 45.833\n",
      "# Iteration   908 -> Loss: 2.466163061976823 \t| Accuracy: 45.822\n",
      "# Iteration   909 -> Loss: 2.4655490751325533 \t| Accuracy: 45.817\n",
      "# Iteration   910 -> Loss: 2.4648649881958353 \t| Accuracy: 45.817\n",
      "# Iteration   911 -> Loss: 2.464330052442886 \t| Accuracy: 45.838\n",
      "# Iteration   912 -> Loss: 2.463841318494786 \t| Accuracy: 45.848\n",
      "# Iteration   913 -> Loss: 2.4640470360423667 \t| Accuracy: 45.825\n",
      "# Iteration   914 -> Loss: 2.463277639434211 \t| Accuracy: 45.872\n",
      "# Iteration   915 -> Loss: 2.462854076564531 \t| Accuracy: 45.877\n",
      "# Iteration   916 -> Loss: 2.462179050383883 \t| Accuracy: 45.885\n",
      "# Iteration   917 -> Loss: 2.46180661408931 \t| Accuracy: 45.890\n",
      "# Iteration   918 -> Loss: 2.461723696609965 \t| Accuracy: 45.870\n",
      "# Iteration   919 -> Loss: 2.4609336224950384 \t| Accuracy: 45.903\n",
      "# Iteration   920 -> Loss: 2.4604253464538233 \t| Accuracy: 45.930\n",
      "# Iteration   921 -> Loss: 2.459906142052394 \t| Accuracy: 45.922\n",
      "# Iteration   922 -> Loss: 2.459351025035901 \t| Accuracy: 45.812\n",
      "# Iteration   923 -> Loss: 2.459099369967377 \t| Accuracy: 45.823\n",
      "# Iteration   924 -> Loss: 2.458608713404403 \t| Accuracy: 45.827\n",
      "# Iteration   925 -> Loss: 2.458556048896538 \t| Accuracy: 45.813\n",
      "# Iteration   926 -> Loss: 2.4577671360922553 \t| Accuracy: 45.842\n",
      "# Iteration   927 -> Loss: 2.4572912548608725 \t| Accuracy: 45.860\n",
      "# Iteration   928 -> Loss: 2.456494278976946 \t| Accuracy: 45.865\n",
      "# Iteration   929 -> Loss: 2.4561029774499277 \t| Accuracy: 45.870\n",
      "# Iteration   930 -> Loss: 2.4556289520960473 \t| Accuracy: 45.890\n",
      "# Iteration   931 -> Loss: 2.4549036442839163 \t| Accuracy: 45.908\n",
      "# Iteration   932 -> Loss: 2.4544116891821 \t| Accuracy: 45.920\n",
      "# Iteration   933 -> Loss: 2.4541669196663256 \t| Accuracy: 45.957\n",
      "# Iteration   934 -> Loss: 2.453631272394008 \t| Accuracy: 45.975\n",
      "# Iteration   935 -> Loss: 2.45312743759009 \t| Accuracy: 45.987\n",
      "# Iteration   936 -> Loss: 2.452570909688186 \t| Accuracy: 46.010\n",
      "# Iteration   937 -> Loss: 2.452117278646056 \t| Accuracy: 45.942\n",
      "# Iteration   938 -> Loss: 2.451651004231921 \t| Accuracy: 45.975\n",
      "# Iteration   939 -> Loss: 2.4514030358820125 \t| Accuracy: 45.965\n",
      "# Iteration   940 -> Loss: 2.451192905967284 \t| Accuracy: 45.985\n",
      "# Iteration   941 -> Loss: 2.4504526065089647 \t| Accuracy: 45.978\n",
      "# Iteration   942 -> Loss: 2.4500353431353084 \t| Accuracy: 46.025\n",
      "# Iteration   943 -> Loss: 2.449886868204962 \t| Accuracy: 45.995\n",
      "# Iteration   944 -> Loss: 2.4492187235100125 \t| Accuracy: 46.017\n",
      "# Iteration   945 -> Loss: 2.4490179832472134 \t| Accuracy: 46.002\n",
      "# Iteration   946 -> Loss: 2.447952956494193 \t| Accuracy: 46.028\n",
      "# Iteration   947 -> Loss: 2.447288884113829 \t| Accuracy: 46.042\n",
      "# Iteration   948 -> Loss: 2.44731694214847 \t| Accuracy: 46.042\n",
      "# Iteration   949 -> Loss: 2.4467179968051314 \t| Accuracy: 46.045\n",
      "# Iteration   950 -> Loss: 2.4464626406311534 \t| Accuracy: 46.048\n",
      "# Iteration   951 -> Loss: 2.4456462228710274 \t| Accuracy: 46.067\n",
      "# Iteration   952 -> Loss: 2.445052008415893 \t| Accuracy: 46.092\n",
      "# Iteration   953 -> Loss: 2.4448192961704773 \t| Accuracy: 46.077\n",
      "# Iteration   954 -> Loss: 2.4444258448175322 \t| Accuracy: 46.103\n",
      "# Iteration   955 -> Loss: 2.4442426936109367 \t| Accuracy: 46.075\n",
      "# Iteration   956 -> Loss: 2.443086700932385 \t| Accuracy: 46.113\n",
      "# Iteration   957 -> Loss: 2.442530460027336 \t| Accuracy: 46.152\n",
      "# Iteration   958 -> Loss: 2.441970402313398 \t| Accuracy: 46.143\n",
      "# Iteration   959 -> Loss: 2.4414004389837913 \t| Accuracy: 46.177\n",
      "# Iteration   960 -> Loss: 2.440886589594394 \t| Accuracy: 46.178\n",
      "# Iteration   961 -> Loss: 2.4404170832290633 \t| Accuracy: 46.198\n",
      "# Iteration   962 -> Loss: 2.440447075366788 \t| Accuracy: 46.160\n",
      "# Iteration   963 -> Loss: 2.4393895221405626 \t| Accuracy: 46.208\n",
      "# Iteration   964 -> Loss: 2.4390539137063723 \t| Accuracy: 46.207\n",
      "# Iteration   965 -> Loss: 2.438895582566142 \t| Accuracy: 46.205\n",
      "# Iteration   966 -> Loss: 2.438129528982668 \t| Accuracy: 46.238\n",
      "# Iteration   967 -> Loss: 2.437779853414161 \t| Accuracy: 46.225\n",
      "# Iteration   968 -> Loss: 2.436982043499868 \t| Accuracy: 46.295\n",
      "# Iteration   969 -> Loss: 2.436749449737307 \t| Accuracy: 46.252\n",
      "# Iteration   970 -> Loss: 2.4355774146740483 \t| Accuracy: 46.312\n",
      "# Iteration   971 -> Loss: 2.4352118002921266 \t| Accuracy: 46.315\n",
      "# Iteration   972 -> Loss: 2.434555268443653 \t| Accuracy: 46.335\n",
      "# Iteration   973 -> Loss: 2.434298013289879 \t| Accuracy: 46.355\n",
      "# Iteration   974 -> Loss: 2.433854978018496 \t| Accuracy: 46.357\n",
      "# Iteration   975 -> Loss: 2.4328971067323018 \t| Accuracy: 46.380\n",
      "# Iteration   976 -> Loss: 2.4323519841023127 \t| Accuracy: 46.407\n",
      "# Iteration   977 -> Loss: 2.432056819446857 \t| Accuracy: 46.390\n",
      "# Iteration   978 -> Loss: 2.431841023673511 \t| Accuracy: 46.408\n",
      "# Iteration   979 -> Loss: 2.4312079549667787 \t| Accuracy: 46.407\n",
      "# Iteration   980 -> Loss: 2.4305614328592697 \t| Accuracy: 46.458\n",
      "# Iteration   981 -> Loss: 2.42989981131961 \t| Accuracy: 46.452\n",
      "# Iteration   982 -> Loss: 2.4293040705161073 \t| Accuracy: 46.478\n",
      "# Iteration   983 -> Loss: 2.428987433257689 \t| Accuracy: 46.488\n",
      "# Iteration   984 -> Loss: 2.428426556670048 \t| Accuracy: 46.493\n",
      "# Iteration   985 -> Loss: 2.4276662632253583 \t| Accuracy: 46.522\n",
      "# Iteration   986 -> Loss: 2.427491013753746 \t| Accuracy: 46.520\n",
      "# Iteration   987 -> Loss: 2.4271689642953898 \t| Accuracy: 46.537\n",
      "# Iteration   988 -> Loss: 2.426524930593037 \t| Accuracy: 46.570\n",
      "# Iteration   989 -> Loss: 2.425393458890636 \t| Accuracy: 46.592\n",
      "# Iteration   990 -> Loss: 2.424835787359956 \t| Accuracy: 46.603\n",
      "# Iteration   991 -> Loss: 2.4243685174094267 \t| Accuracy: 46.607\n",
      "# Iteration   992 -> Loss: 2.4238794054818498 \t| Accuracy: 46.627\n",
      "# Iteration   993 -> Loss: 2.4236032136917136 \t| Accuracy: 46.643\n",
      "# Iteration   994 -> Loss: 2.4231943898822506 \t| Accuracy: 46.660\n",
      "# Iteration   995 -> Loss: 2.422772659211507 \t| Accuracy: 46.663\n",
      "# Iteration   996 -> Loss: 2.4227442740826506 \t| Accuracy: 46.652\n",
      "# Iteration   997 -> Loss: 2.421817751877091 \t| Accuracy: 46.702\n",
      "# Iteration   998 -> Loss: 2.4209490572324905 \t| Accuracy: 46.723\n",
      "# Iteration   999 -> Loss: 2.420491202587389 \t| Accuracy: 46.750\n",
      "# Iteration  1000 -> Loss: 2.4203732294763634 \t| Accuracy: 46.738\n",
      "# Iteration  1001 -> Loss: 2.420017884429001 \t| Accuracy: 46.755\n",
      "# Iteration  1002 -> Loss: 2.419378573466766 \t| Accuracy: 46.768\n",
      "# Iteration  1003 -> Loss: 2.4188626416934897 \t| Accuracy: 46.777\n",
      "# Iteration  1004 -> Loss: 2.4183318132522813 \t| Accuracy: 46.762\n",
      "# Iteration  1005 -> Loss: 2.417771334442023 \t| Accuracy: 46.793\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Iteration  1006 -> Loss: 2.4175736235528094 \t| Accuracy: 46.798\n",
      "# Iteration  1007 -> Loss: 2.4170756452050126 \t| Accuracy: 46.803\n",
      "# Iteration  1008 -> Loss: 2.4164940734149956 \t| Accuracy: 46.815\n",
      "# Iteration  1009 -> Loss: 2.416156663923709 \t| Accuracy: 46.825\n",
      "# Iteration  1010 -> Loss: 2.4155800041957813 \t| Accuracy: 46.847\n",
      "# Iteration  1011 -> Loss: 2.4149648203250376 \t| Accuracy: 46.857\n",
      "# Iteration  1012 -> Loss: 2.4147456464689316 \t| Accuracy: 46.882\n",
      "# Iteration  1013 -> Loss: 2.414173273870408 \t| Accuracy: 46.872\n",
      "# Iteration  1014 -> Loss: 2.413868882977265 \t| Accuracy: 46.890\n",
      "# Iteration  1015 -> Loss: 2.4133980143826563 \t| Accuracy: 46.880\n",
      "# Iteration  1016 -> Loss: 2.4128843380166147 \t| Accuracy: 46.907\n",
      "# Iteration  1017 -> Loss: 2.4123238362781128 \t| Accuracy: 46.933\n",
      "# Iteration  1018 -> Loss: 2.412165867508362 \t| Accuracy: 46.908\n",
      "# Iteration  1019 -> Loss: 2.4114145385417602 \t| Accuracy: 46.958\n",
      "# Iteration  1020 -> Loss: 2.411022614100221 \t| Accuracy: 46.945\n",
      "# Iteration  1021 -> Loss: 2.410704841944089 \t| Accuracy: 46.963\n",
      "# Iteration  1022 -> Loss: 2.4104823014471273 \t| Accuracy: 46.968\n",
      "# Iteration  1023 -> Loss: 2.410210584410143 \t| Accuracy: 46.958\n",
      "# Iteration  1024 -> Loss: 2.409899988674077 \t| Accuracy: 46.938\n",
      "# Iteration  1025 -> Loss: 2.409235512016252 \t| Accuracy: 46.988\n",
      "# Iteration  1026 -> Loss: 2.4088718863615997 \t| Accuracy: 46.992\n",
      "# Iteration  1027 -> Loss: 2.408341336974945 \t| Accuracy: 47.017\n",
      "# Iteration  1028 -> Loss: 2.4079584375268386 \t| Accuracy: 47.030\n",
      "# Iteration  1029 -> Loss: 2.4073347459477046 \t| Accuracy: 47.055\n",
      "# Iteration  1030 -> Loss: 2.4071240715732425 \t| Accuracy: 47.052\n",
      "# Iteration  1031 -> Loss: 2.4071098033906666 \t| Accuracy: 47.035\n",
      "# Iteration  1032 -> Loss: 2.4068686247598565 \t| Accuracy: 47.052\n",
      "# Iteration  1033 -> Loss: 2.406068569751517 \t| Accuracy: 47.060\n",
      "# Iteration  1034 -> Loss: 2.405542117049215 \t| Accuracy: 47.073\n",
      "# Iteration  1035 -> Loss: 2.4053931487575073 \t| Accuracy: 47.087\n",
      "# Iteration  1036 -> Loss: 2.404800208737136 \t| Accuracy: 47.087\n",
      "# Iteration  1037 -> Loss: 2.4044800554828467 \t| Accuracy: 47.092\n",
      "# Iteration  1038 -> Loss: 2.4042539206330846 \t| Accuracy: 47.087\n",
      "# Iteration  1039 -> Loss: 2.403746888214598 \t| Accuracy: 47.093\n",
      "# Iteration  1040 -> Loss: 2.402988468875641 \t| Accuracy: 47.127\n",
      "# Iteration  1041 -> Loss: 2.4027946983987496 \t| Accuracy: 47.133\n",
      "# Iteration  1042 -> Loss: 2.402410036115867 \t| Accuracy: 47.142\n",
      "# Iteration  1043 -> Loss: 2.4023240028583315 \t| Accuracy: 47.127\n",
      "# Iteration  1044 -> Loss: 2.40181993356658 \t| Accuracy: 47.117\n",
      "# Iteration  1045 -> Loss: 2.401359107300669 \t| Accuracy: 47.158\n",
      "# Iteration  1046 -> Loss: 2.4007660062073883 \t| Accuracy: 47.135\n",
      "# Iteration  1047 -> Loss: 2.4001900720349365 \t| Accuracy: 47.183\n",
      "# Iteration  1048 -> Loss: 2.3998705521432937 \t| Accuracy: 47.183\n",
      "# Iteration  1049 -> Loss: 2.3994954863056357 \t| Accuracy: 47.207\n",
      "# Iteration  1050 -> Loss: 2.39911864287202 \t| Accuracy: 47.205\n",
      "# Iteration  1051 -> Loss: 2.398665431152769 \t| Accuracy: 47.238\n",
      "# Iteration  1052 -> Loss: 2.3985391222441774 \t| Accuracy: 47.200\n",
      "# Iteration  1053 -> Loss: 2.3977380960343955 \t| Accuracy: 47.228\n",
      "# Iteration  1054 -> Loss: 2.397459987116836 \t| Accuracy: 47.238\n",
      "# Iteration  1055 -> Loss: 2.397727972493788 \t| Accuracy: 47.198\n",
      "# Iteration  1056 -> Loss: 2.3972169597793935 \t| Accuracy: 47.245\n",
      "# Iteration  1057 -> Loss: 2.3966019792878885 \t| Accuracy: 47.237\n",
      "# Iteration  1058 -> Loss: 2.3962937257315184 \t| Accuracy: 47.252\n",
      "# Iteration  1059 -> Loss: 2.395677146333752 \t| Accuracy: 47.267\n",
      "# Iteration  1060 -> Loss: 2.3953136114187656 \t| Accuracy: 47.255\n",
      "# Iteration  1061 -> Loss: 2.3951614788771463 \t| Accuracy: 47.262\n",
      "# Iteration  1062 -> Loss: 2.3943393240678246 \t| Accuracy: 47.293\n",
      "# Iteration  1063 -> Loss: 2.3939123141654375 \t| Accuracy: 47.277\n",
      "# Iteration  1064 -> Loss: 2.3937683356297117 \t| Accuracy: 47.303\n",
      "# Iteration  1065 -> Loss: 2.393197378617878 \t| Accuracy: 47.295\n",
      "# Iteration  1066 -> Loss: 2.3926758008889872 \t| Accuracy: 47.320\n",
      "# Iteration  1067 -> Loss: 2.3925104088757423 \t| Accuracy: 47.317\n",
      "# Iteration  1068 -> Loss: 2.3919213996071735 \t| Accuracy: 47.323\n",
      "# Iteration  1069 -> Loss: 2.3913127162998453 \t| Accuracy: 47.325\n",
      "# Iteration  1070 -> Loss: 2.390842638449085 \t| Accuracy: 47.398\n",
      "# Iteration  1071 -> Loss: 2.39036102340478 \t| Accuracy: 47.407\n",
      "# Iteration  1072 -> Loss: 2.3902192162763582 \t| Accuracy: 47.412\n",
      "# Iteration  1073 -> Loss: 2.389970391569722 \t| Accuracy: 47.420\n",
      "# Iteration  1074 -> Loss: 2.38950870943657 \t| Accuracy: 47.430\n",
      "# Iteration  1075 -> Loss: 2.3895254224252143 \t| Accuracy: 47.427\n",
      "# Iteration  1076 -> Loss: 2.389007889837919 \t| Accuracy: 47.435\n",
      "# Iteration  1077 -> Loss: 2.3887022942510083 \t| Accuracy: 47.453\n",
      "# Iteration  1078 -> Loss: 2.3879866130936915 \t| Accuracy: 47.468\n",
      "# Iteration  1079 -> Loss: 2.387573948399236 \t| Accuracy: 47.460\n",
      "# Iteration  1080 -> Loss: 2.387106423970363 \t| Accuracy: 47.607\n",
      "# Iteration  1081 -> Loss: 2.3870520033476117 \t| Accuracy: 47.603\n",
      "# Iteration  1082 -> Loss: 2.3868749223005534 \t| Accuracy: 47.615\n",
      "# Iteration  1083 -> Loss: 2.386678848317332 \t| Accuracy: 47.595\n",
      "# Iteration  1084 -> Loss: 2.386372996282045 \t| Accuracy: 47.618\n",
      "# Iteration  1085 -> Loss: 2.385814164888444 \t| Accuracy: 47.648\n",
      "# Iteration  1086 -> Loss: 2.3851526205827462 \t| Accuracy: 47.693\n",
      "# Iteration  1087 -> Loss: 2.3845418538477516 \t| Accuracy: 47.698\n",
      "# Iteration  1088 -> Loss: 2.3839789685931367 \t| Accuracy: 47.720\n",
      "# Iteration  1089 -> Loss: 2.383865188656198 \t| Accuracy: 47.725\n",
      "# Iteration  1090 -> Loss: 2.383912111618366 \t| Accuracy: 47.715\n",
      "# Iteration  1091 -> Loss: 2.3838600642217243 \t| Accuracy: 47.685\n",
      "# Iteration  1092 -> Loss: 2.3828677973056243 \t| Accuracy: 47.735\n",
      "# Iteration  1093 -> Loss: 2.3825672741914268 \t| Accuracy: 47.752\n",
      "# Iteration  1094 -> Loss: 2.3822830851041297 \t| Accuracy: 47.752\n",
      "# Iteration  1095 -> Loss: 2.3820966245569584 \t| Accuracy: 47.765\n",
      "# Iteration  1096 -> Loss: 2.3814966268768294 \t| Accuracy: 47.783\n",
      "# Iteration  1097 -> Loss: 2.3810832128465 \t| Accuracy: 47.795\n",
      "# Iteration  1098 -> Loss: 2.380545484275513 \t| Accuracy: 47.797\n",
      "# Iteration  1099 -> Loss: 2.38030595392754 \t| Accuracy: 47.828\n",
      "# Iteration  1100 -> Loss: 2.379465682732299 \t| Accuracy: 47.828\n",
      "# Iteration  1101 -> Loss: 2.3794135428186225 \t| Accuracy: 47.852\n",
      "# Iteration  1102 -> Loss: 2.3787015611128974 \t| Accuracy: 47.905\n",
      "# Iteration  1103 -> Loss: 2.378093141133183 \t| Accuracy: 47.908\n",
      "# Iteration  1104 -> Loss: 2.377732083970989 \t| Accuracy: 47.935\n",
      "# Iteration  1105 -> Loss: 2.3776700522898 \t| Accuracy: 47.928\n",
      "# Iteration  1106 -> Loss: 2.377370952375271 \t| Accuracy: 47.932\n",
      "# Iteration  1107 -> Loss: 2.3765762854213435 \t| Accuracy: 47.995\n",
      "# Iteration  1108 -> Loss: 2.3760641429861367 \t| Accuracy: 47.987\n",
      "# Iteration  1109 -> Loss: 2.375939203745187 \t| Accuracy: 47.978\n",
      "# Iteration  1110 -> Loss: 2.3752403609346904 \t| Accuracy: 48.002\n",
      "# Iteration  1111 -> Loss: 2.374680729463985 \t| Accuracy: 48.010\n",
      "# Iteration  1112 -> Loss: 2.3744713958468844 \t| Accuracy: 48.000\n",
      "# Iteration  1113 -> Loss: 2.374957212384784 \t| Accuracy: 48.008\n",
      "# Iteration  1114 -> Loss: 2.374541053584578 \t| Accuracy: 47.990\n",
      "# Iteration  1115 -> Loss: 2.373882818476225 \t| Accuracy: 48.027\n",
      "# Iteration  1116 -> Loss: 2.372871685829282 \t| Accuracy: 48.068\n",
      "# Iteration  1117 -> Loss: 2.372663622337456 \t| Accuracy: 48.058\n",
      "# Iteration  1118 -> Loss: 2.372331405652222 \t| Accuracy: 48.088\n",
      "# Iteration  1119 -> Loss: 2.371800436110224 \t| Accuracy: 48.073\n",
      "# Iteration  1120 -> Loss: 2.371405345202229 \t| Accuracy: 48.087\n",
      "# Iteration  1121 -> Loss: 2.3713122963816176 \t| Accuracy: 48.095\n",
      "# Iteration  1122 -> Loss: 2.3711524803860513 \t| Accuracy: 48.087\n",
      "# Iteration  1123 -> Loss: 2.370856129277482 \t| Accuracy: 48.125\n",
      "# Iteration  1124 -> Loss: 2.3697876494473795 \t| Accuracy: 48.152\n",
      "# Iteration  1125 -> Loss: 2.3697332433423597 \t| Accuracy: 48.365\n",
      "# Iteration  1126 -> Loss: 2.3691678625666213 \t| Accuracy: 48.363\n",
      "# Iteration  1127 -> Loss: 2.3685031941184596 \t| Accuracy: 48.410\n",
      "# Iteration  1128 -> Loss: 2.367979882855387 \t| Accuracy: 48.417\n",
      "# Iteration  1129 -> Loss: 2.3674864148403123 \t| Accuracy: 48.422\n",
      "# Iteration  1130 -> Loss: 2.3672016999099945 \t| Accuracy: 48.452\n",
      "# Iteration  1131 -> Loss: 2.3668058256488265 \t| Accuracy: 48.438\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Iteration  1132 -> Loss: 2.366367646551208 \t| Accuracy: 48.453\n",
      "# Iteration  1133 -> Loss: 2.3658040845422046 \t| Accuracy: 48.482\n",
      "# Iteration  1134 -> Loss: 2.365503331786766 \t| Accuracy: 48.453\n",
      "# Iteration  1135 -> Loss: 2.3646703236939217 \t| Accuracy: 48.475\n",
      "# Iteration  1136 -> Loss: 2.36393643536235 \t| Accuracy: 48.522\n",
      "# Iteration  1137 -> Loss: 2.363620746727259 \t| Accuracy: 48.518\n",
      "# Iteration  1138 -> Loss: 2.3632925112605756 \t| Accuracy: 48.535\n",
      "# Iteration  1139 -> Loss: 2.362944809169129 \t| Accuracy: 48.568\n",
      "# Iteration  1140 -> Loss: 2.3626591387241134 \t| Accuracy: 48.533\n",
      "# Iteration  1141 -> Loss: 2.3615492565322054 \t| Accuracy: 48.600\n",
      "# Iteration  1142 -> Loss: 2.3613309311815542 \t| Accuracy: 48.585\n",
      "# Iteration  1143 -> Loss: 2.361215833816067 \t| Accuracy: 48.595\n",
      "# Iteration  1144 -> Loss: 2.3604965349227736 \t| Accuracy: 48.622\n",
      "# Iteration  1145 -> Loss: 2.3598896614723093 \t| Accuracy: 48.650\n",
      "# Iteration  1146 -> Loss: 2.359595914694571 \t| Accuracy: 48.652\n",
      "# Iteration  1147 -> Loss: 2.3594798543447673 \t| Accuracy: 48.642\n",
      "# Iteration  1148 -> Loss: 2.3589061554483464 \t| Accuracy: 48.660\n",
      "# Iteration  1149 -> Loss: 2.357847684363895 \t| Accuracy: 48.713\n",
      "# Iteration  1150 -> Loss: 2.357617188015715 \t| Accuracy: 48.783\n",
      "# Iteration  1151 -> Loss: 2.3571109029740263 \t| Accuracy: 48.805\n",
      "# Iteration  1152 -> Loss: 2.356586386209261 \t| Accuracy: 48.822\n",
      "# Iteration  1153 -> Loss: 2.356293799100828 \t| Accuracy: 48.850\n",
      "# Iteration  1154 -> Loss: 2.3561966639436878 \t| Accuracy: 48.838\n",
      "# Iteration  1155 -> Loss: 2.355472889187285 \t| Accuracy: 48.880\n",
      "# Iteration  1156 -> Loss: 2.3551193339198857 \t| Accuracy: 48.890\n",
      "# Iteration  1157 -> Loss: 2.354602838474068 \t| Accuracy: 48.910\n",
      "# Iteration  1158 -> Loss: 2.3542946290640865 \t| Accuracy: 48.903\n",
      "# Iteration  1159 -> Loss: 2.353606777653608 \t| Accuracy: 48.922\n",
      "# Iteration  1160 -> Loss: 2.3530015386342917 \t| Accuracy: 48.940\n",
      "# Iteration  1161 -> Loss: 2.35238919904982 \t| Accuracy: 48.985\n",
      "# Iteration  1162 -> Loss: 2.352121471013542 \t| Accuracy: 48.972\n",
      "# Iteration  1163 -> Loss: 2.351800382958706 \t| Accuracy: 48.987\n",
      "# Iteration  1164 -> Loss: 2.350904498067612 \t| Accuracy: 49.037\n",
      "# Iteration  1165 -> Loss: 2.350511423017508 \t| Accuracy: 49.003\n",
      "# Iteration  1166 -> Loss: 2.3502342800012115 \t| Accuracy: 49.013\n",
      "# Iteration  1167 -> Loss: 2.3500845175457177 \t| Accuracy: 49.028\n",
      "# Iteration  1168 -> Loss: 2.3490442326033762 \t| Accuracy: 49.095\n",
      "# Iteration  1169 -> Loss: 2.3486572657310893 \t| Accuracy: 49.097\n",
      "# Iteration  1170 -> Loss: 2.3478070531638187 \t| Accuracy: 49.145\n",
      "# Iteration  1171 -> Loss: 2.347117177526489 \t| Accuracy: 49.142\n",
      "# Iteration  1172 -> Loss: 2.3464685563830283 \t| Accuracy: 49.183\n",
      "# Iteration  1173 -> Loss: 2.346196146644739 \t| Accuracy: 49.175\n",
      "# Iteration  1174 -> Loss: 2.3452674238608506 \t| Accuracy: 49.235\n",
      "# Iteration  1175 -> Loss: 2.345018650631 \t| Accuracy: 49.233\n",
      "# Iteration  1176 -> Loss: 2.3445225127861797 \t| Accuracy: 49.243\n",
      "# Iteration  1177 -> Loss: 2.3442879264723135 \t| Accuracy: 49.255\n",
      "# Iteration  1178 -> Loss: 2.3439971368769204 \t| Accuracy: 49.248\n",
      "# Iteration  1179 -> Loss: 2.3432209641529966 \t| Accuracy: 49.313\n",
      "# Iteration  1180 -> Loss: 2.343165177105098 \t| Accuracy: 49.280\n",
      "# Iteration  1181 -> Loss: 2.342255142798066 \t| Accuracy: 49.328\n",
      "# Iteration  1182 -> Loss: 2.3419133579421056 \t| Accuracy: 49.315\n",
      "# Iteration  1183 -> Loss: 2.3419355358845326 \t| Accuracy: 49.317\n",
      "# Iteration  1184 -> Loss: 2.3413497078964998 \t| Accuracy: 49.343\n",
      "# Iteration  1185 -> Loss: 2.3410053754034403 \t| Accuracy: 49.365\n",
      "# Iteration  1186 -> Loss: 2.340727225805391 \t| Accuracy: 49.360\n",
      "# Iteration  1187 -> Loss: 2.3398514886117026 \t| Accuracy: 49.405\n",
      "# Iteration  1188 -> Loss: 2.3395197850618077 \t| Accuracy: 49.398\n",
      "# Iteration  1189 -> Loss: 2.3393234196530766 \t| Accuracy: 49.392\n",
      "# Iteration  1190 -> Loss: 2.3389144898948766 \t| Accuracy: 49.403\n",
      "# Iteration  1191 -> Loss: 2.3382977169675034 \t| Accuracy: 49.443\n",
      "# Iteration  1192 -> Loss: 2.337875172923116 \t| Accuracy: 49.435\n",
      "# Iteration  1193 -> Loss: 2.337159142138956 \t| Accuracy: 49.477\n",
      "# Iteration  1194 -> Loss: 2.336746307868078 \t| Accuracy: 49.465\n",
      "# Iteration  1195 -> Loss: 2.3368477378166386 \t| Accuracy: 49.483\n",
      "# Iteration  1196 -> Loss: 2.336827465738243 \t| Accuracy: 49.468\n",
      "# Iteration  1197 -> Loss: 2.336123280020473 \t| Accuracy: 49.513\n",
      "# Iteration  1198 -> Loss: 2.335186080138177 \t| Accuracy: 49.535\n",
      "# Iteration  1199 -> Loss: 2.3350253053547982 \t| Accuracy: 49.535\n",
      "# Iteration  1200 -> Loss: 2.3346747830100227 \t| Accuracy: 49.552\n",
      "# Iteration  1201 -> Loss: 2.333967379112875 \t| Accuracy: 49.575\n",
      "# Iteration  1202 -> Loss: 2.3334649030101433 \t| Accuracy: 49.605\n",
      "# Iteration  1203 -> Loss: 2.332788096624348 \t| Accuracy: 49.605\n",
      "# Iteration  1204 -> Loss: 2.332374877751845 \t| Accuracy: 49.642\n",
      "# Iteration  1205 -> Loss: 2.331842326553737 \t| Accuracy: 49.665\n",
      "# Iteration  1206 -> Loss: 2.33126356268875 \t| Accuracy: 49.657\n",
      "# Iteration  1207 -> Loss: 2.3306930009395743 \t| Accuracy: 49.735\n",
      "# Iteration  1208 -> Loss: 2.330147780611069 \t| Accuracy: 49.740\n",
      "# Iteration  1209 -> Loss: 2.3295778501462903 \t| Accuracy: 49.753\n",
      "# Iteration  1210 -> Loss: 2.3290783033528717 \t| Accuracy: 49.772\n",
      "# Iteration  1211 -> Loss: 2.3287526011345836 \t| Accuracy: 49.788\n",
      "# Iteration  1212 -> Loss: 2.3285709878706378 \t| Accuracy: 49.790\n",
      "# Iteration  1213 -> Loss: 2.3279151336595003 \t| Accuracy: 49.803\n",
      "# Iteration  1214 -> Loss: 2.3278094127592306 \t| Accuracy: 49.818\n",
      "# Iteration  1215 -> Loss: 2.3270436743905267 \t| Accuracy: 49.857\n",
      "# Iteration  1216 -> Loss: 2.3269232702625526 \t| Accuracy: 49.825\n",
      "# Iteration  1217 -> Loss: 2.3262740586124093 \t| Accuracy: 49.860\n",
      "# Iteration  1218 -> Loss: 2.326223696413301 \t| Accuracy: 49.843\n",
      "# Iteration  1219 -> Loss: 2.325752934678447 \t| Accuracy: 49.853\n",
      "# Iteration  1220 -> Loss: 2.325315548202874 \t| Accuracy: 49.853\n",
      "# Iteration  1221 -> Loss: 2.3243498229700275 \t| Accuracy: 49.885\n",
      "# Iteration  1222 -> Loss: 2.3239274905550698 \t| Accuracy: 49.922\n",
      "# Iteration  1223 -> Loss: 2.324021951940906 \t| Accuracy: 49.885\n",
      "# Iteration  1224 -> Loss: 2.323213156863589 \t| Accuracy: 49.922\n",
      "# Iteration  1225 -> Loss: 2.32299665056921 \t| Accuracy: 49.945\n",
      "# Iteration  1226 -> Loss: 2.322202744307872 \t| Accuracy: 49.952\n",
      "# Iteration  1227 -> Loss: 2.3215836051012424 \t| Accuracy: 49.957\n",
      "# Iteration  1228 -> Loss: 2.3212632954613572 \t| Accuracy: 49.982\n",
      "# Iteration  1229 -> Loss: 2.321145788329233 \t| Accuracy: 49.953\n",
      "# Iteration  1230 -> Loss: 2.320581607692359 \t| Accuracy: 49.987\n",
      "# Iteration  1231 -> Loss: 2.320192560582459 \t| Accuracy: 49.985\n",
      "# Iteration  1232 -> Loss: 2.320182318655718 \t| Accuracy: 50.013\n",
      "# Iteration  1233 -> Loss: 2.3188269382644857 \t| Accuracy: 50.043\n",
      "# Iteration  1234 -> Loss: 2.3187146792221007 \t| Accuracy: 50.042\n",
      "# Iteration  1235 -> Loss: 2.318068505424927 \t| Accuracy: 50.058\n",
      "# Iteration  1236 -> Loss: 2.3177727948842555 \t| Accuracy: 50.057\n",
      "# Iteration  1237 -> Loss: 2.3175489169448396 \t| Accuracy: 50.068\n",
      "# Iteration  1238 -> Loss: 2.3172602840079404 \t| Accuracy: 50.117\n",
      "# Iteration  1239 -> Loss: 2.316195031943398 \t| Accuracy: 50.133\n",
      "# Iteration  1240 -> Loss: 2.315940525032045 \t| Accuracy: 50.153\n",
      "# Iteration  1241 -> Loss: 2.3155335751985207 \t| Accuracy: 50.153\n",
      "# Iteration  1242 -> Loss: 2.3154377008094715 \t| Accuracy: 50.153\n",
      "# Iteration  1243 -> Loss: 2.3146310199226217 \t| Accuracy: 50.172\n",
      "# Iteration  1244 -> Loss: 2.3143941467084073 \t| Accuracy: 50.228\n",
      "# Iteration  1245 -> Loss: 2.3140368136023937 \t| Accuracy: 50.223\n",
      "# Iteration  1246 -> Loss: 2.3140270664362714 \t| Accuracy: 50.253\n",
      "\n",
      "!!! Convergence reached !!!\n",
      "# Iteration 1245 #\n",
      "Cross-Entropy Loss:      2.3140270664362714\n",
      "Accuracy (Training Set): 50.253%\n",
      "Weights\n",
      "S -> H:\n",
      " [[-1.3252345  -1.05556705 -0.2353226  ... -1.41048214 -1.54707226\n",
      "   0.37177735]\n",
      " [-0.47617447  0.65582558 -0.88966674 ...  0.11871211 -0.62968451\n",
      "  -1.28111377]\n",
      " [ 0.62263485  0.83822567  0.52327135 ... -0.92012374  0.11670495\n",
      "   0.47938016]\n",
      " ...\n",
      " [ 0.91172053  0.25104513 -1.51479238 ...  0.84992821  1.23260175\n",
      "   0.18833845]\n",
      " [-0.48037523 -0.15969927 -0.90882356 ...  0.15253514  1.22950193\n",
      "   0.5360275 ]\n",
      " [ 0.75621875 -0.19863944  1.25874298 ... -1.44509926 -0.38698545\n",
      "   1.70846805]] \n",
      "H -> O:\n",
      " [[-5.38124546e-01 -3.79393346e-01 -1.08256107e-01 -1.03911775e+00\n",
      "  -1.98776470e-01 -4.50359128e-01 -1.41263982e+00 -9.21142006e-01\n",
      "  -2.71799196e+00  8.35374878e-01]\n",
      " [-8.31445013e-01 -2.02938135e+00 -1.39203002e+00  2.23094200e-01\n",
      "   1.80432494e+00 -4.45769046e-01 -4.88377733e-01  2.04105331e+00\n",
      "  -1.13915716e+00  1.59065865e+00]\n",
      " [ 9.28433532e-01 -1.47802720e+00  4.62004951e-01  1.84085316e-01\n",
      "  -2.69903689e-01 -1.38850987e-02 -9.45187665e-01 -4.87676436e-01\n",
      "  -1.61742278e-01 -1.36183044e-01]\n",
      " [-7.38869243e-01 -6.17984292e-02 -1.08647769e+00  7.50647085e-01\n",
      "  -1.56421318e+00  6.19960852e-01 -1.08253402e+00 -6.38109392e-01\n",
      "   5.68342765e-01 -6.81107430e-01]\n",
      " [ 5.29214716e-01 -2.64981961e+00 -7.58373454e-01 -8.99625685e-01\n",
      "  -1.62611418e-01  5.01639926e-01  5.54538008e-02 -7.11887593e-01\n",
      "   8.51697191e-01 -5.57168610e-01]\n",
      " [-4.81840843e-02 -3.18187554e-01 -1.25280277e+00  5.39246975e-02\n",
      "  -1.45688418e+00 -1.10999979e+00 -3.15161825e-01 -1.32987286e+00\n",
      "  -2.18709470e-01 -3.13837795e+00]\n",
      " [-1.36568973e+00 -1.02682823e+00 -2.79009175e-01  2.86182982e-01\n",
      "  -1.30775654e+00 -1.33133792e-01 -1.50195022e-01 -7.84964315e-02\n",
      "  -2.25288001e-03 -1.22242707e+00]\n",
      " [-2.63119421e+00  1.90198012e+00 -6.31916490e-01 -5.99903765e-01\n",
      "  -6.29189945e-01 -4.96755883e-01 -1.54367274e+00  9.58503843e-01\n",
      "   8.28168697e-01  7.96968121e-01]\n",
      " [ 6.75291086e-01  3.82654007e-01  1.60819737e+00  1.11563405e-01\n",
      "   4.92969232e-01 -1.22081051e-01  6.88521868e-01  5.35435618e-01\n",
      "  -1.46293778e+00 -2.14699493e-01]\n",
      " [-2.12974484e+00  7.06776829e-01  7.78891479e-01 -9.30707151e-01\n",
      "   9.79767355e-01 -1.51612316e+00  3.61826131e-01 -1.15155928e+00\n",
      "  -2.85910460e-01  6.21996741e-01]\n",
      " [ 1.32620441e+00 -7.88479211e-01  6.88848209e-01 -2.09724371e+00\n",
      "  -3.00536235e-01 -1.08649611e+00  1.12028696e+00 -8.22050464e-01\n",
      "  -8.67707151e-01 -1.08397854e-02]]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Creates the Neural Network with 2 Input Neurons, 10 Hidden Neurons and 4 Output Neuron\n",
    "# (The weights are randomnly initiated)\n",
    "brain = MultilayerPerceptron(n_neurons=[28*28, 10, 10])\n",
    "\n",
    "# Run the Gradient Descent and returns the Error History for the training.\n",
    "errorHist = brain.train(X_train, y_train_ohe, alpha=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Testing the Network\n",
    "\n",
    "Once our network is properly trained, we can use it to classify the digits. In this case, since we are dealing with high-dimensional data, we will not be able to generate visualizations for the entire space as we did before, since it would be infeasible to try to plot the digits into two-dimensional spaces.\n",
    "\n",
    "We can, however, try to sample some digits from the dataset and classify them, in order to see the network in practice. Remember, also, that we can rely in the _Evaluation Metrics_ to understand the quality of the models we trained.\n",
    "\n",
    "**In the cell below, try to sample some examples from the dataset, classify them using the network, and visualize the correct answer to evaluate the results:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels:\n",
      "\n",
      "\t[7]\t[7]\t[4]\t[7]\t[4]\t\n",
      "\t[6]\t[2]\t[0]\t[1]\t[1]\t\n",
      "\t[3]\t[0]\t[9]\t[4]\t[7]\t\n",
      "\n",
      "Images:\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 15 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Exhibits the images in the dataset\n",
    "print(\"Labels:\")\n",
    "\n",
    "plt.figure()\n",
    "for i in range(0,3):\n",
    "    print(end=\"\\n\\t\")\n",
    "    for j in range(0,5):\n",
    "        plt.subplot(3,5,j + 5*i +1)\n",
    "        \n",
    "        # Selects a random example from the dataset\n",
    "        idx = np.random.randint(0, 60000)\n",
    "        \n",
    "        # Classify the example sampled\n",
    "        y_hat = brain.predict(X_train[:,idx:idx+1])\n",
    "        print(y_hat, end=\"\\t\")\n",
    "        \n",
    "        plt.imshow(X_train[:,idx].reshape(28,28))\n",
    "        plt.axis('off')\n",
    "\n",
    "\n",
    "print(\"\\n\\nImages:\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also evaluate the quality of the model by classifying the Testing Dataset and evaluating the overall accuracy over all examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## RESULTS ##\n",
      "Overall Accuracy: 51.690\n"
     ]
    }
   ],
   "source": [
    "# Prediction for all the data in the Testing Set\n",
    "y_hat = brain.predict(X_test)\n",
    "\n",
    "# Calculating the overall accuracy\n",
    "overallAcc = brain.accuracy(y_test_ohe, y_hat)\n",
    "\n",
    "print(\"## RESULTS ##\")\n",
    "print(\"Overall Accuracy: {0:.3f}\".format(overallAcc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
