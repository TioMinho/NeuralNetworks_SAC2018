{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks - A Practical Introduction\n",
    "by _Minho Menezes_  \n",
    "\n",
    "---\n",
    "\n",
    "## Neural Networks - Learning\n",
    "\n",
    "In this second notebook, we build the intelligent algorithms that will learn the optimal set of weights for the Neural Network task. This is the Supervisioned Learning approach, and it is fundamental in Machine Learning.\n",
    "\n",
    "* [1. Evaluating Performance](#1.-Evaluating-Performance)  \n",
    "* [2. Backpropagation](#2.-Backpropagation)  \n",
    "* [3. Gradient Descent Training](#3.-Gradient-Descent-Training)  \n",
    "* [4. Training a MLP for Binary Classification](#4.-Training-a-MLP-for-Binary-Classification)  \n",
    "* [5. Training a MLP for Multiclass Classification](#5.-Training-a-MLP-for-Multiclass-Classification)  \n",
    "\n",
    "---\n",
    "\n",
    "### Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## LIBRARIES ##\n",
    "import numpy as np                         # Library for Numerical and Matricial Operations\n",
    "import matplotlib.pyplot as plt            # Library for Generating Visualizations\n",
    "import pandas as pd                        # Library for Handling Datasets\n",
    "from tools.tools import Tools as tl        # Library for some Utilitary Tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## CLASS: Multilayer Perceptron ##\n",
    "class MultilayerPerceptron:\n",
    "    \n",
    "    # CLASS CONSTRUCTOR\n",
    "    def __init__(self, n_neurons=[2, 5, 1]):\n",
    "        if(len(n_neurons) < 2):\n",
    "            raise ValueError(\"The network must have at least two layers! (The input and the output layers)\")\n",
    "        \n",
    "        # Network Architecture\n",
    "        self.hidden_layers = len(n_neurons)-2\n",
    "        self.n_neurons = n_neurons\n",
    "        self.W = []\n",
    "        \n",
    "        # Adjusting the Network architecture\n",
    "        for i in range(1, len(n_neurons)):\n",
    "            self.W.append( np.random.randn(self.n_neurons[i-1]+1 , self.n_neurons[i]) )\n",
    "        \n",
    "    # ACTIVATION FUNCTION\n",
    "    def activate(self,Z):\n",
    "        return 1 / (1 + np.exp(-Z))\n",
    "    \n",
    "    # FORWARD PROPAGATION\n",
    "    def forward(self, X):\n",
    "        # Activation List\n",
    "        A = []\n",
    "        \n",
    "        # Input Layer Activation\n",
    "        A.append( np.vstack([np.ones([1, X.shape[1]]), X]) )\n",
    "        \n",
    "        # Hidden Layer Activation\n",
    "        for i in range(0, self.hidden_layers):\n",
    "            Z = np.matmul(self.W[i].T, A[-1])\n",
    "            Z = self.activate(Z)\n",
    "            \n",
    "            A.append( np.vstack([np.ones([1, Z.shape[1]]), Z]) )\n",
    "        \n",
    "        # Output Layer Activation\n",
    "        Z = np.matmul(self.W[-1].T, A[-1])\n",
    "        Z = self.activate(Z)\n",
    "\n",
    "        A.append(Z)\n",
    "        \n",
    "        return A\n",
    "    \n",
    "    # CLASSIFICATION PREDICTION\n",
    "    def predict(self, X):\n",
    "        A = self.forward(X)\n",
    "        \n",
    "        if(self.n_neurons[-1] > 1):\n",
    "            return A[-1].argmax(axis=0)\n",
    "        else:\n",
    "            return (A[-1] > 0.5).astype(int)\n",
    "    \n",
    "    # LOSS FUNCTION\n",
    "    def loss(self, y, y_hat):\n",
    "        pass\n",
    "    \n",
    "    # ACCURACY FUNCTION\n",
    "    def accuracy(self, y, y_hat):\n",
    "        pass\n",
    "    \n",
    "    # BACKPROPAGATION\n",
    "    def backpropagate(self, A, y):\n",
    "        pass\n",
    "    \n",
    "    # GRADIENT DESCENT TRAINING\n",
    "    def train(self, X_train, y_train, alpha=1e-3, maxIt=50000, tol=1e-5, verbose=False):\n",
    "        pass\n",
    "        \n",
    "## ---------------------------- ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 1. Evaluating Performance\n",
    "\n",
    "One of the first things to be defined in any Supervisioned Learning approach is the evaluation and error metrics used to tell the model how it performed, and allow it to correct his parameters.\n",
    "\n",
    "An important metric is the **Loss Function**, that direcly be used in the training. In our case, we will use the function known as _Cross-Entropy Loss_:\n",
    "\n",
    "$$\n",
    "    \\mathcal{L}(W) = -\\frac{1}{m} \\sum y\\ log( \\hat{y} ) + (1-y)\\ log(1 - \\hat{y} )\n",
    "$$\n",
    "\n",
    "While it is mathematically useful for the training, its values are hard to interpret. So, a more human-like performance metric consists in the **Accuracy Function**, that can be calculated as:\n",
    "\n",
    "$$\n",
    "    \\text{Acc}(W) = -\\frac{100}{m} \\sum ( y = \\hat{y} )\n",
    "$$\n",
    "\n",
    "Implement the two evaluation metrics below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(self, y, y_hat):\n",
    "    # YOUR CODE HERE\n",
    "\n",
    "def accuracy(self, y, y_hat):\n",
    "    # YOUR CODE HERE\n",
    "\n",
    "MultilayerPerceptron.loss = loss\n",
    "MultilayerPerceptron.accuracy = accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, use these metrics to evaluate the performance of classifiction for the examples in the matrix $X$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([[ 5,  1, -2, -1],\n",
    "              [ 4,  2,  0,  4],\n",
    "              [ 3,  3,  1,  4],\n",
    "              [ 2,  4, -1, -3]])    \n",
    "\n",
    "y = np.array([[1, 1, 0, 0]])\n",
    "\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Backpropagation\n",
    "\n",
    "The **Backpropagation** algorithm is one of the most popular and powerful techniques in traning multilayer models.\n",
    "\n",
    "This algorithm allows the misclassification error to be distributed to the entire network, responsabilizing each neuron individually for its contribution to the faults. The algorithm work as follows.\n",
    "\n",
    "The error in the output layer is directly the difference between the real value of the class for each sample subtracted by the probabilities calculated by the network. For all the subsequent layers, the error in each neuron is equal to:\n",
    "\n",
    "$$\n",
    "    e_i^{(l)} = \\left( e_1^{(l+1)} W_1^{(l)} + e_2^{(l+1)} W_2^{(l)} + \\cdots + e_n^{(l+1)} W_n^{(l)} \\right) \\cfrac{d \\varphi(S_{i_\\text{net}}^{(l)})}{dW}\n",
    "$$\n",
    "\n",
    "Where the derivative is equal to:\n",
    "\n",
    "$$\n",
    "    \\cfrac{d \\varphi(S_{i_\\text{net}}^{(l)})}{dW} = A_i^{(l)}(1-A_i^{(l)})\n",
    "$$\n",
    "\n",
    "Using matrix multiplication, the backpropagation of the error between two layers is:\n",
    "\n",
    "$$\n",
    "    \\mathbf{E}^{(l)} = \\mathbf{W}^{(l)} \\mathbf{E}^{(l+1)} \\mathbf{A}^{(l)} (1 - \\mathbf{A}^{(l)})\n",
    "$$\n",
    "\n",
    "Implement the backpropagation method in the cell below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backpropagate(self, A, y):\n",
    "    # YOUR CODE HERE\n",
    "\n",
    "MultilayerPerceptron.backpropagate = backpropagate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, experiment the _backpropagation()_ method in the example below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([[ 5,  1, -2, -1],\n",
    "              [ 4,  2,  0,  4],\n",
    "              [ 3,  3,  1,  4],\n",
    "              [ 2,  4, -1, -3]])    \n",
    "\n",
    "y = np.array([[1, 1, 0, 0]])\n",
    "\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Gradient Descent Training\n",
    "\n",
    "The **Gradient Descent** is the most popular iterative algorithm to optimize loss functions.\n",
    "\n",
    "The algorithm works as follows:\n",
    "\n",
    "Given a Multilayer Neural Network with $L$ layers, where $A^{(l)}$ is the activation matrix in each layer $l$, and $E^{(l)}$ is the error matrix for each layer $l$:\n",
    "\n",
    "1. Calculate the activation $\\mathbf{A}^{(l)}$ and the errors $\\mathbf{E}^{(l)}$ using $\\mathbf{W}^{(i)}$;  \n",
    "\n",
    "2. Evaluate the current performance using the _Loss Function_ and the _Accuracy Metric_;\n",
    "   \n",
    "3. Update the network weights:\n",
    "\n",
    "    $$ \n",
    "        \\mathbf{W}^{(i+1)} = \\mathbf{W}^{(i+1)} - \\alpha \\nabla \\mathcal{L}(\\mathbf{W}^{(i)})\n",
    "    $$\n",
    "    \n",
    "    where $\\alpha$ is a scaling factor (mostly between 0 and 1) called _Learning Rate_, and $\\nabla \\mathcal{L}(\\mathbf{W}^{(i)})$ represents the gradient of the _cost function_, that can be calculated as:\n",
    "    \n",
    "    $$ \n",
    "        \\nabla \\mathcal{L}(\\mathbf{W}^{(i)}) = (A^{(l)}) E^{(l)^T}\n",
    "    $$  <br>\n",
    "    \n",
    "4. Print the training results at each 50 epochs  <br>\n",
    "\n",
    "5. Check for convergence by comparing the decrease in the _Loss Function_; <br>\n",
    "\n",
    "6. If the training did not converged, go back to Step 1. <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(self, X_train, y_train, alpha=1e-3, maxIt=50000, tol=1e-5, verbose=False):\n",
    "    # YOUR CODE HERE\n",
    "\n",
    "MultilayerPerceptron.train = train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Training a MLP for Binary Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## CARREGANDO E VISUALIZANDO OS DADOS ##\n",
    "X_train, X_test, y_train, y_test = tl.loadData(\"data/toy_data_01.csv\")\n",
    "tl.plotData(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Training a MLP for Multiclass Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## CARREGANDO E VISUALIZANDO OS DADOS ##\n",
    "X_train, X_test, y_train, y_test = tl.loadData(\"data/toy_data_02.csv\")\n",
    "tl.plotData(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
