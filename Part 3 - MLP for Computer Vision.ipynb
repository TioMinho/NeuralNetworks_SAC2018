{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks - A Practical Introduction\n",
    "by _Minho Menezes_  \n",
    "\n",
    "---\n",
    "\n",
    "## Multilayer Perceptron for Computer Vision\n",
    "\n",
    "In this third and last notebook, we will get an experience about how to work with image data and use our Neural models for Computer Vision tasks. This will give you a brief introduction in one of the fields where Neural Networks are most widely used, and will show you that this Multilayer Perceptron class that we built really works. \n",
    "\n",
    "* [1. Image Data Extraction](#1.-Image-Data-Extraction)  \n",
    "* [2. Loading the MNIST Dataset](#2.-Loading-the-MNIST-Dataset)  \n",
    "* [3. Training the Network](#3.-Training-the-Network)  \n",
    "* [4. Testing the Network](#4.-Testing-the-Network)  \n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "### Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## LIBRARIES ##\n",
    "import numpy as np                         # Library for Numerical and Matricial Operations\n",
    "import matplotlib.pyplot as plt            # Library for Generating Visualizations\n",
    "import pandas as pd                        # Library for Handling Datasets\n",
    "from tools.tools import Tools as tl              # Library for some Utilitary Tools\n",
    "from glob import glob                      # Library for File Searching\n",
    "\n",
    "# Function for loading the MNIST dataset into a Numpy Matrix\n",
    "import pickle\n",
    "\n",
    "def loadMNIST():\n",
    "    with open(\"data/mnist/mnist.pkl\",'rb') as f:\n",
    "        mnist = pickle.load(f)\n",
    "    return mnist[\"training_images\"].T, mnist[\"test_images\"].T, mnist[\"training_labels\"].T, mnist[\"test_labels\"].T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Neural Networks Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## CLASS: Multilayer Perceptron ##\n",
    "class MultilayerPerceptron:\n",
    "    \n",
    "    # CLASS CONSTRUCTOR\n",
    "    def __init__(self, n_neurons=[2, 5, 1]):\n",
    "        if(len(n_neurons) < 2):\n",
    "            raise ValueError(\"The network must have at least two layers! (The input and the output layers)\")\n",
    "        \n",
    "        # Network Architecture\n",
    "        self.hidden_layers = len(n_neurons)-2\n",
    "        self.n_neurons = n_neurons\n",
    "        self.W = []\n",
    "        \n",
    "        # Adjusting the Network architecture\n",
    "        for i in range(1, len(n_neurons)):\n",
    "            self.W.append( np.random.randn(self.n_neurons[i-1]+1 , self.n_neurons[i]) )\n",
    "        \n",
    "    # ACTIVATION FUNCTION\n",
    "    def activate(self,Z):\n",
    "        return 1 / (1 + np.exp(-Z))\n",
    "    \n",
    "    # FORWARD PROPAGATION\n",
    "    def forward(self, X):\n",
    "        # Activation List\n",
    "        A = []\n",
    "        \n",
    "        # Input Layer Activation\n",
    "        A.append( np.vstack([np.ones([1, X.shape[1]]), X]) )\n",
    "        \n",
    "        # Hidden Layer Activation\n",
    "        for i in range(0, self.hidden_layers):\n",
    "            Z = np.matmul(self.W[i].T, A[-1])\n",
    "            Z = self.activate(Z)\n",
    "            \n",
    "            A.append( np.vstack([np.ones([1, Z.shape[1]]), Z]) )\n",
    "        \n",
    "        # Output Layer Activation\n",
    "        Z = np.matmul(self.W[-1].T, A[-1])\n",
    "        Z = self.activate(Z)\n",
    "\n",
    "        A.append(Z)\n",
    "        \n",
    "        return A\n",
    "    \n",
    "    # CLASSIFICATION PREDICTION\n",
    "    def predict(self, X):\n",
    "        A = self.forward(X)\n",
    "        \n",
    "        if(self.n_neurons[-1] > 1):\n",
    "            return A[-1].argmax(axis=0)\n",
    "        else:\n",
    "            return (A[-1] > 0.5).astype(int)\n",
    "    \n",
    "    # LOSS FUNCTION\n",
    "    def loss(self, y, y_hat):\n",
    "        m = y.shape[1]\n",
    "        return -(1/m) * np.sum(y * np.log(y_hat) + (1-y) * np.log(1 - y_hat))\n",
    "    \n",
    "    # ACCURACY FUNCTION\n",
    "    def accuracy(self, y, y_hat):\n",
    "        m = y.shape[1]\n",
    "        if(y.shape[0] > 1):\n",
    "            y = y.argmax(axis=0)\n",
    "        \n",
    "        return (1/m) * np.sum(y == y_hat) * 100\n",
    "    \n",
    "    # BACKPROPAGATION\n",
    "    def backpropagate(self, A, y):\n",
    "        # Calculates the error in the Output Layer (difference between the real and the predicted)\n",
    "        E = []\n",
    "        E.append( A[-1] - y )\n",
    "\n",
    "        # Backpropagates the error to all the Hidden Layers\n",
    "        for i in range(self.hidden_layers, 0, -1):\n",
    "            E.append( np.matmul(self.W[i], E[-1]) * A[i] * (1-A[i]) )\n",
    "            E[-1] = E[-1][1:,:]\n",
    "\n",
    "        # Returns the list of Error Matrices\n",
    "        return E[::-1]\n",
    "    \n",
    "    # GRADIENT DESCENT TRAINING\n",
    "    def train(self, X_train, y_train, alpha=1e-3, maxIt=50000, tol=1e-5, verbose=True):\n",
    "        # Returns the total number of samples in the training\n",
    "        m = X_train.shape[1]\n",
    "\n",
    "        # Defines the Error History and other auxiliary variables\n",
    "        errorHist = []\n",
    "\n",
    "        # Gradient Descent Loop\n",
    "        for it in range(0, maxIt):\n",
    "            # 1. Calculates all the activations and prediction (Forward Propagation) and \n",
    "            #    the errors (Backpropagation), using the current weights self.WË†(i)\n",
    "            A = self.forward(X_train)\n",
    "            E = self.backpropagate(A, y_train)\n",
    "            P = self.predict(X_train)\n",
    "\n",
    "            # 2. Calculates the Evaluation Metrics\n",
    "            actualLoss = self.loss(y_train, A[-1])\n",
    "            actualAcc = self.accuracy(y_train, P)\n",
    "            errorHist.append(actualLoss)\n",
    "\n",
    "            # 3. Updates the Neural Networks weights\n",
    "            for i in range(0, self.hidden_layers+1):\n",
    "                self.W[i] = self.W[i] - (alpha/m) * np.matmul(A[i], E[i].T)\n",
    "\n",
    "            # 4. Prints the training results\n",
    "            if(verbose): \n",
    "                print(\"# Iteration {0:5} -> Loss: {1:} \\t| Accuracy: {2:.3f}\".format(it+1, actualLoss, actualAcc))\n",
    "\n",
    "            # 5. Check for convergence and prints the final result for the training.\n",
    "            if(it > 1 and abs(errorHist[-1] - errorHist[-2]) <= tol):\n",
    "                print(\"\\n!!! Convergence reached !!!\")\n",
    "                print(\"# Iteration\", it, \"#\")\n",
    "                print(\"Cross-Entropy Loss:      {}\".format(actualLoss))\n",
    "                print(\"Accuracy (Training Set): {0:.3f}%\".format(actualAcc))\n",
    "                print(\"Weights\\nS -> H:\\n\", self.W[0], \"\\nH -> O:\\n\", self.W[1])\n",
    "                print(\"\\n\")\n",
    "                break;\n",
    "\n",
    "        # End of the Training\n",
    "        return errorHist\n",
    "        \n",
    "## ---------------------------- ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 1. Image Data Extraction\n",
    "\n",
    "When working with Computer Vision, the first thing you need, of course, is to be able to open image data and format it to some shape that your Neural Network can understand. This is a step in this field known as **Feature Extraction**. Consider the case of the following image:\n",
    "\n",
    "<img src=\"data/mnist/mnist1.png\" alt=\"binary multilayer perceptron\" width=\"100px\"/>\n",
    "\n",
    "There are, mainly, two ways that we can feed this image to a Neural Network:\n",
    "\n",
    "1. We can extract, in advance, some informations that descripts the image (mean of colors, borders positions, center of mass, etc.) and build a vector comprising these descriptions. This vector is then used as a Input Layer to the Neural Network.\n",
    "\n",
    "2. The entire image (28x28 pixels, in this case) is arranged into a vector shape. This vector (that will have dimension 784x1) is then used as a Input Layer to the Neural Network.\n",
    "\n",
    "Of course, in the first case it is possible that we form a set of features that is much smaller than 784 (in the second case), hence performing faster. However, this process can be time-consuming and maybe some information will still be missing. The second case requires more memory and processing power, but usually yields a better performance. We will stick to the second approach, now, because this are actually one of the reasons that Neural Networks are popular right now.\n",
    "\n",
    "**To build your own dataset from image data, first use the function _plt.imread()_ to capture the matrices of a image and the function _plt.imshow()_ to visualize it:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open an image and returns the pixel matrices\n",
    "X = plt.imread(\"data/mnist/mnist1.png\")\n",
    "\n",
    "# Selects only the pixel intensity for one of the RGBA channels\n",
    "X = X[:,:,0]\n",
    "\n",
    "# Exhibits the image loaded in such matrix\n",
    "plt.figure()\n",
    "plt.imshow(X)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To flatten the entire image to an array of specific dimensions, we use the method _.reshape()_ from the Numpy matrices. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## YOUR CODE HERE ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is only one image. In order to built an entire dataset, we need to iteratively load and reshape all the images that we have in our database:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Returns the name of all PNG files in the MNIST folder\n",
    "filenames = glob(\"data/mnist/*.png\")\n",
    "\n",
    "## YOUR CODE HERE ##\n",
    "    \n",
    "# Prints part of the dataset\n",
    "print(\"Dataset:\\n\", X)\n",
    "print(\"\\nDimensions:\", X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we need to indicate the real class of each image. This work is usually done manually to label each image by actually looking at it.\n",
    "\n",
    "In this case, we first visualize each example of our dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## YOUR CODE HERE ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we manually create the labels vector $Y$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## YOUR CODE HERE ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Loading the MNIST Dataset\n",
    "\n",
    "You could be worried if a dataset with only 9 samples is enough to train a Neural Network. You are right.\n",
    "\n",
    "The number of samples needed to train a Neural Network varies from the problem and the architecture of the network. The intention is to get as much data as possible, but they also must have great variance to expose the network to several possible cases. In the Computer Vision case, however, the number of input is too great, and the Networks tends to be proportionally as big. In this case we need very big datasets.\n",
    "\n",
    "The **[MNIST dataset](http://yann.lecun.com/exdb/mnist/)** is a very popular dataset of 70,000 images of handwritten digits that are used as benchmark data for several machine learning models. Actually the images that you used before are samples of this dataset.\n",
    "\n",
    "**We will then, work with this data:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loads the entire MNIST dataset\n",
    "X_train, X_test, y_train, y_test = loadMNIST()\n",
    "\n",
    "# Creating the One-Hot Encoding labels\n",
    "y_train_ohe = np.zeros([10, y_train.shape[0]])\n",
    "for i in range(0, y_train.shape[0]):\n",
    "    y_train_ohe[y_train[i], i] = 1\n",
    "\n",
    "y_test_ohe = np.zeros([10, y_test.shape[0]])\n",
    "for i in range(0, y_test.shape[0]):\n",
    "    y_test_ohe[y_test[i], i] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also visualize samples from this dataset by using the _.reshape()_ and the _plt.imshow()_ functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## YOUR CODE HERE ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Training the Network\n",
    "\n",
    "Now, we have a entire dataset and we already built our Neural Network class. We are ready to train a network to work with this data.\n",
    "\n",
    "A quick reminder: the Network will have _748_ Input Labels, a arbitrary number _N_ of Random Neurons and _10_ Output Neurons. This means that we will have _748xN + N*10_ weights to update! Furthermore, we have 60000 training examples. So, you can imagine that this training will be a lot slower (and harder!) than those of the previous notebooks.\n",
    "\n",
    "But fear not! The network can still learn something if we choose a significantly higher number of Hidden Neurons; it will work, but will take a lot of time. Use a small number of Hidden Neurons in order to visualize the training, and (if you computer is fast enough) you can use more neurons later.\n",
    "\n",
    "**The training itself should be equal to the previous cases, just remember to match the dimensions:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## YOUR CODE HERE ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Testing the Network\n",
    "\n",
    "Once our network is properly trained, we can use it to classify the digits. In this case, since we are dealing with high-dimensional data, we will not be able to generate visualizations for the entire space as we did before, since it would be infeasible to try to plot the digits into two-dimensional spaces.\n",
    "\n",
    "We can, however, try to sample some digits from the dataset and classify them, in order to see the network in practice. Remember, also, that we can rely in the _Evaluation Metrics_ to understand the quality of the models we trained.\n",
    "\n",
    "**In the cell below, try to sample some examples from the dataset, classify them using the network, and visualize the correct answer to evaluate the results:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## YOUR CODE HERE ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also evaluate the quality of the model by classifying the Testing Dataset and evaluating the overall accuracy over all examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## YOUR CODE HERE ##\n",
    "\n",
    "print(\"## RESULTS ##\")\n",
    "print(\"Overall Accuracy: {0:.3f}\".format(overallAcc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
